quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:341); 	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2192); 	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2225); 	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2024); 	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:779); 	at com.mysql.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:3907,failure,failure,3907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['failure'],['failure']
Availability," 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2449); 	at com.mysql.jdbc.StatementImpl.executeInternal(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:3478,failure,failures,3478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['failure'],['failures']
Availability, 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2203,Error,ErrorOr,2203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['Error'],['ErrorOr']
Availability," "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:36948,echo,echo,36948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability," "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""50"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File cat_txt = ""merged.txt""; File cat_md = ""merged.md""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:2469,Down,DownstreamConsumer,2469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,6,"['Down', 'down', 'echo', 'failure']","['DownstreamConsumer', 'downstream', 'echo', 'failure']"
Availability," ""IpcMode"": ""private"",; ""Cgroup"": """",; ""Links"": null,; ""OomScoreAdj"": 0,; ""PidMode"": """",; ""Privileged"": false,; ""PublishAllPorts"": false,; ""ReadonlyRootfs"": false,; ""SecurityOpt"": null,; ""UTSMode"": """",; ""UsernsMode"": """",; ""ShmSize"": 67108864,; ""Runtime"": ""runc"",; ""ConsoleSize"": [; 0,; 0; ],; ""Isolation"": """",; ""CpuShares"": 0,; ""Memory"": 0,; ""NanoCpus"": 0,; ""CgroupParent"": """",; ""BlkioWeight"": 0,; ""BlkioWeightDevice"": [],; ""BlkioDeviceReadBps"": null,; ""BlkioDeviceWriteBps"": null,; ""BlkioDeviceReadIOps"": null,; ""BlkioDeviceWriteIOps"": null,; ""CpuPeriod"": 0,; ""CpuQuota"": 0,; ""CpuRealtimePeriod"": 0,; ""CpuRealtimeRuntime"": 0,; ""CpusetCpus"": """",; ""CpusetMems"": """",; ""Devices"": [],; ""DeviceCgroupRules"": null,; ""DeviceRequests"": null,; ""KernelMemory"": 0,; ""KernelMemoryTCP"": 0,; ""MemoryReservation"": 0,; ""MemorySwap"": 0,; ""MemorySwappiness"": null,; ""OomKillDisable"": false,; ""PidsLimit"": null,; ""Ulimits"": null,; ""CpuCount"": 0,; ""CpuPercent"": 0,; ""IOMaximumIOps"": 0,; ""IOMaximumBandwidth"": 0,; ""MaskedPaths"": [; ""/proc/asound"",; ""/proc/acpi"",; ""/proc/kcore"",; ""/proc/keys"",; ""/proc/latency_stats"",; ""/proc/timer_list"",; ""/proc/timer_stats"",; ""/proc/sched_debug"",; ""/proc/scsi"",; ""/sys/firmware""; ],; ""ReadonlyPaths"": [; ""/proc/bus"",; ""/proc/fs"",; ""/proc/irq"",; ""/proc/sys"",; ""/proc/sysrq-trigger""; ]; },; ""GraphDriver"": {; ""Data"": {; ""LowerDir"": ""/var/lib/docker/overlay2/aa7c784c947752f9736d649c2f7f1d1ff992e94a295a7d8b3281eb18b60192f0-init/diff:/var/lib/docker/overlay2/pi10oyxckwgkkcbtbhtopthgo/diff:/var/lib/docker/overlay2/ijx4ivzmz9j7r2z9sqnxxkfr2/diff:/var/lib/docker/overlay2/jv5021rm0ro1ncxdxk9z7z4z2/diff:/var/lib/docker/overlay2/l7ti7s4rs2dxrvvlh4xry72fn/diff:/var/lib/docker/overlay2/0ecrq5dfyezvcc19ewmxvgohh/diff:/var/lib/docker/overlay2/vpue5u7q3ouhkmqlyrwbg5n75/diff:/var/lib/docker/overlay2/eh958b0fn0cjj2btiaxfhimxg/diff:/var/lib/docker/overlay2/luqo2vxej2gtqb1i70juiilf3/diff:/var/lib/docker/overlay2/jn9pv5erse0hvoixil8mb2h0k/diff:/var/lib/docker/overlay2/trwrj89a2zln5c2wh9ci255nm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:4788,Mask,MaskedPaths,4788,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['Mask'],['MaskedPaths']
Availability, (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1633,recover,recoverWith,1633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['recover'],['recoverWith']
Availability," (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colon$plus$colon;@52b558ea,SchemaDefRequirement).; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowD",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4398,error,error,4398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,2,['error'],['error']
Availability," (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the softwa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:3909,down,down,3909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['down'],['down']
Availability," -AF |grep ag457|grep sync; ag457 3388 3387 0 26974 364 9 Oct26 ? 00:00:00 sync; ag457 5036 5035 0 26974 364 2 Oct27 ? 00:00:00 sync; ag457 21705 21704 0 26974 360 11 Oct28 ? 00:00:00 sync; ag457 22304 22303 0 26974 364 12 Oct28 ? 00:00:00 sync; ag457 22345 22344 0 26974 364 26 Oct28 ? 00:00:00 sync; ; and they have been running from a minimum of 46 hours to 85 hours for the oldest, so they are definitely stuck.; ; Bhanu and Jason (the DevOps team leader) checked the node and found out that you have several duplicate processes that are running and doing exactly the same thing.; ; This is never good and it is likely what is causing ""sync"" to hang.; ; More in detail:; ; those are all the PID associated with your username:; ; rp189@compute-p-17-32:~ ps -AF |grep ag457|awk '{printf ""%s "",$2}';echo""""; 2904 2906 3387 3388 5035 5036 12888 12890 14814 15375 15377 21704 21705 21850 21852 22303 22304 22344 22345 24802 24804 29768 29770; ; Looking at the commands being executed by some of those processes we found several duplicates which likely corrupted the file(s) where command sync was potentially flushing the buffer data, those below are the PID and associated commands (cat /proc/PID/cmdline),; you can see how there are 5 duplicates, exactly as the number of stuck ""sync"" commands:; ; 15377, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S003_M1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/d41833e7-2c4a-4a92-a9d7-fd194d3059d3/call-MergeBamAlignment/shard-0/execution/script; 21704, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S003_M1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/d41833e7-2c4a-4a92-a9d7-fd194d3059d3/call-MergeBamAlignment/shard-0/execution/script; 22344, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S005_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/b004cf90-7c72-498e-b378-d49cd0b0c2cb/call-SortAndFixTags/execution/script; 2906, /bin/bash/n/no_bac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347:4987,echo,echo,4987,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347,1,['echo'],['echo']
Availability," -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2ï¼š; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4703:1493,Error,Error,1493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703,1,['Error'],['Error']
Availability," 1 workflows from the WorkflowStoreActor; 2018-01-17 20:52:54,947 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - MaterializeWorkflowDescriptorActor [UUID(e71c769c)]: Call-to-Backend assignments: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SplitLargeRG.SumSplitAlignedSizes -> JES, SplitLargeRG.GatherBamFiless -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.SamSplitter -> JES, SplitLargeRG.Alignment -> JES; 2018-01-17 20:52:56,323 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-e71c769c-948f-4bd7-8cbe-064a18375966 [UUID(e71c769c)]: Starting calls: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1; 2018-01-17 20:53:02,487 cromwell-system-akka.dispatchers.backend-dispatcher-44 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: `# not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because; # the sed may also fail with that error and that is something we actually want to fail on.; /usr/gitc/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //'`; 2018-01-17 20:53:04,348 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: job id: operations/EPXh4LeQLBjT9Z2WvIiz-QggqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:53:15,636 cromwell-system-akka.dispatchers.backend-dispatcher-56 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from - to Running; 2018-01-17 20:56:25,285 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(e71c769c)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from Running to Success; 2018-01-17 20:56:28,223 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:9204,error,error,9204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['error'],['error']
Availability," 12:24:32,94] [info] BT-322 b303ae23:expanse_figures.CBL_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:24:32,97] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 is eligible for call caching with read = true and write = true; [2023-03-29 12:35:42,07] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1-20000000023 [b303ae23expanse_figures.CBL_hom_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,07] [info] BT-322 b303ae23:expanse_figures.CBL_hom_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:1276,failure,failures,1276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability," 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:1569,ERROR,ERROR,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability," 2016-08-03 15:20:01,502 cromwell-system-akka.dispatchers.backend-dispatcher-107 INFO - $a [UUID(eaeaa32d)DeliciousFileSpam.StringSpam:215:1]: JesAsyncBackendJobExecutionActor [UUID(eaeaa32d):DeliciousFileSpam.StringSpam:215:1] Status change from Running to Success; 2016-08-03 15:20:01,923 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: Job DeliciousFileSpam.StringSpam:215:1 succeeded!; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionFailedState. Shutting down.; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] done. Shutting down.; 2016-08-03 15:20:03,593 cromwell-system-akka.dispatchers.engine-dispatcher-85 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from ExecutingWorkflowState to FinalizingWorkflowState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationPendingState to FinalizationInProgressState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationInProgressState to FinalizationSucceededState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-138 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from FinalizingWorkflowState to WorkflowFailedState; 2016-08-03 15:20:03,594 cromwell-system",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2887:1191,down,down,1191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2887,1,['down'],['down']
Availability," 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4985,checkpoint,checkpointClose,4985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5442,checkpoint,checkpointClose,5442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5899,checkpoint,checkpointClose,5899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6356,checkpoint,checkpointClose,6356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6813,checkpoint,checkpointClose,6813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9426,checkpoint,checkpointClose,9426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9883,checkpoint,checkpointClose,9883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10340,checkpoint,checkpointClose,10340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10797,checkpoint,checkpointClose,10797,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11254,checkpoint,checkpointClose,11254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""wr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11711,checkpoint,checkpointClose,11711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability," 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue = ""xxx""; String? sge_project = ""xxx""; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l num_proc="" + cpu + "",virtual_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; -binding ${""linear:"" + cpu} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:1710,alive,alive,1710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,4,['alive'],['alive']
Availability," 24 was b71f06f. This works on 19 (yes we're still on 19 =/). Task (stuff removed):. ```; task ValidateSamFile {; File input_bam; File? input_bam_index; String report_filename; File? ref_dict; File? ref_fasta; File? ref_fasta_index; Int? max_output; Array[String]? ignore; Int disk_size; Int preemptible_tries. command {; java -Xmx4000m -jar stuff.jar blah; }; runtime {; memory: ""7 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File report = ""${report_filename}""; }; }; ```. Call (note there is no value supplied for max_output):. ```; call ValidateSamFile as ValidateReadGroupSamFile {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.colle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:1184,error,error,1184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['error'],['error']
Availability," = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1906,error,error,1906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability," = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the cluster; providers { ; # For running jobs on an interactive node; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; runtime-attributes = """"""; String? docker; """"""; submit = ""/usr/bin/env bash ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:4712,echo,echo,4712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['echo'],['echo']
Availability," = 22009; }. call extract_field as year_of_birth { input :; script_dir = script_dir,; id = 34; }. call extract_field as month_of_birth { input :; script_dir = script_dir,; id = 52; }. call extract_field as date_of_death { input :; script_dir = script_dir,; id = 40000; }. call extract_field as phenotype { input :; script_dir = script_dir,; id = phenotype_id; }. scatter (categorical_covariate_id in categorical_covariate_ids) {; call extract_field as categorical_covariates { input :; script_dir = script_dir,; id = categorical_covariate_id; }; }. call platform_agnostic_workflow.main { input:; script_dir = script_dir,. phenotype_name = phenotype_name,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariates.data,; is_binary = is_binary,; is_zero_one_neg_nan = is_zero_one_neg_nan,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,. fam_file = fam_file, # Could instead create a task for downloading this with ukbgene; withdrawn_sample_list = withdrawn_sample_list,. sc_white_brits = white_brits.data,; sc_ethnicity_self_report = ethnicity_self_report.data,; sc_sex_aneuploidy = sex_aneuploidy.data,; sc_genetic_sex = genetic_sex.data,; sc_reported_sex = reported_sex.data,; sc_kinship_count = kinship_count.data,; sc_assessment_ages = assessment_ages.data,; sc_pcs = pcs.data,; sc_year_of_birth = year_of_birth.data,; sc_month_of_birth = month_of_birth.data,; sc_date_of_death = date_of_death.data,; sc_phenotype = phenotype.data; }. 	output {; 		Array[File] out_sample_lists = main.out_sample_lists; 	}; }; ```. platform_agnostic_workflow.wdl; ```; # platform agnostic workflow. version 1.0. import ""tasks.wdl"". workflow main {. input {; String script_dir. String phenotype_name; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; Boolean is_binary; Boolean is_zero_one_neg_nan; String date_of_most_recent_first_occurrence_update. File fam_file # task for generating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:3390,down,downloading,3390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['down'],['downloading']
Availability," > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4226,failure,failure,4226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failure']
Availability," Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actually shows the issue twice -- once in the section starting with `if(defined(variant_call_after_earlyQC_filtering.errorcode)) {` and once in the",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1354,error,errorcode,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,2,"['echo', 'error']","['echo', 'errorcode']"
Availability," CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2371,failure,failures,2371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['failure'],['failures']
Availability," Cromwell runs the WDL tasks with the correct command line, but somehow the arguments after the initial command aren't being picked up by the binary inside the docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(Pipelines",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1127,error,error,1127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['error'],['error']
Availability," Denied (Service: S3Client; Status; > Code: 403; Request ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1203,Fault,FaultHandling,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability," File filtered_vcf_idx = select_first([FilterAlignmentArtifacts.filtered_vcf_idx, Filter.filtered_vcf_idx]); File filtering_stats = Filter.filtering_stats; File mutect_stats = MergeStats.merged_stats; File? contamination_table = CalculateContamination.contamination_table. File? funcotated_file = Funcotate.funcotated_output_file; File? funcotated_file_index = Funcotate.funcotated_output_file_index; File? bamout = MergeBamOuts.merged_bam_out; File? bamout_index = MergeBamOuts.merged_bam_out_index; File? maf_segments = CalculateContamination.maf_segments; File? read_orientation_model_params = LearnReadOrientationModel.artifact_prior_table; }. }; }. task CramToBam {; input {; File ref_fasta; File ref_fai; File ref_dict; #cram and crai must be optional since Normal cram is optional; File? cram; File? crai; String name; Int disk_size; Int? mem; }. Int machine_mem = if defined(mem) then mem * 1000 else 6000. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools view -h -T ~{ref_fasta} ~{cram} |; samtools view -b -o ~{name}.bam -; samtools index -b ~{name}.bam; mv ~{name}.bam.bai ~{name}.bai; }. runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai""; }; }. task SplitIntervals {; input {; File? intervals; File ref_fasta; File ref_fai; File ref_dict; Int scatter_count; String? split_intervals_extra_args. # runtime; Runtime runtime_params; }. command {; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. mkdir interval-files; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" SplitIntervals \; -R ~{ref_fasta} \; ~{""-L "" + intervals} \; -scatter ~{scatter_count} \; -O interval-files \; ~{split_intervals_extra_args}; cp interval-files/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:19569,failure,failure,19569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['failure'],['failure']
Availability," Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then an empty file gets passed downstream\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s $",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:38561,down,downstream,38561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['downstream']
Availability," Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2014,error,errors,2014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['errors']
Availability," Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3262,error,error,3262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['error'],['error']
Availability," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1514,failure,failures,1514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['failure'],['failures']
Availability," SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6855,alive,alive,6855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['alive'],['alive']
Availability," The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf2b-49a2-be87-483; 9e96a08eb/call-TestOutOfMemoryRetry/stderr; Killed ; ``` . We have also noticed that in the out of memory case, no retrurnCode is written to the metadata. **Test wdl for reproduction:**; `version 1.0. workflow MemoryRetryTest {; input {; String message = ""Killed""; }; call TestOutOfMemoryRetry {}; call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; command <<<; echo ""Killed"" >&2; tail /dev/zero; >>>; runtime {; docker: ""ubuntu:latest""; cpu: ""1""; memory: ""1 GB""; disks:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:1520,error,error,1520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,1,['error'],['error']
Availability," The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defined() check still is not enough like is the case for setting not_optional_error_code, then that should be caught before runtime. ## backends effected; The womtool validation bug affects at least Terra-womtool and local-womtool. Runtime error happened",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2445,error,errorcode,2445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1652,error,error,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['error'],['error']
Availability," WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:21:09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e705",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:4056,echo,echo,4056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-1/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-0/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12895,echo,echo,12895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4760,Checkpoint,Checkpoint,4760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability," [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm mo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:2185,ERROR,ERROR,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability," [e22c6324test_opt_array.t1:1:1]: Status change from - to WaitingForReturnCode; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: Status change from - to WaitingForReturnCode; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: Status change from - to Done; [2018-10-25 21:17:28,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: Status change from - to Done; [2018-10-25 21:17:30,83] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:34,04] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:34,39] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: Status change from WaitingForReturnCode to Done; [2018-10-25 21:17:38,14] [error] WorkflowManagerActor Workflow e22c6324-5aec-4694-8750-f62160e2ca81 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'test_opt_array.arr2' (reason 1 of 1): Evaluating select_first([t1.out, arr1]) failed: assertion failed: base member type WomMaybeEmptyArrayType(WomAnyType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:510); at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:73); at cats.instances.ListInstances$$anon$1.loop$2(list.scala:63); at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:63); at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:276); at cats.Eval$.loop$1(Eval.scala:338); at cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); at cats.Eval$Defer.va",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:16128,error,error,16128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['error'],['error']
Availability," `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2198,error,errorcode,2198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1998,error,error,1998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,6,['error'],"['error', 'errors']"
Availability," a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1545,echo,echo,1545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['echo'],['echo']
Availability," a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chrom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1935,error,errors,1935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['error'],['errors']
Availability," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1592,error,error,1592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,4,"['error', 'failure']","['error', 'failures']"
Availability," actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; \""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\""; ""; """"""; kill-docker = ""scancel ${job_id}"". filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; check-sibling-md5: true; hasing-strategy: ""fingerprint""; fingerprint-size: 1048576 # 1 MB ; }; }; }. }; }; }}; ```. Note: there are some WDL parameters relevant to DNANexus's dxCompiler. I'm hoping this code will be able to run on that system eventually, but I understand that those parameters are not relevant to cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:6287,echo,echo,6287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,2,['echo'],['echo']
Availability," and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Buc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2166,error,error,2166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability," be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13687,down,downloading,13687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['down'],['downloading']
Availability," builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:1041,Error,Errors,1041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['Error'],['Errors']
Availability," clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6023,down,down,6023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['down'],['down']
Availability," com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1040); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1350); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:716); 	... 27 common frames omitted; ```. Initially nothing was persisting to the database making debugging tricky, so I updated the database to increase the column size of METADATA_ENTRY.METADATA_KEY from varchar(255) to something silly like varchar(3000) and re-ran the offending script. The culprit showed itself as:. ```; SELECT length(METADATA_KEY) FROM cromwell.METADATA_ENTRY, x.METADATA_KEY xWHERE length(METADATA_KEY) = (SELECT max(length(METADATA_KEY)) FROM METADATA_ENTRY). 323, ""inputs:batch_files:/mnt/data/cromwell-executions/build_bob_ep/40573452-6a92-4e26-8f0d-02bd980970b7/call-build_bob/build_bob/b6e60c8e-2d0a-4db7-8b70-b71cde217b30/call-building_taxonomy/building_taxonomy/c2e537ff-e231-4b51-a423-f750604dca7c/call-classify_f33ef23grwsg32fgv/inputs/2065711490/GTDB_GB_GCA_123456789.1.mask.fasta""; ```. Basically, as the complexity of the workflows increases the potential length of the inputs increases and the limit of varchar(255) is exceeded. Going forward, this will not be our most complicated workflow so I expect to hit this more frequently. So firstly, am I doing anything wrong?. Secondly would it be possible to increase the maximum size of the column METADATA_KEY that can accommodate increasing levels workflow complexity? I can do this post deployment using ansible but that feels a little bit messy. (I have also posted this on your JIRA backlog as Im not sure which is the best place to raise this: https://broadworkbench.atlassian.net/browse/CROM-6721). Best,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:4957,mask,mask,4957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,1,['mask'],['mask']
Availability," creating a `CallableTaskDefinition` structure. This structure has an empty meta section. When rewriting `foo` in draft-2, this works as expected. . ```wdl; version 1.0. task foo {; input {; String buf ; }; command {}; output {; String s = buf; } ; meta {; type : ""native"",; id: ""applet-xxxx""; }; }; ```. This is the code used to parse the task: ; ```scala; // Extract the only task from a namespace ; def getMainTask(bundle: WomBundle) : CallableTaskDefinition = {; // check if the primary is nonempty ; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }; task match {; case Some(x) => x; case None =>; // primary is empty, check the allCallables map ; if (bundle.allCallables.size != 1); throw new Exception(""WDL file must contains exactly one task""); val (_, task) = bundle.allCallables.head; task match {; case task : CallableTaskDefinition => task; case exec : ExecutableTaskDefinition => exec.callableTaskDefinition; case _ => throw new Exception(""Cannot find task inside WDL file""); 		}; }; }. def parseWdlTask(wfSource: String) : CallableTaskDefinition = {; val languageFactory =; if (wfSource.startsWith(""version 1.0"") ||; wfSource.startsWith(""version draft-3"")) {; new WdlDraft3LanguageFactory(ConfigFactory.empty()); } else {; new WdlDraft2LanguageFactory(ConfigFactory.empty()); }. val bundleChk: Checked[WomBundle] =; languageFactory.getWomBundle(wfSource, ""{}"", List.empty, List(languageFactory)); val womBundle = bundleChk match {; case Left(errors) => throw new Exception(s""""""|WOM validation errors: ; | ${errors} ; |"""""".stripMargin); case Right(bundle) => bundle; }; val task: Option[CallableTaskDefinition] = bundle.primaryCallable match {; case Some(task : CallableTaskDefinition) => Some(task); case Some(exec : ExecutableTaskDefinition) => Some(exec.callableTaskDefinition); case _ => None; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4709:1718,error,errors,1718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4709,3,['error'],['errors']
Availability," default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be create",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2701,alive,alive,2701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['alive'],['alive']
Availability," definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:1310,recover,recoverWith,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['recover'],['recoverWith']
Availability," e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] WorkflowManagerActor Successfully started WorkflowActor-e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:17:13,86] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/sh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12529,echo,echo,12529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability," even create the `.list` file:; ```bash; #!/bin/sh; umask 0000; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution; base_name=$(basename /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam .bam); java -Xmx4g -jar $picard CollectHsMetrics \; I=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam \; O=$base_name.hs_metrics \; PER_TARGET_COVERAGE=$base_name.per_target_coverage \; TI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; BI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; R=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; ); echo $? > /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution/rc.tmp; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution. ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028:2005,echo,echo,2005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028,1,['echo'],['echo']
Availability," events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2117,error,error,2117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4798,error,error,4798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability," field seems to have an inconsistent format:. Compare the failures sections for the following:. ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""{...},; ""calls"": {; ""echo_strings.echo_files"": [{; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""machineType"": ""us-central1-c/n1-standard-1"",; ""googleProject"": ""broad-dsde-dev"",; ""executionBucket"": ""gs://cromwell-dev/cromwell-executions"",; ""zone"": ""us-central1-c"",; ""instanceName"": ""ggp-3462354720519617596""; },; ""runtimeAttributes"": {...},; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""jobId"": ""operations/EJiq_oWfKxi8-N-X4qiwhjAgw7vetLsXKg9wcm9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:1050,failure,failures,1050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,2,"['error', 'failure']","['error', 'failures']"
Availability, file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Standard,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1495,recover,recover,1495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Availability," files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.ref153positions; Offsets compression type: bitpack64; Allocating memory for ref offset pointers, kmer 15, interval 3...Attached existing memory (2 attached) for defuse-data/gmap/cdna/cdna.ref153offsets64meta...done (134,217,744 bytes, 0.00 sec); Allocating memory for ref offsets, kmer 15, interval 3...Attached new memory for defuse-data/gmap/cdna/cdna.ref153offsets64strm...done (234,475,312 bytes, 0.23 sec); Pre-loading ref positions, kmer 15, interval 3......done (276,173,052 bytes, 0.05 sec); Starting alignment; Failed attempt to alloc 18446744073709550532 bytes; Exception: Allocation Failed raised at indexdb.c:2885; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/log/defuse.12.sh: line 6: 7481 Segmentation fault (core dumped) /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; ; real 0m1.262s; user 0m0.046s; sys 0m0.564s. ```. Run within the docker container but not using Cromwell, the output of that command looks like this:; ```; Starting defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /mnt/Workflow_runs/2_fusion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.cdna.psl m; issing; Success for defuse command:; /usr/local/bin/gmap -D Program_required_data/deFuse/defuse-data/gmap -d est4 -f psl /mnt/Workflow_runs/2_fus; ion_detection_tools/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa > /mnt/Workflow_runs/2_fusion_detection_to; ols/BT474/BT474_deFuse_0.8.1/jobs/breakpoints.split.001.fa.est.4.psl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:2768,fault,fault,2768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['fault'],['fault']
Availability," for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service_account"",; ""project_id"": ""project-test1"",; ""private_key_id"": ""59b66448c3ab730097135e1dba83b375a6b57ea3"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\n(Omitted)\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""project-service@project-test1.iam.gserviceaccount.com"",; ""client_id"": ""104927211954691424974"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/project-service%40project-test1.iam.gserviceaccount.com""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:5618,down,downloaded,5618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['down'],['downloaded']
Availability," from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:1183,Error,Error,1183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,1,['Error'],['Error']
Availability," function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.v",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1144,error,error,1144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability," had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.white_brits_sample_list:-1:1-20000000013 [788d8048main.white_brits_sample_list:NA:1]: Unrecognized runtime attribute keys:; shortTask, dx_timeout; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.low_genotyping_quality_sample_list:-1:1-20000000014 [788d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36049,failure,failures,36049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a072",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136:1472,Failure,Failure,1472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136,1,['Failure'],['Failure']
Availability," if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:17:00.575023Z""; },; {; ""startTime"": ""2018-08-14T16:13:13.678Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-08-14T16:13:13.678Z""; },; {; ""startTime"": ""2018-08-14T16:16:42.510303Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:43.002063Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.575023Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:19262,echo,echo,19262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:12282,error,errors,12282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,4,['error'],"['error', 'errors']"
Availability," is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1071,error,error,1071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,2,"['Error', 'error']","['Error', 'error']"
Availability," itself will hold most of this granularity, it should be useful for Cromwell to group certain actions into a higher level grouping concept based on its understanding of the job. For example, here's the execution events from a single run: ; ```; ""executionEvents"": [; {; ""startTime"": ""2018-08-14T16:16:40.069663Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:42.510303Z""; },; {; ""startTime"": ""2018-08-14T16:16:43.002063Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:1111,echo,echo,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability," line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm modifying the CWL to ask for a bit more memory but I'm wondering if there's something else that Cromwell is not doing right",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:2339,ERROR,ERROR,2339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability," list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19152,echo,echo,19152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2210,ERROR,ERROR,2210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['ERROR'],['ERROR']
Availability," make memory retry work on our system without sucess. ; Read all docs and previous issues I could find, but it still doesn't work for us. I have written a test wdl with two tasks, both write ""Killed"" to stderr, and supposed to get retried with more memory. The first task, **TestBadCommandRetry** is designed to fail regularly with rc 127, due to a bad command.; The purpose of this task is to prove the memory-retry mechanism is configured correctly in our system. Result of TestBadCommandRetry:; The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:1067,error,error,1067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,1,['error'],['error']
Availability," may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:887,error,error,887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2583,avail,available,2583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['avail'],['available']
Availability," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:2413,alive,alive,2413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,"['alive', 'error']","['alive', 'error']"
Availability," name-for-call-caching-purposes: PAPI; slow-job-warning-time: ""24 hours""; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600; request-workers = 3; genomics {; auth = ""application-default""; endpoint-url = ""https://genomics.googleapis.com/""; location = ""us-west1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""application-default""; project = ""xxxx""; caching {; duplication-strategy = ""copy""; }; }; http { }; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b""]; }; include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```; When I run with the above config using:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json; ```; I am getting the following error message:; ```; [2021-08-24 22:05:33,60] [info] WorkflowManagerActor: Workflow 6cc303b4-295d-49fa-a996-b5cf7ec9beea failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Execution failed: allocating: creating instance: inserting instance: Invalid value for field 'resource.networkInterfaces[0].network': ''. The referenced network resource cannot be found.; ```; I have tried passing the vpc and subnet id using the following config:; ```; virtual-private-cloud {; network-label-key = ""xxx""; subnetwork-label-key = ""xxx""; auth = ""application-default""; }; ```. The above values are my actual vpc and subnet id/name. However, it is still giving me that error message. Is there something I am missing from a configuration perspective. Any help would be greatly appreciated. Our VPC network's are not created in auto mode and that is not something we have control over unfortunately. Thanks,; -Simran",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:1759,error,error,1759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,3,['error'],['error']
Availability," new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1040,error,errorcode,1040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) â€” You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1917,Error,Error,1917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability," of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:87710,avail,available,87710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['avail'],['available']
Availability," on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2664,recover,recoverWith,2664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['recover'],['recoverWith']
Availability," on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1740,echo,echo,1740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,4,['echo'],['echo']
Availability," optional value: overwrite_given_value. **Example**; _For this examples I use womtool 84 and WDL 1.0. I also tried with womtool 53.1 and 83 which showed the same issue_; The following files are used to show case the issue.; The main workflow:; ```; version 1.0. import ""SubWorkflow.wdl"" as SubWorkflow. workflow MainWorkflow {; input {; String value_2_give = ""default value""; }; call MainTask {; input:; given_value = value_2_give; }. call SubWorkflow.SubWorkflow {; input:; value_2_give = value_2_give; }; }. task MainTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. The subworkflow:; ```; version 1.0. workflow SubWorkflow {; input {; String value_2_give = ""default value""; String? overwrite_value_2_give; }; call SubTask {; input:; given_value = select_first([overwrite_value_2_give, value_2_give]); }; }. task SubTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. To be sure I ran the validation mode of womtools:; ```; $ java -jar womtool-84.jar validate MainWorkflow.wdl; Success!; $ java -jar womtool-84.jar validate SubWorkflow.wdl; Success!; ```. After creating these files, I ran womtool with the ""inputs"" option getting the following output:; ```; $ java -jar womtool-84.jar inputs MainWorkflow.wdl; {; ""MainWorkflow.SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""MainWorkflow.MainTask.overwrite_given_value"": ""String? (optional)"",; ""MainWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; This output json shows which variables you can (or must) provide in order to be able to run in this case the main workflow. here we see that we are able to provide values for the Mainworkflow, MainTask and SubWorkflow but not the SubTask.; If we do the same for just the subworkflow:; ```; $ java -jar womtool-84.jar inputs SubWorkflow.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:1210,echo,echo,1210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,1,['echo'],['echo']
Availability," pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - â€˜cromwell-36.jarâ€™ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1205,down,download,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['down'],['download']
Availability," provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.â€‚â€‚. Current deploy modes supported for any spark job:; â€‚â€‚a - Client deploy mode using the spark standalone cluster manager; â€‚â€‚b - Cluster deploy mode using the spark standalone cluster manager; â€‚â€‚c - Client deploy mode using Yarn resource manager; â€‚â€‚d - Cluster deploy mode using Yarn resource manager; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.; â€‚â€‚Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:; â€‚â€‚In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.; â€‚â€‚Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:1721,error,error,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,1,['error'],['error']
Availability," runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1791,echo,echo,1791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,7,"['alive', 'echo']","['alive', 'echo']"
Availability, scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4602,Error,ErrorOr,4602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability," script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsync",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1381,error,error,1381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['error'],['error']
Availability," submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:1682,down,down,1682,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['down'],['down']
Availability," the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInsta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1471,error,error,1471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability," the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a6eeab"" >&2 &; (; cd /cromwell_root. /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R /cromwell_root/required-files/references/b37/human_g1k_v37_decoy.fasta \; -I /cromwell_root/temporary-files/XXXXXX-001/workspace/SingleSampleGenotyping/415cf327-c799-4d1d-a726-272028b4e8c5/call-ubam2bam/from_ubam.to_bam_workflow/36aabe2e-6ff6-456b-a7cf-d76cb7b93173/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/e4a6eeab-c2ff-4940-b1aa-1ae64e63e1ae/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam \; --useOriginalQualities \; -O XXXXXX-001.recal_data.csv \; -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf \; -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf \; -L 12:1+ -L 13:1+; ) > ""$oute4a6eeab"" 2> ""$erre4a6eeab""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync. ); mv /cromwell_root/rc.tmp /cromwell_root/rc; ```. I appreciate the help",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:4040,echo,echo,4040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,1,['echo'],['echo']
Availability," the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:18886,echo,echo,18886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability," the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcompone",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2470,Down,Downloading,2470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloading']
Availability," the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1266,alive,alive,1266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['alive'],['alive']
Availability," to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); cromwell_1 | 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260). this code line causes the error above:; https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataConfig.scala#L38:. `PathBuilderFactory.instantiatePathBuilders(pathBuilderFactories.values.toList, WorkflowOptions.empty)`. `WorkflowOptions.empty` probably doesn't have a user_service_account_json set...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:2463,error,error,2463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,1,['error'],['error']
Availability," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:3956,echo,echo,3956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['echo'],['echo']
Availability," two runs at the same time since cromwell db gets locked by the previous run until it is finished? If yes, is there any other way to do it?. PS: I understand that cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1375,alive,alive,1375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,2,['alive'],['alive']
Availability, type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1469,recover,recoverWith,1469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['recover'],['recoverWith']
Availability," useful. Thanks. Workflow Id:. `129f0510-5d6b-4c4c-b266-116a9a52f325`. Step meta data:. ```. {; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:1015,failure,failures,1015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability," what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:10016,down,down,10016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['down'],['down']
Availability," with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.pcs' (scatter index: None, attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.ethnicity_self_report' (scatter index: None, attempt 1); [2022-12-15 21:28:01,17] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Starting main.white_brits_sample_list, main.sex_aneuploidy_sample_list, main.low_genotyping_quality_sample_list, m; ain.sex_mismatch_sample_list, main.load_shared_covars; [2022-12-15 21:28:03,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 5; [2022-12-15 21:28:03,69]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:32083,failure,failures,32083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability," work happens inside several levels of sub-workflows. ![image](https://github.com/broadinstitute/cromwell/assets/47044104/c2fa4113-3f6b-47aa-ad8d-62ab55fb374c). ## Details about the example WDL. * A root workflow (`main_workflow.wdl`) creates a subworkflow (LEVEL_1), `call outer.outer_workflow`; ``` wdl; import ""outer_subworkflow.wdl"" as outer. workflow main_workflow {; call outer.outer_subworkflow; }; ```; * LEVEL_1 `outer_subworkflow.wdl` then creates a scatter of 2 across another subworkflow (`call inner.inner_subworkflow`/LEVEL_2A and LEVEL_2B); ``` wdl; import ""inner_subworkflow.wdl"" as inner. workflow outer_subworkflow {; scatter (i in range(2)) {; call inner.inner_subworkflow as inner_subworkflow; }; }; ```; * `inner_subworkflow.wdl`/LEVEL_2A and LEVEL_2B then runs a task with a scatter and a scatter of 3 across a final subworkflow (`call sub_workflow.sub_subworkflow`/ LEVEL_2_X__3_Y); ``` wdl; import ""sub_subworkflow.wdl"" as sub_subworkflow. task hello_world {; command {; echo 'Hello, world!'; echo 'blah' > output.txt ; }. output {; String message = read_string(stdout()); File outputFile = ""output.txt""; }. runtime {; docker: ""ubuntu:latest""; }; }. workflow inner_subworkflow {; scatter (i in range(4)) {; call hello_world; }; scatter (i in range(3)) {; call sub_subworkflow.sub_subworkflow; }; }; ```; * This final `sub_subworkflow.wdl` then runs a scatter across a task:; ``` wdl; task sub_hello_world {; command {; echo 'Hello from sub.sub_workflow, world!'; }. output {; String message = read_string(stdout()); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow sub_subworkflow {; scatter (i in range(2)) {; call sub_hello_world; }; }; ```. In tree form you have something like this:; * ROOT_WORKFLOW `main_workflow.wdl`; * LEVEL_1 `outer_subworkflow.wdl`; * LEVEL_2A `inner_subworkflow.wdl`; * LEVEL_2_A__3_A `sub_subworkflow.wdl`; * LEVEL_2_A__3_B `sub_subworkflow.wdl`; * LEVEL_2_A__3_C `sub_subworkflow.wdl`; * LEVEL_2B `inner_subworkflow.wdl`; * LEVEL_2_B__3_A `su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7387:2772,echo,echo,2772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7387,2,['echo'],['echo']
Availability," workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seems like it's casting the 3 to 3.0 etc.). In any case this only seems to be a problem with defining structs inline. The conversion seems to work okay if you use input.json or if you read_json from some params.json at runtime (however I want to fix parameters inline for my current use case). The current workaround is to use object definition inline:. ```; Params p = object {; boolean: true,; float: 0.04; } ; ```. This is okay but it seems like the object syntax is going to be deprecated in newer versions (and the WDL 1.0 spec doesn't have any examples of this). Also it's generally awkward that the JSON syntax works for some combinations but not others. Thanks for any insights!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:1879,echo,echo,1879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['echo'],['echo']
Availability," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1651,error,error,1651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,5,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability," {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2519,down,downloading,2519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['down'],['downloading']
Availability," {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); ```; in metadata:; ```; failures: [; {; causedBy: {; message: ""Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; },; message: ""java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:2611,failure,failures,2611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['failure'],['failures']
Availability," {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1870,error,error,1870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15910,failure,failure,15910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['failure'],['failure']
Availability,"!. I've followed the instructions at https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/ and I've run into the following errors during 'sbt assembly': . [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:999,error,error,999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,![download](https://user-images.githubusercontent.com/961771/150234480-e61224c2-c7e6-49cd-9bc2-fb721c682eee.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200,1,['down'],['download']
Availability,![download](https://user-images.githubusercontent.com/961771/27764039-f12b841e-5e5d-11e7-9c9e-2d12766fbacd.jpg). ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2408/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688,1,['down'],['download']
Availability,![download](https://user-images.githubusercontent.com/961771/37727961-6b031812-2d0f-11e8-8150-cf0f1d2bbc00.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3442#issuecomment-375038040:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3442#issuecomment-375038040,1,['down'],['download']
Availability,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:216,Ping,Pingdom,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,2,"['Ping', 'ping']","['Pingdom', 'pingdom']"
Availability,"![image](https://user-images.githubusercontent.com/45682016/93409722-13f30f80-f8ca-11ea-89cd-bc544cea69ad.png). cromwell version: 53. config file. [aws.txt](https://github.com/broadinstitute/cromwell/files/5235741/aws.txt). ###wdl part. ```; version 1.0. task task1 {. input {; File simg; }. command {; du /cromwell_root/; du /yuce/; find *.simg; singularity exec ${simg} echo hello > hello.txt; du /cromwell_root/; }. runtime{; docker:""kongdeju/singularity:v3.4.0""; }. output {; File outfile = ""hello.txt""; }; }. ```. ### json part. ```; {; ""test.task2.simg"": ""s3://yuce/simgs/alpine.simg"",; ""test.task1.simg"": ""s3://yuce/simgs/alpine.simg""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5861:372,echo,echo,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5861,1,['echo'],['echo']
Availability,"![screen shot 2015-06-14 at 8 36 19 am](https://cloud.githubusercontent.com/assets/58551/8148606/0f7b94f6-1273-11e5-8f6e-8fb7b23aa935.png). No rush to review this. This is ancillary to the sprint but it'd be nice if we could get it in by the end of the sprint. Changes:. 1) SLF4J logging hooked in with the actor system too. 2) Two modes of logging, set by the Java Property `CROMWELL_LOGGER=[SERVER|CONSOLE]`:; - In SERVER mode, it logs to a rolling file appender with all the bells and whistles. This will default to DEBUG level.; - In CONSOLE mode, there's code in `cromwell.logging` that handles these messages from SLF4J and prints them out to the console is as human-readable way as possible. I welcome comments about how to make it more readable. Though, if you are going to do that make sure you first run it so you can see the colors, which are an important aspect of this! CONSOLE logs on INFO, WARN, ERROR.; - The modes are toggled either by explicitly setting CROMWELL_LOGGER, or based on the CLI sub-command you chose: `server` will do SERVER logging and every other sub-command uses CONSOLE logging. 3) I've tried to establish some conventions for logging:; - INFO, WARN, ERROR is meant to be read by _users_ to debug their WDL executions. It should equally be helpful for _developers_ to debug many issues. We must keep in mind that these are also show up in the server logs so they could also help us add context to debugging an issue easier if we're used to these messages from the command line.; - Messages should contain the workflow UUID wherever appropriate. Anything that exists only in a context of a workflow execution: CallActors, WorkflowActors, SymbolStores, etc.; - Messages should be chosen to craft a story about how a workflow is progressing. Highlight the big points (something starts, something finishes, something is launched, symbol store entry is updated, etc)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44:911,ERROR,ERROR,911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44,2,['ERROR'],['ERROR']
Availability,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4689:498,error,error,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689,1,['error'],['error']
Availability,"""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvalu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:2158,error,error,2158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['error'],['error']
Availability,""": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""des",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1504,failure,failures,1504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['failure'],['failures']
Availability,"""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095284). I think `Pair`s are declared with parenthesis and not brackets. Does . ```; output {; Pair[File, Array[File]] p = (unmapped_bam, fastqs); }; ```. work ?. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132). Also, as long as you don't declared the zip as a workflow variable you should be fine. For example, this should work:. ```; task t {; command {; echo ""hello""; echo ""world""; }; output {; Array[String] o = read_lines(stdout()); }; }. task t2 {; Array[Pair[String, String]] p; command {; #do something; }; output {; Array[Pair[String, String]] o = p; }; }. workflow w {; call t; call t as u; call t2 { input: zip(t.o, u.o) }; }; ```. ---. @meganshand commented on [Thu Nov 17 2016](https://github.com/br",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:1770,error,error,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['error'],['error']
Availability,"""No coercion defined"" error in optional output in CWL with wildcard glob.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['error'],['error']
Availability,"""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s = read_string(stdout()) }; }. task mirror {; Array[String] s; command {}; output { Array[String] out = s }; }. workflow helloArray {; Array[Int] ints = range(100); scatter(i in ints) {; call helloWorld; }; call mirror { input: s = helloWorld.s }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461:1959,echo,echo,1959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461,1,['echo'],['echo']
Availability,"""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-metadata-db/;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; }; }. services {; MetadataService {; metadata-read-row-number-safety-threshold = 5000000; }; }; ```; The main issue that I can see is that Cromwell is ignoring the increased metadata row count. this is despite my separating out the metadata database and increasing the thresholds on both databases. Prior to running the changes listed above I have ensured that the working directory is completely purged of logs and metadata so as to ensure an unobstructed run. The documentation currently provides no additional guidance on how to overcome the error. Any assistance will be appreciated.; Best wishes,. Matthieu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:4124,error,error,4124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['error'],['error']
Availability,"""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.bam\""},\""normal_sample\"":\""seqcii_N020\"",\""output_dir\"":\""output\"",\""promiscuous_five_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousFive.csv\""},\""promiscuous_three_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousThree.csv\""},\""reference\"":{\""class\"":\""File\"",\""location\"":\""/efs/umccr-refdata/bwa/hg38.fa\""},\""replication_origins_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/heli_rep_origins.bed\""},\""sample_name\"":\""SBJ_seqcii_020\"",\""snvvcf\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.vcf.gz\""},\""tumor_bam\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020_tumor.bam\""},\""tumor_sample\"":\""seqcii_T020\"",\""viral_hosts_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/viral_host_ref.csv\""}}"",; ""workflowUrl"": """",; ""labels"": ""{}""; },; ""calls"": {},; ""outputs"": {},; ""actualWorkflowLanguage"": ""CWL"",; ""id"": ""8681f8fa-7624-4bba-bc94-a697d1d2d179"",; ""inputs"": {},; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-8681f8fa-7624-4bba-bc94-a697d1d2d179""; },; ""submission"": ""2020-09-02T09:23:04.304Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""FileStepUUID(file:///tmp/tmp.Olzr17Zus8_cromwell/cwl_temp_dir_5247448030953921891/cwl_temp_file_8681f8fa-7624-4bba-bc94-a697d1d2d179.cwl,Some(main),out_vcf,gridss_step,) (of class cwl.FileStepUUID)""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ""workflowLog"": ""wf_logs/workflow.8681f8fa-7624-4bba-bc94-a697d1d2d179.log"",; ""end"": ""2020-09-02T09:23:06.270Z"",; ""start"": ""2020-09-02T09:23:04.925Z""; }; ```. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:129882,failure,failures,129882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['failure'],['failures']
Availability,"""cromwell --help"" should return zero exit code (success) rather than one (failure)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1870:74,failure,failure,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1870,1,['failure'],['failure']
Availability,"""failures"": [{""causedBy"": [{""causedBy"": [],""message"": ""the local copy message must have path set.""}],""message"": ""Unable to complete JES Api Request""}]. See workflow metadata at: https://cromwell-v29.dsde-methods.broadinstitute.org/api/workflows/v1/4ff9cb8a-cade-482a-8492-66ea3b7a2eaa/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791:1,failure,failures,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791,1,['failure'],['failures']
Availability,"""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 358",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2183,ERROR,ERROR,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:4825,heartbeat,heartbeat,4825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"""string"",; ""inputBinding"": {; ""prefix"": ""-A="",; ""separate"": false; }; },; {; ""name"": ""itemB"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-B="",; ""separate"": false; }; }; ]; }; }; ],; ""outputs"": {; ""example_out"": {; ""type"": ""stdout""; }; },; ""stdout"": ""output.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error] WorkflowManagerActor Workflow de935a6c-85a6-476f-845f-cf5360bbef03 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error when parsing file /tmp/cwl_temp_dir_9897655526044348367/cwl_temp_file_de935a6c-85a6-476f-845f-cf5360bbef03.cwl; DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: String; ``` ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:1995,error,error,1995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['error'],['error']
Availability,"""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/workflow.logs/workflow.3608d6ca-fbb4-4232-b197-268058470bfc.log"",; ""end"": ""2016-12-02T15:05:42.868Z"",; ""start"": ""2016-12-02T15:05:40.873Z""; }; ```. Here there's no ""message"" and there are ""timestamp"" and ""failure"". ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""account_name\"": \""abaumann.firecloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5\""\n}"",; ""inputs"": ""{\""aggregate_data_workflow.aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:6037,echo,echo,6037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['echo'],['echo']
Availability,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817:1667,reliab,reliably,1667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817,1,['reliab'],['reliably']
Availability,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:606,avail,available,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495,1,['avail'],['available']
Availability,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:485,failure,failure,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,3,"['failure', 'robust']","['failure', 'failures', 'robust']"
Availability,"# Thursday. 1. Jobs ""queued in cromwell""; 2. Doug started job, 8 hours to start; 3. Graphite under-reporting. # Friday. 1. No improvement; 2. User1 suggests his own wf running w/ 25k jobs; 3. disk quota hit; 4. **No limit on # jobs running / project**; 4. Console quotas page revealed this; 5. User1 aborts wf; 6. 10 minutes of improvement, then back down to low throughput; 7. **Difficult to understand who is running what**; 8. We found a crude query to ask ^; 9. Bubbled up sub-wf's manually; 10. Found User2 running a large job; 11. User had upped quotas on # cpus, persistent disk and IP's; 12. **Exhausted IP quota, PAPI throttles itself as a result of too many API calls**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:351,down,down,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['down'],['down']
Availability,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105:716,Reboot,Rebooted,716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105,5,"['Error', 'Reboot', 'error']","['Error', 'Rebooted', 'error']"
Availability,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:65,mainten,maintenance,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,7,"['degraded', 'down', 'error', 'mainten', 'outage', 'resilien']","['degraded', 'down', 'errors', 'maintenance', 'outages', 'resilient']"
Availability,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4138:627,resilien,resiliency,627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138,2,"['failure', 'resilien']","['failures', 'resiliency']"
Availability,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@370f3e3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## develop #2657 +/- ##; ==========================================; Coverage ? 64.03% ; ==========================================; Files ? 381 ; Lines ? 8893 ; Branches ? 193 ; ==========================================; Hits ? 5695 ; Misses ? 3198 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [...backend/standard/StandardAsyncExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree#diff-YmFja2VuZC9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL3N0YW5kYXJkL1N0YW5kYXJkQXN5bmNFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `67.44% <100%> (Ã¸)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165,2,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2663?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@839ea1e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## develop #2663 +/- ##; ==========================================; Coverage ? 64.32% ; ==========================================; Files ? 381 ; Lines ? 8892 ; Branches ? 195 ; ==========================================; Hits ? 5720 ; Misses ? 3172 ; Partials ? 0; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801,2,['error'],['error-reference']
Availability,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (Ã¸)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119,2,['error'],['error-reference']
Availability,"## About this PR; ðŸ“¦ Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:1005,down,down,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from `0.40.10` to `0.40.17`. ðŸ“œ [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.17) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7270:1416,down,down,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7270,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from `0.14.1` to `0.14.6`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.6) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.6). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7292:1299,down,down,1299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7292,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.http4s:http4s-ember-client](https://github.com/http4s/http4s); * [org.http4s:http4s-ember-server](https://github.com/http4s/http4s). from `0.21.31` to `0.21.34`. ðŸ“œ [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7311:1095,down,down,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7311,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.junit.jupiter:junit-jupiter-api](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-engine](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-params](https://github.com/junit-team/junit5). from `5.9.3` to `5.10.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.junit.jupiter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.junit.jupiter"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7312:1041,down,down,1041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7312,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from `2.7.0` to `2.10.0`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.10.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.10.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7320:1384,down,down,1384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7320,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [cglib:cglib-nodep](https://github.com/cglib/cglib) from `3.2.7` to `3.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""cglib"", artifactId = ""cglib-nodep"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""cglib"", artifactId = ""cglib-nodep"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7259:864,down,down,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7259,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from `2.18.0` to `2.33.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7269:938,down,down,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7269,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-http-okhttp](https://github.com/Azure/azure-sdk-for-java) from `1.11.10` to `1.11.17`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7262:910,down,down,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7262,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from `1.7.1` to `1.11.9`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7263:1156,down,down,1156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7263,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-test](https://github.com/Azure/azure-sdk-for-java) from `1.18.0` to `1.18.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.18.0).; You might want to review and update them manually.; ```; cloud-nio/cloud-nio-impl-drs/src/test/scala/cloud/nio/impl/drs/DrsPathResolverSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7264:1208,down,down,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7264,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core](https://github.com/Azure/azure-sdk-for-java) from `1.40.0` to `1.45.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7261:884,down,down,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7261,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity-extensions](https://github.com/azure/azure-sdk-for-java) from `1.1.4` to `1.1.10`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7266:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7266,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from `1.9.1` to `1.9.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7265:890,down,down,890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7265,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-blob](https://github.com/Azure/azure-sdk-for-java) from `12.23.0-beta.1` to `12.23.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7267:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7267,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-common](https://github.com/Azure/azure-sdk-for-java) from `12.22.0-beta.1` to `12.22.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-common"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-common"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7268:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7268,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` âš . ðŸ“œ [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.fasterxml.jackson.dataformat:jackson-dataformat-xml](https://github.com/FasterXML/jackson-dataformat-xml) from `2.13.3` to `2.13.5`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7272:962,down,down,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7272,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.github.cb372:sbt-explicit-dependencies](https://github.com/cb372/sbt-explicit-dependencies) from `0.2.16` to `0.3.1`. ðŸ“œ [GitHub Release Notes](https://github.com/cb372/sbt-explicit-dependencies/releases/tag/v0.3.1) - [Version Diff](https://github.com/cb372/sbt-explicit-dependencies/compare/v0.2.16...v0.3.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7273:1126,down,down,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7273,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.github.sbt:junit-interface](https://github.com/sbt/junit-interface) from `0.13.2` to `0.13.3`. ðŸ“œ [GitHub Release Notes](https://github.com/sbt/junit-interface/releases/tag/v0.13.3) - [Version Diff](https://github.com/sbt/junit-interface/compare/v0.13.2...v0.13.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7274:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7274,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client) from `2.1.4` to `2.2.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v2.2.0) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v2.1.4...v2.2.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7276:1142,down,down,1142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7276,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-batch-v1](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7277:1130,down,down,1130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7277,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-resourcemanager-v3](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7278:1214,down,down,1214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7278,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.api:gax-grpc](https://github.com/googleapis/sdk-platform-java) from `2.25.0` to `2.38.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7275:1146,down,down,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7275,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from `1.5.3` to `1.20.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.20.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.20.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7281:1149,down,down,1149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7281,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-batch](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7282:1106,down,down,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7282,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-bigquery](https://github.com/googleapis/java-bigquery) from `2.25.0` to `2.34.2`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-bigquery/releases/tag/v2.34.2) - [Version Diff](https://github.com/googleapis/java-bigquery/compare/v2.25.0...v2.34.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7283:1352,down,down,1352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7283,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7284:1190,down,down,1190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7284,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-storage](https://github.com/googleapis/java-storage) from `2.17.2` to `2.29.1`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-storage/releases/tag/v2.29.1) - [Version Diff](https://github.com/googleapis/java-storage/compare/v2.17.2...v2.29.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.17.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7285:1347,down,down,1347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7285,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [com.typesafe:config](https://github.com/lightbend/config) from `1.4.2` to `1.4.3`. ðŸ“œ [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.3) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.2...v1.4.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7286:1033,down,down,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7286,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-codec:commons-codec](https://github.com/apache/commons-codec) from `1.15` to `1.16.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.15).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_wdlom.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-codec"", artifactId = ""commons-codec"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-codec"", artifactId = ""commons-codec"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7287:1363,down,down,1363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7287,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-io:commons-io](https://commons.apache.org/proper/commons-io/) from `2.11.0` to `2.15.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.11.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-io"", artifactId = ""commons-io"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-io"", artifactId = ""commons-io"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7288:1140,down,down,1140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7288,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from `3.8.0` to `3.10.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7289:892,down,down,892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7289,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [eu.timepit:refined](https://github.com/fthomas/refined) from `0.10.1` to `0.10.3`. ðŸ“œ [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.10.3) - [Version Diff](https://github.com/fthomas/refined/compare/v0.10.1...v0.10.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7290:1033,down,down,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7290,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-config](https://github.com/circe/circe-config) from `0.8.0` to `0.10.1`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7291:965,down,down,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7291,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-optics](https://github.com/circe/circe-optics) from `0.14.1` to `0.15.0`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-optics/releases/tag/v0.15.0) - [Version Diff](https://github.com/circe/circe-optics/compare/v0.14.1...v0.15.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-optics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-optics"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7293:1300,down,down,1300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7293,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from `1.54.1` to `1.54.2`. ðŸ“œ [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.54.2) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.54.1...v1.54.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7295:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7295,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.projectreactor:reactor-test](https://github.com/reactor/reactor-core) from `3.4.29` to `3.4.34`. ðŸ“œ [GitHub Release Notes](https://github.com/reactor/reactor-core/releases/tag/v3.4.34) - [Version Diff](https://github.com/reactor/reactor-core/compare/v3.4.29...v3.4.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7296:1072,down,down,1072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7296,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from `5.7.4` to `7.0.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/7.0.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...7.0.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7297:1057,down,down,1057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7297,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from `1.0.56` to `1.0.68`. ðŸ“œ [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.68) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.68). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7298:1080,down,down,1080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7298,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from `2.4.10` to `2.4.11`. ðŸ“œ [GitHub Release Notes](https://github.com/netplex/json-smart-v2/releases/tag/2.4.11) - [Version Diff](https://github.com/netplex/json-smart-v2/compare/2.4.10...2.4.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.minidev"", artifactId = ""json-smart"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""net.minidev"", artifactId = ""json-smart"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7300:1056,down,down,1056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7300,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.apache.commons:commons-lang3](https://commons.apache.org/proper/commons-lang/) from `3.12.0` to `3.14.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7301:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7301,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.apache.tika:tika-core](https://tika.apache.org/) from `2.3.0` to `2.9.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7303:1382,down,down,1382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7303,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from `3.1.7` to `3.1.11`. ðŸ“œ [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.7...3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7307:1151,down,down,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7307,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. ðŸ“œ [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:1562,down,down,1562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from `4.1.1` to `4.4.0`. ðŸ“œ [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.4.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7309:967,down,down,967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7309,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.hsqldb:hsqldb](http://hsqldb.org) from `2.6.1` to `2.7.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7310:1101,down,down,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7310,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from `2.7.4` to `2.7.11`. ðŸ“œ [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.11) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7314:1235,down,down,1235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7314,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from `4.11.0` to `5.7.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.7.0) - [Version Diff](https://github.com/mockito/mockito/compare/v4.11.0...v5.7.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7315:1044,down,down,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7315,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from `1.13.1` to `1.13.6`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7316:895,down,down,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7316,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from `2.13.9` to `2.13.12`. ðŸ“œ [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.12) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.12). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7317:1044,down,down,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7317,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from `3.2.15` to `3.2.17`. ðŸ“œ [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.17) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.15...release-3.2.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7318:1076,down,down,1076,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7318,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from `2.0.4` to `2.0.9`. ðŸ“œ [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.9) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.9). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7319:1070,down,down,1070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7319,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from `2.3.2` to `3.1.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.1.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.1.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7321:1293,down,down,1293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7321,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from `1.0.11` to `1.2.2`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.2) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7322:1267,down,down,1267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7322,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [org.yaml:snakeyaml](https://bitbucket.org/snakeyaml/snakeyaml/src) from `1.33` to `2.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. <sup>; labels: test-library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7324:1484,down,down,1484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7324,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates [se.marcuslonnberg:sbt-docker](https://github.com/marcuslonnberg/sbt-docker) from `1.9.0` to `1.11.0`. ðŸ“œ [GitHub Release Notes](https://github.com/marcuslonnberg/sbt-docker/releases/tag/v1.11.0) - [Changelog](https://github.com/marcuslonnberg/sbt-docker/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/marcuslonnberg/sbt-docker/compare/v1.9.0...v1.11.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7325:1417,down,down,1417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7325,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates bio.terra:workspace-manager-client from `0.254.452-SNAPSHOT` to `0.254.966-SNAPSHOT`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7258:1153,down,down,1153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7258,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-cloudkms from `v1-rev20230421-2.0.0` to `v1-rev20231012-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7279:913,down,down,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7279,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-lifesciences from `v2beta-rev20220916-2.0.0` to `v2beta-rev20230707-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7280:929,down,down,929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7280,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates mysql:mysql-connector-java from `8.0.28` to `8.0.33`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7299:849,down,down,849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7299,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.apache.commons:commons-text from `1.10.0` to `1.11.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-text"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-text"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7302:859,down,down,859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7302,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-2147824`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7304:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7304,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-5781917`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9ac858c7e61f43ed3648f0fabc7104d0951cce67/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7331:909,down,down,909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7331,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-model from `0.15-f9f0d4c` to `0.19-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7305:907,down,down,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7305,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-util from `0.6-65bba14` to `0.10-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7306:904,down,down,904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7306,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.liquibase:liquibase-core from `4.8.0` to `4.25.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7313:852,down,down,852,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7313,1,['down'],['down']
Availability,"## About this PR; ðŸ“¦ Updates org.webjars:swagger-ui from `4.5.2` to `4.19.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7323:840,down,down,840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7323,1,['down'],['down']
Availability,"## Bug. I am trying to run a workflow using the GCP backend, however no matter what set of configurations I use, I am unable to have it succeed. The workflows Batch task appears to fail on the 3rd task, just after the `Setup Container`. This is basically causing every task to fail for some strange reason. ```; docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons.; ```. [This](https://cromwellhq.slack.com/archives/CGQ7WK5A6/p1697484861117659) thread suggested that the logic in [these two lines](https://github.com/broadinstitute/cromwell/blob/86/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/runnable/RunnableBuilder.scala#L63-L64) may be the culprit under specific condirtions. ## Information. Cromwell Version: 87-c9d4ce4; <!-- Which backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. task hello {. input {; String name; }; command <<<; echo 'hello ~{name}!'; >>>. output {; File response = stdout(); }. runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:990,echo,echo,990,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['echo'],['echo']
Availability,"## Bug; Cromwell appears to be improperly tokenizing string interpolations when using a variable that is prepended by the letters: `if` in any format. It seems to think that this is actually the start of a conditional `if _ then _ else` block instead of a variable name. The parser does not appear to be discriminating against the lack of whitespace in variables with the form `if[a-zA-Z0-9_]+` and fails to parse with an error. ### How to reproduce. ```; cat <<EOF > test.wdl; version 1.0; task test_task {; String ifl_token=""a""; command <<<; echo ""~{ifl_token}""; >>>; }. workflow test {; call test_task; }; EOF. java -jar cromwell-84.jar run. test.wdl; ```. ### Expected; 1. The workflow to parse correctly and to echo `a` when running. ### Actual Error. The workflow fails to parse with the following error:. ```; [2022-11-25 11:10:39,68] [info] WorkflowManagerActor: Workflow 45701495-7113-40d6-ac32-dab5247f37e7 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Unexpected symbol (line 5, col 20) when parsing 'e'. Expected then, got ""}"". echo ""~{ifl_token}""; ^. $e = :identifier <=> :lparen $_gen23 :rparen -> FunctionCall( name=$0, params=$2 ); ; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:257); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:227); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:222); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); ```. ### Environment:; Tested on:; - Cromwell 84; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6956:422,error,error,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6956,5,"['Error', 'echo', 'error']","['Error', 'echo', 'error']"
Availability,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:918,reliab,reliable,918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['reliab'],['reliable']
Availability,"## From @yfarjoun . I lost a whole day on this stupid bug....; when the input file specified in a JSON is missing, submitting the job takes a long time and returns with:. Resource representation is only available with these Content-Types:; text/plain; charset=UTF-8; text/plain. Which, needless to say is not pointing at the problem...I only managed to figure this out by slowly transitioning from a functioning JSON to the ""problematic"" one... Cheers,. Yossi.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703:203,avail,available,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703,1,['avail'],['available']
Availability,"## I want to. ## (i) scatter scatters as requested by @skazakoff above (aka ""nested scatter"") and also. ## (ii) re-array and scatter again (what I'm calling criss-cross scatter). As I show in this diagram: . ![img_4460](https://cloud.githubusercontent.com/assets/11543866/18210288/d489f7b4-7105-11e6-9737-59f75d4a6949.JPG). My attempt to ask WDL/Cromwell to do the first scatter-of-scatter above gives the following error:. ```; [2016-09-02 16:30:43,699] [error] WorkflowActor [e1b18d33]: Call failed to initialize: Failed to start call: Nested Scatters are not supported (yet).; ```. It was unclear how I would go about criss-cross scattering. My current solution around this is to run two separate WDL workflows. Between the two workflows, I use a simple python script to organize the files for the second scatter. The first WDL workflow is per sample, while the second WDL workflow acts on all the outputs of the multiple runs of the first workflow (multiple samples). ---. ## Example of workflows that I want to run within a single WDL workflow. These are scripts that run on the cloud that I would like to run within a single WDL script using the nest-scatter and criss-cross-scatter features. Currently, I run the first script per sample (1) for multiple samples, then run a helper script to organize all the outputs (1.5), and finally run the second WDL script across the multi-sample outputs per genomic interval (2). . I would like to be able to scatter the samples, scatter per interval, then run a differently organized (criss-cross) scatter across all the samples per interval. ### (1) First WDL workflow. #### UltimateScatterHaplotypeCaller_cloud_quicktest.wdl. ```; # ScatterHaplotypeCaller.wdl #############################################################; # Must use GATK v3.6, especially with hg38 where contig names have colons,; # as a bug in prior versions of GATK strips this off.; # Each BAM file represents a single different sample.; # Option to include an additional file to H",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995:416,error,error,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995,4,['error'],['error']
Availability,"## Symptom; I can run test and assembly tasks succesfully on cromwellApiClient subproject, but If I write my own small test class that uses `cromwell.api.CromwellClient` it fails at runtime with:; ```; Detected java.lang.NoSuchMethodError error, which MAY be caused by incompatible Akka versions on the classpath. Please note that a given Akka version MUST be the same across all modules of Akka that you are using, e.g. if you use akka-actor [2.5.3 (resolved from current classpath)] all other core Akka modules MUST be of the same version. External projects like Alpakka, Persistence plugins or Akka HTTP etc. have their own version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:239,error,error,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,4,"['down', 'error']","['down', 'error']"
Availability,## What it does do; Gives you the field that does not parse. (This is better than the previous `CanBuildFrom` brick wall.). ## What it does not do; * Tell you the offending value; * Distinguish between CWL's when multiple are submitted in one file. This is because `id` field is optional and made random by SALAD preprocessing. Example of 2 errors combined:; ```; DecodingFailure at .inputs[1].inputBinding.position: Int; DecodingFailure at .stdout: DecodingFailure at .stdout: DecodingFailure at .stdout: String; ```; So not amazing (don't know why `at .stdout` is repeated thrice) but better.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3180:341,error,errors,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3180,1,['error'],['errors']
Availability,"## basic issue; `womtool validate` does not catch all scenarios where you are defining a new variable based on an optional variable. Instead, Cromwell fails at runtime -- even if it is actually impossible for that optional variable to be undefined. [A working example is available](https://github.com/aofarrel/myco/commit/e7f9ba6951d1b0fe5b3c1a650835312dd2b6e68f), but it is a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.er",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:271,avail,available,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,4,"['avail', 'error']","['available', 'errorcode']"
Availability,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:100,error,errors,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,7,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"### Description. As part of preparing for the fall 2024 audit, we were asked to fix the permissions on the various healthcheck buckets such as `gs://cromwell-ping-me-dev`. It would have taken some tinkering to make sure the permissions are secure enough _and_ the healthcheck still works, so I decided to drop the healthcheck. I am not aware of any times it's helped us and doesn't pass the ""would we add this today"" test. The only notable bucket we'd want to make sure Cromwell itself has permissions on is the workflow archiver - and it uses [a separate service account from the rest of Cromwell](https://github.com/broadinstitute/terra-helmfile/blob/master/charts/cromwell/templates/config/_cromwell.conf.tpl#L267-L271), so it's not a valid test. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533:158,ping,ping-me-dev,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533,1,['ping'],['ping-me-dev']
Availability,"### Description. Currently, and as described in https://github.com/broadinstitute/cromwell/issues/7535, only general-purpose machine types are supported in Google Backend, which prevents running wdl workflows on many machine types available on GCP, including those provisioned with modern GPUs. I believe the simplest and most general solution would be to pass the machine type directly from the wdl configuration to the Google Batch API. The idea is that this approach would be more resilient to machine types being added or deprecated on GCP, as users would only need to update their wdl workflows in such cases. An alternative approach of mapping machine specs (e.g.: cpu platform and gpu requirements) to standard machine types would potentially introduce an additional layer of maintenance with little benefit. This PR adds support for a new standardMachineType key in the runtime section, which is only parsed for the Google backend. ### Testing. I deployed this internally and verified I can successfully run the following wdl workflow:. ```; version 1.0. task nvidia_smi {; input {; String docker_version; }. command <<<; nvidia-smi. touch .done; echo ""Finished at $(date)""; >>>. runtime {; docker: <internal image>; disks: ""local-disk 50 SSD""; memory: ""32G""; preemptible: 0; gpuCount: 1; gpuType: ""nvidia-tesla-a100""; standardMachineType: ""a2-highgpu-1g""; }. output {; File done = "".done""; }; }. workflow nvidia_smi_wf {; input {; String docker_version; }; ; call nvidia_smi as nvidia_smi_call {; input:; docker_version = docker_version; }. output {; File done = "".done""; }; }; ```. ### Next steps. - [ ] Confirm this approach is in the right direction with the cromwell team.; - [ ] Work on proper unit tests and get this PR ready to be merged. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545:231,avail,available,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545,3,"['avail', 'mainten', 'resilien']","['available', 'maintenance', 'resilient']"
Availability,"### Description. Fixes CI failures caused by errors like:; ```; $ docker pull quay.io/broadinstitute/cromwell-docker-test:centaur; centaur: Pulling from broadinstitute/cromwell-docker-test; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/broadinstitute/cromwell-docker-test:centaur to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. The very old images need to be updated to get around this. For Python, we can use a newer version. For the Quay image we manage, using a different one because I fear push access to the current one has been lost in the mists of time. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,2,"['error', 'failure']","['errors', 'failures']"
Availability,"### Description. Fixes job recovery on restart for GCP Batch, addresses #7495. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recovery']
Availability,### Description. Part 2 of https://github.com/broadinstitute/cromwell/pull/7432. Detects and retries the new fatal quota errors we've been seeing. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7439:121,error,errors,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7439,1,['error'],['errors']
Availability,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes ðŸŽ‰ ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:317,redundant,redundant,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,1,['redundant'],['redundant']
Availability,"### Description. The PR exercises the ""retry with more memory"" Centaur tests on the GCP Batch backend. Minimal changes to production code, all of which are in the GCP Batch backend:. - The constant`RunnableUtils#MountPoint` was created with value `/mnt/disks/cromwell_root` and applied where appropriate.; - A copy/paste bug in code brought over from PAPIv2 was corrected (the `/cromwell_root` of PAPIv2 has become `/mnt/disks/cromwell_root` in Batch), using the constant described above.; - If a job fails, the *last* event message is now propagated rather than the first event message. The first event message is often a benign state transition, while the last event message is more likely to contain the actual reason for job failure.; ; Unfortunately Cromwell does not allow for dynamic backend selection (i.e. the backend name cannot be a variable), which necessitated copy/paste/renaming the Centaur test WDLs from their PAPIv2 versions, hence the magnitude of these diffs. The existing `preemptible_and_memory_retry ` Centaur test is heavily tailored to the quirks of Papi v2: a preemptible PAPI VM deletes itself and depends on the Lifesciences system mistaking that for a preemption event. tbh this is kind of a weird test and as I don't know how to induce a preemption on demand, I simply `ignore`d the GCPBATCH version. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:729,failure,failure,729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,1,['failure'],['failure']
Availability,"### Description. The `google_legacy_machine_selection` workflow option that exists on PAPI v2 to pick JES-shaped machines was not completely wired in to the GCP Batch backend. However when I completed the wiring job and ran the `hello_google_legacy_machine_selection` Centaur test I got this:. ```; Task wf_hello.hello:NA:1 failed: Job failed when Batch tries to schedule it:; Batch Error: code - CODE_MACHINE_TYPE_NOT_FOUND, description - ; machine type predefined-1-2048 for job job-xyz, project 8675309, region us-central1, zones (if any) us-central1-b is not available.; ```. So basically `google_legacy_machine_selection` does not seem to work on GCP Batch. If anyone is using this undocumented feature to emulate the behavior of JES from many years ago, we should let them know that this support is going to be dropped when GCP Batch is rolled out. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7547:383,Error,Error,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7547,2,"['Error', 'avail']","['Error', 'available']"
Availability,"### Description. The current Centaur backend selection mode within a *.test file is 'all', which means if a given backend does not have all the backends enumerated in the backend specification then the test will be ignored. IMHO this is a dangerous default behavior because it can cause tests to be unexpectedly ignored, creating a false sense of confidence in the correctness of the code that was intended to be tested. These changes make the default backend selection mode 'any'. The failure mode here is that a test may run on backends that the test author did not intend, but if this happens that should be a noisy failure that the test author cannot ignore and must correct. Test authors can always explicitly override the `backendsMode` in a Centaur .test file if they want. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7513:486,failure,failure,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7513,2,['failure'],['failure']
Availability,"### Description. This should have no effect on our existing Bard usage, just removes errors logged when NOT using Bard. * Base config was incorrect, so Bard was not registered in the Service Registry by default; * `BardEventingActor.receive` had no handling for receiving a `BardEvent` when eventing was disabled... and also nothing has ever checked for enablement before creating and sending these events. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7566:85,error,errors,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7566,1,['error'],['errors']
Availability,### Description. Turn on 90ish Centaur tests for GCPBATCH. In all but one case this was just adding the GCPBATCH backend to the Centaur .test file. The one exception involved different error message text coming from the Batch system than what we get from Lifesciences. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7496:185,error,error,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7496,1,['error'],['error']
Availability,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:793,failure,failure,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,1,['failure'],['failure']
Availability,"### Description. We make this query constantly, looks like the single most frequent one against metadata. All it does is check whether the workflow ID is valid by checking whether >1 metadatum exists for it. We already started down the path of checking summary instead of metadata, see https://github.com/broadinstitute/cromwell/pull/4617. It just makes way more sense to me to check a table with 77M rows than 36B. ```; select ; exists(; select ; `CALL_FQN`, ; `METADATA_KEY`, ; `WORKFLOW_EXECUTION_UUID`, ; `METADATA_TIMESTAMP`, ; `JOB_SCATTER_INDEX`, ; `METADATA_JOURNAL_ID`, ; `JOB_RETRY_ATTEMPT`, ; `METADATA_VALUE_TYPE`, ; `METADATA_VALUE` ; from ; `METADATA_ENTRY` ; where ; `WORKFLOW_EXECUTION_UUID` = '602a4913-d666-4182-b2f1-242fbda817d2'; );; ```. It is potentially implicated in the 2021 database migration that failed at the very end, you can see a bunch of them in this screenshot (2021-11-09):. <img width=""1792"" alt=""Screen Shot 2021-11-09 at 1 14 03 AM"" src=""https://github.com/user-attachments/assets/d3eee3da-9636-4406-a6f9-9862d33cd650"">. ```; Lock wait timeout exceeded; try restarting transaction; [for Statement ""RENAME TABLE `cromwell`.`METADATA_ENTRY` TO `cromwell`.`_METADATA_ENTRY_old`,; `cromwell`.`_METADATA_ENTRY_new` TO `cromwell`.`METADATA_ENTRY`""]; at /usr/bin/pt-online-schema-change line 10922.; ```; [Slack link to contemporary discussion.](https://broadinstitute.slack.com/archives/C02LCC8968N/p1636439602084200); [Contemporary analysis in JIRA.](https://broadworkbench.atlassian.net/browse/WM-906?focusedCommentId=53394). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575:227,down,down,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575,1,['down'],['down']
Availability,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,4,"['down', 'echo', 'error']","['downloadArtifacts', 'echo', 'error', 'errors']"
Availability,"### Description. When Cromwell restarts during a failure, it must fail the 'next' upcoming tasks in order to cleanly terminate the workflow. During this process, it was logging an error that isn't really an error (and was very confusing to users). . This PR: ; - Changes the failure reason to something more relevant.; - Removes the failure reason from the 'Workflow' level failures, since it was not the reason the workflow failed. The failure reason is still attached to the task that never ran. . ### Release Notes Confirmation; #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7449:49,failure,failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7449,7,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"### Description; Google has shut down their Genomics (a.k.a. PAPI v2Alpha1) API, this PR cleans up associated code. This is not to be confused with the Cloud Life Sciences API (a.k.a. PAPI v2beta, deprecated, but still in use for the next few months) or the Google Batch backend (the new hotness). . #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7532:33,down,down,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7532,1,['down'],['down']
Availability,"### EDIT: See https://www.traviscistatus.com/incidents/kyf149kl6bvp. Multiple builds are displaying timeouts trying to run the dockerScripts tests. These builds have a heartbeat that give more information as to the timeout:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197403844; - https://travis-ci.com/broadinstitute/cromwell/jobs/197407990; - https://travis-ci.com/broadinstitute/cromwell/jobs/197412193; - https://travis-ci.com/broadinstitute/cromwell/jobs/197420904. May be something that broke in our repo, or an upstream transient error?. Edit:. Just commenting out the `sbt assembly` is not enough. Commenting out just the assembly leads to downstream build errors such as:; - https://travis-ci.com/broadinstitute/cromwell/jobs/197523977",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4933:168,heartbeat,heartbeat,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4933,4,"['down', 'error', 'heartbeat']","['downstream', 'error', 'errors', 'heartbeat']"
Availability,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:582,error,errors,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,1,['error'],['errors']
Availability,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:724,error,errors,724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527,3,"['Ping', 'error']","['Pinging', 'errors']"
Availability,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:98,failure,failures,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,3,['failure'],['failures']
Availability,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,5,['error'],"['error', 'errors']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1280,error,errors,1280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1233,Error,Error,1233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,3,"['Error', 'error']","['Error', 'error']"
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. I'm using cromwell to kick off 1000 mutect2 jobs at a time, but right now the workflow doesn't kick off all 1000 sub-workflows simultaneously. When I look at my google cloud quotas none are close to being full. I'm wondering what settings I might need to change in the system or some other section of the google conf to better use all the compute I have available. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; I'm kicking off google cloud jobs using cromwell from a local VM.; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; # vi : arrays of normal bams; # scatter_count: number of parallel jobs when scattering over intervals; # pon_name: the resulting panel of normals is {pon_name}.vcf; # m2_extra_args: additional command line parameters for Mutect2. This should not involve --max-mnp-distance,; # which the wdl hard-codes to 0 because GenpmicsDBImport can't handle MNPs. #import """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:591,avail,available,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['avail'],['available']
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,. My workflow runs out of memory. I'm not sure where exactly though. I'm trying to run Mutect2 on a large cohort of samples, but it keeps crashing with an out of memory error. Any help would be extremely helpful as to how I can avoid this issue. I'm using `-Xmx32g` when I call cromwell, and am using GCS as the backend. Here the error:; ```; ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,2,['error'],['error']
Availability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. I'm trying to run the workflow below. There is a working version for a single sample (https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl), but I want to run the workflow on many samples concurrently. My attempt at creating a workflow to do so failed with the following error:. ```; Failed to process scatter block (reason 1 of 1): No conversion defined for Ast with name Outputs to WorkflowGraphElement; ```. Any help would be greatly appreciated. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how ma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['error'],['error']
Availability,"#2030 fixes this specific issue. There remains a greater tech debt of actor supervision, where unexpected errors like this shouldn't leave the workflows in a running state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614:106,error,errors,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614,1,['error'],['errors']
Availability,#3984 restored streaming logs. A/C for this ticket would add a regression test for the functionality. One possible centaur implementation for a task:; - Writes to local stdout & stderr; - Calculates the gcloud stdout & stderr path; - Sleep a bit to allow streaming; - Use gsutil to download the stdout & stderr from gcloud; - Validate that the stdout & stderr are as expected,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4187:282,down,download,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4187,1,['down'],['download']
Availability,"$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"",",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2003,error,error,2003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['error'],['error']
Availability,$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1448,Robust,RobustClientHelperSpec,1448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(Wom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:6708,Error,ErrorOr,6708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5789,error,errors,5789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:5213,error,error,5213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['error'],['error']
Availability,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:2070,echo,echo,2070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['echo'],['echo']
Availability,"(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:3298,error,error,3298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Then I see:. ```; [ERROR] [05/01/2017 17:36:04.203] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 5; 3e95ead-9026-4c13-89f9-f6c675214523 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the follow; ing files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n; File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.; py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrappin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:2592,error,error,2592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['error'],['error']
Availability,"(First, I've registered at; https://gatkforums.broadinstitute.org/wdl/categories/wdl#; but I cannot figure out how to post a question there. If you help me with that, I would be happy to re-ask in the ""user forum"" there, but so far I do not see how.). My question is on `write_json` versus `write_lines`. The following works fine (using Cromwell-36):; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_lines(results)}""; echo ${files_fn}; >>>; }; ```; But this fails:; ```; task foo {; input {; Array[File] results; }; command <<<; echo ~{sep=',' results}; files_fn=""~{write_json(results)}""; echo ${files_fn}; >>>; }; ```; It says `write_json` needs an ""Object"". Am I doing something wrong?; ```; Failed to process task definition 'foo' (reason 1 of 1): Failed to process expression 'write_js; on(results)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(results)'. Expected 'Object' but got 'Array[File]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4625:415,echo,echo,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625,4,['echo'],['echo']
Availability,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14676,error,errors,14676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"(JdbcBackend.scala:491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] Backg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3928,Error,Error,3928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['Error'],['Error']
Availability,"(Mailbox.scala:268); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | ; cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 1280",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:6778,echo,echo,6778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['echo'],['echo']
Availability,"(No links to failed builds, as clicking Travis ""Retry Build"" erased the logs.). Travis has been having network errors recently attempting to download artifacts from maven central, and in at least one case downloading the ubuntu:latest docker image. In the various scripts under src/bin/travis, the scripts should retry:. - `sbt update` before any other sbt commands; - `docker pull ubuntu:latest`; - possibly: `sudo apt-get â€¦`. The retries should wait some time (60 seconds?) between failures, and try at least twice? Possibly this could be put into a utility bash script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791:111,error,errors,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791,4,"['down', 'error', 'failure']","['download', 'downloading', 'errors', 'failures']"
Availability,"(Probably related to #4081). . Using cromwell v36. I had the following code block in my wdl:. ```; call MergePileupSummaries as MergeTumorPileups {; input:; input_tables = TumorPileups.pileups,; output_name = , // <--- forgot to specify this input; ref_dict = ref_dict,; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible_attempts = preemptible_attempts,; max_retries = max_retries,; disk_space = ceil(SumSubVcfs.total_size * large_input_to_output_multiplier) + disk_pad; }; ```. Cromwell error didn't give me anything, and `womtool validate` threw an exception:. ```; tsato@gsa5:fc-read-orientation: java -jar womtool-36.jar validate mutect2.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl.draft2.model.WdlExpression$.toString(WdlExpression.scala:117); 	at wdl.draft2.model.WdlExpression.toString(WdlExpression.scala:200); 	at wdl.draft2.model.WdlExpression.toWomString(WdlExpression.scala:203); 	at wom.values.WomValue.valueString(WomValue.scala:50); 	at wom.values.WomValue.valueString$(WomValue.scala:50); 	at wdl.draft2.model.WdlExpression.valueString(WdlExpression.scala:185); 	at wdl.draft2.model.WdlWomExpression.sourceString(WdlExpression.scala:219); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$4(ExpressionNode.scala:66); 	at cats.data.NonEmptyList.map(NonEmptyList.scala:76); 	at wom.graph.expression.ExpressionNode$.$anonfun$buildFromConstructor$3(ExpressionNode.scala:66); 	at cats.data.Validated.leftMap(Validated.scala:203); 	at wom.graph.expression.ExpressionNode$.buildFromConstructor(ExpressionNode.scala:66); 	at wom.graph.expression.AnonymousExpressionNode$.fromInputMapping(AnonymousExpressionNode.scala:17); 	at wdl.draft2.model.WdlWomExpression$.$anonfun$toAnonymousExpressionNode$1(WdlExpression.scala:293); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flat; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:510,error,error,510,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,2,"['Error', 'error']","['ErrorOr', 'error']"
Availability,"(during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:1206,error,error,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['error'],['error']
Availability,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:368,failure,failure,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503,1,['failure'],['failure']
Availability,) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]; > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74]; > at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74]; > Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.error.Error.error(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Constraint.getException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.index.IndexAVLMemory.insert(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.persist.RowStoreAVL.indexRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.TransactionManagerMVCC.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Table.insertSingleRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDML.insertRowSet(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementInsert.getResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDMQL.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.executeCompiledStatement(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > ... 23 common frames omitted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:4285,error,error,4285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['error'],['error']
Availability,"); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3361,error,errors,3361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['error'],['errors']
Availability,); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:86); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(Part,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:4092,robust,robustExecuteOrRecover,4092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['robust'],['robustExecuteOrRecover']
Availability,); 	at cromwell.backend.wdl.ReadLikeFunctions$class.read_string(ReadLikeFunctions.scala:62); 	at cromwell.backend.impl.jes.JesExpressionFunctions.read_string(JesExpressionFunctions.scala:16); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:181); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:180); 	... 38 more; Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoog,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:7178,Error,Error,7178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['Error'],['Error']
Availability,"* Addresses the ""programmer error"" in JES API manager; * Makes the logging in these classes more usable and traceable; * Replaces strings like `""JES API polling worker""` with strings like `""PAPI request worker""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671,1,['error'],['error']
Availability,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,1,['error'],['error']
Availability,"* LEVEL_1 `outer_subworkflow.wdl` then creates a scatter of 2 across another subworkflow (`call inner.inner_subworkflow`/LEVEL_2A and LEVEL_2B); ``` wdl; import ""inner_subworkflow.wdl"" as inner. workflow outer_subworkflow {; scatter (i in range(2)) {; call inner.inner_subworkflow as inner_subworkflow; }; }; ```; * `inner_subworkflow.wdl`/LEVEL_2A and LEVEL_2B then runs a task with a scatter and a scatter of 3 across a final subworkflow (`call sub_workflow.sub_subworkflow`/ LEVEL_2_X__3_Y); ``` wdl; import ""sub_subworkflow.wdl"" as sub_subworkflow. task hello_world {; command {; echo 'Hello, world!'; echo 'blah' > output.txt ; }. output {; String message = read_string(stdout()); File outputFile = ""output.txt""; }. runtime {; docker: ""ubuntu:latest""; }; }. workflow inner_subworkflow {; scatter (i in range(4)) {; call hello_world; }; scatter (i in range(3)) {; call sub_subworkflow.sub_subworkflow; }; }; ```; * This final `sub_subworkflow.wdl` then runs a scatter across a task:; ``` wdl; task sub_hello_world {; command {; echo 'Hello from sub.sub_workflow, world!'; }. output {; String message = read_string(stdout()); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow sub_subworkflow {; scatter (i in range(2)) {; call sub_hello_world; }; }; ```. In tree form you have something like this:; * ROOT_WORKFLOW `main_workflow.wdl`; * LEVEL_1 `outer_subworkflow.wdl`; * LEVEL_2A `inner_subworkflow.wdl`; * LEVEL_2_A__3_A `sub_subworkflow.wdl`; * LEVEL_2_A__3_B `sub_subworkflow.wdl`; * LEVEL_2_A__3_C `sub_subworkflow.wdl`; * LEVEL_2B `inner_subworkflow.wdl`; * LEVEL_2_B__3_A `sub_subworkflow.wdl`; * LEVEL_2_B__3_B `sub_subworkflow.wdl`; * LEVEL_2_B__3_C `sub_subworkflow.wdl`. LEVEL_2 `inner_subworkflow.wdl` task outputs end up here:; ```; cromwell-executions/main_workflow/ecb081a4-0166-4f9f-a2a8-20a50f8e9b19/; call-outer_subworkflow/outer.outer_subworkflow/53f62151-1c36-4e2f-8bff-0a2a90d7d8c5/; call-inner_subworkflow/shard-0/inner.inner_subworkflow/cd65e57c-12ee-4213-a698-c98dd0a9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7387:3220,echo,echo,3220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7387,1,['echo'],['echo']
Availability,* Re-pin conformance test hash to the current HEAD of master.; * Allow workflow inputs to be recycled back as outputs (part 1/2 fixing conformance 20).; * Allow arrays to be coerced to Anys (part 2/2 fixing conformance 20).; * Fix prefix handling with empty arrays (fix conformance 125).; * Adjust expected conformance failures for all of the above.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3467:319,failure,failures,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3467,1,['failure'],['failures']
Availability,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5262:314,error,errors,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262,1,['error'],['errors']
Availability,"* Wiring in the remaining draft 3 engine functions' value evaluators; * Modified value evaluation to keep a running track of its side-effect file generation; * Also allowed value evaluation to run value mapping if used in placeholder interpolation. Note: red thumb required because there's some changes to WomIdentifier undoing some of the changes in the forkjoin PR and making them again, but correcter this time. . This PR ended up with far wider-ranging changes than I originally expected (even requiring changes in draft 2 and CWL) so if you see something that smells wrong... âš ï¸ â—ï¸ â—ï¸ âš ï¸ . ~~NB: this PR will go down by about 700 lines once the `forkjoin` PR merges~~ DONE!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3505:617,down,down,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3505,1,['down'],['down']
Availability,"* `cromwell-30.jar`; * local; * single workflow mode. ```; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true	; 	if ( b0 ) {; 		if ( b2 ) {; 			call t0 as t2 { input: i=2 }; 		}; 		if ( b1 ) {; 			call t0 as t1 { input: i=1 }			; 		}; 		#Boolean tmp = b1 && b2; 		#if ( tmp ) {	; 		if ( b1 && b2 ) {; 			call t0 as t12 { input: i=12 }				; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo test > ${i}.txt; 	}; 	output {; 		File out = glob('*.txt')[0]; 	}; }; ```; This code does not work.; ```; $ java -jar ../cromwell-30.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 09:40:22,36] [info] Running with database db.url = jdbc:hsqldb:mem:ee347d5b-2cdf-4b76-b68a-dc5d09a93aeb;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:28,42] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 09:40:28,44] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 09:40:28,54] [info] Running with database db.url = jdbc:hsqldb:mem:68a1b424-aa08-4f22-bc04-952c5eb83e7e;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:29,02] [info] Slf4jLogger started; [2017-12-05 09:40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:411,echo,echo,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['echo'],['echo']
Availability,"* disabled build failures due to deprecation warnings... it's a hotfix, deadend branch!; * support public http-based imports in cromwell; * fixed a few necessary classes due to newer cats being pulled in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2734:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2734,1,['failure'],['failures']
Availability,"* lost a few hours debugging this test, made it more robust",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2749:53,robust,robust,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2749,1,['robust'],['robust']
Availability,"** /dev""; ls -l /dev; ; if [ -d /dev/disk ]; then; echo; echo ""**** /dev/disk""; ls /dev/disk; fi; ; echo; echo ""**** /proc/mounts""; cat /proc/mounts; ; echo; echo ""**** /etc/mtab""; cat /etc/mtab; ; echo; echo ""**** /sys/block""; find -L /sys/block -maxdepth 2; ; echo; echo ""**** /sys/block/sdb/size (converted to integer GB)""; echo ""$(($(cat /sys/block/sdb/size) * 512 / 2**30))""; ; echo; echo ""**** /sys/devices""; find -L /sys/devices -maxdepth 3; >>>; ; runtime {; docker: ""talkowski/delly""; memory: ""1.7 GB""; cpu: ""1""; disks: ""local-disk 250 HDD""; preemptible: 3; }; }; ```; Snips of relevant output from cromwell 36 (edited for brevity):; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/disk/by-id/google-local-disk; 245.1G 60.0M 245.0G 0% /cromwell_root; **** /dev; total 0; lrwxrwxrwx 1 root root 11 Nov 14 21:16 core -> /proc/kcore; lrwxrwxrwx 1 root root 13 Nov 14 21:16 fd -> /proc/self/fd; crw-rw-rw- 1 root root 1, 7 Nov 14 21:16 full; drwxrwxrwt 2 root root 40 Nov 14 21:16 mqueue; crw-rw-rw- 1 root root 1, 3 Nov 14 21:16 null; lrwxrwxrwx 1 root root 8 Nov 14 21:16 ptmx -> pts/ptmx; drwxr-xr-x 2 root root 0 Nov 14 21:16 pts; crw-rw-rw- 1 root root 1, 8 Nov 14 21:16 random; drwxrwxrwt 2 root root 40 Nov 14 21:16 shm; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stderr -> /proc/self/fd/2; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdin -> /proc/self/fd/0; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdout -> /proc/self/fd/1; crw-rw-rw- 1 root root 5, 0 Nov 14 21:16 tty; crw-rw-rw- 1 root root 1, 9 Nov 14 21:16 urandom; crw-rw-rw- 1 root root 1, 5 Nov 14 21:16 zero. **** /proc/mounts; /dev/disk/by-id/google-local-disk /cromwell_root ext4 rw,relatime,data=ordered 0 0. **** /etc/mtab; /dev/disk/by-id/google-local-disk /cromwell_root ext4 rw,relatime,data=ordered 0 0. **** /sys/block/sdb/size (converted to integer GB); 250; ```. Whereas on cromwell 30; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/sdb 246.0G 59.1M 233.4G 0% /cromwell_root; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388:3145,Avail,Available,3145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388,1,['Avail'],['Available']
Availability,"**Backend**: PAPIv2; **Cromwell version**: 38-6725312. When a task output is referenced by a variable, `read_*` functions fail to delocalize a file if the file name is referenced by a variable, instead of as a literal string. This might be related to https://github.com/broadinstitute/cromwell/issues/3698. Example:; ```wdl; version 1.0. workflow TestFailureDelocalize {; call Test. output {; String test = Test.out; }; }. task Test {; String testFile = ""test.txt"". command {; echo OK > ~{testFile}; }. output {; String out = read_string(testFile); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. If I change the syntax to; ```wdl; String out = read_string(""~{testFile}""); ```; then the file is delocalized successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4901:477,echo,echo,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901,1,['echo'],['echo']
Availability,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:624,Error,Error,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,1,['Error'],['Error']
Availability,"**Command**:; `sudo java -jar cromwell-84.jar run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. **Platform**:; Ubuntu 20.04 via WSL2 on Windows 10 pro. **Java**:; openjdk 11.0.17 2022-10-18; OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu220.04); OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu220.04, mixed mode, sharing). **Docker**:; Docker version 20.10.22, build 3a2c30b; Docker desktop v.4.16.3. **Inputs & Options JSON:**; Please find attached in the zip.; [forgithub.zip](https://github.com/broadinstitute/cromwell/files/10610687/forgithub.zip). **Workflow & Error**:; I am running the main workflow (WDL attached) named ""local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl"". It first successfully finishes the first task from ""BamProcessing.wdl"" (WDL attached) specifically running the task ""GenerateSubsettedContaminationResources"" via docker: ""us.gcr.io/broad-gotc-prod/bedtools:2.27.1"". In the next step it starts the next task from ""paired-fastq-to-unmapped-bam.wdl"" (WDL attached) via docker: ""broadinstitute/gatk:latest"". Inspection of the container's log during this step shown below:. ```; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/out.1': Operation not supported; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/err.1': Operation not supported; 2023-02-05 12:55:4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7002:793,Error,Error,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002,1,['Error'],['Error']
Availability,"**Edit:** I've encountered more issues here, so putting this on pause. ---. **Short version:** rather than an InstrumentationPath being a `NonEmptyList[String]`, it is now [a more complex type](https://github.com/broadinstitute/cromwell/blob/DDO-1728-handle-metric-variance/services/src/main/scala/cromwell/services/instrumentation/CromwellInstrumentation.scala#L48). Everything else is refactoring to match that change. **Long version:**. There's basically two competing models for metric names--""lots of metrics, no labels"" and ""smaller a number of metrics, with labels for variation"". Statsd uses the first, and Prometheus (and Stackdriver too, ideally) use the second. By way of example:. ```; api.response.count.200; ```. versus. ```; api_response_count{code=""200""}; ```. This boils down to how the different systems query metrics. In a StatsD world, you can have a query like `api.response.count.*`, but that's not possible for Prometheus/Stackdriver--the actual metric name needs to be fully static, and you conceptually do `api_response_count{code=""*""}`. Cromwell metric names right now are this:. ```scala; type InstrumentationPath = NonEmptyList[String]. CromwellBucket(prefix: List[String], path: InstrumentationPath); ```. Metric names are guaranteed to not be empty by the path. The path is what is actually assembled in code and passed around, and prefix (or an empty list) is added at the outer edge--most of Cromwell treats the `NonEmptyList[String]` as the metric name. When the metrics are actually sent to StatsD, the strings are joined with periods and sent off. I spent several days trying to figure out a way to reliably parse labels out from these names, and I couldn't figure it out. There's two key pieces of info known at the time the `NonEmptyList[String]` is assembled that are not recorded:; - When a particular string being added to the list is expected to vary frequently (and thus needs to be a label); - What that string means (labels are key-value pairs and we're usu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6681:788,down,down,788,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681,1,['down'],['down']
Availability,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762:243,failure,failure,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762,4,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,"**Issue:**; Frequently during Travis builds a single test may fail. Often after a restart the test will pass, but then perhaps a different test will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:322,error,error,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,"**NB** - we're happy to have more than one person working on this as long as it's temporally close. If you find this interesting and there's already someone assigned speak up. Also we're happy to have people doing this in spare cycles unofficially (as long as there's at least one official person), so if that's you also speak up. Timeboxed to 1 week. Take a deep dive into CWL in whatever form you think will be the most effective for you. The output of this should be some form of show & tell to the group, whatever you think will be most effective for how you did this. To give a little bit of guidance: Imagine a spectrum between becoming a generalized CWL expert on one side or having a perfect proof of concept of how we could implement one specific thing. The desired outcome would be to trend towards the former. Our goal here is to have a goto person (or persons) available when we start putting shovels to the ground.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2238:873,avail,available,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2238,1,['avail'],['available']
Availability,"**PR**: https://github.com/broadinstitute/cromwell/pull/7224. **Idea:** Add an auth mode for GCP that allows Cromwell to impersonate a service account and use [short-lived credentials](https://cloud.google.com/iam/docs/create-short-lived-credentials-direct) for calls to GCP. . **Context:** Service account impersonation is now the [defacto recommendation](https://cloud.google.com/docs/authentication#auth-decision-tree) when choosing an authentication mechanism that relies on service accounts. Some teams (such as Verily) might prefer this approach over the current option to rely on downloaded service account keys. **How it works**; Users would specify an auth block in the .conf file with one of the following formats. The first specifies . - Application default credentials used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; }; ```. - A specified JSON file used for **source service account**; ```; {; name = ""user-service-account""; scheme = ""user_service_account_impersonation""; json-file= ""path/to/file.json""; }; ```. Users would then add the following option when making workflow requests:; ```; {; ""user_service_account_email"": ""someemail@domain.com; }; ```. Cromwell would use the **source service account** to impersonate the **target service account** from the workflowOptions. It would then mint and refresh access tokens from this target service account for all GCP requests. . In order for this to work, the source service account would need the IAM role roles/iam.serviceAccountTokenCreator. **If the team would prefer a smaller PR**; We could change this to only ever use applicationDefaultCredentials, in which case we could remove parts of the code that are altering ServiceAccountMode",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7223:587,down,downloaded,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7223,1,['down'],['downloaded']
Availability,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But thereâ€™s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Donâ€™t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Donâ€™t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesnâ€™t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, thereâ€™s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:1244,avail,available,1244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531,1,['avail'],['available']
Availability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = â€¦ ++ assemblySettings ++ â€¦`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:1231,down,downstream,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,1,['down'],['downstream']
Availability,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:42,failure,failures,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427,10,"['error', 'failure']","['error', 'failures']"
Availability,"**WORKAROUND:** Explicitly `mkdir` (as necessary) and `export` a new `$TMPDIR` at the top of your task command, for example `export TMPDIR=/tmp` should work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3647:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647,1,['error'],['error']
Availability,"**Was the imports zip the same in each workflow?**; Yes, we were running the same workflow so the imports zip is the same. We get those import files by downloading them from github and adding them to a cache so that we don't have to download them for each workflow. **Were the workflows all the same?**; Yes, all of the workflows were the same . **Do you have any logs from the sender to check that a zip was indeed sent?**; No, we just have logs that a workflow was submitted to Cromwell ðŸ˜ž. **Were they submitted as a series of 999 separate submits or as a single batch submit POST?** ; They were submitted as a series of 999 individual requests in the ""On Hold"" status, and a separate process sent requests to Cromwell to started one of those on-hold workflows every 10 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649:152,down,downloading,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649,2,['down'],"['download', 'downloading']"
Availability,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:124,recover,recovered,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['recover'],['recovered']
Availability,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:161,recover,recovered,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,2,['recover'],['recovered']
Availability,"*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".unmapped.bam$\"", \""\"") + \"".validation_report\"",\n disk_size = validate_sam_file_disk_size\n }\n\n output {\n RevertSam.*\n ValidateSamFile.*\n }\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""engle-macarthur-ccdd\"",\n \""auth_bucket\"": \""gs://cromwell-auth-engle-macarthur-ccdd\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs\"",\n \""account_name\"": \""kcibul@broadinstitute.org\"",\n \""jes_gcs_root\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""a714b11b-0162-4585-afa5-abbd7433af51"",; ""inputs"": {; ""BamToUnmappedBams.input_bam"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam""; },; ""submission"": ""2017-01-19T18:17:12.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Google credentials are invalid: connect timed out""; }],; ""workflowLog"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs/workflow.a714b11b-0162-4585-afa5-abbd7433af51.log"",; ""end"": ""2017-01-19T18:17:39.673Z"",; ""start"": ""2017-01-19T18:17:19.606Z""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:4250,failure,failures,4250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['failure'],['failures']
Availability,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:201,error,error,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,4,['error'],['error']
Availability,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:165,error,error,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487,2,['error'],['error']
Availability,"+1 on `tmpfs`. Currently, we have to create a directory under `/dev/` and rely on the assumption that that directory gets mounted by default as a `tmpfs` with 1/2 of the available RAM (at least on GCP). This is obviously not ideal. Delocalization of such files is problematic as well. Our use case is exactly the same, to unpack/process tens or hundreds of thousands of small files (in a BCL). Doing so with any ""normal"" disk is much slower than with RAM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155:170,avail,available,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155,1,['avail'],['available']
Availability,+1 on this feature (or one like it) -- it's really helpful for writing robust and cheap workflows,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064:71,robust,robust,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064,1,['robust'],['robust']
Availability,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:490,failure,failure,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563,2,['failure'],"['failure', 'failures']"
Availability,"+1 to this. I'm getting the same warning, which looks like a failure but actually seems to work fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434580781:61,failure,failure,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434580781,1,['failure'],['failure']
Availability,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:359,error,error,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414,4,['error'],['error']
Availability,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016,2,['error'],['error']
Availability,", ggp-1817239588482439788, us-east1-c/n1-standard-2; > broad-wgs-prod5, 2018-01-16T23:46:08Z, 2018-01-17T14:57:19Z, ggp-3754421722448645101, us-east1-d/n1-standard-2. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #11 Jan 17, 2018 12:32PM ; > Mike, ; > ; > For comparison - the previous reported workflow also had a Message 14: type pre-emption. Which is what cromwell normally detects as pre-emption. Not sure what is difference between the above pre-emptions and the one below. Info as follows:; > ; > OPSID; > operations/ENOi-PyPLBioyJKO-s3GhY0BIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > ; > broad-wgs-prod5, 2018-01-16T15:37:17Z, 2018-01-16T16:43:22Z, ggp-10163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've bee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:9693,down,down,9693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['down'],['down']
Availability,", not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:1069,echo,echo,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,1,['echo'],['echo']
Availability,",89] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-01 20:01:02,94] [info] Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 submitted.; [2017-12-01 20:01:02,94] [info] SingleWorkflowRunnerActor: Workflow submitted 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Successfully started WorkflowActor-132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-01 20:01:03,27] [info] MaterializeWorkflowDescriptorActor [132d7527]: Call-to-Backend assignments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-01 20:01:04,82] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: echo test1 > test1.txt; echo test2 > test2.txt; [2017-12-01 20:01:04,86] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: executing: /bin/bash /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/script; [2017-12-01 20:01:04,91] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:2510,echo,echo,2510,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['echo'],['echo']
Availability,",; ""Mutect2_Multi.unfiltered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-unfilteredOutputList/execution/unfiltered.list"",; ""Mutect2_Multi.ob_filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-16-0-unknown-operation#-371454906]] terminated abruptly; [2017-03-20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3587,error,error,3587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,",; ""doc"": ""Minimum spread within candidate purities before somatics can be used. Default 0.15\n"",; ""id"": ""#somatic_min_purity_spread_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\n"",; ""id"": ""#somatic_min_total_purple""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""id"": ""#somatic_penalty_weight_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#somatic_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#structural_vcf_purple""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#sv_recovery_vcf_purple""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads used for amber step\n"",; ""id"": ""#threads_amber""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to run cobalt command\n"",; ""id"": ""#threads_cobalt""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads to use - set to 8 by default"",; ""id"": ""#threads_gridss""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""id"": ""#threads_purple""; },; {; ""type"": ""File"",; ""doc"": ""tumour BAM file\n"",; ""secondaryFiles"": [; "".bai""; ],; ""id"": ""#tumor_bam""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""sample name of tumor. Must match the somatic snvvcf sample name. (Default: \\${sample}_T)\n"",; ""id"": ""#tumor_sample""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""htsjdk SAM/BAM validation level (STRICT (default), LENIENT, o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:46840,recover,recovered,46840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:117,down,down,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['down'],['down']
Availability,"- 0.23; - SGE backend; - single workflow; - no docker. The wdl in question (a simpler WDL can be made easily). Look at how it scatters over a variable that does not exist. I would expect cromwell to give an error message and exit.; ```; # This is **broken.wdl**; # This simple, *unsupported* WDL takes in a VCF from M2 and a tumor bam file.; # It produces a new VCF with the filtering results. workflow test_ob_filter {; # tsv; # entity_id vcf tumor_bam_file; File input_table; Array[Array[String]] m2_vcfs = read_tsv(input_table); File db_snp; String gatk_jar; File ref_fasta. scatter (row in THIS_VAR_DOES_NOT_EXIST) {; call CollectSequencingArtifactMetrics {; input:; entity_id=row[0],; bam_file=row[2],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; output_location_prepend=row[0]; }; call FilterByOrientationBias {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; m2_vcf=row[1],; pre_adapter_detail_metrics=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics; }; }. call MakeSummaryFileList {; input:; files=FilterByOrientationBias.orientation_bias_vcf_summary,; output_file=""summary_table.txt""; }; }. task CollectSequencingArtifactMetrics {; String entity_id; File bam_file; String output_location_prepend; String gatk_jar; # /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; File ref_fasta. command {; java -jar ${gatk_jar} CollectSequencingArtifactMetrics -I ${bam_file} -O ${output_location_prepend} -R ${ref_fasta} --VALIDATION_STRINGENCY SILENT; }. output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File bait_bias_summary_metrics = ""${output_location_prepend}.bait_bias_summary_metrics""; }; }. task FilterByOrientationBias {; String entity_id; String gatk_jar; File m2_vcf; File pre_adapter_detail_metrics. command {; java -jar ${ga",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774,1,['error'],['error']
Availability,"- 0.23; - Single Workflow; - SGE backend. Task fails, but cromwell, never realizes and simply hangs. SGE has already exited cleanly. In this case, I believe cromwell would simply exit with non-zero return code. stderr:; ```; /var/spool/sge/node1403/job_scripts/9226232: line 16: warning: here-document at line 4 delimited by end-of-file (wanted `CODE'); /var/spool/sge/node1403/job_scripts/9226232: line 17: syntax error: unexpected end of file. ```. Task is broken due to unforeseen tab issues (unlikely to be reproduced correctly here):. ```wdl; command {; python <<CODE; 	 	import shutil; 		import os; 		str_file_list = """"""${sep=""\n"" files}""""""; 		fp = fopen(${output_file}, 'w'); 		fp.write(str_file_list); 		fp.close(); CODE; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1773:415,error,error,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1773,1,['error'],['error']
Availability,"- 0.24; - SGE backend. I accidentally gave an Int parameter a String value in the json. I would prefer an error specific to parameter type, rather than a generic invalid runtime attribute error message (below). Proposed solution: ; ``Task m1_task was given an invalid type for cpu = ""${cpu}"". A String was given, though parameter is an Int``. Current error message:; ```; [ERROR] [02/08/2017 10:38:57.225] [cromwell-system-akka.dispatchers.engine-dispatcher-8] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 07a3f007-8c62-4cd4-8668-6ac034ff42f1 failed (during InitializingWorkflowState): Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; java.lang.IllegalArgumentException: Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:106,error,error,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"- 0.24; - SGE backend. When used for a list of files (as Strings), I received a lot of file not found errors. Seems like CRLF (0D0A) is interpreted as two lines. Proposed solution: have ``read_lines`` do a strip/trim on each line before returning the Array of String/File. Offending WDL:. `` Array[String] oncotated_m1_files = read_lines(input_oncotated_m1)``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876:102,error,errors,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876,1,['error'],['errors']
Availability,"- 0.24; - SGE backend; - single workflow mode; - sub-workflows involved; - no docker. I can provide exact WDL, but replicating this may need new WDL. Feel free to ask me questions. I have a full WDL that calls two subworkflows, serially. . - The first takes a really long time and completed successfully on the first run. ; - The second subworkflow failed on the first run. This was due to an error in the call block of the WDL (marked below); - Fixed the error ; - Ran it again for a second run. Unexpected: In the second run, the first subworkflow call did not call cache and I do not know why not. Nothing was changed in the call block (nor parameters, etc etc). ```; import ""dl_ob_training.wdl"" as dl_ob_training; import ""m1/m1.wdl"" as m1. workflow full_dl_ob_training_with_m1 {. .....snip.....; Array[Pair[File, File]] tumor_bam_pair = zip(tumor_bam_files, tumor_bam_indices); scatter (p in tumor_normal_pairs) {. # All of the m1.m1 calls completed just fine; call m1.m1 {; input: ; tumorBam=p.left.left,; tumorBamIdx=p.left.right,; ....snip....; ; }. call dl_ob_training.dl_ob_training {; input:; # I fixed an error in the two lines below. Below is the corrected output; bam_file=p.left.left,; bam_file_index=p.left.right,; .....snip..........; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970:393,error,error,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970,3,['error'],['error']
Availability,"- 0.24; - single workflow mode; - JES backend. When I run the workflow, I get a localization permission error, but when I try again from the command line, there is no issue.; From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:104,error,error,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,2,['error'],['error']
Availability,"- 0.25; - Local+docker backend (though I doubt it matters); - single workflow (though I doubt it matters). Workflow output is optional and sometimes both ``oncotate_m2_ob.oncotated_m2_vcf`` and ``oncotate_m2_no_ob.oncotated_m2_vcf`` are not populated. Proposed solution: select_first returns null if no inputs are populated. Offending workflow output:; ```; File? oncotated_m2_vcf = select_first([oncotate_m2_ob.oncotated_m2_vcf, oncotate_m2_no_ob.oncotated_m2_vcf]); ```. Error message:; ```; [ERROR] [02/17/2017 14:18:45.923] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow e241e2bc-95cd-4a8c-a814-20bb8852c6b5 failed (during ExecutingWorkflowState): select_first failed. All provided values were empty.; java.lang.IllegalArgumentException: select_first failed. All provided values were empty.; at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at scala.Option.getOrElse(Option.scala:121); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:175); at scala.util.Success.flatMap(Try.scala:231); at wdl4s.expression.WdlStandardLibraryFunctions$class.select_first(WdlStandardLibraryFunctions.scala:175). ....snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2005:473,Error,Error,473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2005,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4744:218,failure,failures,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744,2,"['error', 'failure']","['error', 'failures']"
Availability,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['recover'],['recovery']
Availability,- Added the `referenceDiskManifestBuilderApp` test to GithubActions; - Implemented workaround for an issue related to Github Action Runners and their shell environments. See [WX-938](https://broadworkbench.atlassian.net/browse/WX-938?atlOrigin=eyJpIjoiZTU2YWJhMGVjZTM3NDg2NmFiZThkNmI2NGQwYzUwMTEiLCJwIjoiaiJ9) for a full run down. . [WX-938]: https://broadworkbench.atlassian.net/browse/WX-938?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7017:325,down,down,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7017,1,['down'],['down']
Availability,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:199,failure,failures,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['failure'],['failures']
Availability,- Adds a `JesError` class that maps some known JES errors to custom Exceptions to provide better error messages. Simplistic for now but avoid unnecessary stacktrace and give more explicit error messages.; - Tries to read the return code regardless of the final status of the JES job (even if it failed). If it can read it then the return code will be available in metadata.; - Sets the exec.sh content-type to `text/plain` in gcs so it opens in the browser instead of downloading a files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:51,error,errors,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,5,"['avail', 'down', 'error']","['available', 'downloading', 'error', 'errors']"
Availability,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:487,redundant,redundant,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['redundant'],['redundant']
Availability,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4021:126,alive,alive,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021,1,['alive'],['alive']
Availability,"- Backend: AWS; - Cromwell: 36. When running certain highly parallel WDL workflows, I'm getting the error `cromwell.core.CromwellFatalException: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:100,error,error,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,- Closes #4158 ; - [x] @ruchim could you confirm that [this error message](https://github.com/broadinstitute/cromwell/pull/4174/files#diff-aade89887d9abbfe15dc3bf8b809aec5R31) meets your AC?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4174:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4174,1,['error'],['error']
Availability,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:86,redundant,redundant,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,1,['redundant'],['redundant']
Availability,"- Disabled redundant `lots_of_inputs.test` test, which uses `lots_of_inputs.wdl` like `lots_of_inputs_papiv2.test` and makes analysis confusing; - Removed unused configs, mostly from PAPIv2 Alpha; - Removed unused suites, mostly from PAPIv2 Alpha; - Removed Travis, Jenkins, and CircleCI references from `test.inc.sh`. This includes `case` statements, as well as all functions that were called exclusively in the removed `case` statements.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7336:11,redundant,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7336,1,['redundant'],['redundant']
Availability,"- Fix broken calls to any workflow that is already in the workflow call tree (including itself).; - This would allow the equivalent of `do-while` loops via recursion and conditionals. E.g. we could iteratively improve a result until it is ""good enough""; - Here's an example that would allow us to work out square roots to a predefined accuracy:; ```; task refine {; 	Int number; 	Float currentApproximation. 	command {}. 	output {; 		Float nextApproximation = 0.5 * (currentApproximation + (1.0 / currentApproximation * number)); 	}; }. workflow sqrt {; 	Int number; 	Float currentApproximation. 	Float errorAllowed = 0.05 . 	if (currentApproximation * currentApproximation - number > errorAllowed) {; 		call refine { input: number = number, currentApproximation = currentApproximation }; 		call sqrt as recurse { input: number = number, currentApproximation = refine.nextApproximation }; 	}. 	output {; 		Float result = select_first( [ recurse.result, currentApproximation ] ); 	}; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1842:603,error,errorAllowed,603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1842,2,['error'],['errorAllowed']
Availability,- Fixes #4081 ; - Fixes the draft-2 error and makes slight readability improvements to the `1.0` and later messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4175:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4175,1,['error'],['error']
Availability,"- If the job itself fails to start, this can be reported back immediately by Cromwell (although please do raise this as a separate issue with a reproducing WDL if that's what you mean).; - If Cromwell can start the job but the bash script will fail immediately, then that's out of Cromwell's hands.; - The overhead of running jobs on JES means that even `echo hello` has a minimum overhead of around 5-10 minutes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761:355,echo,echo,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761,1,['echo'],['echo']
Availability,"- JES Backend; - v29. See #2844 . This error causes cromwell to enter a funny state when the error described in #2844 occurs in a subworkflow. Cromwell states that the workflow (and subworkflow) are running, though the server log shows the exception. Not only that, the backend status is `Success` for the task that generated the invalid filename, though the task status remains `Running`, just like the workflows. Without looking at the server logs, a user cannot determine what is going on, easily.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845,2,['error'],['error']
Availability,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754:474,error,error,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754,1,['error'],['error']
Availability,"- JES backend; - 0.24; - single workflow mode. When JES returns a 403 AccessDeniedException, should cromwell keep retrying? It delays the result getting back to the user and should have no way of recovering with retries. Proposed solution: When AccessDeniedException is seen from JES, simply end there, instead of initiating any retries...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961:196,recover,recovering,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961,1,['recover'],['recovering']
Availability,"- JES backend; - cromwell server; - localhost mysql; - cromwell-27-c89c83f-SNAP.jar; - I set the database queue size to 3000.; - I have *not* changed the metadata batch size. *Should I attempt to restart this workflow?* This took over 4 hours to get this error message and I do not want to incur the cost if it will fail the same way again. Side issues:; - My workflow failed and yet cromwell is still *mauling* the mysql server.; - The call cache lookups are taking >1 hour per task. Main issue:. I do not understand the error messages, but my workflow has entered a Failed state and I am not sure why. First, I see a bunch of NPE:; ```; [ERROR] [05/01/2017 17:36:00.055] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:255,error,error,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"- JES/PAPI backend; - Cromwell v29. I have a task that dropped its outputs into ""./"", since output directory was a parameter and that is how it was set. So when cromwell/PAPI saw an output file `...././my_output.txt`, they created a bucket that actually contains the ""."" ... . The task was allowed to complete successfully and then downstream tasks failed. Proposed solutions:; - check the output location either when the first/upstream task completes (or, if possible, before it starts).; - Simple prune the ./ from the output path. ```; java.lang.IllegalArgumentException: I/O not allowed on dot-dirs or extra slashes when !permitEmptyPathComponents: /lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/25df0c1d-5a13; -4bc1-8712-0cf90a78dfda/call-cnvPair/CNVSomaticPairWorkflow/6fe3dc67-c5f0-4c61-8881-474ceac4c8d7/call-ModelSegmentsTumor/./G25783.TCGA-55-6986-01A-11D-1945-08.2.modelFinal.seg; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844:332,down,downstream,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844,1,['down'],['downstream']
Availability,- Prefix default locations in CWL with `gs://...` root for PAPI conformance tests; - Retrieve size of files early to avoid unnecessary I/O (there's still too much redundant I/O but it's a step); - Uses `WomObject` to map back JS objects instead of `WomMap` that needs homogoneous value type (or it ends up being `WomAnyType`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3492:163,redundant,redundant,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3492,1,['redundant'],['redundant']
Availability,"- Refactored the DrsLocalizer to better handle multiple large downloads.; - Now, the DrsLocalizer will resolve all URLs up front, and then invoke the `getm` tool with a manifest containing all files to download. This improves download performance.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7214:62,down,downloads,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7214,3,['down'],"['download', 'downloads']"
Availability,- Remove the throttling of jobs during PapiV1 cron.; - Assemble jars if they haven't been already.; - Ensure an error is printed before exiting.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3681:112,error,error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3681,1,['error'],['error']
Availability,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:364,avail,available,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262,1,['avail'],['available']
Availability,"- Removes the ability to abort Finalization Actor; - Ensures that the finalization actor runs if the Workflow reaches the `Initialize` state, regardless of what happens next (failure, success, abort), or when it happens (initialization, execution).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/916:175,failure,failure,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/916,1,['failure'],['failure']
Availability,"- Removes the awkward plateauing of running jobs at 2k, 4k, 6k, etc when running several thousand jobs concurrently.; - Does not introduce very long delays into execution store processing like the previous attempt to ""fix"" the execution store.; - Allows us to get a more accurate count of total jobs queued in the system because they will express themselves as EJEAs waiting for tokens rather than pre-queue-queued items of which we have no visibility.; - Adds a dummy backend to test all of the above. Review Notes:; * Start with the `Remove redundant WaitingForQueueSpace status` commit. That's the one which fixes the bug. Everything else is just dummy backend and test infrastructure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047:543,redundant,redundant,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047,1,['redundant'],['redundant']
Availability,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4132:272,down,down,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132,1,['down'],['down']
Availability,- Renames the existing `Parse` to `IOChecked` <- I changed the name because it is now used in places that have nothing to do with parsing but this is still not a great name so suggestions welcome; - Uses `IOChecked` in place of `ErrorOr` or `Checked` to keep the operation async as long as possible. It's not all the way async yet but at least pushes the sync call further up; - Provides a value for the `cats.Parallel` typeclass over `CheckedIO` using `CheckedIOPar`. This gives the ability to use `parTraverse` and `parSequence` on collections and have the IO execute in parallel over the elements of the collection.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4423:229,Error,ErrorOr,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4423,1,['Error'],['ErrorOr']
Availability,"- Retries the creation of the `Pipeline` and `Run`; - If all retries fail, a proper failure is propagated and the call will fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/227:84,failure,failure,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/227,1,['failure'],['failure']
Availability,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4123:420,down,down,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123,1,['down'],['down']
Availability,"- SGE backend (though I bet backend does not matter); - server mode; - cromwell 29. WDL takes in a list of filenames and scatters over a read_lines call. Each line is a file.; If the list file has DOS line endings, read_lines preserves the `\r` character in the file name. After running dos2unix, the issue disappeared. Here is the error message and you can even see the appended `\r`... ```; Could not localize /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r:\n\t/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r doesn't exists\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r""; ```. Hash error:; ```; ""Cannot hash file /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r because it can't be found"". ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:332,error,error,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,2,['error'],['error']
Availability,"- SGE backend; - 0.24; - single workflow; - call caching is on. Failure of intermediate task (CollectSequencingArtifactMetrics) on a single shard caused the entire workflow to stop immediately and cromwell to exit. Hence, successful tasks could not cache results, though those did complete. . The next tasks in the series (ExtractReadInfo) do not appear to ever be run, even for samples that did not fail CollectSequencingArtifactMetrics. ```; [ERROR] [01/23/2017 14:00:09.277] [cromwell-system-akka.dispatchers.engine-dispatcher-74] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 92c98fd6-003e-4f1b-b61b-9ab610f4961d failed (during ExecutingWorkflowState): Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3; java.lang.RuntimeException: Call dl_ob_training.dl_ob_training.CollectSequencingArtifactMetrics:NA:1: return code was 3. ```. Command:; ```; java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell.jar \; run full_dl_ob_training.wdl \; full_dl_ob_training.json \; sge_runtimes \; full_dl_ob_training.metadata; ```. full_dl_ob_training.wdl:; ```; import ""dl_ob_training.wdl"" as dl_ob_training. workflow full_dl_ob_training {. ....snip.... scatter (p in variant_files_pair) {; call dl_ob_training.dl_ob_training {; input:; ....snip....; }; }; }. ```. dl_ob_training.wdl:; ```; workflow dl_ob_training {. ....snip.... call CollectSequencingArtifactMetrics {; input:; .....snip.....; }. call CreateObIntervalList {; input:; .....snip.....; }. call ExtractReadInfo {; input:; ....snip.....; }. output {; ExtractReadInfo.read_infos; }; }. task CollectSequencingArtifactMetrics {; ....snip....; output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895:64,Failure,Failure,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,"- SGE backend; - cromwell v29. The following WDL works:; ```; # Runtime parameters; Int? mem; String gatk_docker; Int? preemptible_attempts; Int? disk_space_gb. Int final_mem=select_first([mem, 3]); ... snip....; runtime {; memory: select_first([mem, 3]) + "" GB""; ....snip....; ```. BUT the below WDL gives me an error that the + operator is not supported for optional variables, please use select_first. However, the variable final_mem is not optional:. ```; ....; # Everything is the same as the working WDL, except:; runtime {; memory: final_mem + "" GB""; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643:313,error,error,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643,1,['error'],['error']
Availability,- Some IoCommands cannot be created so return a `Failure` when that happens; - FYI: in some cases throwing-and-re-catching the first `Failure`; - No longer passing `overwrite = true` since it was always true; - Resealed `GcsBatchIoCommand` by moving the test instance into main,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6019:49,Failure,Failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6019,2,['Failure'],['Failure']
Availability,- Starter-for-10 PR.; - Lets us start up Cromwell with a simple; ```; # cromwell.server start; ```; - Lets us stop Cromwell with a simple; ```; # cromwell.server stop; ```; - Lets us see whether Cromwell is already running with a simple; ```; # cromwell.server status; Cromwell is stopped. # echo $?; 1; ```; - My ultimate dream is to make this part of the [cromwell quickstart process](https://github.com/broadinstitute/cromwell/issues/2624) to get people up and ready for server mode in just a few simple commands.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2635:292,echo,echo,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2635,1,['echo'],['echo']
Availability,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3892:58,error,errors,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,- To the changelog:; - Added entry for already released 62; - Added placeholder for next release 63; - For homebrew:; - Remove extra slash added to generated URLs; - Changed default publishing instructions to include homebrew; - Added validation of brew style according to guidelines; - Fixed casing of 'cromwell' in PR name; - For the publishing GitHub token scopes:; - updated instructions; - updated validation; - gracefully error with helpful messages; - added an example image; - Removed attempt to publish from dbms tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6338:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6338,1,['error'],['error']
Availability,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4133:264,error,errors,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133,1,['error'],['errors']
Availability,- Wait longer for containers to download and startup; - Use the latest PostgreSQL jdbc driver; - Updated liquibase and schema comparison tests; - Empty LOBs are always stored as null; - Fixed test description for empty lobs; - Test the various databases with centaur local; - Don't start a db when CI checking publishing; - Help liquibase by using valid location of S3FSP; - Longer leeway for logging events for longer liquibase,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6345:32,down,download,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6345,1,['down'],['download']
Availability,"- We have a lot of actors.; - The actor hierarchy can be quite deep in places; - Each actor is given a name, which must make the overall path unique; - Unwieldy actor paths are making debugging awkward. The error lines are often hundreds of characters long!; - We should decide how to name actors, in a way that ensures they have workflow, call and attempt information in the name... but **at most** once.; - We should also retroactively update the actor naming so that all existing actors have appropriate paths.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1367:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1367,1,['error'],['error']
Availability,- Workaround Travis Docker issue; - Added heartbeats to docker tests; - DRYed conformance tests; - Fixed centaur tests where an empty CBCTAP early terminated getopts; - Introduced non-Travis-specific variable for GitHub PR branch names; - Switched from cd to pushd/popd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4930:42,heartbeat,heartbeats,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4930,1,['heartbeat'],['heartbeats']
Availability,"- [ ] Usage message; - Update the usage message to better describe how to use the commands. - [ ] Parameters; - Switch from positional parameters to parameter arguments so that users explicitly include inputs and other parameters (workflow options, metadata, imports, lables, etc).; - Make all parameters optional, so if a user doesn't include the parameter argument, then no error.; - Get rid of functionality that Cromwell looks for files with specific extensions, like `.imputs`, `.options`, etc. To Be Continued.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1939:376,error,error,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1939,1,['error'],['error']
Availability,"- [ ] `missing_optional_output`:; The error message is now `""No input x.out_except_undeclared found evaluating inputs for expression x.out_except_undeclared""` which is significantly less friendly than the previous:; ```; out_except_undeclared is not declared as an output of the task x.; Make sure to declare it as an output to be able to use it in the workflow.; ```. - [x] `missing_input_failure`:; We used to get information saying which call, and which input, were given an invalid file. Now we just get `""Workflow Failed""` caused by: `""nonexistingbucket/path/doesnt/exist""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871,1,['error'],['error']
Availability,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:777,avail,available,777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,1,['avail'],['available']
Availability,- [x] Some of the error messages will change/improve following #3628 (or vice versa); - Red thumb required for the womtool changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3641:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3641,1,['error'],['error']
Availability,"- branch 0.19_hotfix a6f7c00b71dd22485d5e95c9a30f3dedd2ddeaba; - running with default application.conf. If I abort a running job via POST to the API endpoint `worflows/v1/<uuid>/abort`, this appears in the server logs:. > 2016-05-23 10:21:55,192 cromwell-system-akka.actor.default-dispatcher-6 INFO - CallActor [UUID(87ebf02f):Godot]: Abort function called.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: Beginning transition from Running to Aborting.; > 2016-05-23 10:21:55,201 cromwell-system-akka.actor.default-dispatcher-5 INFO - WorkflowActor [UUID(87ebf02f)]: transitioning from Running to Aborting.; > 2016-05-23 10:22:00,175 cromwell-system-akka.actor.default-dispatcher-8 INFO - LocalBackend [UUID(87ebf02f):Godot]: Return code: 0; > 2016-05-23 10:22:00,313 cromwell-system-akka.actor.default-dispatcher-2 ERROR - WorkflowActor [UUID(87ebf02f)]: Completion work failed for call Godot.; > java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementProxy.executeUpdate(PreparedStatementProxy.java:61) ~[cromwell-0.19.jar:0.19]; > at com.zaxxer.hikari.proxy.PreparedStatementJavassistProxy.executeUpdate(PreparedStatementJavassistProxy.java) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$anonfun$run$8$$anonfun$apply$1.apply(JdbcActionComponent.scala:520) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:882,ERROR,ERROR,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['ERROR'],['ERROR']
Availability,"- cromwell 0.25; - local backend + docker; - single workflow mode; - call caching disabled. Seems like the workflow completed just fine, but I still get an error. *This is transient* I have run the exact same workflow in exact same configuration multiple times and the error appears to happen ~50% of the time. ```; ....snip....; [2017-03-20 15:30:35,10] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""Mutect2_Multi.contamination_tables"": [""null"", ""null""],; ""Mutect2_Multi.filtered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-0/Mutect2/7b579210-dfee-4740-ab6e-c1f65bc64014/call-Filter/execution/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal-filtered.vcf"", ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-Filter/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal-filtered.vcf""],; ""Mutect2_Multi.filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-filteredOutputList/execution/filtered.list"",; ""Mutect2_Multi.unfiltered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-0/Mutect2/7b579210-dfee-4740-ab6e-c1f65bc64014/call-MergeVCFs/execution/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal.vcf"", ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-MergeVCFs/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal.vcf""],; ""Mutect2_Multi.ob_filtered_vcf_files"": [""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-987",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:185,error,error,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,8,"['Error', 'error', 'recover']","['Error', 'error', 'errors', 'recoverWith']"
Availability,"- cromwell 26; - call caching is on (local mysql); - server mode, but only running one workflow; - lots of subworkflows; - JES backend. This workflow has > 20k tasks. Most of the questions are in the title. I ran a lot of tasks and the workflow eventually failed with the same error as reported in issue #2215 . Therefore, I am not sure whether this is a side effect of the failure. This could also just be an issue with the timing diagram. Regardless, see attached image. . Suggestion (which you probably thought, already): Do not investigate until after #2215 is remedied. . Once I run with a fix for #2215 , I'll check to see if this is still an issue. . Is there a parameter that would let me increase the dispatch rate? Would that alleviate this issue?. ![queuedincromwellissue](https://cloud.githubusercontent.com/assets/2152339/25530922/f9208b78-2bf5-11e7-9553-5a7f69a79dc4.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2216:277,error,error,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2216,2,"['error', 'failure']","['error', 'failure']"
Availability,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3039:34,failure,failures,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039,1,['failure'],['failures']
Availability,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:214,failure,failure,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,3,"['down', 'error', 'failure']","['download', 'error', 'failure']"
Availability,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,2,['error'],"['error', 'errors']"
Availability,"- cromwell v27; - SGE backend; - server mode. Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464:200,reliab,reliable,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464,1,['reliab'],['reliable']
Availability,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646:436,Error,Error,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"- cromwell-25-c398490; - Local backend; - single workflow ; - using docker; - Google VM. This has happened multiple times. Three days apart. Reading google buckets from a local backend has worked fine in the past. The error states that a file cannot be found, yet the error messages look more like a HTTP 500. ``gsutil ls ...`` shows that the file is there. . Apologies if I am missing something obvious (this may just be a misleading error message)... ```; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:218,error,error,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,3,['error'],['error']
Availability,"- cromwell-27-aab4763-SNAP.jar; - JES backend ; - localhost MySQL with call caching enabled.; - server mode. This error is above my ability to debug. Cromwell failed after that. It did not exit, but was no longer responsive. ```[ERROR] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 6c383c35-d791-4971-aecd-0723726c8a7b failed (during ExecutingWorkflowState): JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; java.lang.RuntimeException: JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:147); at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:114,error,error,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,3,"['ERROR', 'Fault', 'error']","['ERROR', 'FaultHandling', 'error']"
Availability,"- cromwell-27-c89c83f-SNAP; - server mode; - JES backend; - call caching on localhost mysql server. Is this a matter of hitting some sort of ceiling in number of concurrent jobs? Can I increase this?. ```; ....snip....; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ""message"": ""JobStore write failure: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:221,failure,failures,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,2,['failure'],"['failure', 'failures']"
Availability,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:141,error,error,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,4,"['Error', 'error']","['Error', 'error']"
Availability,"- develop branch from 0.22; - local backend; - single workflow. I think one of the tasks failed, but got the message below. cromwell did not exit and I believe it should have. ```; [2016-10-24 14:44:19,47] [error] head of empty list; java.util.NoSuchElementException: head of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,1,['error'],['error']
Availability,"- develop; - server mode. ```; SELECT MAX(aggregated) as group_concat_max_len FROM; (; SELECT cche.CALL_CACHING_ENTRY_ID, SUM(LENGTH(cche.HASH_VALUE)) AS aggregated; FROM CALL_CACHING_HASH_ENTRY cche; GROUP BY cche.CALL_CACHING_ENTRY_ID; ) aggregation; ```. yields a value of 1440. Yet, even when I set ``group_concat_max_len`` to 30000, I get the error message that the migration cannot be completed successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2262:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2262,1,['error'],['error']
Availability,- more logging in EjeaMultipleCallCacheCopyAttemptsSpec; - more logging of the centaur config during app startup; - all tests now output the heartbeat after common setup; - shorter centaur max workflow length for non-cron jobs; - exit horicromtal test when any of the containers fail,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6202:141,heartbeat,heartbeat,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6202,1,['heartbeat'],['heartbeat']
Availability,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4663,failure,failure,4663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failure']
Availability,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748:505,error,error,505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748,1,['error'],['error']
Availability,"- v28; - JES backend; - continue while possible is true. I would expect the below workflow to have failed. However, cromwell listed many tasks as running that had failed (quickly) with an out-of-storage error. Metadata is attached. ; [lee_metadata_bd884082.json.txt](https://github.com/broadinstitute/cromwell/files/1205626/lee_metadata_bd884082.json.txt). Remaining running tasks look like this one (as near as I can tell):. ```; ....snip....; 2017/08/07 17:03:25 E: command failed: [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; CommandException: Some components of /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam were not downloaded successfully. Please retry this download.; CommandException: 1 file/object could not be transferred.; (exit status 1); 2017/08/07 17:03:27 W: cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam, command failed: [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; [Errno 28] No space left on device; CommandException: Some components of /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/KIRP/DNA/WGS/BCM/ILLUMINA/TCGA-A4-A48D-10A-01D-A25F-10_wgs_Illumina.bam were not downloaded successfully. Please retry this download.; CommandException: 1 file/object could not be transferred.; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2526:203,error,error,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2526,5,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,"- v29; - Local backend. `java -jar cromwell.jar test.wdl test.json` is missing the `-i` flag and yet cromwell returns a zero error code. Since this is invalid, it should return non-zero.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3094:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3094,1,['error'],['error']
Availability,"--annotation-override "" else """"; String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"; String excluded_fields_args = if defined(funcotator_excluded_fields) then "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:36676,echo,echo,36676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['echo'],['echo']
Availability,"--mem-per-cpu=${requested_memory_mb_per_core} causes WomLong error on Cromwell 45.1, as reported here: . https://github.com/ENCODE-DCC/chip-seq-pipeline2/issues/59. Editting syntax to --mem-per-cpu ${requested_memory_mb_per_core} fixes this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5151:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151,1,['error'],['error']
Availability,"-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the asy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7122,down,down,7122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['down'],['down']
Availability,"-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1775,ERROR,ERROR,1775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"-12-15 21:14:45,71] [info] dataFileCache open start; [2022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4416,checkpoint,checkpointClose,4416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:5875,down,down,5875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,6,['down'],['down']
Availability,"-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5703,down,down,5703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['down'],['down']
Availability,"-306de3037265; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from - to Initializing; [2019-01-10 17:34:35,81] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from - to Initializing; [2019-01-10 17:36:24,60] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Initializing to Running; [2019-01-10 17:36:32,19] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Initializing to Running; [2019-01-10 18:21:56,23] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:1:1]: Status change from Running to Succeeded; [2019-01-10 18:23:09,43] [info] AwsBatchAsyncBackendJobExecutionActor [127f691eSomaticSNVInDel.MarkDuplicates:0:1]: Status change from Running to Succeeded; [2019-01-10 18:23:10,45] [error] WorkflowManagerActor Workflow 00d0c2df-8f87-42af-9439-b45593930c84 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3://s3.amazonaws.com/s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-branford-dev-nphi-valinor-v2/cromwell-execution/tigris_workflow/00d0c2df-8f87-42af-9439-b45593930c84/call-SomaticSNVInDel/vc.SomaticSNVInDel/127f691e-ea2e-441f-9d58-deba01a84c5e/call-MarkDuplicates/shard-0/MarkDuplicates-0-rc.txt: s3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4537:3915,error,error,3915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4537,1,['error'],['error']
Availability,-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traver,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:1150,Robust,RobustClientHelperSpec,1150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,1,['Robust'],['RobustClientHelperSpec']
Availability,-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traver,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1413,Robust,RobustClientHelperSpec,1413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,1,['Robust'],['RobustClientHelperSpec']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55290,down,download,55290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151272,down,download,151272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:98900,down,download,98900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:54042,down,download,54042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133217,down,download,133217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87057,down,download,87057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:40947,down,download,40947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12267,down,download,12267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169007,down,download,169007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:163985,down,download,163985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52175,down,download,52175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:185187,down,download,185187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77091,down,download,77091,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:70845,down,download,70845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz; 1608597234131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:80827,down,download,80827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49046,down,download,49046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:126969,down,download,126969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6644,down,download,6644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29094,down,download,29094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:36578,down,download,36578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145066,down,download,145066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125721,down,download,125721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:39699,down,download,39699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85188,down,download,85188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz; 1608596956029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:4773,down,download,4773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz; 1608597345651,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:110739,down,download,110739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141326,down,download,141326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3185,recover,recoverAsync,3185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:29723,down,download,29723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:165870,down,download,165870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19130,down,download,19130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:151901,down,download,151901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:77720,down,download,77720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:167755,down,download,167755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2823,ERROR,ERROR,2823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,1,['ERROR'],['ERROR']
Availability,-dc48-4409-bbd1-6f1411211f42 failed (during ExecutingWorkflowState): Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(Shar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:1477,Failure,Failures,1477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Failure'],['Failures']
Availability,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:2185,recover,recoverWith,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['recover'],['recoverWith']
Availability,-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.confi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:1835,Error,Error,1835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,4,['Error'],['Error']
Availability,"-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - â€˜cromwell-36.jarâ€™ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as CWL v1.0; [2018-11-09 10:25:19,60] [info] Pre-Processing /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1310,down,download,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:117602,down,download,117602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:12886,down,download,12886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:5392,down,download,5392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:8519,down,download,8519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:42822,down,download,42822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:52794,down,download,52794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:81446,down,download,81446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:88932,down,download,88932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172753,down,download,172753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140697,down,download,140697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi; 1608597219467,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:76462,down,download,76462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:161487,down,download,161487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93918,down,download,93918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz; 1608597066051,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:33469,down,download,33469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:125092,down,download,125092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:45927,down,download,45927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi; 1608597046875,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:28465,down,download,28465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz; 1608597577251,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:176483,down,download,176483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:98271,down,download,98271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"-queries-per-100-seconds = 1000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Number of workers to assign to PAPI requests; request-workers = 3. genomics {; # A reference to an auth defined in the `google` stanza at the top.; # This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:3126,error,error,3126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,"['error', 'failure']","['error', 'failure']"
Availability,"-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi. glob-b34dfc006a981a93d6da067cf50036fe:; total 512; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file. glob-ce2a0ab5d8c37a6d061c814f835853ee:; total 3.6M; -rw-r--r-- 1 root root 277 Jan 14 20:32 cromwell_glob_control_file; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; ```; As you can see, here `vcf.gz` and `vcf.gz.tbi` are stored under different directories.; However, the next `picard sort` step will be only looking at the directory where all `vcf.gz` live, which leads to the error:; ```; Could not localize /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/genomicsdb-0.vcf.gz.tbi -> /mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-picard_sortvcf/inputs/2004815296/genomicsdb-0.vcf.gz.tbi:',; u""\t/mnt/glusterfs/genomel-cohort-cwl/cromwell-executions/cwl_temp_file_6dfd1508-a107-491d-9cc2-8984f8e84977.cwl/6dfd1508-a107-491d-9cc2-8984f8e84977/call-gatk4_cohort_genotyping/shard-0/gatk4_cohort_genotyping.cwl/09c59ed5-8631-415c-97bc-896553cd775a/call-genomel_pdc_gatk4_cohort_genotyping/execution/glob-ce2a0ab5d8c37a6d061c814f835853ee/gen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4546:4497,error,error,4497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546,1,['error'],['error']
Availability,"-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor Successfully started WorkflowActor-cfc7b055-b8a3-40c4-a0eb-b4636d5c7286; 2018-08-02 02:23:03,861 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-08-02 02:23:03,863 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - MaterializeWorkflowDescriptorActor [UUID(cfc7b055)]: Parsing workflow as CWL v1.0; 2018-08-02 02:23:03,869 INFO - Pre-Processing /tmp/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.temp.1620266732058368635/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.cwl; 2018-08-02 02:23:05,504 INFO - Pre-Processing /modules/file-parsers/parsetxtxy.cwl; 2018-08-02 02:23:07,089 INFO - Pre-Processing /modules/processing-modules/13C-NMR.cwl; 2018-08-02 02:23:08,703 INFO - Pre-Processing /modules/core-modules/uploadsamples.cwl; 2018-08-02 02:23:10,570 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow cfc7b055-b8a3-40c4-a0eb-b4636d5c7286 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Input frequency is required and is not bound to any value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3958:2202,ERROR,ERROR,2202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958,1,['ERROR'],['ERROR']
Availability,"-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3675,ERROR,ERROR,3675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,". # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The defau",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12899,avail,available,12899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['avail'],['available']
Availability,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2429,rollback,rollback,2429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['rollback'],['rollback']
Availability,". engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://mybucket/cromwell-execution""; 	auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<account-id>:job-queue/MyHighPriorityQue-ae4256f76f07d96"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }. call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users.; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This is used to blacklist cache hit paths based on the; # prefixes of cache hit paths that Cromwell previously failed to copy for authorization reasons.; enabled: false; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 1000; }; }. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://cromwell-db-rdscluster.cluster.us-east-1.rds.amazonaws.com/cromwell""; user = ""myuser""; password = ""my password""; connectionTimeout = 5000; }; }. my hello.wdl is:; task hello {; String name; command {; echo 'Hello ${name}!' > ""hello${name}.txt""; }; output {; File response = ""hello${name}.txt""; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow test {; call hello; }. My hello_inputs.json is:; {; ""test.hello.name"": ""World"",; }. I ran java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl several times, each time there is a new job submitted to aws batch. Should it submit aws batch only once when first time ran it? . Your assistance will be highly appreciated. Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412:1767,echo,echo,1767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412,1,['echo'],['echo']
Availability,".$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1721,ERROR,ERROR,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['ERROR'],['ERROR']
Availability,... but it's still broken. Closing this down,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2834#issuecomment-342886123:40,down,down,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2834#issuecomment-342886123,1,['down'],['down']
Availability,...and if there was an error.. they just retry later,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227521672:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227521672,1,['error'],['error']
Availability,"...because we're seeing failures with gsutil with the newest image. Need to fix that for sure, but this is more like a temporary patch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2370:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2370,1,['failure'],['failures']
Availability,"...so if the 403 comes back with that information, that's great!. On Mon, Jul 11, 2016 at 8:27 AM, Yossi Farjoun farjoun@broadinstitute.org; wrote:. > Indeed. but the problem was that we couldn't tell that a file was missing; > and indeed which file it was.; > ; > On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; > wrote:; > ; > > IIRC the error in question was passing the 403 Forbidden back from Google; > > ; > > â€”; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > > or mute the thread; > > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259:363,error,error,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259,1,['error'],['error']
Availability,...until such time as 100KB can be read from OSS in under 60 seconds. Multiple PRs are backed up behind this chronic test failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5102:122,failure,failure,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5102,1,['failure'],['failure']
Availability,".5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:1521,down,download,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,3,"['Error', 'down', 'error']","['Error', 'download', 'error']"
Availability,".<snip>......... scatter (p in tumor_normal_pairs) {; # This call works fine; call m2.Mutect2 as m2_tn {; input:; gatk4_jar = ""/root/gatk-protected.jar"",; intervals = mutectIntervals,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fai,; ref_dict = ref_dict,; tumor_bam = p.left.left,; tumor_bam_index = p.left.right,; tumor_sample_name = sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; normal_bam = p.right.left,; normal_bam_index = p.right.right,; normal_sample_name = sub(sub(p.right.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; scatter_count = scatter_count,; dbsnp = dbSNPVCF,; dbsnp_index = dbsnp_index,; cosmic = cosmicVCF,; cosmic_index = cosmic_index,; is_run_orientation_bias_filter = true,; is_run_oncotator = false,; oncotator_docker = ""broadinstitute/oncotator:1.9.2.0-eval-gatk-protected"",; m2_docker = ""broadinstitute/gatk-protected@sha256:08bf9835dabb5b694164dae8312bac8d8012b9d907341f30d3c8e262a0f121d6"",; preemptible_attempts = preemptible,; onco_ds_local_db_dir = ""/root/onco_dbdir/"",; artifact_modes = [""G/T"", ""C/T""],; picard_jar = picard_jar; }; # New WDL added here that calls a task, not a workflow; # NOTE: Even when I put the VcfToIntervals task inline in this WDL file, I get the same exact error.; call dl_ob_training_m2.VcfToIntervals as vcf2i {; input:; vcf = m2_tn.filtered_vcf,; entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"); }; ### End new WDL; }. output {; Array[File] unfiltered_vcf = m2_tn.unfiltered_vcf; Array[File] unfiltered_vcf_index = m2_tn.unfiltered_vcf_index; Array[File] filtered_vcf = m2_tn.filtered_vcf; Array[File] filtered_vcf_index = m2_tn.filtered_vcf_index; }; }; ```. Here is the error message I get using wdltool 0.8 and 0.12 (and cromwell):. ```; ERROR: Expression will not evaluate (line 82, col 42):. entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"). ```. I tried a few things:; - ``entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """")`` --> ``entity_id=p.left.left`` gives same error; - ``entity_id=sub(sub",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334:2383,error,error,2383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334,1,['error'],['error']
Availability,".ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,912 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:9816,down,down,9816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['down'],['down']
Availability,".GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1737,error,errors,1737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['errors']
Availability,".JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:39); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted. Â  | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.impl.MetadataServiceActor - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:5891,Error,Error,5891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['Error'],['Error']
Availability,.JesAsyncBackendJobExecutionActor.jobPaths(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardCachingActorHelper$class.startMetadataKeyValues(StandardCachingActorHelper.scala:76); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:516); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.executeOrRecover(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.core.retry.Retry$.withRetry(Retry.scala:36); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:50); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:77); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunct,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:4311,robust,robustExecuteOrRecover,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['robust'],['robustExecuteOrRecover']
Availability,".aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:7062,failure,failures,7062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failures']
Availability,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:5919,failure,failure,5919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['failure'],['failure']
Availability,".backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references:/mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references \; --bind ${cwd}:${docker_cwd} \; --bind /tmp:/tmp \; /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; ${job_shell} \; ${docker_script}""'; """""". kill = ""'scancel ${job_id}'"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:2130,echo,echo,2130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['echo'],['echo']
Availability,".com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1158,error,error,1158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute v,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:4948,Fault,FaultHandling,4948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6223,recover,recover,6223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5649,error,error,5649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['error'],['error']
Availability,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:10129,error,errors,10129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:83447,down,down,83447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['down']
Availability,".html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19325,echo,echo,19325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,".impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I received yet another error: . ```; 2019-02-25 18:46:46,698 cromwell-system-akka.actor.default-dispatcher-3 ERROR - Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:2431,error,error,2431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['error'],['error']
Availability,".java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. ```; mysql> select * from WORKFLOW_METADATA_SUMMARY_ENTRY;; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | WORKFLOW_METADATA_SUMMARY_ENTRY_ID | WORKFLOW_EXECUTION_UUID | WORKFLOW_NAME | WORKFLOW_STATUS | START_TIMESTAMP | END_TIMESTAMP | SUBMISSION_TIMESTAMP | PARENT_WORKFLOW_EXECUTION_UUID | ROOT_WORKFLOW_EXECUTION_UUID | METADATA_ARCHIVE_STATUS |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | 2 | 796f3949-47e6-497e-9458-59ab53a063c6 | wf_hello | Succeeded | 2020-06-04 21:40:10.924000 | 2020-06-04 21:43:38.055000 | 2020-06-04 21:40:10.726000 | NULL | NULL | TooLargeToArchive |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; ```. But there was a small bug:; ```; 2020-06-04 21:43:43,512 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Programmer Error! The CarboniteWorkerActor cannot convert this into a completion metric: CarboniteWorkflowComplete(796f3949-47e6-497e-9458-59ab53a063c6,TooLargeToArchive); ```; I created a ticket for it: https://broadworkbench.atlassian.net/browse/BA-6471",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:4317,ERROR,ERROR,4317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,".jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.nativeUpsert(JdbcActionComponent.scala:561); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.f$1(JdbcActionComponent.scala:544); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$InsertOrUpdateAction.run(JdbcActionComponent.scala:557); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); 2017-07-13 22:14:36,622 cromwell-system-akka.actor.default-dispatcher-552 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Deadlock found when trying to get lock; try restarting transaction; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:951); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:9811,ERROR,ERROR,9811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,".jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4815,failure,failures,4815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['failure'],['failures']
Availability,".phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter inde",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:26523,failure,failures,26523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:4540,alive,alive,4540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,"['alive', 'fault']","['alive', 'fault']"
Availability,".runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:17036,error,error,17036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,".scala:49); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2019-01-07 16:21:23,91] [info] WorkflowManagerActor WorkflowActor-18de8166-5f29-4288-9fa4-6741565446fd is in a terminal state: WorkflowFailedState; [2019-01-07 16:21:30,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-07 16:21:32,96] [info] Workflow polling stopped; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-01-07 16:21:32,99] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-01-07 16:21:33,02] [info] Aborting all running workflows.; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7084,down,down,7084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,2,['down'],['down']
Availability,".scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncEx; > ecutionActor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1360,Fault,FaultHandling,1360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,".stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:182); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCommandLineWomFile(SharedFileSystemAsyncJobExecutionActor.scala:145); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCommandLineWomFile$(SharedFil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1970,Error,Error,1970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['Error'],['Error']
Availability,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:5247,Error,ErrorOr,5247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2896,Error,ErrorOr,2896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,".withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4366,Error,Error,4366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,['Error'],['Error']
Availability,".xml::call_job_identifiers::rmunshi: Column METADATA_JOURNAL.METADATA_CALL_INDEX renamed to JOB_SCATTER_INDEX; 2018-06-07 12:16:10,762 INFO - changelog.xml: changesets/standardize_column_names.xml::call_job_identifiers::rmunshi: Column METADATA_JOURNAL.METADATA_CALL_ATTEMPT renamed to JOB_RETRY_ATTEMPT; 2018-06-07 12:16:10,762 INFO - changelog.xml: changesets/standardize_column_names.xml::call_job_identifiers::rmunshi: ChangeSet changesets/standardize_column_names.xml::call_job_identifiers::rmunshi ran successfully in 1ms; 2018-06-07 12:16:10,778 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::entry_or_journal_existence_xor::mcovarr: ChangeSet changesets/embiggen_metadata_value.xml::entry_or_journal_existence_xor::mcovarr ran successfully in 15ms; 2018-06-07 12:16:10,789 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_entry::mcovarr: Marking ChangeSet: changesets/embiggen_metadata_value.xml::embiggen_metadata_entry::mcovarr ran despite precondition failure due to onFail='MARK_RAN':; changelog.xml : Table PUBLIC.METADATA_ENTRY does not exist. 2018-06-07 12:16:10,792 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr: METADATA_JOURNAL.METADATA_VALUE datatype was changed to LONGTEXT; 2018-06-07 12:16:10,793 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr: ChangeSet changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr ran successfully in 3ms; 2018-06-07 12:16:10,795 INFO - changelog.xml: changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi: Table CALL_CACHING_JOB_DETRITUS created; 2018-06-07 12:16:10,795 INFO - changelog.xml: changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi: ChangeSet changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi ran successfully in 1ms; 2018-06-07 12:16:10,797 INFO - changelog.xml: changesets/call_caching_job_d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:43916,failure,failure,43916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,"/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_â€¦ . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostExcepti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2582,Down,Downloading,2582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloading']
Availability,"/257) and associated cromwell PRs. But if the uninitialized optional declaration on [this line](https://github.com/broadinstitute/centaur/pull/242/files#diff-cc04c14d68a6a1a6d8d8366fc0c2f88cR48) is uncommented, the workflow fails. (Deliberately not quoting since lines don't wrap and anyway the thumbs downs are apropos). 2017-11-14 18:00:05,062 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-11-14 18:00:05,093 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - MaterializeWorkflowDescriptorActor [UUID(4b725606)]: Call-to-Backend assignments: decls.sub_decls.second_task -> Local, decls.sub_decls.first_task -> Local; 2017-11-14 18:00:06,129 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd [UUID(4b725606)]: Starting calls: SubWorkflow-sudecls:-1:1; 2017-11-14 18:00:06,130 cromwell-system-akka.actor.default-dispatcher-50 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreRegisterSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#-388497585] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/SubWorkflowExecutionActor-SubWorkflow-sudecls:-1:1#-1890869436] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; 2017-11-14 18:00:06,132 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - WorkflowManagerActor Workflow 4b725606-6d2a-4cf2-b23b-e5971f52b7dd failed (during ExecutingWorkflowState): ; 2017-11-14 18:00:06,133 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd is in a terminal state: WorkflowFailedState",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2902:2288,ERROR,ERROR,2288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2902,1,['ERROR'],['ERROR']
Availability,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1533,Fault,FaultHandling,1533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,6,['Fault'],['FaultHandling']
Availability,"/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that warning should guide the user as to how it will be interpreted at runtime.; 4. The same workflow running in Cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:886,error,error,886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,2,['error'],['error']
Availability,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:21335,recover,recoverWith,21335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['recover'],['recoverWith']
Availability,"/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using the AWS CLI as follows, it works fine. ```; aws batch register-job-definition --cli-input-json file://event_history.json; {; ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator:207"",; ""jobDefinitionName"": ""PreProcessingForVariantDiscovery_GATK4-B",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1876,Error,Error,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"/codes.html#StaticLoggerBinder for further details."",; ""endTime"": ""2018-08-14T16:16:33.718991Z""; },; {; ""startTime"": ""2018-08-14T16:13:13.678Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-08-14T16:13:14.570Z""; },; {; ""startTime"": ""2018-08-14T16:14:33.113135Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 60; done\"""",; ""endTime"": ""2018-08-14T16:14:33.759759Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.755Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-08-14T16:13:25.071851Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.571Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-08-14T16:13:14.755Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.570Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-08-14T16:13:14.571Z""; },; {; ""startTime"": ""2018-08-14T16:16:37.479167Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:8683,echo,echo,8683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"/cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) example is out of date. In `google.conf` it still lists the configuration for ""JES"" backend. 2) In the same tutorial ([Setting up PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2)), the instructions for which roles to assign to the GCP service account are outdated. 3) Once the user puzzles together which parts to replace, the execution is still failing (for me at least).; I run the following command ` java -Dconfig.file=cromwell.BROADexamples.v4.conf -jar cromwell-66.jar run hello.wdl -i hello.inputs`, which results in the following `Request contains an invalid argument.` error (abbreviated to the relevant section):; ```; [2021-08-13 10:44:39,31] [info] Running with database db.url = jdbc:hsqldb:mem: ...; ...; [2021-08-13 10:44:54,04] [info] Reference disks feature for PAPIv2 backend is not configured.; [2021-08-13 10:44:54,46] [info] Slf4jLogger started; [2021-08-13 10:44:54,73] [info] Workflow heartbeat configuration:; ...; [2021-08-13 10:44:55,42] [info] Running with 3 PAPI request workers; ...; [2021-08-13 10:44:55,79] [info] Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:1497,heartbeat,heartbeat,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['heartbeat'],['heartbeat']
Availability,"/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:1274,recover,recoverWith,1274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['recover'],['recoverWith']
Availability,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:19155,heartbeat,heartbeat,19155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.standard.StandardAsyncExecutionActor$class.commandScriptContents(StandardAsyncExecutionActor.scala:170); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:2317,Failure,Failure,2317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['Failure'],['Failure']
Availability,"/opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${bo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:3154,echo,echo,3154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['echo'],['echo']
Availability,"/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mdoc[0m) Scaladoc generation failed[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mcompileIncremental[0m) Compilation failed[0m; [0m[[31merror[0m] [0mTotal time: 273 s, completed Oct 25, 2017 1:03:49 PM[0m. restoring stty: 500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0. travis_time:end:1f45f95e:start=1508935437087656153,finish=1508936629752201761,duration=1192664545608; [0Ktravis_fold:end:after_success; [0K[33;1mSkipping a deployment with the script provider because this is not a tagged commit[0m. Done. Your build exited with 0.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:2057,error,errors,2057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['error'],['errors']
Availability,"/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); cromwell_1 | 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); cromwell_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:1312,error,error,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"/www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:968,heartbeat,heartbeat,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,4,"['error', 'failure', 'heartbeat']","['error', 'failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"0 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,81] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,84] [info] Database closed; [2019-02-11 10:13:36,84] [info] Stream materializer shut down; [2019-02-11 10:13:36,84] [info] WDL HTTP import resolver closed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:17396,down,down,17396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,3,['down'],['down']
Availability,"0.0-alpha1.2.4.jar.; 2017/03/20 15:37:09 I: Copying gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar to /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; 2017/03/20 15:37:09 I: Running command: sudo gsutil -q -m cp gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar /mnt/local-disk/gs:/bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar; ```. And the `exec.sh` that it generates is:; ```bash; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp. (; cd /cromwell_root; if [ false = false ]; \; then java -Xmx1g -jar gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar PadTargets --targets /cromwell_root/broad-dsde-methods/th/target/ice_targets.tsv --output targets.padded.tsv \; --padding 250 --help false --version false --verbosity INFO --QUIET false; \; else touch targets.padded.tsv; \; fi; ); echo $? > /cromwell_root/PadTargets-rc.txt.tmp; (; cd /cromwell_root. ); mv /cromwell_root/PadTargets-rc.txt.tmp /cromwell_root/PadTargets-rc.txt; ```. The WDL that has this issue is:; ```; workflow BrokenFilePath {; File targets = ""gs://broad-dsde-methods/th/target/ice_targets.tsv""; File GATK_protected_jar = ""gs://bg_tag_team/Tumor_Only_Resources/gatk-protected-1.0.0.0-alpha1.2.4.jar""; Boolean isWGS = false; Int padding = 250. call PadTargets {; input:; target_file=targets,; gatk_jar=GATK_protected_jar,; isWGS=isWGS,; mem=1,; padding=padding; }; }. task PadTargets {; File target_file; Int padding; File gatk_jar; Boolean isWGS; Int mem. command {; if [ ${isWGS} = false ]; \; then java -Xmx${mem}g -jar ${gatk_jar} PadTargets --targets ${target_file} --output targets.padded.tsv \; --padding ${padding} --help false --version false --verbosity INFO --QUIET false; \; else touch targets.padded.tsv; \; fi; }. output {; File padded_target_file = ""targets.padded.tsv""; }. runtime {; docker: ""broadinstitute/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078:1399,echo,echo,1399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078,1,['echo'],['echo']
Availability,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1103:61,down,download,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103,2,['down'],['download']
Availability,"0.5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:1403,down,download,1403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,1,['down'],['download']
Availability,"0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:15182,echo,echo,15182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"019-05-22 19:19:26,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: job id: 7c2d29c2-f04e-4b3f-8579-915a6fbc9033; [2019-05-22 19:19:26,76] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:10398,error,error,10398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['error'],['error']
Availability,"022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4471,checkpoint,checkpointClose,4471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:42:08.436 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:05.5983839 Bandwidth: 264.71 MiB/s; 2023-12-20 18:42:08.439 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 1,553,926,298; /cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/execution; ```. `develop` with foreign Blob URL:. ![Screenshot 2023-12-20 at 11 41 33](https://github.com/broadinstitute/cromwell/assets/1087943/b46da630-ad80-4388-9642-867e11516177)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:2813,Down,Download,2813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['Down'],['Download']
Availability,"02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1232,ERROR,ERROR,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"04 using conda-installed Cromwell:. `cromwell run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. Getting the error:. ```; [2023-02-04 08:55:00,61] [info] Running with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] SingleWorkflowRunnerActor: Workflow submit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:966,heartbeat,heartbeat,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"0748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:19498,echo,echo,19498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz; 1608597579845,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:178344,down,download,178344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:86428,down,download,86428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:84559,down,download,84559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:70216,down,download,70216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz; 1608597587281,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:178965,down,download,178965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32844,down,download,32844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:139451,down,download,139451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz; 1608597293653,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:96398,down,download,96398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi; 1608597651575,*** COMPLETED LOCALIZATION ***; 1608597657265,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597658212,[W::hts_idx_load2] The index file is older than the data file: /tmp/scratch/fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:197060,down,download,197060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz; 1608597289714,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:94539,down,download,94539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14757,down,download,14757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:118842,down,download,118842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz; 1608597232457,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:78960,down,download,78960,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz; 1608597101844,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:44062,down,download,44062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:156268,down,download,156268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A657136E0208581A7B6A7F020F, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,68] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.white_brits:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.year_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.phenotype' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.date_of_death' (scatter index: None, attempt 1); [2022-12-15 21:23:02,52] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.white_brits' (scatter index: None, attempt 1); [2022-12-15 21:23:03,67] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 3; [2022-12-15 21:23:03,69] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 is eligible for call caching with read = true and write = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:27158,failure,failures,27158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394:95,error,error,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394,1,['error'],['error']
Availability,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3399:142,heartbeat,heartbeats,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3399,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"1. Takes a `cromwell_id` from config or will just make up a `cromwell_id` based off a randomly generated UUID if none is provided.; 2. Clears `cromwell_id`s and heartbeats on workflow store entries on clean shutdown.; 3. Any workflow with a null or expired heartbeat is fair game when sweeping for workflows.; 4. Actual SQL generated with Slick's`forUpdate` looks like the following when using the MySQL profile: ```select `WORKFLOW_EXECUTION_UUID`, `WORKFLOW_DEFINITION`, `WORKFLOW_ROOT`, `WORKFLOW_TYPE`, `WORKFLOW_TYPE_VERSION`, `WORKFLOW_INPUTS`, `WORKFLOW_OPTIONS`, `WORKFLOW_STATE`, `SUBMISSION_TIME`, `IMPORTS_ZIP`, `CUSTOM_LABELS`, `CROMWELL_ID`, `HEARTBEAT_TIMESTAMP`, `WORKFLOW_STORE_ENTRY_ID` from `WORKFLOW_STORE_ENTRY` where (`HEARTBEAT_TIMESTAMP` is null) or (`HEARTBEAT_TIMESTAMP` < ?) order by `SUBMISSION_TIME` limit ? for update```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3401:161,heartbeat,heartbeats,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3401,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"1. When the AWS backend job actually failed with memory error, instead of returning that error code the commandScript returned zero. Fixed it to return the error code so that it a failed job in batch.; 2. Added AWS-EFS expression post mapping function so the output expressions with files with relative paths get mapped to the correct path.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5516:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516,3,['error'],['error']
Availability,"1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:1927,down,down,1927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,1,['down'],['down']
Availability,"1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-4 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:9184,ERROR,ERROR,9184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,270 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:5107,ERROR,ERROR,5107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1016:3972,error,error,3972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016,1,['error'],['error']
Availability,"1.splicegraph"": ""<absolute_path_to_file2>""; }. ```; I tried with and without the ""runtime"" spec block (local run with voila on system path) with the same result. . In the cromwell log as it runs I see it says it runs this command:; ```; voila tsv \ ; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/test.psi.voila \; /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs/613161631/splicegraph.sql \; -f something.tsv; ```. However my program (voila) seems to somehow be receiving an argument for a different file (?) which it can not find:. ```; usage: voila tsv [-h] -f FILE_NAME [--threshold THRESHOLD]; [--non-changing-threshold NON_CHANGING_THRESHOLD]; [--probability-threshold PROBABILITY_THRESHOLD] [--show-all]; [--lsv-types-file LSV_TYPES]; [--lsv-types [LSV_TYPES [LSV_TYPES ...]]]; [--lsv-ids-file LSV_IDS] [--lsv-ids [LSV_IDS [LSV_IDS ...]]]; [--gene-names-file GENE_NAMES]; [--gene-names [GENE_NAMES [GENE_NAMES ...]]]; [--gene-ids-file GENE_IDS]; [--gene-ids [GENE_IDS [GENE_IDS ...]]] [-j NPROC] [--debug]; [-l LOGGER] [--silent]; files [files ...]; ```; voila tsv: error: argument files: cannot find ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/ "". The input files exist correctly at the path /home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/inputs . However, it appears that somehow a third path is being specified as an argument with the path ""/home/pjewell/wdltut/cromwell-executions/myWorkflow1/c9fc1ea1-3904-4bc7-8fc3-186cf99b6926/call-task_voila_tsv/execution/"" ? I don't understand where this is coming from but I am new at the language so most likely it is just a noob mistake. . Can anyone let me know why my program might be receiving the extra argument that causes it to crash?. Os: ubuntu 18.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5285:2490,error,error,2490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285,1,['error'],['error']
Availability,1043c failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:1625,Failure,Failure,1625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,1,['Failure'],['Failure']
Availability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); 	at java.nio.file.Files.newInputStream(Files.java:152); 	at java.nio.file.Files.newBufferedReader(Files.java:2784); 	at java.nio.file.Files.readAllLines(Files.java:3202); 	at java.nio.file.Files.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3857,echo,echo,3857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.ni",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3586,echo,echo,3586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,10K copies of the same error message per Cloud NAT run gets old fast.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5589:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5589,1,['error'],['error']
Availability,"11Z"",; ""description"": ""Stopped pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:28.018319Z""; },; {; ""startTime"": ""2018-08-14T16:14:34.534911Z"",; ""description"": ""Started running \""\/bin\/bash -c mkdir -p \/cromwell_root && chmod -R a+rwx \/cromwell_root\"""",; ""endTime"": ""2018-08-14T16:14:34.810415Z""; },; {; ""startTime"": ""2018-08-14T16:16:45.288207Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:45.309551Z""; },; {; ""startTime"": ""2018-08-14T16:16:39.695039Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:15897,echo,echo,15897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponâ€¦ . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retryi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3906,down,download,3906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,2,"['down', 'error']","['download', 'error']"
Availability,"13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:20718,ERROR,ERROR,20718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['ERROR'],['ERROR']
Availability,"15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4707,checkpoint,checkpointClose,4707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4928,checkpoint,checkpointClose,4928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5385,checkpoint,checkpointClose,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5842,checkpoint,checkpointClose,5842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6299,checkpoint,checkpointClose,6299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6756,checkpoint,checkpointClose,6756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:50,95] [info] checkpointClose start; [2022-12-15 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9369,checkpoint,checkpointClose,9369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9826,checkpoint,checkpointClose,9826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10283,checkpoint,checkpointClose,10283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10740,checkpoint,checkpointClose,10740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11197,checkpoint,checkpointClose,11197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11654,checkpoint,checkpointClose,11654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; concurrent-job-limit = 1000; root = ""s3://cromwell-aws-hello/cromwell-execution""; auth = ""default""; default-runtime-attributes {; queueArn = ""arn:aws:batch:us-east-1:XXXXXXXXX:job-queue/python-batch"" ,; scriptBucketName = ""cromwell-aws-hello"" ; }; filesystems {; s3 {; auth = ""default""; }; }; # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: 24 hours; }; }; }; }. [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026013/Cromwell-Error.docx); ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153040332-625cb61a-062b-4766-96ea-8e129efb2b20.PNG); [config file.docx](https://github.com/broadinstitute/cromwell/files/8026025/config.file.docx). How to give Timeout options for Job definitions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:3017,Error,Error,3017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,2,['Error'],['Error']
Availability,"16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-For: 69.173.127.107, User-Agent: Java/1.8.0, Host: app:8000),Empty,HTTP/1.1); com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3434) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3334) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3774) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Mysq",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:2191,failure,failure,2191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,1,['failure'],['failure']
Availability,"17](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:2956,down,down,2956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,"182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES inputs (found 1):; Input(Some(commandScript),Some(wf_hello.hello.commandScript),None,/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; sync. ); mv /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:4640,echo,echo,4640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['echo'],['echo']
Availability,"18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2941,echo,echo,2941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['echo'],['echo']
Availability,"19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1574,ERROR,ERROR,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2266,recover,recoverAsync,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Availability,"19:02:19.372170Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Failed to complete master workflow, error code: 1; [2018-11-04T19:02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1129,ERROR,ERROR,1129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObjec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:1155,error,errors,1155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,1,['error'],['errors']
Availability,"1: I attempted to use your Jira tracker, it let me log in but told me I don't have permission to see anything or do anything; 2: https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team no longer exists, and the support staff respond to questions with ""we only answer GATK isues""; 3: I am using womtool 65 and Cromwell 62. I get the same failure in both, which is that if the first line of my file is:. `version development`. As per the [WDL specifications](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#versioning) I get the error:. `ERROR: Finished parsing without consuming all tokens.`. If I do not include that line, then I get this error:. ```; Expected rbrace, got Directory.; Directory	OutputDir; ```. Does Cromwell support WDL versions pther than the default? if so, how do I specify which version to use?. Thank you,; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIV",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438:351,failure,failure,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438,4,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"1] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3823,Failure,Failure,3823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['Failure'],['Failure']
Availability,"1bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz; 1608597615294,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz; 1608597617667,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:186423,down,download,186423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1036,Failure,Failure,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Failure'],['Failure']
Availability,"2 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4943,echo,echoHelloWorld,4943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,2 workflows failing with JES 503 errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,1,['error'],['errors']
Availability,2); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq:; 	fastq doesn't exist; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	File not found fastq; 	File not found /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:14402,Error,Error,14402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['Error'],['Error']
Availability,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11235,error,errors,11235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['errors']
Availability,"2+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3042,echo,echo,3042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"2-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4823,Checkpoint,Checkpoint,4823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4873,checkpoint,checkpointClose,4873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5217,Checkpoint,Checkpoint,5217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5330,checkpoint,checkpointClose,5330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5787,checkpoint,checkpointClose,5787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5674,Checkpoint,Checkpoint,5674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6131,Checkpoint,Checkpoint,6131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6244,checkpoint,checkpointClose,6244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6588,Checkpoint,Checkpoint,6588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6701,checkpoint,checkpointClose,6701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:50,95] [info] Checkpoint start; [2022-12-15 21:14:50,95] [info] checkpointClose start; [2022-12-15 21:14:50,95] [info] checkpointClose synched; [2022-12-15 21:14:50,98] [info] checkpointClose script done; [2022-12-15 21:14:50,98] [info] dataFileCache commit start; [2022-12-15 21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9314,checkpoint,checkpointClose,9314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9658,Checkpoint,Checkpoint,9658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9771,checkpoint,checkpointClose,9771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10228,checkpoint,checkpointClose,10228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10115,Checkpoint,Checkpoint,10115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10572,Checkpoint,Checkpoint,10572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10685,checkpoint,checkpointClose,10685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11029,Checkpoint,Checkpoint,11029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11142,checkpoint,checkpointClose,11142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11486,Checkpoint,Checkpoint,11486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,"2-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11599,checkpoint,checkpointClose,11599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"2-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O XXXXXX-001.recal_data.csv -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf -L 10:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.bdd4ff39; [Global flags]. (...EXECUTION LOGS...). 16:35:03.610 INFO ProgressMeter - 10:135446829 2.3 3269161 1437710.1; 16:35:03.614 INFO ProgressMeter - Traversal complete. Processed 3269161 total reads in 2.3 minutes.; 16:35:03.819 INFO BaseRecalibrator - Calculating quantized quality scores...; 16:35:03.882 INFO BaseRecalibrator - Writing recalibration report...; 16:35:04.992 INFO BaseRecalibrator - ...done!; 16:35:04.996 INFO BaseRecalibrator - Shutting down engine; [October 31, 2018 4:35:05 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 2.34 minutes.; Runtime.totalMemory()=4054515712; Tool returned:; 3269161; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication.; ```. The only thing that is not default in bucket is that we have set lifecycle option to delete objects after 5 days. Our workflow takes ~6 hours to end so it should not be a problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126:1992,down,down,1992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126,1,['down'],['down']
Availability,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1875,heartbeat,heartbeat,1875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"2/call-lo; ad_shared_covars/execution/stderr.; [First 3000 bytes]:Traceback (most recent call last):; File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 87, in <module>; load_covars(); File ""/home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py"",; line 51, in load_covars; assert not np.any(np.isnan(data)); AssertionError. [2022-12-15 21:28:38,49] [info] WorkflowManagerActor: Workflow actor for 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff completed with status 'Failed'. The workflow will be removed from the workflow store.; [2022-12-15 21:28:52,23] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2022-12-15 21:28:53,46] [info] Workflow polling stopped; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49140,down,down,49140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['down'],['down']
Availability,"21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5164,checkpoint,checkpointClose,5164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5621,checkpoint,checkpointClose,5621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6078,checkpoint,checkpointClose,6078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6535,checkpoint,checkpointClose,6535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022-12-15 21:14:50,85] [info] dataFileCache commit end; [2022-12-15 21:14:50,87] [info] checkpointClose end; [2022-12-15 21:14:50,88] [info] Checkpoint end - txts: 101879; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6992,checkpoint,checkpointClose,6992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:50,99] [info] dataFileCache commit end; [2022-12-15 21:14:51,01] [info] checkpointClose end; [2022-12-15 21:14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9605,checkpoint,checkpointClose,9605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10062,checkpoint,checkpointClose,10062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10519,checkpoint,checkpointClose,10519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10976,checkpoint,checkpointClose,10976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11433,checkpoint,checkpointClose,11433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"22-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script done; [2022-12-15 21:14:46,71] [info] dataFileCache commit start; [2022-12-15 21:14:47,14] [info] dataFileCache commit end; [2022-12-15 21:14:47,20] [info] checkpointClose end; [2022-12-15 21:14:47,37] [info] Checkpoint start; [2022-12-15 21:14:47,37] [info] checkpointClose start; [2022-12-15 21:14:47,37] [info] checkpointClose synched; [2022-12-15 21:14:47,44] [info] checkpointClose script done; [2022-12-15 21:14:47,44] [info] dataFileCache commit start; [2022-12-15 21:14:47,45] [info] dataFileCache commit end; [2022-12-15 21:14:47,48] [info] checkpointClose end; [2022-12-15 21:14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:4528,checkpoint,checkpointClose,4528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['checkpoint'],['checkpointClose']
Availability,"23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility2_run_create_seg_gt_table:NA:1#1336176467] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,67] [error] WorkflowManagerActor Workflow 54e13b6c-33e4-4777-a4bd-f7b2876c2df5 failed (during ExecutingWorkflowState): java.lang.Exception: Call crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:3:1: return code was -1. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:3072,error,error,3072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,3,['error'],['error']
Availability,"235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: Call failed to initialize: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log: None.get; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(88b21d2d)]: persisting status of PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log to Failed.; 2016-06-01 16:10:15,230 cromwell-system-akka.actor.default-dispatcher-20 INFO - WorkflowActor [UUID(88b21d2d)]: Beginning transition from Running to Failed.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:4353,ERROR,ERROR,4353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['ERROR'],['ERROR']
Availability,"23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] /build/cos-tools: \nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:4222,Down,Downloading,4222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:1744,ERROR,ERROR,1744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:182,failure,failure,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['failure'],['failure']
Availability,"27). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diabetesgenetics.org/>, Broad Institute; <http://www.broadinstitute.org/>. ---",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:3024,down,down,3024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,"28000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.; cache {; enabled: true; # Guava cache concurrency.; concurrency: 1; # How long entries in the cache should live from the time they are added to the cache.; ttl: 20 minutes; # Maximum number of entries in the cache.; size: 100000; }; }. # Cromwell reads this value into the JVM's `networkaddress.cache.ttl` setting to control DNS cache expiration; dns-cache-ttl: 3 minutes; }. docker {; hash-lookup {; # Set this to match your available quota against the Google Container Engine API; #gcr-api-queries-per-100-seconds = 1000. # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again; #cache-entry-ttl = ""20 minutes"". # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache; #cache-size = 200. # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub, gcr, gar, quay; #method = ""remote""; enabled = ""false""; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:4038,avail,available,4038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['avail'],['available']
Availability,"31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:2870,error,error,2870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['error'],['error']
Availability,"333-8f3c-7c69317ba0a2/call-PairedFastQsToUnmappedBAM/inputs/-2135135022/S000021_S7367Nr1.2.fastq.gz --OUTPUT S7367Nr1.unmapped.bam --READ_GROUP_NAME S7367Nr1 --SAMPLE_NAME S4431Nr1 --LIBRARY_NAME TwistCore+RefSeq+Mito-Panel --PLATFORM_UNIT platform_unit --PLATFORM Illumina --SEQUENCING_CENTER CeGaT --RUN_DATE 2021-10-10T06:00:00+0000 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; 2023-02-03 12:38:34 [Fri Feb 03 09:38:34 GMT 2023] Executing as root@d65fc5b7d470 on Linux 5.15.49-linuxkit amd64; OpenJDK 64-Bit Server VM 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0; 2023-02-03 12:38:35 INFO 2023-02-03 09:38:35 FastqToSam Auto-detected quality format as: Standard.; 2023-02-03 12:39:08 INFO 2023-02-03 09:39:08 FastqToSam Processed 1,000,000 records. Elapsed time: 00:00:32s. Time for last 1,000,000: 32s. Last read position: */*`. I tried via Java 18.0.1.1 JDK and also later with 1.8.0_202 JDK. I also tried with the conda installation where Java dependency of OpenJDK 11.0.15 is automatically installed. I also tried combinations with Cromwell 69, 80 and 84. None of them works. They all have the same problem. It only works if I use Cromwell version 55 along with Java 1.8.0_202 JDK. It would be amazing if you look into this, as we would love to use the latest Cromwell versions and benefit from the conda environment. Thanks!. Machine info: `Darwin Ibrahims-MacBook-Pro.local 22.2.0 Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103 arm64`. MacOS = Ventur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998:2678,avail,available,2678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998,1,['avail'],['available']
Availability,"34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:2630,error,error,2630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability,"3:25.071851Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.571Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2018-08-14T16:13:14.755Z""; },; {; ""startTime"": ""2018-08-14T16:13:14.570Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2018-08-14T16:13:14.571Z""; },; {; ""startTime"": ""2018-08-14T16:16:37.479167Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:39.695039Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.673071Z"",; ""description"": ""Started running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:10061,echo,echo,10061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"3:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file; '/usr/local/nvidia/nvidia-installer.log' for details. You may find; suggestions on fixing installation problems in the README available; on the Linux driver download page at www.nvidia.com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5375,ERROR,ERROR,5375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,5,"['ERROR', 'avail', 'down', 'error']","['ERROR', 'available', 'download', 'error']"
Availability,"3] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7199,down,down,7199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['down'],['down']
Availability,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2883,error,error,2883,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['error'],['error']
Availability,"4+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3313,echo,echo,3313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:134465,down,download,134465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:156895,down,download,156895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193276,down,download,193276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:137588,down,download,137588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:183935,down,download,183935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:108253,down,download,108253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz; 1608597031084,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:25352,down,download,25352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:112608,down,download,112608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:119469,down,download,119469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170884,down,download,170884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:100775,down,download,100775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97025,down,download,97025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:20999,down,download,20999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi; 1608597362768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:114477,down,download,114477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123846,down,download,123846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz; 1608597156183,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:59643,down,download,59643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:82692,down,download,82692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4/19 PM Hi Evan, a patch went out to fix this at 10 AM this morning. Can you confirm that you no longer see this?. 4/19 AM Hi Evan - were you signed into the forum when you got this error? Can you send me the url of the page you were on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047:182,error,error,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047,1,['error'],['error']
Availability,406 errors from status endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775,1,['error'],['errors']
Availability,"46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] WorkflowManagerActor WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 is in a terminal state: WorkflowFailedState; [2016-07-13 10:12:49,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:2911,down,down,2911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,2,"['down', 'error']","['down', 'error']"
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz; 1608597060059,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:32227,down,download,32227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:162737,down,download,162737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:18501,down,download,18501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:106384,down,download,106384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:194526,down,download,194526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60268,down,download,60268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116983,down,download,116983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:129467,down,download,129467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30973,down,download,30973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:47798,down,download,47798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:172134,down,download,172134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:50927,down,download,50927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62145,down,download,62145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:7271,down,download,7271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi; 1608597606470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:184562,down,download,184562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:14140,down,download,14140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143199,down,download,143199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37205,down,download,37205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi; 1608597159853,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:60895,down,download,60895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d915b85b25fc;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,59] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-15 15:09:22,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-15 15:09:22,67] [info] Running with database db.url = jdbc:hsqldb:mem:52af65c3-a08f-4d3a-a6bc-c97a3d7e1a3c;shutdown=false;hsqldb.tx=mvcc; [2019-01-15 15:09:22,97] [info] Slf4jLogger started; [2019-01-15 15:09:23,24] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-d961aae"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; .; .; [2019-01-15 15:09:29,85] [error] WorkflowManagerActor Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.job.preparation.JobPreparationActor$$anonfun$1$$anon$1: Call input and runtime attributes evaluation failed for ls:; Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""womtool-31.jar""' of type 'File' to 'Array[File]'.; .; .; Workflow 5bc372e9-61f6-45fd-b178-60ed25529216 transitioned to state Failed. [issue.zip](https://github.com/broadinstitute/cromwell/files/2759990/issue.zip). I've attached the `wdl` and `json` files I've used, the input filenames are the `jar` files of both cromwell and womtool used to run the test workflow. I'm not attaching those because they are very large.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:2505,heartbeat,heartbeat,2505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,3,"['error', 'heartbeat']","['error', 'heartbeat', 'heartbeatInterval']"
Availability,48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2932,recover,recover,2932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.trans,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:4781,Error,ErrorOr,4781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2430,Error,ErrorOr,2430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"49.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/Evi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159399,down,download,159399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$kb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitializationActor-7218c3a1-5155-4921-9adb-d96c52c32200] State is now terminal. Shutting down.; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.544] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$mb] $$mb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$mb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transitioning from In",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:2853,down,down,2853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"4:main.year_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 09247459DDA5EA8DF661D5F490C81E8B, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,84] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.year_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,36] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.phenotype:-1:1-20000000025 [9e4f5894main.phenotype:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,36] [info] BT-322 9e4f5894:main.phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629C24E64D33DEB38610, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,37] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.date_of_death:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,67] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.white_brits:-1:1-20000000000 [9e4f5894main.white_brits:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,68] [info] BT-322 9e4f5894:main.white_brits:-1:1 cache hit copying success with aggregated hashes: initial = EB2F16A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:25880,failure,failures,25880,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 358986",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2526,error,error,2526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['error'],['error']
Availability,"4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF4C1AEE5AC6D327D9290D15E, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:56,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.ethnicity_self_report:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.categorical_covariates' (scatter index: Some(0), attempt 1); [2022-12-15 21:27:56,51] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:31406,failure,failures,31406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:3039,ERROR,ERROR,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['ERROR'],['ERROR']
Availability,500 Internal error post success processing of task (call caching turned off),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,1,['error'],['error']
Availability,500 Internal error when trying to get task hash for call caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['error'],['error']
Availability,"54-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-FilterByOrientationBias/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal.ob_filtered.vcf""],; ""Mutect2_Multi.unfiltered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-unfilteredOutputList/execution/unfiltered.list"",; ""Mutect2_Multi.ob_filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3351,error,error,3351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"55ee7c; [2018-10-25 21:21:09,93] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:21:09,93] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:21:09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:3934,echo,echo,3934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"5sbg5oeqevqw-6cylhedz/job-1/a7123170_f41bbba17a6f4409940127a60234695d-1/wd/wd/cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/inputs/lz304a1e79fd7359e5327eda.blob.core.windows.net/sc-705b830a-d699-478e-9da6-49661b326e77/inputs/Rocky-9.2-aarch64-dvd.iso; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:1319,Down,Download,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['Down'],['Download']
Availability,6); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:6591,robust,robustExecuteOrRecover,6591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['robust'],['robustExecuteOrRecover']
Availability,"6-08-08 08:33:10,291] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [â†[38;5;1merrorâ†[0m] BackendCallExecutionActor [â†[38;5 ;2m4e20eafcâ†[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:9338,error,error,9338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['error'],['error']
Availability,"613). Admittedly, I have never written a parser before, so I don't know how feasible this is, but can you write the grammar/parser such that everything on a line after a `#` character is ignored?. The only edge cases I imagine are when the `#` character is in a quoted string. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200505343). @tmdefreitas Yes, that is definitely possible. However, we try to not make assumptions about the type of characters that your script can have in it. I'm perhaps being a little overly cautious, but I'd hate for there to be a case where somebody wants to use a `#` in their command but it gets interpreted as a comment. That could lead to the same kind of confusion that we're seeing now. I vacillate on this because I also see the pragmatism in implementing your suggestion for the common case. In most cases I can think of, a `#` is a comment. Maybe some approach like Eddie's where I can have the parser give a better error message is the best solution. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200508578). Scott,. Not only can I imagine a command where # is used (and in a WDL no less),; but I have actually written such a command. In the merge step for mutect, I use ""#"" to select for header lines in; merging call_stats files and VCF files!. command <<<; #increase verbosity; set -x. #mutect1 call_stats merging; MUTECT1_CS=""MuTect1.call_stats.txt""; head --lines=2 ${mutect1_cs[0]} > $MUTECT1_CS; cat ${sep =' ' mutect1_cs} | grep -Pv '#'|grep -Pv '^contig' >> $MUTECT1_CS. #mutect2 call_stats merging; MUTECT2_CS=""MuTect2.call_stats.txt""; cat ${mutect2_cs[0]} |grep -P '^#' > $MUTECT2_CS ;; cat ${sep=' ' mutect2_cs} |grep -Pv '^#' >> $MUTECT2_CS ;; -eddie. On Wed, Mar 23, 2016 at 3:25 PM, Scott Frazer notifications@github.com; wrote:. > @tmdefreitas https://github.com/tmdefreitas Yes, that is definitely; > possible.; >",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:3006,error,error,3006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['error'],['error']
Availability,"636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:195153,down,download,195153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz; 1608597186282,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:68359,down,download,68359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:57157,down,download,57157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz; 1608597208254,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:73960,down,download,73960,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi; 1608597008325,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:19751,down,download,19751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:107005,down,download,107005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz; 1608597070520,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi; 1608597074143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:35953,down,download,35953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111360,down,download,111360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz.tbi; 1608597049133,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz; 1608597051239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:30344,down,download,30344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:45308,down,download,45308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:103267,down,download,103267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:104513,down,download,104513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16003,down,download,16003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz; 1608597175918,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:65250,down,download,65250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:92672,down,download,92672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz; 1608597567063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175237,down,download,175237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9146,down,download,9146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120715,down,download,120715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz; 1608597619328,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:188288,down,download,188288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:158141,down,download,158141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:147566,down,download,147566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz; 1608596985970,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz.tbi; 1608596987867,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz.tbi; 1608596990438,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:13513,down,download,13513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190153,down,download,190153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102021,down,download,102021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22245,down,download,22245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:2287,down,download,2287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27219,down,download,27219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz; 1608597594378,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182066,down,download,182066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90178,down,download,90178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"64-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135711,down,download,135711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:62766,down,download,62766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131340,down,download,131340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:170257,down,download,170257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:22866,down,download,22866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:142572,down,download,142572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:55911,down,download,55911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:123219,down,download,123219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi; 1608597455771,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz; 1608597458007,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz.tbi; 1608597459482,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:141947,down,download,141947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz; 1608597430528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:133838,down,download,133838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi; 1608597252335,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz; 1608597255692,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz; 1608597256940,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:85809,down,download,85809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:136961,down,download,136961,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:113229,down,download,113229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz; 1608596960348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz.tbi; 1608596962113,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz.tbi; 1608596964153,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:6019,down,download,6019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz; 1608597642095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz.tbi; 1608597643768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:193897,down,download,193897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi; 1608597451192,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz; 1608597453442,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:140078,down,download,140078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:192028,down,download,192028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128217,down,download,128217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105134,down,download,105134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz; 1608597448688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:138834,down,download,138834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17251,down,download,17251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi; 1608597371711,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:116354,down,download,116354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi; 1608597552897,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz; 1608597554503,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz.tbi; 1608597557397,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:171505,down,download,171505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:182687,down,download,182687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:72093,down,download,72093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz.tbi; 1608597162870,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz; 1608597164800,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz.tbi; 1608597167270,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:61516,down,download,61516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:131969,down,download,131969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:145693,down,download,145693,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz; 1608597369233,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:115733,down,download,115733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:41574,down,download,41574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi; 1608597198607,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz.tbi; 1608597201756,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:71472,down,download,71472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:164612,down,download,164612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz.tbi; 1608597259619,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:87684,down,download,87684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi; 1608597409505,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:127596,down,download,127596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160868,down,download,160868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi; 1608597303071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:99527,down,download,99527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:49673,down,download,49673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:166497,down,download,166497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz; 1608597218252,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:75843,down,download,75843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:130094,down,download,130094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi; 1608597489587,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:152528,down,download,152528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz.tbi; 1608597053171,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz.tbi; 1608597054920,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz.tbi; 1608597057531,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:31600,down,download,31600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:83319,down,download,83319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi; 1608597120193,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz.tbi; 1608597122029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:50302,down,download,50302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi; 1608597548876,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz; 1608597550096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:169636,down,download,169636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz; 1608597651470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-96/cacheCopy/SR00c.HG03864.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:196439,down,download,196439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.genetic_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:54,15] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.genetic_sex' (scatter index: None, attempt 1); [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:29933,failure,failures,29933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:47169,down,download,47169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz; 1608597327069,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz; 1608597328928,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz.tbi; 1608597331137,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:105755,down,download,105755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:68976,down,download,68976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz; 1608597154187,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:57776,down,download,57776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz; 1608597003743,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz.tbi; 1608597005866,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:17872,down,download,17872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz; 1608597322033,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz.tbi; 1608597324182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:103886,down,download,103886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-151/cacheCopy/SR00c.NA20845.txt.gz; 1608597411470,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz; 1608597413700,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz.tbi; 1608597415362,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:128838,down,download,128838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz; 1608597317794,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz.tbi; 1608597320422,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:102640,down,download,102640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:188907,down,download,188907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz.tbi; 1608597432512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz; 1608597434353,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:135084,down,download,135084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi; 1608597442529,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz; 1608597443863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-54/cacheCopy/SR00c.HG02272.txt.gz.tbi; 1608597446991,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:138207,down,download,138207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz; 1608597312994,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz.tbi; 1608597315649,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:101394,down,download,101394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi; 1608597182124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:65869,down,download,65869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi; 1608597381292,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz; 1608597382846,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:120088,down,download,120088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146939,down,download,146939,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:1660,down,download,1660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:38451,down,download,38451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi; 1608597507274,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz; 1608597508511,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:157514,down,download,157514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:155028,down,download,155028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz.tbi; 1608597462345,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:143818,down,download,143818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz.tbi; 1608597125075,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz; 1608597127083,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz.tbi; 1608597129434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:51546,down,download,51546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz; 1608597018214,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz.tbi; 1608597021127,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:21618,down,download,21618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz; 1608597343594,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:108872,down,download,108872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi; 1608596949182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz; 1608596954593,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:2906,down,download,2906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:11019,down,download,11019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initializing engine; 21:45:16.367 INFO ProgressMeter - Starting traversal; 21:45:16.368 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 21:45:17.033 INFO PrintReads - No reads filtered by: WellformedReadFilter; 21:45:17.035 INFO ProgressMeter - chr6:96496576 0.0 5660 509909.9; 21:45:17.036 INFO ProgressMeter - Traversal complete. Processed 5660 total reads in 0.0 minutes.; 21:45:17.414 INFO PrintReads - Shutting down engine; ```. This is running on Google Cloud on Cromwell 32-c7bcab8.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3929:2534,down,down,2534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929,1,['down'],['down']
Availability,"73364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1671,failure,failure,1671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['failure'],['failure']
Availability,"7c74-4c5c-4314-8463-072e7055ee7c is in a terminal state: WorkflowSucceededState; [2018-10-25 21:21:44,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test_opt_array.t1.out"": [""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-0/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-3/execution/out.txt"", ""/users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-4/execution/out.txt""]; },; ""id"": ""0bb77c74-4c5c-4314-8463-072e7055ee7c""; }; ```. But I got the following error when I tried with 36.; ```; $ java -jar ~/cromwell-36.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:17:04,83] [info] Running with database db.url = jdbc:hsqldb:mem:bb200ed8-7db5-49a0-a250-ca46b3332697;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,03] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:17:12,04] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:17:12,13] [info] Running with database db.url = jdbc:hsqldb:mem:c7a7ec22-dec6-4fae-a53b-6c9933402fa9;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,59] [info] Slf4jLogger started; [2018-10-25 21:17:12,88] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-f5ccf1c"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:17:12,90] [info] Metadata summary refreshi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:9533,error,error,9533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['error'],['error']
Availability,"7d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2100,avail,available,2100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['avail'],['available']
Availability,"7e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35187,ERROR,ERROR,35187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi; 1608597544559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz.tbi; 1608597545740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:168382,down,download,168382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi; 1608597470124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-44/cacheCopy/SR00c.HG01982.txt.gz.tbi; 1608597472441,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz; 1608597473978,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:146320,down,download,146320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi; 1608597394986,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz.tbi; 1608597397902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz; 1608597399545,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:124473,down,download,124473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153782,down,download,153782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi; 1608596972311,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:9773,down,download,9773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:191407,down,download,191407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz.tbi; 1608597076382,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz.tbi; 1608597078723,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:37832,down,download,37832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi; 1608597571205,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:175864,down,download,175864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi; 1608597497211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:154409,down,download,154409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz.tbi; 1608597491461,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-29/cacheCopy/SR00c.HG01396.txt.gz.tbi; 1608597492748,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz.tbi; 1608597495158,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:153155,down,download,153155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-60/cacheCopy/SR00c.HG02489.txt.gz.tbi; 1608597475239,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148193,down,download,148193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121969,down,download,121969,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi; 1608597042942,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz; 1608597045131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:27846,down,download,27846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi; 1608596998999,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz; 1608597001436,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:16630,down,download,16630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz.tbi; 1608597384680,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz.tbi; 1608597386501,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:121342,down,download,121342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz.tbi; 1608596974147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz.tbi; 1608596976528,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:10400,down,download,10400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:90805,down,download,90805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi; 1608597297393,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz.tbi; 1608597299791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz; 1608597301211,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:97652,down,download,97652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:74587,down,download,74587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi; 1608597629983,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-46/cacheCopy/SR00c.HG02010.txt.gz.tbi; 1608597632017,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:190780,down,download,190780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"86c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi; 1608597281347,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-70/cacheCopy/SR00c.HG02855.txt.gz; 1608597283995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:93299,down,download,93299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"8:33:10,121] [â†[38;5;220mwarnâ†[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [â†[38;5;220mwarnâ†[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [â†[38;5;2m4e20eafcâ†[0m:hello]: â†[38;5;5mecho 'Hello String!'â†[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [â†[38;5;2m4e20eafcâ†[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [â†[38;5;1merrorâ†[0m] BackendCallExecutionActor [â†[38;5 ;2m4e20eafcâ†[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDisp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:7952,error,error,7952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['error'],['error']
Availability,"8d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.low_genotyping_quality_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.sex_mismatch_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,35] [info] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main; /788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_cov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:36508,failure,failures,36508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"9-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignments: printHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:2308,down,down,2308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['down'],['down']
Availability,929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsert,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5661,error,error,5661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['error'],['error']
Availability,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:3137,error,error,3137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,4,"['down', 'error']","['downloading', 'error']"
Availability,"9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files.log""; },; ""start"": ""2017-01-30T19:00:03.896Z""; }]; },; ""outputs"": {. },; ""workflowRoot"": ""/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/"",; ""id"": ""c386672d-0248-4968-9b1a-114f5f5c4706"",; ""inputs"": {...; },; ""submission"": ""2017-01-30T19:00:00.796Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:2539,failure,failures,2539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,2,"['error', 'failure']","['error', 'failures']"
Availability,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:2019,ERROR,ERROR,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,1,['ERROR'],['ERROR']
Availability,: cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:1644,Failure,Failure,1644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['Failure'],['Failure']
Availability,:+1: (minus the failure),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/57#issuecomment-119694172:16,failure,failure,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/57#issuecomment-119694172,1,['failure'],['failure']
Availability,":+1: It seems there's a potential to reduce some of the boilerplate around instances dealing with `ExpressionElement`. It seems like those instances exist to refine the type down to the ""leaf"" level where the more specific type can do its thing. I would try to eliminate one of these and see if you can parameterize the callers with a `[T]` or `[T <: ExpressionElement]` to save you from the trouble of specializing/refining/narrowing/casting (not sure the right word) the type yourself. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911:174,down,down,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911,1,['down'],['down']
Availability,:+1: Thank you @delocalizer this is great! ðŸ˜„ Can you please squash down to one commit before we merge?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2049/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732:67,down,down,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732,1,['down'],['down']
Availability,":+1: although I could not agree more with @geoffjentry's sentiments on slimming down `WorkflowDescriptor`, I think that's better done in pluggable backend PRs or maybe completely separate PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-190957501:80,down,down,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-190957501,1,['down'],['down']
Availability,:+1: barring the test failure... I looked into it briefly but I didn't figure anything out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/573/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609:22,failure,failure,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609,1,['failure'],['failure']
Availability,:+1: total failure. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2203/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333,1,['failure'],['failure']
Availability,":+1:. Very nice! I need to go back and re-read this after my brain cools down, but the tests give me a lot of confidence it's all working. ðŸ™‚ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1969/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986:73,down,down,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986,1,['down'],['down']
Availability,":-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = tru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2688,failure,failures,2688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability,":. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2145,Error,ErrorOr,2145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:1159,error,error,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['error']
Availability,":09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-3/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:4178,echo,echo,4178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7388,Error,ErrorOr,7388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,":14:47,48] [info] Checkpoint end - txts: 101676; [2022-12-15 21:14:47,72] [info] Checkpoint start; [2022-12-15 21:14:47,72] [info] checkpointClose start; [2022-12-15 21:14:47,72] [info] checkpointClose synched; [2022-12-15 21:14:47,78] [info] checkpointClose script done; [2022-12-15 21:14:47,78] [info] dataFileCache commit start; [2022-12-15 21:14:47,79] [info] dataFileCache commit end; [2022-12-15 21:14:47,84] [info] checkpointClose end; [2022-12-15 21:14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5280,Checkpoint,Checkpoint,5280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:47,84] [info] Checkpoint end - txts: 101746; [2022-12-15 21:14:47,84] [info] Checkpoint start; [2022-12-15 21:14:47,84] [info] checkpointClose start; [2022-12-15 21:14:47,84] [info] checkpointClose synched; [2022-12-15 21:14:47,89] [info] checkpointClose script done; [2022-12-15 21:14:47,89] [info] dataFileCache commit start; [2022-12-15 21:14:47,90] [info] dataFileCache commit end; [2022-12-15 21:14:47,92] [info] checkpointClose end; [2022-12-15 21:14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:5737,Checkpoint,Checkpoint,5737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:47,93] [info] Checkpoint end - txts: 101748; [2022-12-15 21:14:49,99] [info] Checkpoint start; [2022-12-15 21:14:49,99] [info] checkpointClose start; [2022-12-15 21:14:49,99] [info] checkpointClose synched; [2022-12-15 21:14:50,05] [info] checkpointClose script done; [2022-12-15 21:14:50,06] [info] dataFileCache commit start; [2022-12-15 21:14:50,06] [info] dataFileCache commit end; [2022-12-15 21:14:50,08] [info] checkpointClose end; [2022-12-15 21:14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6194,Checkpoint,Checkpoint,6194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:50,09] [info] Checkpoint end - txts: 101803; [2022-12-15 21:14:50,10] [info] Checkpoint start; [2022-12-15 21:14:50,10] [info] checkpointClose start; [2022-12-15 21:14:50,10] [info] checkpointClose synched; [2022-12-15 21:14:50,18] [info] checkpointClose script done; [2022-12-15 21:14:50,18] [info] dataFileCache commit start; [2022-12-15 21:14:50,18] [info] dataFileCache commit end; [2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:6651,Checkpoint,Checkpoint,6651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,02] [info] Checkpoint end - txts: 101887; [2022-12-15 21:14:51,05] [info] Checkpoint start; [2022-12-15 21:14:51,05] [info] checkpointClose start; [2022-12-15 21:14:51,06] [info] checkpointClose synched; [2022-12-15 21:14:51,08] [info] checkpointClose script done; [2022-12-15 21:14:51,08] [info] dataFileCache commit start; [2022-12-15 21:14:51,31] [info] dataFileCache commit end; [2022-12-15 21:14:51,35] [info] checkpointClose end; [2022-12-15 21:14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:9721,Checkpoint,Checkpoint,9721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,35] [info] Checkpoint end - txts: 101957; [2022-12-15 21:14:51,35] [info] Checkpoint start; [2022-12-15 21:14:51,35] [info] checkpointClose start; [2022-12-15 21:14:51,35] [info] checkpointClose synched; [2022-12-15 21:14:51,38] [info] checkpointClose script done; [2022-12-15 21:14:51,38] [info] dataFileCache commit start; [2022-12-15 21:14:51,38] [info] dataFileCache commit end; [2022-12-15 21:14:51,41] [info] checkpointClose end; [2022-12-15 21:14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10178,Checkpoint,Checkpoint,10178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,41] [info] Checkpoint end - txts: 101959; [2022-12-15 21:14:51,63] [info] Checkpoint start; [2022-12-15 21:14:51,63] [info] checkpointClose start; [2022-12-15 21:14:51,63] [info] checkpointClose synched; [2022-12-15 21:14:51,67] [info] checkpointClose script done; [2022-12-15 21:14:51,67] [info] dataFileCache commit start; [2022-12-15 21:14:51,68] [info] dataFileCache commit end; [2022-12-15 21:14:51,70] [info] checkpointClose end; [2022-12-15 21:14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:10635,Checkpoint,Checkpoint,10635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,71] [info] Checkpoint end - txts: 102014; [2022-12-15 21:14:51,72] [info] Checkpoint start; [2022-12-15 21:14:51,72] [info] checkpointClose start; [2022-12-15 21:14:51,72] [info] checkpointClose synched; [2022-12-15 21:14:51,76] [info] checkpointClose script done; [2022-12-15 21:14:51,76] [info] dataFileCache commit start; [2022-12-15 21:14:51,76] [info] dataFileCache commit end; [2022-12-15 21:14:51,79] [info] checkpointClose end; [2022-12-15 21:14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11092,Checkpoint,Checkpoint,11092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,":14:51,79] [info] Checkpoint end - txts: 102077; [2022-12-15 21:14:51,80] [info] Checkpoint start; [2022-12-15 21:14:51,80] [info] checkpointClose start; [2022-12-15 21:14:51,80] [info] checkpointClose synched; [2022-12-15 21:14:51,85] [info] checkpointClose script done; [2022-12-15 21:14:51,85] [info] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11549,Checkpoint,Checkpoint,11549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['Checkpoint'],['Checkpoint']
Availability,:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:11196,Error,ErrorHandling,11196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['Error'],['ErrorHandling']
Availability,:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4374,Error,ErrorHandling,4374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Error'],['ErrorHandling']
Availability,:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.sho,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:4523,Error,ErrorHandling,4523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['Error'],['ErrorHandling']
Availability,":30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1400,down,down,1400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['down'],['down']
Availability,":40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] [info] 1 new workflows fetched; [2017-12-05 09:40:30,69] [info] WorkflowManagerActor Starting workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] WorkflowManagerActor Successfully started WorkflowActor-6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 09:40:31,66] [error] WorkflowManagerActor Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b1; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b2; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b1; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.b2; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:2377,error,error,2377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['error'],['error']
Availability,":469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14519,error,error,14519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,":47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2234,failure,failure,2234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['failure'],['failure']
Availability,":48:20,484 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Starting workflow UUID(075e0cf3-194b-4f53-a43d-d31f0b370f79); 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Successfully started WorkflowActor-075e0cf3-194b-4f53-a43d-d31f0b370f79; 2021-09-27 13:48:20,511 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2021-09-27 13:48:20,547 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; Sep 27, 2021 1:48:20 PM com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 2021-09-27 13:48:21,326 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Parsing workflow as WDL draft-2; 2021-09-27 13:48:22,359 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - MaterializeWorkflowDescriptorActor [UUID(075e0cf3)]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; 2021-09-27 13:48:24,671 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:9916,error,error,9916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['error'],['error']
Availability,":491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3934,error,error,3934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['error'],['error']
Availability,":52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99474,failure,failure,99474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,":54,70] [info] BT-322 12ceda02:test.task_A:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; [2022-10-06 15:14:54,70] [info] 12ceda02-4906-4840-80a2-514af3ccb801-EngineJobExecutionActor-test.task_A:NA:1 [ESC[38;5;2m12ceda02ESC[0m]: Could not copy a suitable cache hit for 12ceda02:test.task_A:-1:1. No copy attempts were made. Based on [StackOverflow, the issue seems to be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2203,echo,echo,2203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['echo'],['echo']
Availability,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16484,down,down,16484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,12,['down'],['down']
Availability,":56:30,264 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - 384e88c5-eba8-400c-aaef-5d618ffdce88-SubWorkflowActor-SubWorkflow-SplitRG:-1:1 [UUID(384e88c5)]: Starting calls: SplitLargeRG.SamSplitter:NA:1; 2018-01-17 20:56:30,293 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: `set -e; mkdir output_dir. total_reads=$(samtools view -c /cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam). java -Dsamjdk.compression_level=2 -Xms3000m -jar /usr/gitc/picard.jar SplitSamByNumberOfReads \; INPUT=/cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam \; OUTPUT=output_dir \; SPLIT_TO_N_READS=48000000 \; TOTAL_READS_IN_INPUT=$total_reads`; 2018-01-17 20:56:36,955 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: job id: operations/EOvc7beQLBiwi6fk-aX5yBEgqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:56:48,780 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: Status change from - to Running; ```. Here is the scattered [SomaticPairedSingleSampleWf.scattered.txt](https://github.com/broadinstitute/cromwell/files/1641317/SomaticPairedSingleSampleWf.scattered.txt) runnable version that gets stuck running. and the non scattered [SomaticPairedSingleSampleWf.single.txt](https://github.com/broadinstitute/cromwell/files/1641318/SomaticPairedSingleSampleWf.single.txt) runnable version that works great. Here is the dependencies zip [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1641320/SomaticPairedSingleSampleWfDependencies.zip). @kcibul i was asked to ping you on this issue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:12252,ping,ping,12252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['ping'],['ping']
Availability,:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1218,recover,recover,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,":NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13084,error,error,13084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,"['Error', 'error']","['Error', 'error']"
Availability,; 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:7688,robust,robustExecuteOrRecover,7688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['robust'],['robustExecuteOrRecover']
Availability,"; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:441); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:365); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:307); 	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:293); 	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:270); 	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:266); 	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:95); 	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java); 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:352); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:37439,ERROR,ERROR,37439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipelin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:1737,down,download,1737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['down'],['download']
Availability,"; > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwell.backend.standard.St",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1281,Fault,FaultHandling,1281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,"; Int max_retries_or_default = select_first([max_retries, 2]). Boolean compress = select_first([compress_vcfs, false]); Boolean run_ob_filter = select_first([run_orientation_bias_mixture_model_filter, false]); Boolean make_bamout_or_default = select_first([make_bamout, false]); Boolean run_funcotator_or_default = select_first([run_funcotator, false]); Boolean filter_funcotations_or_default = select_first([filter_funcotations, true]). # Disk sizes used for dynamic sizing; Int ref_size = ceil(size(ref_fasta, ""GB"") + size(ref_dict, ""GB"") + size(ref_fai, ""GB"")); Int tumor_reads_size = ceil(size(tumor_reads, ""GB"") + size(tumor_reads_index, ""GB"")); Int gnomad_vcf_size = if defined(gnomad) then ceil(size(gnomad, ""GB"")) else 0; Int normal_reads_size = if defined(normal_reads) then ceil(size(normal_reads, ""GB"") + size(normal_reads_index, ""GB"")) else 0. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = if defined(funco_data_sources_tar_gz) then ceil(size(funco_data_sources_tar_gz, ""GB"") * 3) else 100; Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, ""GB"")) else 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size + select_first([emergency_extra_disk,0]). # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = basename(basename(tumor_reads, "".bam""),"".cram"") #hacky way to strip either .bam or .cram; String unfiltered_name = output_basename + ""-unfiltered""; String filtered_name = output_basename + ""-filtered""; String funcotated_name = output_basename + ""-funcotated"". String output_vcf_name = output_basename + "".vcf"". Int tumor_cram_to_bam_disk = ceil(tumor_reads_size * cram_to_bam_multiplier); Int normal_cram_to_bam_disk = ceil(normal_reads_size * cram_to_bam_multiplier). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:9983,down,downloads,9983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['down'],['downloads']
Availability,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7667,down,down,7667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,26,['down'],['down']
Availability,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:7388,down,down,7388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,13,['down'],['down']
Availability,"; ```java; 2017-02-22 13:49:48,448 cromwell-system-akka.dispatchers.api-dispatcher-23 INFO - Workflow 56b1f228-8054-4947-8dc3-372363c5e94b submitted.; 2017-02-22 13:49:57,791 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - 1 new workflows fetched; 2017-02-22 13:49:57,792 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Starting workflow UUID(56b1f228-8054-4947-8dc3-372363c5e94b); 2017-02-22 13:49:57,798 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Successfully started WorkflowActor-56b1f228-8054-4947-8dc3-372363c5e94b; 2017-02-22 13:49:57,799 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-02-22 13:49:58,220 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - MaterializeWorkflowDescriptorActor [UUID(56b1f228)]: Call-to-Backend assignments: ; 2017-02-22 13:49:58,373 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - Unexpected engine failure; java.lang.RuntimeException: Unexpected engine failure; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:384); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:77); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:75); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:1176,ERROR,ERROR,1176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:1754,error,error,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['error'],['error']
Availability,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Bug is full covered here:; https://github.com/sbt/sbt/issues/7117. The error I was getting was this:; ```; Exception in thread ""sbt-socket-server"" java.lang.NoClassDefFoundError: Could not initialize class org.scalasbt.ipcsocket.JNIUnixDomainSocketLibraryProvider; 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.get(UnixDomainSocketLibraryProvider.java:26); 	at org.scalasbt.ipcsocket.UnixDomainSocketLibraryProvider.maxSocketLength(UnixDomainSocketLibraryProvider.java:31); 	at sbt.internal.server.Server$$anon$1$$anon$2.$anonfun$run$1(Server.scala:75); 	at scala.util.Try$.apply(Try.scala:213); 	at sbt.internal.server.Server$$anon$1$$anon$2.run(Server.scala:63); Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.UnsatisfiedLinkError: darwin/aarch64/libsbtipcsocket.dylib not found on classpath [in thread ""main""]; ```; <!-- Which backend are you running? -->; This was trying to load set 1.10.2 in the cromwell directory. So I edited project/build.properties to have. sbt.version=1.8.2. And everything worked. Share I submit a PR with that change?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7564:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7564,1,['error'],['error']
Availability,"<!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Validating a workflow with circular imports causes a stack overflow in womtool. ## Expected behavior; I'm the fool who wrote a workflow with circular imports, but if womtool is here to check for errors, I'd like for it to suggest exactly how I'm being foolish. Something like miniwdl's output would work, where it follows the trail of imports and eventually says ""hey, this could be circular.""; ```; >miniwdl check ../fairyland/ld-pruning/ld-pruning-wf.wdl. (../fairyland/ld-pruning/ld-pruning-wf.wdl Ln 5 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:300,error,errors,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['error'],['errors']
Availability,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4086:215,error,error,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086,3,['error'],['error']
Availability,"<!-- Which backend are you running? -->; Backend: AWS backend. Link to Jira: https://broadworkbench.atlassian.net/browse/CROM-6712. Issue: ; From time to time I get the following error running a workflow on AWS. ```java; Could not build the path \""s3://bean-cromwell/cromwell-execution\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Failures: \ns3: Unable to load region from any of the providers in the chain software.amazon.awssdk.regions.providers.DefaultAwsRegionProviderChain@7c812b7e: [software.amazon.awssdk.regions.providers.SystemSettingsRegionProvider@759440a5: Unable to load region from system settings. Region must be specified either via environment variable (AWS_REGION) or system property (aws.region)., software.amazon.awssdk.regions.providers.AwsProfileRegionProvider@2a8c03c1: No region provided in profile: default, software.amazon.awssdk.regions.providers.InstanceProfileRegionProvider@1bafe7dd: Unable to contact EC2 metadata service.] (SdkClientException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; ```. This usually happens to one task generated from a scatter task.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6233:179,error,error,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6233,2,"['Failure', 'error']","['Failures', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:891,error,error,891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:917,error,error,917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:798,error,error,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:842,echo,echo,842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['echo'],['echo']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend: Local. Several basic array functions are not working on optional arrays. Test code passes wdltool validate with no errors.; Tested types: Array[String]? and Array[Int]?; ### Length(); WDL code:; ```; Array[String]? strings; Int num = length(strings); ```; Error:; ```; [2018-10-08 13:12:09,55] [error] WorkflowManagerActor Workflow 3dfb9c92-4e2e-4754-a35e-cfcbf9d6c006 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.num:; length() expects one parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:767,error,errors,767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,3,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:1263,error,error,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Hi all,. When running a workflow with `write_objects(Array[Struct])` in a task, the workflow fails with **runtime** error `Failed to evaluate input 'out' (reason 1 of 1): Failed to write_objects(...) (reason 1 of 1): Cannot TSV serialize a Array[WomCompositeType {\n a -> String \n}] (valid types are Array[Primitive], Array[Array[Primitive]], or Array[Object])` **if the array is empty**. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->; ```wdl; version 1.0. struct Input {; String a; }. workflow Test {; call test; }. task test {; input {; Array[Input] inputs = []; }. File out = write_objects(inputs). command {; cat '~{out}'; }. runtime {; docker: 'debian:stable-slim'; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Our Cromwell build is `37-a52c415-SNAP` (config available upon request)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595:718,error,error,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595,2,"['avail', 'error']","['available', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:685,error,error,685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,3,"['Error', 'error']","['Error', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->; We are looking for a general guidance regarding how to use AWS EFS system with cromwell. ; Specifically, guidance of setup cromwell to recognize mounted EFS files in the backend and run jobs on AWS batch. . Details of what we have attempted to run the workflow using EFS on AWS:; - We tried specifying a aws EFS file system as one of the filesystems both within backend and engine constructs in addition to S3. But we get this error: ""Cannot find a filesystem with name efs in the configuration. Available filesystems: ftp, s3, demo-dos, gcs, oss, http"". If I just specify EFS, Cromwell does not start, errors out looking for S3. - Also, the EFS is mounted on my AWS batch computes. How do I specify the mount point to the container(I am asking this because, I donâ€™t have control over creating AWS job definitions)?. We have checked lots of resources online but could not find any. And we have tried to ask questions on gatk forum with no luck: https://gatkforums.broadinstitute.org/wdl/discussion/23380/using-aws-backend-with-efs. Searched github issues, we found a similar issue opened here 7 days ago with no answer #4579 AWS backend with own source path to mount. Thanks for looking into this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4602:932,error,error,932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602,3,"['Avail', 'error']","['Available', 'error', 'errors']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:862,error,error,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:812,error,error,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Default application.json not found in classpath in precompiled jar on github (affecting multiple releases tested version 86 and 85). ; Tested to not be affected version 79/56. Main error below. Should be an easy fix. . ```; Exception in thread ""main"" com.typesafe.config.ConfigException$IO: application: application.conf: java.io.IOException: resource not found on classpath: application.conf, application.json: java.io.IOException: resource not found on classpath: application.json, application.properties: java.io.IOException: resource not found on classpath: application.properties; at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:236); at com.typesafe.config.impl.ConfigImpl.parseResourcesAnySyntax(ConfigImpl.java:133); at com.typesafe.config.ConfigFactory.parseResourcesAnySyntax(ConfigFactory.java:1083); at com.typesafe.config.impl.SimpleIncluder.includeResourceWithoutFallback(SimpleIncluder.java:123); at com.typesafe.config.impl.SimpleIncluder.includeResources(SimpleIncluder.java:109); at com.typesafe.config.impl.ConfigParser$ParseContext.parseInclude(ConfigParser.java:181); at com.typesafe.config.impl.ConfigParser$ParseContext.parseObject(ConfigParser.java:237); at com.typesafe.config.impl.ConfigParser$ParseContext.parseValue(ConfigParser.java:103); at com.typesafe.config.impl.ConfigParser$ParseContext.parse(ConfigParser.java:415); at com.typesafe.config.impl.ConfigParser.parse(ConfigParser.java:25); at com.typesafe.config.impl.Parseable.rawParseValue(Parseable.java:263); at com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:652,error,error,652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hi!. We've been looking into migrating from PAPIv2 backend to GCPBATCH backend. Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:950,error,error,950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; The GCP batch backend preemption handling seems to have issue. When preemption happens the job had very high possibility to be error. The typical error would be : time=â€œâ€¦â€ level=error msg=â€œerror waiting for container:â€ . It will take the preempt events as the error from Cromwell logs. However, in the google batch console, it shows clearly ""preemption notice has received and will be processed"". . <!-- Which backend are you running? -->; GCP batch ; <!-- Paste/Attach your workflow if possible: -->; The workflow works perfectly in GCP life science backend; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407:598,error,error,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407,5,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:553,error,error,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,3,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi,. Since last week, our cromwell server instance on GCP started to encounter the following error in all the jobs:. ```; 2024-07-31 19:08:59 cromwell-system-akka.dispatchers.backend-dispatcher-35 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:09:33 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:10:06 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); ```. However, with `Unknown Error` message, I don't know where to go for help. Do you have any suggestion?. Here are the configurations:. * Cromwell v85; * Genomics API; * PAPIv2 with `actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""`. Many thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:459,error,error,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,11,"['Error', 'error', 'failure']","['Error', 'error', 'failures']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I believe there is an error in the information provided by the Reference Disk Support in the documents for using reference disk images in gcpbatch. I believe there is a ""["" missing or this bit is additional as there are 3 ""["" present in the example. When trying to run this on my cromwell config, I get a syntax error. This is regarding using GCPBatch and not PAPI V2 as mentioned in some examples:. This is what I think it should look like:. ``` ; reference-disk-localization-manifests = [ ; {; ""imageIdentifier"": ""projects/pmc-bdc-private-test-cromwell/global/images/omics-reference-disk-image"",; ""diskSizeGb"": 10, ; ""files"": [ ; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""crc32c"": 2158779318; },; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""crc32c"": 420322484; },; {; ""path"": ""pmc-bdc-test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""crc32c"": 1970999569; }; ]; }; ] ; ```; Not sure if reference-disk-localization = [] is also correct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7486:388,error,error,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7486,2,['error'],['error']
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. I'm using Cromwell v87 on GCP Genomics API. When submitting a job, the error I got is the following:. ```; Caused by: java.lang.IllegalStateException: You are currently running with version 2.2.0 of google-api-client. You need at least version 1.31.1 of google-api-client to run version 1.32.1 of the Genomics API library.; at com.google.common.base.Preconditions.checkState(Preconditions.java:534); at com.google.api.client.util.Preconditions.checkState(Preconditions.java:113); at com.google.api.services.genomics.v2alpha1.Genomics.<clinit>(Genomics.java:44); ... 12 common frames omitted; ```. It seems that I need to downgrade the version of `google-api-client`. However, I don't know how to do it on my machine. Could anyone help? Thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481:437,error,error,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481,2,"['down', 'error']","['downgrade', 'error']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; Hi, I would like to get help on troubleshooting.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; I am running the workflow [here](https://github.com/PacificBiosciences/pb-human-wgs-workflow-wdl) using Cromwell 73-04a69e5 on Azure with GA4GH TES backend.; The workflow fails with the following stack trace, but I couldn't find a clue what specific tasks/jobs Cromwell had the issues with. The backend doesn't look to log any failures. Would you guide me how to identify the tasks/jobs where the error occurred? Thanks!. > Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: 2022-09-01 16:44:46,864 cromwell-system-akka.dispatchers.engine-dispatcher-27538 INFO - **WorkflowManagerActor: Workflow 2cd0993c-94df-4663-923d-48bbce3feead failed (during ExecutingWorkflowState): java.lang.Exception: The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.**; Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:275); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:209); Sep 1 16:44:47 vmce33268581 container_name/cromwellazure_cromwell_1[2296]: #011at scala.PartialFunction$OrElse.apply(Parti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:848,failure,failures,848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,2,"['error', 'failure']","['error', 'failures']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I am using `checkpointFile` in the `runtime` section of a WDL `task`. . I accidentally included a space in the checkpoint file name, and I see in the logs that this (probably) breaks checkpointing. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Log file shows; ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical monocyte_H_shard0.csv; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/monocyte_H_shard0.csv': No such file or directory; cp: can't create 'monocyte_H_shard0.csv-tmp/noise_prompting_classical': No such file or directory; CHECKPOINTING: Uploading new checkpoint content; ```. <!-- Which backend are you running? -->; Running on GCP via Terra. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When I remove the space in the filename, I see this in the logs, which appears to be working fine:. ```; CHECKPOINTING: Making local copy of /cromwell_root/noise_prompting_classical_monocyte_H_shard0.csv; CHECKPOINTING: Uploading new checkpoint content; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:378,checkpoint,checkpointFile,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,9,"['CHECKPOINT', 'checkpoint']","['CHECKPOINTING', 'checkpoint', 'checkpointFile', 'checkpointing']"
Availability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I think the minimum to reproduce the bug is just. ```; Array[File] foo = []; Array[String]? bar = foo; ```. which fails with. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 'bar' (reason 1 of 1): Evaluating foo failed: assertion failed: base member type WomMaybeEmptyArrayType(WomStringType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ],; ```. Interestingly enough, this passes if the array is non-empty, or if the target is not optional, or if the source is type `Array[String]`. I am running cromwell ""v85 (ish)"" according to the administrator. Backend is AWS batch.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7399:498,failure,failures,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7399,1,['failure'],['failures']
Availability,"<< being discussed in Google Doc, copy here when it settles down >>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1342:60,down,down,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1342,1,['down'],['down']
Availability,"= 8BB8C81C27BFD2533FC9743A70F55DB1, file = 51C3D11209F9A7985345B2FD76E1C699.; [2022-12-15 21:28:23,82] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:4:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,86] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:0:1-20000000038 [788d8048main.all_qced_sample_lists:0:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,86] [info] BT-322 788d8048:main.all_qced_sample_lists:0:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 801EC388A847FBAB78685AE96643853A.; [2022-12-15 21:28:23,86] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:0:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(5), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(1), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(3), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(2), attempt 1); [2022-12-15 21:28:26,55] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:46110,failure,failures,46110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"= [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c504",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:1615,echo,echo,1615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"= {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `defaultSchemaName` are all defined for the HSQLDB database so maybe it is something there?. Anyway it also does not run properly outside of the test configuration. Running cromwell (compiled from this branch) with the following configuration:; ```; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:""; }; ```. Gives the following error:; ```; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:941); at org.sqlite.core.DB.newSQLException(DB.java:953); at org.sqlite.core.DB.throwex(DB.java:918); ```; Which is rather non-descript. . I have been digging for the entire afternoon, but I can not find the root cause. The other database types seem to work fine without lots of additional configuration, so I did not get much inspiration from that. Could somebody help me out? I will then do all the rest of the work, write the documentation etc. . Thanks!. Best regards,; Ruben",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5453:2390,error,error,2390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453,3,['error'],['error']
Availability,"=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); out5d4c4459=""${tmpDir}/out.$$"" err5d4c4459=""${tmpDir}/err.$$""; mkfifo ""$out5d4c4459"" ""$err5d4c4459""; trap 'rm ""$out5d4c4459"" ""$err5d4c4459""' EXIT; tee '/cromwell_root/stdout' < ""$out5d4c4459"" &; tee '/cromwell_root/stderr' < ""$err5d4c4459"" >&2 &; (; cd /cromwell_root. /app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq; ) > ""$out5d4c4459"" 2> ""$err5d4c4459""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 2> /dev/null ) || ( ln /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-c36f18b89b7c5f50e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:5175,echo,echo,5175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"=/). Task (stuff removed):. ```; task ValidateSamFile {; File input_bam; File? input_bam_index; String report_filename; File? ref_dict; File? ref_fasta; File? ref_fasta_index; Int? max_output; Array[String]? ignore; Int disk_size; Int preemptible_tries. command {; java -Xmx4000m -jar stuff.jar blah; }; runtime {; memory: ""7 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File report = ""${report_filename}""; }; }; ```. Call (note there is no value supplied for max_output):. ```; call ValidateSamFile as ValidateReadGroupSamFile {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:1283,ERROR,ERROR,1283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['ERROR'],['ERROR']
Availability,"=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:2042,failure,failures,2042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failures']
Availability,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hiï¼ŒHave you solved this problem ï¼Ÿ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:212,reliab,reliable,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657,1,['reliab'],['reliable']
Availability,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:166,failure,failures,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038,1,['failure'],['failures']
Availability,"> > Looks very promising! ðŸ˜„; > > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788; > ; > Thanks for your review!; > In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok. We have provided a new custom ECS image with local `ubuntu:latest` docker image(`src/ci/bin/test_bcs.inc.sh`). Please run Centaur CI for a new test. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308:281,failure,failures,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308,2,"['avail', 'failure']","['available', 'failures']"
Availability,"> @alartin You're spot on in regards to array jobs. Unfortunately the internal design of Cromwell makes it difficult to do this, it's come up before for HPC backends as well. Something we should address some day but it'd be a fairly major undertaking. Hi @geoffjentry I wonder if there is any guide for developer, especially for the backend impl. Or is there some doc/slide available for that? I am willing to implement backend for other public cloud vendor and curious about how to get it started quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480:374,avail,available,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480,2,['avail'],['available']
Availability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:220,error,errors,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['error'],['errors']
Availability,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119,4,"['avail', 'error']","['available', 'error', 'errored']"
Availability,"> A couple of other observations:; > ; > 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded. I like that a bit less, since in Travis, if one of the subbuilds failed after 1 second, you can independently restart it immediately. But in CircleCI you would need to wait for all subbuilds to finish in order for it to let you restart the failed onces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564:55,failure,failures,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564,1,['failure'],['failures']
Availability,"> Also have this error, using Cromwell 52, installed using this manual :; > ; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > ; > logs say : fetch_and_run.is is a directory. Extra info : cloning job & resubmitting through aws console runs fine. so it seems to be a temporary issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613,1,['error'],['error']
Availability,"> Are there any config properties which you know of that might help with this?. Not that I can think of unfortunately :/ One quick thing we could maybe do before the fixing it ""the right way"" would be to enable retries at the GCS library level. We've disabled it because we have our own retry mechanism which is more reliable and asynchronous (but WDL functions couldn't use it so far, which is why they're not retried). We're about to release Cromwell 30 imminently so I don't think this can make it before then but we could consider hotfixing it if this is really becoming urgent. Edit: actually looking at it more closely, even though we don't supply an ""retry settings"" to the GCS library they have default ones allowing for 6 attempts. However like I said we've found their retries to not always be reliable so it might be that for some particular errors it's not retried at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774:317,reliab,reliable,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774,3,"['error', 'reliab']","['errors', 'reliable']"
Availability,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:517,down,download,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634,2,['down'],['download']
Availability,"> Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory"". Also have this error. Anyone figure out what the issue is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269:126,error,error,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269,1,['error'],['error']
Availability,"> Did we update the akka http library in the last few days?. FYI, the import resolver, that during centaur tries to download the non-existent file from GitHub, [doesn't use akka-http](https://github.com/broadinstitute/cromwell/blob/49/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L191) like most of cromwell/cromiam/centaur/etc. It uses yet another jvm http client called [sttp](https://github.com/softwaremill/sttp#readme). Still, I think what you're running into is that GitHub changed their 404 response. See `curl -i https://raw.githubusercontent.com/broadinstitute/cromwell/develop/my_workflow`. I don't know how stable the change is either... they may change the body of the 404 response again [without notice](https://xkcd.com/1172/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242:116,down,download,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242,1,['down'],['download']
Availability,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:239,down,downloads,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['down'],['downloads']
Availability,"> FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied.; > ; > Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275:327,failure,failures,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275,1,['failure'],['failures']
Availability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:25,avail,available,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,1,['avail'],['available']
Availability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. I built cromwell 88. Then I ran two large preemptible runs, one with cromwell 87 and the other with 88. The e2 with standard machines ran faster, and was pre-empted less, than the n1 with custom machines.; 11% faster, and 424 preemptions vs 276. Many of the 424 were preemptions that happened really early in the run process. So it may be that I just had bad luck with someone kicking off a large non-spot run while my 88 run was going but teh 87 was finished. But I've been advised that custom machine types are much more likely to be preempted than standard ones. Would it be possible for you to default to n1 or n2 standard machines, rather than custom ones?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654:25,avail,available,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654,1,['avail'],['available']
Availability,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:897,error,error,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,1,['error'],['error']
Availability,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:527,alive,alive,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,3,['alive'],['alive']
Availability,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > [â€¦](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:313,down,down,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,2,['down'],['down']
Availability,"> Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code.; Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry. @cjllanwarne I found the problem. I did not have a `final_workflow_outputs_dir` set in the options.json files for the centaur tests. If this path is not set `use_relative_output_paths` is of course not used... :man_facepalming: That is fixed now and it works as expected. Colliding outputs will return as a workflow failure. Since I got the local testing working I was able to add more advanced tests and make sure these are correct as well. The error message is tested when the outputs are colliding. In order for that test to work I had to make the output order of colliding paths in the error message deterministic. (Otherwise it would fail randomly). Also my colleague @DavyCats showed me some centaur tests were file outputs are tested. I used these as an example to also test for the outputs. ; All the behavior that this PR affects is now properly tested, which means that these tests should be able to discover regressions in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876:121,failure,failure,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876,4,"['error', 'failure']","['error', 'failure']"
Availability,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:77,error,errors,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402,1,['error'],['errors']
Availability,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:329,error,errors,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,2,['error'],"['error', 'errors']"
Availability,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,3,"['error', 'failure']","['error', 'failure']"
Availability,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:567,down,down,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746,1,['down'],['down']
Availability,"> I have tried Float memory_gb = 1.0 as the runtime attribute and ${""-l mem="" + memory_gb + ""GB""} as the submit string but this fails with qsub: Illegal attribute or resource value Resource_List.mem error. `Int memory = 1` is the equivalent of `Int memory_b = 1` and is generating values in **bytes**. A WDL specifying gigs of memory will therefore generate very large values, with 4GB generating the string `-l mem=4294967296""GB""`. If you navigate within the cromwell-executions directory and find the `submit*` files that contain the generated qsub command, you should see something like that. `cd` to the directory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB â€¦` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g â€¦` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb â€¦` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; St",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:199,error,error,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,2,['error'],['error']
Availability,> I wasn't sure how the sbt magic you put in for the API docs worked. Pass the word on. Currently available both in the updated [wiki](https://github.com/broadinstitute/cromwell/wiki/DevZone#generating-a-markdown-document-of-the-swagger-yaml) and the more comprehensive mega doc-on-docs in our [team drive](https://drive.google.com/drive/u/1/folders/0By3wA7o30lk3VmF3aldBNkEzaDg). EDIT: Maybe we can leave a link at the top of the cromwell.yaml?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690:98,avail,available,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690,1,['avail'],['available']
Availability,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1005,error,error-prone,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936,1,['error'],['error-prone']
Availability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:653,error,error,653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['error'],['error']
Availability,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:400,down,down,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156,1,['down'],['down']
Availability,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:225,avail,available,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,4,['avail'],['available']
Availability,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,recover,recovery,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208,1,['recover'],['recovery']
Availability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:153,error,error,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['error'],['error']
Availability,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing ðŸ“– (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... ðŸ¤”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:816,avail,available,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755,1,['avail'],['available']
Availability,> Just curious if putting a sleep [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L341) or [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L198) are viable solutions until something more generic in the core of Cromwell can be implemented. Any update and interim solution available? I am curious if there is an alternative like python aiohttp concurrent requests number limit rather than sleep.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911:555,avail,available,555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911,1,['avail'],['available']
Availability,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:104,failure,failure,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921,3,"['error', 'failure']","['errors', 'failure']"
Availability,"> Looks very promising! ðŸ˜„; > ; > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > ; > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788. Thanks for your review!; In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732:283,failure,failures,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732,2,"['avail', 'failure']","['available', 'failures']"
Availability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:138,reliab,reliability,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"> OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario. Actually, that work has already been done and merged into develop. The goal of this ticket was to verify that 1.000.000 rows chosen as default limit is a sane choice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679:130,resilien,resilient,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679,1,['resilien'],['resilient']
Availability,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; â­© â†“ â­¨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,redundant,redundant,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['redundant'],['redundant']
Availability,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655,2,['error'],['error']
Availability,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:200,failure,failure,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,1,['failure'],['failure']
Availability,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:282,error,error,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732,1,['error'],['error']
Availability,"> Thanks for the report, we'll check it out.; > ; > It seems that something about running inside Cromwell causes the tool to attempt allocating `18446744073709550532 bytes`, or 18.45 exabytes!. Hiï¼Œi also got this problem while running the gmap on Linux shell ,it shows that is need such memory.; Failed attempt to alloc 18446744073709551600 bytes; Exception: Allocation Failed raised at indexdb.c:2886; Segmentation fault; .Could you please tell me how you solve this problem ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-598648632:416,fault,fault,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-598648632,1,['fault'],['fault']
Availability,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:97,avail,available,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140,2,['avail'],['available']
Availability,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:112,error,errors,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,2,['error'],['errors']
Availability,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:251,down,down-featured,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500,2,['down'],['down-featured']
Availability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:1114,rollback,rollbackException,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,3,"['error', 'rollback']","['error', 'rollbackException']"
Availability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:707,error,error,707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,1,['error'],['error']
Availability,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:977,down,down,977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['down'],['down']
Availability,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:358,repair,repair,358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,1,['repair'],['repair']
Availability,"> We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?. Interesting, I'm running a local build from current `develop` and I seem to have the code you've linked above, but I don't see either the `""cromwell-workflow-id""` or `""goog-batch-worker""` labels on my task logs ðŸ¤”. . In `.labels` I have `hostname`, `job_uid`, `task_group_name`, and `task_id` keys.; In `.resource.labels` I have `job_id`, `location`, and `resource_container` keys. For the proposed additional labels, with respect to `GcpBatchRequestFactoryImpl#createAllocationPolicy`:. - Root workflow id is in `data.createParameters.jobDescriptor.workflowDescriptor.rootWorkflowId`.; - Everything else is in the `BackendJobDescriptorKey` via `data.createParameters.jobDescriptor.key`:; - task name in `call.identifier.localName` (I think); - shard in `index`; - attempt in `attempt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046:307,avail,available,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046,1,['avail'],['available']
Availability,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978,1,['error'],['error']
Availability,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:328,ERROR,ERROR,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"> Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. For the ""docker pull failed during a very large scatter"" case, retries are pretty much always successful (except when the image actually doesn't exist of course). Would it be reasonable for this to be handled, especially since this isn't a rare failure when scattering more than 300x?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263:91,error,errors,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263,2,"['error', 'failure']","['errors', 'failure']"
Availability,"> every backend should have the option to return one. :+1: . > Add â€œreason(s) for failureâ€ to database, metadata. :+1:, but happy if storing strings waits for a future ticket",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184756416:82,failure,failure,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184756416,1,['failure'],['failure']
Availability,"> it is trying to resubmit jobs to the local engine. Do you mean jobs that were running when you stopped Cromwell were ""restarted"" on the local backend ?; Or new downstream jobs for the same workflow were then submitted to the local backend ?; Or both ?. I imagine you did not change your configuration in between the stop/start ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539:162,down,downstream,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539,1,['down'],['downstream']
Availability,"> it seems like we would want to continue seeing it; @aednichols; The bug in Life Sci API is that the ssh server is supposed to be disabled on the VM, but in some cases it is not, causing the `address already in use` problem. Since the ssh server is not disabled, ssh access to the VM is in fact possible. The error then becomes meaningless: the dockerized ssh server is unrelated to the wdl workflow, and users can still ssh to the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597:310,error,error,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597,1,['error'],['error']
Availability,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:298,alive,alive,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929,5,"['alive', 'down', 'ping']","['alive', 'down', 'ping']"
Availability,> not sure why this was showing all green with only one review ðŸ¤”. PullApprove audits are always available via the `code-review/pullapprove` [Details](https://pullapprove.com/broadinstitute/cromwell/pull-request/3691/) links. In this case the change fell into `groups.one_reviewer` because of `groups.two_reviewers.conditions.files.exclude: centaur/*`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770:96,avail,available,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770,1,['avail'],['available']
Availability,> pinning to a known good version is a good idea regardless. Agreed and I'd even say that embedding the cwltool executable in Cromwell is what we should do. Or whatever makes sense such that when I download the latest Cromwell release and try to run a CWL I don't get an error because I don't have cwltool or I have the wrong version.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263:198,down,download,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263,2,"['down', 'error']","['download', 'error']"
Availability,">Does ""5xx HTTP Status Code"" literally appear in real life instead of the actual 500-511 error codes?. No but I think the idea is to make sure the regex that checks for ""5"" matches ""500"" but not ""5 apples""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876,1,['error'],['error']
Availability,>Merging despite the slurm test failure because:. I have considered and endorse this decision.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719:32,failure,failure,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719,1,['failure'],['failure']
Availability,">This change appears to validate the format of the disk requirements but then do nothing with the actual values? Is that correct?. AWS has auto-sizing and auto-expanding disks, so the concept of specifying a disk size is not applicable in this universe. This PR lets Cromwell ignore everything after `local-disk` instead of issuing an error. >Can we update the test cases which now work? I suspect custom_mount_point at least could be re-enabled?. `custom_mount_point` is not on the excluded list in `testCentaurAws.sh`. Are you requesting new coverage by adding `awsbatch` to the backends for `custom_mount_point.test`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441:335,error,error,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441,1,['error'],['error']
Availability,"@AlexMTX @rhpvorderman Am I able to specify the following in a task using draft-2?; ```; meta {; volatile: ""true""; } ; ```. I found that call-caching still occurred for the task with this volatile: ""true"" included in the meta. Is this only available for version 1.0 or am I misunderstanding your issue Alex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651:240,avail,available,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651,1,['avail'],['available']
Availability,"@DavyCats thank you for the report. Would you mind posting the WDL that you're using to induce the error? (Or if it contains sensitive information, a minimal repro case.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927#issuecomment-407519270:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927#issuecomment-407519270,1,['error'],['error']
Availability,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:64,alive,alive,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['alive'],['alive']
Availability,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:558,alive,alive,558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['alive'],['alive']
Availability,"@EvanTheB `reference.conf` is the configuration file used by Cromwell.; > Also is there any way to actually enumerate all the available settings?. I am not sure how we can do that. But you are right, it might be a lot of effort to restructure/modify those files to be able to enumerate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687,1,['avail'],['available']
Availability,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['error'],['error']
Availability,"@Horneth - thanks for the information - this works. . So I had organized my WDL tasks into folders just for organizational purposes (just like you had posted above). Sorry if you have answered this before, but is this going to be supported down the road? Or should is it always going to be recommended to have all WDL tasks are in one folder at the same level? . Thanks again for getting back to me so quickly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-374289503:240,down,down,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-374289503,1,['down'],['down']
Availability,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:59,failure,failures,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728,6,['failure'],"['failure', 'failures']"
Availability,@Horneth I came here to update the latest failure and noticed your comment. Considering that @kshakir noted Liquibase weirdness in #4320 perhaps these are related in some way?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823,1,['failure'],['failure']
Availability,"@Horneth I don't believe this has been improved any further, is that true? Do you have any sense of how often users encounter this error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768,1,['error'],['error']
Availability,"@Horneth I don't think so, although I'm pretty sure what would work would be to add a case similar to the one on 481, but catching `UnsuccessfulRunStatus` w/ the isPreemptible if. I didn't originally go down that road because at the time I thought it wouldn't work, but I misunderstood how things were working and I'd forgotten about it by the end. I'm going to give that a whirl as it'd be a lot easier than digging into the op metadata",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358785007:203,down,down,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358785007,1,['down'],['down']
Availability,@Horneth I have run into the disk full error several times and the workflow appropriately fails,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716,1,['error'],['error']
Availability,"@Horneth I just noticed - this is going to try forever, right? What happens if this error is being thrown appropriately?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4272#issuecomment-431714560:84,error,error,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4272#issuecomment-431714560,1,['error'],['error']
Availability,"@Horneth I pruned my number of jobs dramatically.... I am now down to <5k. And I am still getting this error and very slow call caching. @katevoss Can we tell google, if the crashes are on their end?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460:62,down,down,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460,2,"['down', 'error']","['down', 'error']"
Availability,"@Horneth I think you're right that most reasonable execution backends should have _some_ notion which could be viewed as a numeric return code. Perhaps ""return code"" isn't the right term there, maybe something like ""error code"" would be more apropos. Pinging @mcovarr @gauravs90 and @francares here in case they have thoughts on the matter.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184752207:216,error,error,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184752207,2,"['Ping', 'error']","['Pinging', 'error']"
Availability,@Horneth I totally missed your message here. It was on the methods cromwell 30 instance. I'm not sure how to find logs. My guess is that if they're produced by default they're still available somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854:182,avail,available,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854,1,['avail'],['available']
Availability,"@Horneth I tried it with call-caching based on file path only, and while it was significantly faster than it used to be, it still got bogged down. I restarted it with call-caching turned off completely, and then things seemed to work quite well. . It's running over 1000 jobs with no ""Cromwell Overhead"" in the timing diagram. So I would say this works when call-caching is turned off, but with call-caching it is still slow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596:141,down,down,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596,1,['down'],['down']
Availability,@Horneth Just pinging to see what the status is on this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284:14,ping,pinging,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284,1,['ping'],['pinging']
Availability,@Horneth Out of curiosity has this been addressed. We are using v28.2 and seem to get a lot of these errors when doing `size` lookups,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348286795:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348286795,1,['error'],['errors']
Availability,"@Horneth Per our discussion this morning, it was already a blocking call, this just makes retrieving the return code more robust in case of disconnect.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3411#issuecomment-373500372:122,robust,robust,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3411#issuecomment-373500372,1,['robust'],['robust']
Availability,"@Horneth Thanks for letting me know. It would be great if this was prioritized, we are unable to fully do call caching at the moment, so having 503 errors can get quite expensive for us. Are there any config properties which you know of that might help with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128:148,error,errors,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128,1,['error'],['errors']
Availability,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:205,fault,fault,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,4,"['down', 'error', 'fault', 'toler']","['down', 'error', 'fault', 'tolerance']"
Availability,"@Horneth What, if anything, are the downsides of this change?. IOW is this creating a tradeoff where we're deciding this improvement is overall better than the negative change elsewhere, or is this purely positive?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422898224:36,down,downsides,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422898224,1,['down'],['downsides']
Availability,"@Horneth although we're already mapping something else to preemptible in the spot where i'm doing it now, so this would mean we're mapping ""other"" errors to preemptible in 2 different spots",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358788264:147,error,errors,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358788264,1,['error'],['errors']
Availability,@Horneth and @mcovarr can you review when you get a chance? This should now fix the Tyburn failures on the `develop` branch,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191825712:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191825712,1,['failure'],['failures']
Availability,@Horneth available for review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/238#issuecomment-148163666:9,avail,available,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/238#issuecomment-148163666,1,['avail'],['available']
Availability,"@Horneth check out DSDEEPB-2876. Having thought about this and having seen that new issue in our backlog (Add â€œreason(s) for failureâ€ to database, metadata) I think this would be better reported as a stringly-typed ""reason for failure"" error message rather than an explicit ""backend return code"" field. It would then be the job of the backend to convert its ""backend error code"" into a useable and sensible message to report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747:125,failure,failure,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747,4,"['error', 'failure']","['error', 'failure']"
Availability,@Horneth commented on [Mon Sep 11 2017](https://github.com/broadinstitute/wdl4s/issues/204). CWL has a lot of meta information that is currently not available in WOM.; This could be part of a larger work and include re-work of WDL runtime attributes but those bits need to make it to Cromwell somehow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2726:149,avail,available,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2726,1,['avail'],['available']
Availability,"@Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48). ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261072803). The attached wdl results in an error message:. `Workflow has invalid declarations: : AggregatedException: : VariableNotFoundException: Variable 'generateArray' not found`. [scratch_3.wdl.txt](https://github.com/broadinstitute/wdl4s/files/595927/scratch_3.wdl.txt). ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261089538). @meganshand . This is actually a different problem - Cromwell doesn't support (yet) Workflow Declarations that reference call outputs. This wouldn't work either:. ```; task t {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:246,error,error,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,2,"['echo', 'error']","['echo', 'error']"
Availability,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:831,echo,echo,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"@Horneth my suggestion for this PR (happy to be overruled - @geoffjentry @kcibul) would be to get the return code being ONLY the return code. And forget the ""backend return code"" entirely for now until that upcoming ticket (maybe enhance that other ticket to add ""JES return codes appear in failure message"" as another AC?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682:291,failure,failure,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682,1,['failure'],['failure']
Availability,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:93,avail,available,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010,2,['avail'],['available']
Availability,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:117,error,error,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,1,['error'],['error']
Availability,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:359,error,error,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,1,['error'],['error']
Availability,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:274,error,errors,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858,3,"['error', 'ping']","['errors', 'ping']"
Availability,@LeeTL1220 Do you still encounter this error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773,1,['error'],['error']
Availability,"@LeeTL1220 Does the error have the effect on the actual final status of the workflow ? Or does it cause cromwell to exit with a non 0 exit code ? I wasn't able to reproduce the exact same error but I've had similar ones and I've got a branch that should fix it, if you want to try it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429,2,['error'],['error']
Availability,"@LeeTL1220 I just ran the following WDL against `PAPI` and `Local` it worked for both, could you confirm that it fails against SGE?. ```; task foo {; Int? mem; Int final_mem = select_first([mem, 3]). command {; echo hello world; }. runtime {; memory: final_mem + "" GB""; docker: ""ubuntu:latest""; }. output {; Int five = 5; }; }. workflow bar {; call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643#issuecomment-330975479:211,echo,echo,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643#issuecomment-330975479,1,['echo'],['echo']
Availability,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875,2,['echo'],['echo']
Availability,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:43,failure,failure,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648,1,['failure'],['failure']
Availability,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:306,error,errors,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116,3,"['down', 'error', 'failure']","['downstream', 'errors', 'failure']"
Availability,@LeeTL1220 I'm unable to reproduce the error you saw. Have you seen the same issue with more recent versions of Cromwell? What version were you using when you saw the initial error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298,2,['error'],['error']
Availability,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:115,error,errors,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"@LeeTL1220 we have made number of improvements with transient errors in JES, Cromwell (in version 26) will now retry transient errors automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871,2,['error'],['errors']
Availability,"@LeeTL1220 when you get the ""file not found"" errors, does it fail the workflow? Or does it still continue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991,1,['error'],['errors']
Availability,@MartonKN This is almost certainly not a real error but rather some annoying/alarming yet harmless Cromwell messages. Other than this does it appear that your workflow successfully completed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613,1,['error'],['error']
Availability,@MartonKN this error should have been addressed in Cromwell v33 -- please reopen this issue if needed. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-402242786:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-402242786,1,['error'],['error']
Availability,"@MatthewMah commented on [Thu Jun 15 2017](https://github.com/broadinstitute/wdltool/issues/32). The following example passes validation, and I think it should not. I think validation should be able to identify that a nonexistent output field is trying to be read. . ```; workflow ShouldNotValidate{; 	call A{}; 	Array[File] simple = [A.nonexistent]; 	call B{ input:; 		in = simple; 	}; }. task A{; 	command{; 		echo ""A"" > out; 	}; 	output{; 		File out = ""out""; 	}; }. task B{; 	Array[File] in. 	command{; 		cat ${sep=' ' in}; 	}; 	output{; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2881:412,echo,echo,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2881,1,['echo'],['echo']
Availability,@SHuang-Broad Thank you for reporting this. All these execution errors seem to indicate silent copying failures. Have you always seen this percentage of cp style failures or this has been recent?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424945872:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424945872,3,"['error', 'failure']","['errors', 'failures']"
Availability,"@TMiguelT . I separated out the part that is failing into a separate WDL and tried running just that WDL with different inputs, it all failed with the same error message. The WDL is attached so that you can just run to see the error. [cromwell_4356.zip](https://github.com/broadinstitute/cromwell/files/2555124/cromwell_4356.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756,2,['error'],['error']
Availability,"@TMiguelT . The error is happening at `FilterByOrientationBias` stage. Not the `CollectSequencingArtifactMetrics` stage. `CollectSequencingArtifactMetrics` is the previous task where the input file is coming from (`FilterByOrientationBias` uses output files from `CollectSequencingArtifactMetrics` stage) I'm re-pasting the whole error ( as I previously split it out into two) for clarification. ```; [2018-11-02 17:24:33,42] [info] AwsBatchAsyncBackendJobExecutionActor [1651349bSomaticSNVInDel.FilterByOrientationBias:1:1]: gatk FilterByOrientationBias \; \; -V /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,2,['error'],['error']
Availability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:1599,error,error,1599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,3,"['Error', 'error']","['Error', 'error']"
Availability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:305,error,error,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['error'],['error']
Availability,"@TMiguelT I've moved the singularity cache section back down. Hopefully this is all the comments actioned. Sorry @vsoch, I realised I've been resolving your comments as I actioned them, but I probably should've left them open as they're from your review. Also sorry for what was probably a lot of email spam over the past few hours.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558:56,down,down,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558,1,['down'],['down']
Availability,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1878,echo,echo,1878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,4,"['alive', 'echo', 'error']","['alive', 'echo', 'error']"
Availability,"@Xophmeister Ultimately this is going to need changes to the WDL spec and thus a conversation over in the [OpenWDL group](www.openwdl.org), although your issue does start entering the murky grey area between WDL and Cromwell (which admittedly exists due to WDL's heritage as originally being a product of the Cromwell team). You're able to reference values in `runtime` because the WDL spec allows it. However, there's no notion in WDL of `default_runtime_attributes`, that's a purely Cromwell concept. Thus if Cromwell were to allow this, it'd encourage WDLs which would fail elsewhere. . My own $0.02 is that `runtime` is horribly broken and needs to be rebuilt from the ground up (again, over in OpenWDL, not here). Since history has shown that there's always some path to fixing `runtime` which **doesn't** require my burn it all down approach a good first step might be to suggest an official system of defaults over there. I'm going to close this issue as I don't believe there's anything reasonable we can do from the Cromwell side. If, given the context I provided above, you disagree (e.g. perhaps I misunderstood the ask) let me know and we might reopen it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741#issuecomment-472412608:834,down,down,834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741#issuecomment-472412608,1,['down'],['down']
Availability,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:200,error,error,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503,1,['error'],['error']
Availability,"@aednichols All the removals of the `new Integer(n)` calls are changes I made ""while I was in there"" to silence warnings. What with the other errors, I lost track of my intention to check in on whether it was reasonable to change that stuff. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796:142,error,errors,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796,1,['error'],['errors']
Availability,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:481,down,download,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,1,['down'],['download']
Availability,"@aednichols I want to reopen, because Map[String, MyStruct] also crashes and because if you claim it is about types than the error should appear when I validate with latest womtool and not in a runtime!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925,1,['error'],['error']
Availability,"@aednichols Thanks for the new invitation. I did accept it, but when I try to push a branch to this repo I get error 403.; Am I doing something wrong?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424:111,error,error,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424,1,['error'],['error']
Availability,"@aednichols and @rsasch - yes, `Integer.MIN_VALUE` is some huge negative value. I tested with a fetchSize of ""1"" and got the same out of memory errors as when it was 1000. I don't know whether `Integer.MIN_VALUE` is a special sentinel value or any value below 0 would do.... but since it works, I'm inclined to treat it as a magic number, and document it as such.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136:144,error,errors,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136,1,['error'],['errors']
Availability,@aednichols are the Travis CI failures concerning?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754,1,['failure'],['failures']
Availability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,4,['error'],['error']
Availability,"@aednichols thanks for a detailed look into this. With the latest dev branch, and with the commit that contains the fix, I'm getting the below error. I can still run unpacked without error. ```; (p3) [jeremiah@localhost fail_cromwell]$ /usr/lib/jvm/java-11-openjdk/bin/java -Dconfig.file=/home/jeremiah/code/fresh/really/cromwell/cromwell.example.backends/cromwell.examples.conf --illegal-access=warn -jar /home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar run test_wf_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0; [2019-04-18 17:19:09,95] [info] Running with database db.url = jdbc:hsqldb:mem:39c64473-526e-47d6-a015-f9193a0fd4f4;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:143,error,error,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,2,['error'],['error']
Availability,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:92,down,downstream,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481,1,['down'],['downstream']
Availability,"@alexfrieden - I just ran this successfully. With your first error, the fact that you are getting an HTTP response indicates that it's not necessarily a networking issue on the AWS side, and as @Horneth stated something going on with DockerHub. For debugging in that scenario, I would try launching a t2.micro based on the CustomAMI you created with the same Batch instance profile in the VPC used by the Batch compute environment and try `docker pull <image>`. As for your second issue, did your tasks eventually transition from RUNNABLE? I've noticed that sometimes it takes about 5-10min for the Batch scheduler to ""re-warm"" after scaling down instances that were used for previous tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443,2,"['down', 'error']","['down', 'error']"
Availability,@andy7i Those are FireCloud errors because of the resulting missing status. GAWB-1645 has been opened to be more resilient on our side (and thus make those errors go away); this ticket is for Cromwell to deal with why the statuses are missing in the first place.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976:28,error,errors,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976,3,"['error', 'resilien']","['errors', 'resilient']"
Availability,@andy7i pinging you as this is also a great perfomance test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397:8,ping,pinging,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397,1,['ping'],['pinging']
Availability,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:30,ping,ping,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773,2,['ping'],['ping']
Availability,"@anton-khodak commented on [Wed Jan 25 2017](https://github.com/broadinstitute/wdltool/issues/22). I use `wdltool` to parse descriptions from the main repository, for instance, [this one](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ValidateBamsWf_170107.wdl) and [this](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_dsde_workflows/ConvertPairedFastQToUnmappedBamWf_170107.wdl) . Both descriptions have valid syntax (`validate` and `parse` run smoothly). However, when I run `highlight` on either of them, I get the following error:; ``` ; $ java -jar ~/Downloads/wdltool-0.8.jar highlight ""/media/anton/ECFA959BFA95631E/Programming/wdl2cwl/ValidateBamsWf_170107.wdl"" console. Exception in thread ""main"" scala.MatchError: [Declaration type=Array[File] name=validation_reports expr=Some(ValidateBAM.validation_report)] (of class wdl4s.WorkflowOutput); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatScope(SyntaxFormatter.scala:188); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$4.applyOrElse(SyntaxFormatter.scala:153); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:141); at scala.PartialFunction$$anonfun$runWith$1.apply(PartialFunction.scala:140); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.collect(TraversableLike.scala:271); at scala.collection.AbstractTraversable.collect(Traversable.scala:104); at wdl4s.formatter.SyntaxFormatter.wdl4s$formatter$SyntaxFormatter$$formatWorkflow(SyntaxFormatter.scala:153); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.applyOrElse(SyntaxFormatter.scala:73); at wdl4s.formatter.SyntaxFormatter$$anonfun$2.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2878:580,error,error,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2878,2,"['Down', 'error']","['Downloads', 'error']"
Availability,"@antonkulaga -. 1. If you use read_json the type you get will actually be a WDL `Object`. You should be able to coerce to map but only if the values are all the right type. Also look out for better `object` support with draft 3's `struct`s. 2. you're right, some kind of list comprehension (equivalent to a scala map function) has certainly come up before (they'd need to be spec changes in openWDL, but I'd certainly be happy to see those sorts of PR popping up). 3. for values available outside scatters - it's just like call outputs - we add implicit gatherers for Declarations too. The syntax is ugly but hopefully it works for you until we get some sugar around it in the spec via openwdl ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367483310:479,avail,available,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367483310,1,['avail'],['available']
Availability,"@antonkulaga We always welcome PRs! It's not obligated though, even if you use BioWDL :wink: . On-topic: Yes, I think Cromwell could modify the input folder and its contents to be read-only . But that might have some unforeseen consequences down the line. This would need to be tested.; DISCLAIMER: I am not of the cromwell team. So I will not implement this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438:241,down,down,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438,1,['down'],['down']
Availability,"@bfoster-lbl - An example config with the parameters you highlighted is available [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#cromwell-server). That said, I agree that the tutorial in cromwell.readthedocs.io should match.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509:72,avail,available,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509,1,['avail'],['available']
Availability,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999,1,['error'],['error']
Availability,"@chapmanb Somatic completed successfully by bumping the memory (I doubled it to 8GB) :); I have another question about the rnaseq pipeline if you don't mind.; I'm hitting this error on the `pipeline_summary` task:. ```; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/cyvcf2/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/loca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['error'],['error']
Availability,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:160,echo,echo,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981,2,['echo'],['echo']
Availability,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,redundant,redundant,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517,1,['redundant'],['redundant']
Availability,"@cjllanwarne @Horneth I'll just add that we're going to need to start slimming down WorkflowDescriptor and removing its coupling to Backend, so while if it's the only way to do something for now that's one thing but if it's a matter of how one skins the proverbial cat going another pat would be good",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873:79,down,down,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873,1,['down'],['down']
Availability,"@cjllanwarne @aednichols Looks like the tests didnâ€™t complete with the same error, I gave merging from develop another crack without success. Hopefully itâ€™s all good though ðŸ˜¬",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203,1,['error'],['error']
Availability,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,redundant,redundant,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492,2,['redundant'],['redundant']
Availability,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:88,error,errors,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961,2,['error'],"['error', 'errors']"
Availability,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:531,error,errors,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,1,['error'],['errors']
Availability,"@cjllanwarne Does this only require handling the retryable failure in the WorkflowExecutionActor, by re-running the failed call up to a `max` number of retries? Your previous comment somehow indicates this is perhaps more than this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-216631299:59,failure,failure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-216631299,1,['failure'],['failure']
Availability,@cjllanwarne I can't find it now (perhaps it has since been closed as a won't fix) but this has come up before. If you're open to trying to track it down close one as a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287:149,down,down,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287,1,['down'],['down']
Availability,"@cjllanwarne I considered doing this at first and I think I actually tried to implement it, but there was a fundamental inconsistency that I couldn't figure out how to resolve. It might have had to do with going from the `WomLong` type back to a WDL type, but I hit so many errors along the way that I may be mixing them up. The current implementation is what I eventually settled on as the least invasive (not necessarily the most elegant). If the current approach is a deal-breaker I can take another look, but I suspect the blast radius will be larger no matter what. My hope was that CWL users wouldn't actually notice or care about the underlying JVM type - is there a use case for large integer arrays in Cromwell?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335:274,error,errors,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335,1,['error'],['errors']
Availability,"@cjllanwarne I disagree. IMO officially supported backends should be provided by the downloadable fat jar, we should not require a user to need to download multiple jars if all they want to do is fart around w/ the officially supported stuff.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208423531:85,down,downloadable,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208423531,2,['down'],"['download', 'downloadable']"
Availability,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707,1,['failure'],['failure']
Availability,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:31,error,errors,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015,3,"['down', 'error']","['down', 'errors']"
Availability,"@cjllanwarne I know we've added interpolation for certain instances, is this still relevant? I can add this to our list of doc things to fix, but I agree that adding an error/warning would be good too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219:169,error,error,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219,1,['error'],['error']
Availability,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584,1,['error'],['error']
Availability,"@cjllanwarne I'd prefer to see it talking to a single point (currently a single VA, as @Horneth & I discussed, if performance, reliability or complexity call for it, to be morphed into something else). I don't care that much if it happens here, but it's not an onerous change to make on this PR. I _will_ care once something else is talking to a VA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254:127,reliab,reliability,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254,1,['reliab'],['reliability']
Availability,@cjllanwarne I'm going to go out on a limb and say that the test failure had nothing to do with this change. Just a guess.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133:65,failure,failure,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133,1,['failure'],['failure']
Availability,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:395,down,down,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066,2,['down'],['down']
Availability,"@cjllanwarne I've run into this issue also, while I can work around it currently, it will likely become an issue down the road when trying to build workflows comprising of scatter operations when you want to collect up the results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4538#issuecomment-482190924:113,down,down,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538#issuecomment-482190924,1,['down'],['down']
Availability,"@cjllanwarne Indeed different containers might have different requirements, as it will depend on the container what mount points are available. I would like to point out, however, that you can already define this per task using a custom runtime attribute. For example, in my config I could put something like:; ```; runtime-attributes= """"""; String? docker; String? dockerMountPoint = ""/data""; """"""; dockerRoot = ""${dockerMountPoint}""; ```. EDIT: Hmm, nevermind, looks like that wouldn't work. In the submission command, it would, but you'll just end up with `${dockerMountPoint}` in the execution script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684:133,avail,available,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684,1,['avail'],['available']
Availability,"@cjllanwarne Initially the concern was that I've heard, well, concern from firecloud folks here and there about frivolous metadata migrations on our part as they cause downtime they don't always want to incur. It also ties in to my rapidly increasing sentiment that we should not be migrating a supposedly immutable data store for what are merely cosmetic changes - if we have to in order to fix something we know is causing issues w/ programatic clients you gotta do what you gotta do. When we talked with @abaumann he indicated that a) they wouldn't really care if the old data stayed as-is and b) made a similar point as I did about the event store. @Horneth with all of these taken together I don't think we should be migrating. The key constituency has said that it would be at best superfluous and IMO it's part of a habit we should be getting out of anyways.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322329894:168,downtime,downtime,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322329894,1,['downtime'],['downtime']
Availability,"@cjllanwarne Looking into the WorkflowActor I found this comment . ```; // The assumption for restart is that the call will have been found in state Running. Explicitly; // re-writing the Starting status causes the new start date to be persisted, which is desirable to; // provide a realistic elapsed execution time.; ```. which seems contradictory with DSDEEPB-2127..; This PR doesn't change the current behavior. I found a bug indirectly related to this though where the resumable and non-resumable calls where not processed correctly leading to weird DB errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/449#issuecomment-184922009:557,error,errors,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/449#issuecomment-184922009,1,['error'],['errors']
Availability,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744,1,['error'],['error']
Availability,@cjllanwarne Perhaps one is spinning down and the next one has already triggered?. @kcibul :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185928699:37,down,down,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185928699,1,['down'],['down']
Availability,"@cjllanwarne Really it's just that they were developed simultaneously, and it's probably my fault for not going back and cleaning the other one up. In retrospect I knew it existed, but probably just lost track of it. I certainly wouldn't disapprove of converting future-based logic in actors into a more actor-y solution but that's because IMO it's easier to reason about multiple actors (and their messages) than composed Futures. My stance isn't one which is universally held, however",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998:92,fault,fault,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998,1,['fault'],['fault']
Availability,"@cjllanwarne Thanks for looking into it! Our current use case is heavily relying on `""additionalQueryResultFields"": [""labels""]`, with this error we have to loop through all workflows and make X thousands requests each time :( And I assume JMUI is relying on `""additionalQueryResultFields"": ""parentWorkflowId""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-455263017:139,error,error,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-455263017,1,['error'],['error']
Availability,"@cjllanwarne Thanks for picking up those two errors, I fixed them. Should be all set now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235,1,['error'],['errors']
Availability,"@cjllanwarne Thanks for the explanation! Exactly right, this is what the script says: ; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp; cd /cromwell_root. echo ""Hello foobar!"" && exit 1; echo $? > job.rc.txt. @pgrosu The exit 1 was the purpose here - I was modifying the basic hello.wdl test, trying to do a lightweight test simulating the failure of the binary being called. Turns out, as @cjllanwarne explains, that exit 1 is not a good way to simulate that, because it defeats Cromwell's return-code monitoring. Here's a better test, which does work as expected: . task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && head nonexistent; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792:212,echo,echo,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792,4,"['echo', 'failure']","['echo', 'failure']"
Availability,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,2,['error'],['error']
Availability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:136,failure,failures,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,11,"['Error', 'error', 'failure', 'reliab']","['Error', 'error', 'errors', 'failure', 'failures', 'reliably']"
Availability,"@cjllanwarne Would it make sense to send a `0L` metric for `workflowArchiveTotalTimeFailureMetricPath` in [Success cases here](https://github.com/broadinstitute/cromwell/blob/80cfe3e4b653c5ab6f2f935c9b306b34cd4287c9/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataSchedulerActor.scala#L77-L99) so that the graph line can come back to 0 upon a success in the below graph? Currently after a failure metric, it stays at the x seconds and never comes back down. ![Screen Shot 2021-04-28 at 12 28 15 PM](https://user-images.githubusercontent.com/16748522/116439527-a757d000-a81d-11eb-8b2e-1d4238aab769.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501:421,failure,failure,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501,2,"['down', 'failure']","['down', 'failure']"
Availability,@cjllanwarne available for review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/237#issuecomment-148164236:13,avail,available,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/237#issuecomment-148164236,1,['avail'],['available']
Availability,"@cjllanwarne commented on [Fri Sep 15 2017](https://github.com/broadinstitute/wdl4s/issues/217). EG this can be converted from WDL to WOM:; ```; import ""import_me.wdl"" as import_me. workflow outer {; ; Array[Int] xs; scatter (x in xs) {; 	Boolean b; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. But if we move the `b` outside the scatter:; ```; import ""import_me.wdl"" as import_me. workflow outer {; Boolean b; Array[Int] xs; scatter (x in xs) {; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. Then we get an error:; ```; Exception in thread ""main"" java.lang.Exception: Can't build WOM executable from WDL namespace:; No input b found evaluating inputs for expression b; key not found: b; ```. ---. @Horneth commented on [Mon Sep 18 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-330214878). This has implications in Cromwell. Namely if `b` was a Call instead of being a boolean, and `import_me.inner` depended on an output of `b`, when we evaluate the inputs of `import_me.inner` it will make a difference whether or not `b` is a sibling of `import_me.inner`. If it is we want to get the output with the same shard number from the output store, otherwise the output with no index (if we rule out nested scatters). We could simplify and say ""always look for the same index and if it's not there take the output with no index"" but it would be better to know for sure which one we need. ---. @mcovarr commented on [Fri Sep 22 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-331568932). Sorry if this is a dumb question, but do you understand what's going wrong here? It's obvious looking at this statically what `b` is supposed to be whether it's inside or outside the scatter. Also it seems a little weird to me that `b` can even be a `GraphInputNode` inside the scatter...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2724:663,error,error,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2724,1,['error'],['error']
Availability,"@cjllanwarne commented on [Mon May 22 2017](https://github.com/broadinstitute/wdl4s/issues/112). This problem presents itself when using `wdltool` but it looks like it's a match error coming from inside `WDL4S`. null.wdl:; ```; task empty{; command {}; output {; File out = ""${output}""; }; }; ```. On validate:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar validate ~/myWorkflows/null.wdl; null; ```. We can see more details when we try to graph it:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar graph ~/myWorkflows/null.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:44); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.expression.ValueEvaluator.replaceInterpolationTag(ValueEvaluator.scala:20); 	at wdl4s.expression.ValueEvaluator.$anonfun$interpolate$2(ValueEvaluator.scala:33); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['error'],['error']
Availability,@cjllanwarne do you think this was resolved by your improvements to JES transient errors?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959:82,error,errors,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959,1,['error'],['errors']
Availability,"@cjllanwarne got it, thanks. I did note that you said ""one class"" before commenting :) The belief **is** that those WFs are getting stuck and it's not just a dilation issue, so it'll be interesting to see if the failures dry up after this merges",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341726:212,failure,failures,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341726,1,['failure'],['failures']
Availability,"@cjllanwarne helped me with this issue but its led to new ones:; https://github.com/broadinstitute/cromwell/issues/5793. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:418,error,error,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,2,['error'],['error']
Availability,@cjllanwarne not having this consume the imports of a workflow has actually lead us to create our own service for describing workfklows as a json schema. having a JSON schema returned is actually quite practical since we can build some pretty UI's around it with existing tools. Is this still low down on the priority list?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4572#issuecomment-557277899:297,down,down,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4572#issuecomment-557277899,1,['down'],['down']
Availability,"@cjllanwarne note that what I just said in #1777 could be used against my disagreement although I think handling all the different bifurcations here would be trickier than the one over there. At the end of the day the number one priority is that a WDL should always produce the exact same result regardless of where it is run (possibly excepting docker, and that's part of what rubs me the wrong way about runtime attrs). The number two priority should be that a user can't possibly take down a server w/ a rogue wdl. . After that anything can be tunable in terms of how a server achieves the first one whilst not allowing the second one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397:488,down,down,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397,1,['down'],['down']
Availability,"@cjllanwarne regarding testing the ""write to cache"" and ""read from cache""... that feature isn't available to this code, it's on another PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332:96,avail,available,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332,1,['avail'],['available']
Availability,"@cjllanwarne that wdl does look right. I was able to reproduce the error successfully using a similar wdl in Papiv2 backend:; ```; task random_words {. String dollar = ""$""; command <<<; cat << 'EOF' > random.txt; This should fail. ${dollar}{Hello} blah blah; EOF; >>>. runtime {; docker: ""ubuntu:latest""; }. output {; String text = read_string(""random.txt""); }; }. workflow read_random_words {; call random_words. output {; String random_text = random_words.text; }; }; ```. The error thrown by Cromwell is:; ```; ERROR - WorkflowManagerActor Workflow a8de1168-8b70-4d54-83f6-aa41adc8f87e failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'read_random_words.random_text' (reason 1 of 1): Evaluating random_words.text failed: key not found: random_words; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:523); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$2(list.scala:66); 	at cats.Eval$.advance(Eval.scala:271); 	at cats.Eval$.loop$1(Eval.scala:350); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:517); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4928#issuecomment-488786242:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4928#issuecomment-488786242,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"@cjllanwarne yes. This won't protect us from returning 40x for workflows that are not in the store that may have legitimately gone terminal, but at least this won't produce errors for workflows that are obviously OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494904639:173,error,errors,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494904639,1,['error'],['errors']
Availability,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:325,ERROR,ERROR,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,4,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"@cjllanwarne, here is the PR. This is only for workflow definitions, and only for line numbers. I found that it is, as you were saying, hard to extract reliable information from Hermes for column numbers. I *would* like to get the entire extent in the source file covered by an AST. It was slow slog to updates the tests to correctly check line numbers. Let's start with this change, and see how it goes. . Thank you,; Ohad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117:152,reliab,reliable,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117,1,['reliab'],['reliable']
Availability,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:131,failure,failure,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926,1,['failure'],['failure']
Availability,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:108,avail,available,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111,2,['avail'],['available']
Availability,"@coreone Merge at will. If the database isn't updated, the new build will crash with an error containing the SQL that needs to be run. The paths to the scripts have also changed, so even if an old jenkins job tries to run the scripts manually, I suspect it would fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974:88,error,error,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974,1,['error'],['error']
Availability,"@cowmoo is there a way for Cromwell to tell from the result of running the ""begin execution"" command, or from the resulting failure, that the task should be retryable?. I like the idea of retrying certain things, but a blanket ""retry everything n times, regardless of problem"" is probably going to annoy more people than it helps (especially if they're paying for it directly in cloud compute!). One thing we previously considered was a ""retryOnStdoutRegex"" attribute, maybe something like:; ```; command {; # ...; }; runtime {; retryOnStdoutRegex: ""path not found:""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555:124,failure,failure,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555,1,['failure'],['failure']
Availability,"@cpavanrun Your right here, but this can be improved. What can be done here is lower the number of parallel jobs submitted by cromwell. This depends really on the cluster. In our case I did a stress test with 10000 parallel jobs and it still acts fine. Only downside is that the log is getting spammed a bit but it still works like it should. Still in the past (on older hardware) the headnode could not deal with this number of jobs. If this is the case limiting the parallel jobs could be a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773:258,down,downside,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773,1,['down'],['downside']
Availability,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:125,robust,robust,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929,2,['robust'],['robust']
Availability,"@danbills OOC when was this screenshot taken ? Last timed I looked at the requester pays failures, I sampled a few and they were due to the alpine issue IIRC so I'd have expected them to stop happening by now.; Last one I see on develop from sentry was Sept 24th here: https://sentry.io/broad-institute/cromwell/issues/650196050/events/?query=ci_env_branch%3Adevelop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429867335:89,failure,failures,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429867335,1,['failure'],['failures']
Availability,@danbills can you explain more about when the error occurs? Do you have an idea of how much effort it would take to fix?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113,1,['error'],['error']
Availability,@danbills commented on [Thu Aug 17 2017](https://github.com/broadinstitute/wdl4s/issues/177). Once it's available it would be nice to have scaladocs published for CWL as they are for WDL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2715:104,avail,available,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2715,1,['avail'],['available']
Availability,"@danbills yes IMO the ""powers of 2"" you have here is the usual and expected form of exponential backoff. The other way is technically exponential but doesn't slow down nearly as quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403:163,down,down,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403,1,['down'],['down']
Availability,"@danxmoran I'm trying to recreate this - could you list the `womtool` version that you're using and a minimal WDL that reproduces the issue?. Note: I tried to recreate on `develop` using this WDL:; ```wdl; version 1.0. import ""not/a/file.wdl"" as oops. workflow foo {; call oops.not_a_thing; }; ```. And received an error message and an exit code of 1:; ```; Failed to import 'not/a/file.wdl' (reason 1 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/cromwell (without escaping Some([...]/cromwell))' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 2 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/bad_import (without escaping None)' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 3 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'not/a/file.wdl' relative to nothing; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176:315,error,error,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176,1,['error'],['error']
Availability,"@davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87). I have the following toy workflow, which I invoke with `-jar cromwell-25.jar run example.wdl empty_inputs.json`. It reads a tsv that has either one column or two and scatters a task over each row of the tsv. The task prints the second column if it is present. When the input `fake.tsv` has one column, everything is fine. However, when it has two columns eg; ```; 1</TAB>1; 2</TAB>2; ```; it fails with ""Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlIntegerType)) with this value: List(WdlInteger(1), WdlInteger(2))"". (*Side question: why is it trying to make a list out of values in two different scattered rows?*) Using `Int?` in the conditional instead of `Int` does not make a difference. Another bizarre twist: if instead of reading in from a file I hardcode the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > â€”; > You are receiving this because yo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:905,error,error,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['error'],['error']
Availability,@davidbernick @hjfbynara would you please confirm update script has been run so that I can rule out pingdom/firewall issues?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659:100,ping,pingdom,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659,1,['ping'],['pingdom']
Availability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:76,down,down,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,2,"['down', 'echo']","['down', 'echo']"
Availability,"@delocalizer Any chance you still have your ""hacky non-async"" piece of code still? The original link you posted is no longer available, and it might be useful for @caross73. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581:125,avail,available,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581,1,['avail'],['available']
Availability,@delocalizer Thanks for tracking this down ! I'm working on a fix. From what you describe and what I've found out so far it looks like it would not work either even if the value is supplied. Have you hit that case as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047:38,down,down,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047,1,['down'],['down']
Availability,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:145,down,downl,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962,1,['down'],['downl']
Availability,"@dgtester Since @geoffjentry just brought me up to speed on the exciting changes coming down the line with #401 (as well as #413), it would probably make more sense to revisit some of these tests afterwards, if they are still pertinent at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501:88,down,down,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501,1,['down'],['down']
Availability,@dianekaplan could you run `gcloud alpha genomics operations describe operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU` and add in anything that looks like an error message or error code?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050,2,['error'],['error']
Availability,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:632,failure,failure,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,1,['failure'],['failure']
Availability,"@dinvlad it was indeed a blind hunt! So in that sense, 204 permissions is not that much ... it's a pretty refined subset ;-) Previously I was running Cromwell with the `editor` role set, which likely has even more than 204 permissions. Without the `firebase.developAdmin` role, the only error I get is that the tasks start running, then they fail immediately, and the only thing you find in the logs is: `yyyy/mm/dd hh:mm:ss Starting container setup.`. In any case, I wanted to give an answer here to provide publicly available information to other users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333:287,error,error,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333,2,"['avail', 'error']","['available', 'error']"
Availability,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... ðŸ¤·. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API ðŸ¤ž",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:292,error,error,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972,4,['error'],"['error', 'errors']"
Availability,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:122,error,errors,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531,1,['error'],['errors']
Availability,"@dshiga -- can you help me understand one piece of your request:; ``` We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way.```. Why exactly does pagination make it unreliable to find the last from a list?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417326254:214,reliab,reliably,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417326254,1,['reliab'],['reliably']
Availability,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:401,error,errors,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['error'],['errors']
Availability,"@dvoet I wondered if you all were overriding that elsewhere. Have you run into issues w/ the loss of number of inserted elements or do you all just not care about that ever?. The function in question (don't have it in front of me) was using `++=` but that's why I was wondering if perhaps there's something else about our slick code which counteracts this. My slick-fu is likely not strong enough, I'll probably need to rely on bigger guns next week. My guess is that this is the culprit. I was looking at the general query log (I've only been using mysql, not cloudsql) and all I saw were individual inserts, never a batch insert. I can't get jprofiler to work reliably running against a JVM on GCS vms (at least not from home) so wasn't even looking at that :). Another thought is that something upstream is actually calling our slick code per-item instead of per-collection but I don't think that's the case. It's at least something I can double check easily.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991:662,reliab,reliably,662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991,1,['reliab'],['reliably']
Availability,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,1,['error'],['error']
Availability,"@eddiebroad commented on [Wed Aug 10 2016](https://github.com/broadinstitute/wdltool/issues/12). I try to use wdltool validate on a WDL and it seems to _not_ catch; an error in the WDL. In the example ""bad"" WDL attached the call to VCF_to_MAF_task; has the line ""inputVCF=inputVCF"" but the ""inputVCF"" exists only; in the task but not at the workflow level ; but invoking wdltool 0.4 on it seems to _NOT_; cause an error. Shouldn't it be saying the WDL has an error; because inputVCF does _not_ exist at the workflow level?. I downloaded the wdltool from the latest release; https://github.com/broadinstitute/wdltool/releases/download/0.4/wdltool-0.4.jar. the two WDLs are attached. [wdl_files.zip](https://github.com/broadinstitute/wdltool/files/412067/wdl_files.zip). ```; wm8b1-75c:red_bug esalinas$ find *.wdl -exec java -jar wdltool-0.4.jar validate {} \;. wm8b1-75c:red_bug esalinas$ diff good.wdl bad.wdl ; 188c188; < inputVCF=inVCF,. ---; > inputVCF=inputVCF,; wm8b1-75c:red_bug esalinas$ ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2876:168,error,error,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2876,5,"['down', 'error']","['download', 'downloaded', 'error']"
Availability,"@egor-broad commented on [Fri Jun 09 2017](https://github.com/broadinstitute/wdltool/issues/31). The sample script is attached below.; Given a task call like that: ; ```; if (condition) {; call select as selectCondition {; input: ; sampleName=name, ; RefFasta=undeclaredVariable, #this variable does not exist; GATK=gatk, ; RefIndex=refIndex, ; RefDict=refDict, ; type=""SNP"", ; rawVCF=haplotypeCaller.rawVCF; }; }; ```; `wdltool inputs` command will generate the inputs just like everything is fine, but since the undeclaredVariable does not exist, when the task is executed, the run will exit with an error. You can try it with the attached script. ; However, this tool http://pb.opensource.epam.com:10000/ is able to notice the error (try and paste the script there and click ""build"", it will say ""Error: Undeclared variable is referenced: 'undeclaredVariable'). . [simpleVariantSelection.txt](https://github.com/broadinstitute/wdltool/files/1064633/simpleVariantSelection.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2880:602,error,error,602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2880,3,"['Error', 'error']","['Error', 'error']"
Availability,@elerch -- I got IntelliJ fired up and can step through to see if I can track down what is happening...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-397402360:78,down,down,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-397402360,1,['down'],['down']
Availability,@ernoc -- just pinging you again to see if theres movement. Thanks,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-440498404:15,ping,pinging,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-440498404,1,['ping'],['pinging']
Availability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:222,error,errors,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['error'],['errors']
Availability,"@ffinfo As I read it:. - If state is `None`, go to `Running` state; - If state is `Running`, the first thing we do is run `isAlive` (which I don't want to ever do!). That means that I have no way to opt-out of ever running `isAlive` (which is the thing I want before approving this PR). ---. What I was suggesting is (but there are many other ways):; - If I set `pollForAliveness: ""1 minute""` in the config file:; - Use `context.system.scheduler.scheduleOnce` to run an `isAlive` 1 minute in the future (completely separate from `pollStatus`).; - If that `isAlive` is true, schedule again another 1 minute in the future; - If not, record the time at which the job was not alive; - `pollStatus` continues on a different schedule:; - If the job is no longer alive, the `pollStatus` switches to `WaitingForReturnCode`; - If a time limit is set for the `WaitingForReturnCode` state, honor it; - If I set `pollForAliveness: false` in the config file:; - Go straight to `WaitingForReturnCode`; - No time limits for `WaitingForReturnCode`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053:672,alive,alive,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053,2,['alive'],['alive']
Availability,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:285,alive,alive,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,3,['alive'],['alive']
Availability,"@francares @cjllanwarne I think the actual issue here is that the ""/tmp"" assertions on lines 260 and 261 always fail on Mac, regardless of whether Docker is available or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582:157,avail,available,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582,1,['avail'],['available']
Availability,"@francares Cool. My main concern there was that when i did the akka http conversion that I ""fixed"" it by giving bad results, so as long as the results look good I'll stand down my fretting :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631:172,down,down,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631,1,['down'],['down']
Availability,"@francares Definitely - the key though is it'll still need to eventually be rebased on top of develop. There are more than one ways to skin this cat but one possibility is that when you're ready you could squash this down to one commit (or some other smaller number) and then rebase those on top of develop, handing the differences there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174601136:217,down,down,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174601136,1,['down'],['down']
Availability,"@francares I'm not saying the engine itself _needs_ to know (although I could envision a scenario where it'd be useful for it to know), I'm saying that the outside world needs to know information particular to the backend. That information needs to be stored somewhere and right now the only somewhere is in the DB which is accessed purely through engine. I see this PR as a transition point - it's against develop which does _not_ have pluggable backends but starts to remove the direct requirements (i.e. the backend specific tables). It's not the final state things will live in but it makes the ultimate changes smaller down the road.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090:624,down,down,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090,1,['down'],['down']
Availability,"@francares reports the following. He said that it disappeared in 29 but it's concerning that it just disappeared without (I think) intentionally being fixed. Reproduce and track this error down in 28 and then demonstrate that a) this *is* actually resolved in 29 and b) it didn't get ""resolved"" via another bug. ```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450,5,"['down', 'error']","['down', 'error']"
Availability,"@francares the result when run on a mac:. ```; ""#!/bin/sh; cd local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello; docker run -w /workingDir -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:/Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:ro -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello:/outputDir --rm ubuntu/latest ; echo /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T/testFile977065604273058878.out; echo $? > rc"" did not contain ""/tmp:"" (HtCondorJobExecutionActorSpec.scala:261); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075:654,echo,echo,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075,2,['echo'],['echo']
Availability,"@freeseek `firebase.developAdmin` is a pretty wide role (with 204 permissions), so it's not surprising that it gives some permissions that are needed here. What would be helpful is if Google showed the exact permissions in their error messages, though from what it seems, that's not always the case. Then if you have a list of permissions, you can find minimal role(s) that encompass those permissions, rather than through a blind hunt (please correct me if it wasn't entirely blind here..). Btw @freeseek, from my limited experience, GitHub issues here are not often-looked-through, it might be better to create an internal JIRA ticket instead ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141,1,['error'],['error']
Availability,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422,1,['error'],['error']
Availability,"@gauravs90 @geoffjentry ; Re JES Backend - there's a _lot_ of hidden complexity in the JES backend that I think could quite easily end up being more than 5 days work to re-implement under another workflow runner. We currently have a Master branch which lets us run hundreds of Genomes-on-the-Cloud jobs concurrently in JES. If this merge goes ahead before the JES backend is made (and is as robust as it currently exists), we LOSE that ability. I don't think we should underestimate this task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137:391,robust,robust,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137,1,['robust'],['robust']
Availability,"@gauravs90 I don't doubt that passing the actorref around everywhere looked ugly. But in terms of moving parts and other such things my bet is that it's worth it. Your description of this scheme sounds a lot more complex (publishing, assuming everything going to metadata will always track w/ state change, having to shoehorn the data side in, etc) than simply passing the metadata down as is typically suggested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687:382,down,down,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687,2,['down'],['down']
Availability,"@gauravs90 a couple of global comments:; - The symbol store and execution store in the old WorkflowActor were not necessarily database-backed. They just stored which calls were completed and which were in flight.; - The graph is a nice idea but currently isn't as eager as it could be. Consider:. ```; A -> B -> C; X -> Y -> Z; ```; - I believe this will run this in pairs, `(A,X)`, `(B,Y)`, `(C,Z)`. But what if A and B are really quick but X and Y are really slow - we're slowed down from executing C because the unrelated tasks X and Y haven't finished yet.; - I think I would prefer the existing method of determining (after every job completes) the set of jobs which have now become runnable. There's already an implicit DAG there. ; - A major reason is, it has already been shown to work with the scatter/gather and other features which update the graph at run-time and I can't see how that would work with this static graph approach?. _NB Sorry all for the repeated almost-identical edits to the above comment. Markdown is hard... :(_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919:481,down,down,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919,1,['down'],['down']
Availability,@gemmalam I tried to create an account but got an error saying I don't have access,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071,1,['error'],['error']
Availability,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:162,error,error,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,4,['error'],['error']
Availability,"@geoffjentry & @Horneth for review please, including the lenthall patch needed to get this cromwell build repaired",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040:106,repair,repaired,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040,1,['repair'],['repaired']
Availability,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:436,error,errors,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897,3,['error'],"['error-caused-by-a-job-submission-failure', 'errors']"
Availability,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:365,avail,available,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660,2,['avail'],['available']
Availability,"@geoffjentry ; Here is an example of a situation:. ```; workflow wf { ; String firstname; String lastname; call Greeting as GreetingFirstName { input: name=firstname }; call Greeting as GreetingLastName { input: name=lastname }; }; task Greeting {; String name; String greeting; command {; echo ""${greeting} ${name}""; }; }; ```. The idea is to make it possible to give a value to greetings for all aliased tasks in json like so:. ```; {; ""wf.firstname"": ""Andrey"",; ""wf.lastname"": ""Smirnov"",; ""wf.Greeting.greeting"": ""howdy""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1335:290,echo,echo,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335,1,['echo'],['echo']
Availability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:128,fault,fault-tolerant,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,2,['fault'],['fault-tolerant']
Availability,@geoffjentry I didn't dig into to the root cause of the error. I launched two Cromwell servers (via the cfn template on the AWS docs page) against the same AWS Batch setup and tested a hello world wdl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804,1,['error'],['error']
Availability,@geoffjentry I had trouble building this pr:. ```; [error] /work/engine/src/main/scala/cromwell/webservice/SwaggerService.scala:3:35: imported `CromwellApiService' is permanently hidden by definition of object CromwellApiService in package webservice; [error] import cromwell.webservice.routes.CromwellApiService; [error] ^; ```. Any ideas?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866,3,['error'],['error']
Availability,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:383,echo,echo,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274,1,['echo'],['echo']
Availability,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,1,['error'],['error']
Availability,"@geoffjentry In this case I don't think it should be in the ExecutionInfos Table which by my understanding is more of a custom bag of key/values for backend specific info (like JES Run ID and JES status).; If this is general enough to be valid for any backend it should be in the EXECUTION table IMO.; @cjllanwarne Makes sense to have a string yes, which JES returns anyway but for now we only extract the error code from it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184757316:406,error,error,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184757316,1,['error'],['error']
Availability,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:479,error,error,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290,1,['error'],['error']
Availability,"@geoffjentry Not really solved. The pipeline could be terminated by the same error, i just extract the samples that are not processed and run it again. It would be better with local MySQL database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294,1,['error'],['error']
Availability,@geoffjentry Thanks for the quick response. Goal here is to enable rapid failure detection. I'm very open to different approaches to this!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215:73,failure,failure,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215,1,['failure'],['failure']
Availability,"@geoffjentry The main advantage of running these test cases daily vs weekly seems to be that itâ€™s easier to narrow down which change couldve caused this test suite to fail. However, it seems unlikely to me that these tests could find breaking changes everyday that the centaur standard test cases wouldnâ€™t already uncover. I see these tests as a release requirement for Cromwell more than anything else. However, itâ€™s totally upto the team on whatever makes them most comfortable, I donâ€™t have a strong opinion on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368:115,down,down,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368,1,['down'],['down']
Availability,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:83,down,down,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770,3,"['down', 'error']","['down', 'error']"
Availability,"@geoffjentry Yeah so I actually changed the fix because @cjllanwarne suggestion seemed cleaner and I though it would have the same effects, but apparently it doesn't. I can change it back to the first version that worked for you.; On a larger point, I think (hope) this kind of failures will be a lot less happening when we re-factor the test suite infrastructure. IMO it's happening because we keep doing more and more complex tricks in the spec to get it to do what we need but with all the features we keep adding it keeps getting less and less stable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463:278,failure,failures,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463,1,['failure'],['failures']
Availability,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:55,error,errors,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,2,"['error', 'failure']","['errors', 'failure-recovery']"
Availability,"@geoffjentry Yeah, I just had to run 14 `cat` commands before I finally found the (transient) issue... `docker: Error response from daemon: device or resource busy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756:112,Error,Error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756,1,['Error'],['Error']
Availability,"@geoffjentry actually the constructor is at arity 20 now. I think the point Jeff is driving at is that there aren't built-in Spray marshallers for arities > 22, and I'm actually not sure this won't break down completely for arities > 22 (it is possible to have case class constructors of arity > 22, but functions can't have arity > 22 so something else might still break). So beyond the aesthetic issues here this has become like a game of musical chairs; it might be nice to address this proactively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198329111:204,down,down,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198329111,1,['down'],['down']
Availability,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1218,robust,robust,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,2,['robust'],['robust']
Availability,"@geoffjentry commented on [Fri May 05 2017](https://github.com/broadinstitute/wdl/issues/109). **Needs Refinement**. We want to be able to define types for the values of objects. One suggestion was something like the following (note `struct` is using as a possible replacement for `object`, see below):. struct MyType {; o_f: File; x: Array[String]; }. MyType foo = read_object(...). It will coerce to the types it expects and if it can't that's a failure. Open questions:. - Do we make a new construct (e.g. `struct` above), or replace objects; - If replace, who (if anyone) is currently using `object`; - What's the right syntax, regardless of the name of the construct. This needs focus grouping.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283:448,failure,failure,448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283,1,['failure'],['failure']
Availability,"@geoffjentry commented on [Tue Apr 26 2016](https://github.com/broadinstitute/centaur/issues/36). Initially Centaur loaded in its files for each test (wdl, inputs, etc) using a function which would throw an exception if it wasn't there. As the framework has evolved it has moved to a model that is more robust and would more accurately report what was going on there. Modify these file slurps to play nicer with the rest of the system",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2885:303,robust,robust,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2885,1,['robust'],['robust']
Availability,@geoffjentry is it possible to call cache when the original input no longer exists? ; @dheiman have you looked at the [call cache diff endpoint](https://github.com/broadinstitute/cromwell#get-apiworkflowsversioncallcachingdiff)? This is not available in FireCloud but it may have more information about why a workflow cached (or did not).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210:241,avail,available,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210,1,['avail'],['available']
Availability,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:314,error,error,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['error'],['error']
Availability,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902,1,['error'],['error']
Availability,@geoffjentry would this fit under the current work with labels that @ruchim is doing? I agree that it's a good idea and it would help make labels even more robust and useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979:156,robust,robust,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979,1,['robust'],['robust']
Availability,"@geoffjentry yes, this is turning the knob higher and hoping for the best. The downstream tests succeeded except for the one that depended on cross-talking with the failed test...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458:79,down,downstream,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458,1,['down'],['downstream']
Availability,"@geoffjentry, these were real failures. The jobs were marked by JES as failures, so Cromwell (correctly) did not attempt to retry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830,2,['failure'],['failures']
Availability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:218,error,errors,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['error'],['errors']
Availability,"@grsterin yes - I [would expect](https://docs.google.com/document/d/1rvLeQYHJATz17VLGJ7xtJjYA0yqrDW_HQ7ja9WCV2-c/edit) we would do a one-time reading of the manifest at start-up time and use that listing to decide whether a file was available on a reference disk or not. We don't know that there will only be one bucket, and in the future people might want to bring their own reference disks containing files from their own buckets. Instead of making the bucket a magic value that's hard-coded into Cromwell I think it's better to have it as part of the reference file path in the manifest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643:233,avail,available,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643,1,['avail'],['available']
Availability,"@helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103). I'm loading a WDL in wdl4s using `WdlNamespaceWithWorkflow.load(my_wdl, Seq())` -- i.e. passing no import resolvers. If the WDL contains imports, I'd expect it to fail and complain that it can't resolve the imports because I haven't specified any resolvers. (In the current code, that'd be [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L198).). However, what actually happens is that wdl4s tries to turn the import into a `File` object [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/Import.scala#L18). If that's not a valid kind of file path (perhaps because of a custom URI scheme, or because you're running Windows and colons aren't allowed in filenames), wdl4s blows up with some java-native exception. (In the Windows case, that's `java.nio.file.InvalidPathException`.). TLDR: wdl4s should throw a useful error if your WDL contains imports but you haven't specified resolvers. It probably shouldn't attempt to load the imports outside the context of a resolver, either. ---. @helgridly commented on [Thu Apr 13 2017](https://github.com/broadinstitute/wdl4s/issues/103#issuecomment-293936559). I discussed this with @Horneth and he's provided me with a workaround: I can check for the existence of imports in my code by loading the AST directly using something like `Option(ast).map(_.getAttribute(""imports"")).toSeq` (lifted from [here](https://github.com/broadinstitute/wdl4s/blob/develop/src/main/scala/wdl4s/WdlNamespace.scala#L185)). So consider this a non-urgent enhancement rather than a cloud of fire.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2701:992,error,error,992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2701,1,['error'],['error']
Availability,@hjfbynara has started testing the migration from 19 -> 25 and he noticed restart_and_recover_migration.xml seems to be throwing an unhelpful liquibase error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108,1,['error'],['error']
Availability,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:308,error,errors,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['error'],['errors']
Availability,"@huangzhibo ; you may want to check if you see this:; ```; MariaDB [(none)]> SELECT @@SQL_MODE;; +-------------------------------------------------------------------------------------------+; | @@SQL_MODE |; +-------------------------------------------------------------------------------------------+; | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |; +-------------------------------------------------------------------------------------------+; 1 row in set (0.000 sec); ```; as this is the default setup for some linux distros. I get your exact error, and also #3346 with the above SQL_MODE. In https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457 it is suggested to try setting; ```; MariaDB [(none)]> SET GLOBAL sql_mode = 'ANSI_QUOTES';; Query OK, 0 rows affected (0.000 sec); ```; When I do that, both errors no longer occur. Note the above change is not permanent. You can alter; `/etc/my.cnf.d/mariadb-server.cnf`; to something like; ```; [mysqld]; datadir=/var/lib/mysql; socket=/var/lib/mysql/mysql.sock; log-error=/var/log/mariadb/mariadb.log; pid-file=/run/mariadb/mariadb.pid; sql_mode=ANSI_QUOTES; ```; and then restart your db server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031:594,error,error,594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031,3,['error'],"['error', 'errors']"
Availability,"@huangzhibo Hi huangzhibo, this makes no difference for me (for cwl). I only contain tools called by the workflow in my zipped directory and it still does not recognize this as a valid workflow.; Example:; ```; ./echo_cat_wf.cwl; ./tools/echo.cwl; ./tools/cat.cwl. zip -r tools.zip tools; ```; then run:. `java -jar cromwell-44.jar run -i in.yml -p tools.zip --type cwl echo_cat_wf.cwl`; Gives me error.; Thanks,; Dennis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519981728:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519981728,2,"['echo', 'error']","['echo', 'error']"
Availability,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:55,down,downloading,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301,1,['down'],['downloading']
Availability,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:124,failure,failures,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236,1,['failure'],['failures']
Availability,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:582,ping,ping,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,1,['ping'],['ping']
Availability,@illusional I wanted to echo the comments from @vsoch. Thanks to all of you who worked to get this going.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465204072:24,echo,echo,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465204072,1,['echo'],['echo']
Availability,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:190,down,down,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['down'],['down']
Availability,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:260,error,error,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284,2,['error'],['error']
Availability,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:239,reliab,reliable,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307,2,['reliab'],['reliable']
Availability,"@jainh I think you need to rebase, my suspicion is that the test failures are due to not being quite up to date on develop. Also, not for this PR but I'd suggest looking into porting this backend to the standard backend trait, it might help wiht these divergences in the future",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729:65,failure,failures,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729,1,['failure'],['failures']
Availability,"@jainh I would therefore suggest you want to sort the list yourself to make that guarantee robust, rather than assume it's already correctly ordered in the WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731:91,robust,robust,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731,1,['robust'],['robust']
Availability,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:10,echo,echo,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992,1,['echo'],['echo']
Availability,"@jainh Yes, there are multiple `--conf` attributes.; If there is no space in the value of `--conf` attribute, single quote is not needed; otherwise, I think it's needed. However the [gatk-launch](https://github.com/broadinstitute/gatk/blob/70edbb6e4caa2b7cf1b8678450443c0c590a2b76/gatk-launch) in GATK beta 4 does not produce the single quote for such case; but if I run the following without single quote, it leads to error:; >Error: Unrecognized option: -Dsamjdk.use_async_io_read_samtools=false. command:; ```; /opt/spark-latest/bin/spark-submit --master spark://localhost:6066 --deploy-mode cluster \; --driver-cores 4 --driver-memory 8g --executor-memory 4g --total-executor-cores 10 \; --conf 'spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false' \; --conf 'spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true' \; ...; ```; Here is a related [post](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) on stackoverflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862:419,error,error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862,2,"['Error', 'error']","['Error', 'error']"
Availability,@jdidion I talked to a site admin and this setting should be updated. Please let me know if you run into this error again and I will add you manually and continue to communicate with our site admin about this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542,1,['error'],['error']
Availability,@jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999:107,error,error,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999,1,['error'],['error']
Availability,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:105,error,error,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,3,"['error', 'failure']","['error', 'failures']"
Availability,"@jgainerdewar thank you, I didn't check the ""Pull Request"" errors carefully enough (thinking it was just the `ssh_access` problem)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930:59,error,errors,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930,1,['error'],['errors']
Availability,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:215,down,down,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235,2,['down'],"['down', 'downloading']"
Availability,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:142,avail,available,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696,1,['avail'],['available']
Availability,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:354,robust,robust,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,"['alive', 'robust']","['alive', 'robust']"
Availability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:124,down,down,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,2,['down'],"['down', 'download']"
Availability,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:447,error,errors,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898,1,['error'],['errors']
Availability,"@katevoss It'd certainly be more robust and nice to have, however it's unlikely to be some huge fire for poeple",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342:33,robust,robust,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342,1,['robust'],['robust']
Availability,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:209,robust,robustness,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912,1,['robust'],['robustness']
Availability,@katevoss This issue expects that there is some way to gracefully shut down Cromwell and that stopping the docker process does not do it by default. . #1495 looks to me like an edge case and is not related to this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-325717424:71,down,down,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-325717424,1,['down'],['down']
Availability,@katevoss error that is blocking the migration,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409,1,['error'],['error']
Availability,@katevoss heads up that this was reported and requested in the forums here: https://gatkforums.broadinstitute.org/wdl/discussion/10853/consistent-503-error-for-engine-functions#latest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349130801:150,error,error-for-engine-functions,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349130801,1,['error'],['error-for-engine-functions']
Availability,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133,2,"['error', 'failure']","['error', 'failure']"
Availability,"@katevoss nope, this is just making a single test more reliable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174:55,reliab,reliable,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174,1,['reliab'],['reliable']
Availability,"@katevoss seems to be, yeah. This is a specific fix to the problem in #2576 which also comes with extra benefits like throttling and batching to make it generally much more reliable and scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282:173,reliab,reliable,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282,1,['reliab'],['reliable']
Availability,"@katevoss, it not being available in FireCloud is my high-level issue - it turns out that since FireCloud currently implements Cromwell 28, [call_caching_placeholder.txt gets placed, even though it is actually cache-by-copy rather than reference](https://gatkforums.broadinstitute.org/firecloud/discussion/10282/confusing-file-left-in-call-cached-execution-directory). . This makes me believe that it should be trivial to leave a file or log entry with details of _why_ a call was cached, which would be quite useful to me, or anyone else trying to troubleshoot an unexpected occurrence like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147:24,avail,available,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147,1,['avail'],['available']
Availability,"@kcibul - I was thinking of adding a ""robustness"" (or something like that) label for tickets like this and #1762 as these aren't really about scaling but definitely could impact the health of a cromwell server. Figured I'd leave it up to you as to a) if that's the right thing and b) the exact nomenclature if so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975:38,robust,robustness,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975,1,['robust'],['robustness']
Availability,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:72,avail,available,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,2,['avail'],['available']
Availability,"@kcibul @ruchim FYI - this has failed 62 workflows for us (out of 277 total failures, 22%). It is tied for our largest source of failure. We have not seen it since 6/7",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-228104671:76,failure,failures,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-228104671,2,['failure'],"['failure', 'failures']"
Availability,@kcibul I added a volume entry to mount the mysql data directory onto the host so the data survives a `docker-compose down`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008:118,down,down,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008,1,['down'],['down']
Availability,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:89,down,down,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802,1,['down'],['down']
Availability,@kcibul It might not be. I made the same claim at one point and got a lot of hesitant looks from fellow Cromwellians. We're a lot more robust to storing this stuff but at some point it still needs to be gathered into a single array object,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1051#issuecomment-245964356:135,robust,robust,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1051#issuecomment-245964356,1,['robust'],['robust']
Availability,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:51,down,down,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085,6,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,@kcibul This problem doesn't exist in develop (that I can see) as we're not currently supporting restart/recover,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359:105,recover,recover,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359,1,['recover'],['recover']
Availability,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669,1,['down'],['down']
Availability,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:62,failure,failure,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012,1,['failure'],['failure']
Availability,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:376,echo,echo,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340,1,['echo'],['echo']
Availability,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:19,down,downside,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016,1,['down'],['downside']
Availability,"@kcibul why not support empty string as null, for docker, in the backend code? Are you worried about discerning an error where the user wants docker, but accidentally specifies empty string? . If my example wdl/json above is correct, I'd rather not have to delete json entries, since this is a bit more difficult to script around, but I can be convinced.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347,1,['error'],['error']
Availability,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:273,down,down,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111,2,['down'],['down']
Availability,"@kshakir As you said, this was an error in the input file. I successfully completed a run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332,1,['error'],['error']
Availability,"@kshakir Can you tell me a bit more about this ticket? Is it still happening, or is Travis not having network errors anymore?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190:110,error,errors,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190,1,['error'],['errors']
Availability,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:369,alive,alive,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482,8,['alive'],['alive']
Availability,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:110,avail,available,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556,1,['avail'],['available']
Availability,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:164,Down,Downloaded,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906,1,['Down'],['Downloaded']
Availability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/92). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; workflow undeclared_scatter_variable {; scatter (i in undeclared) {}; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2699:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2699,2,['error'],['error']
Availability,"@kshakir commented on [Mon Feb 27 2017](https://github.com/broadinstitute/wdl4s/issues/93). The following wdl currently parses in wdl4s without error, later causing an error within cromwell. ```; task x {; Int i; command { echo $i }; runtime { docker: ""ubuntu"" }; output {; Int out_but_intentionally_misnamed = i; Boolean validOutput = i % 2 == 0; }; }. workflow missing_optional_output {; Array[Int] arr = [0,1,2,3]; scatter (i in arr) {; call x { input: i = i }; if (x.validOutput) {; Int x_out = x.out_except_undeclared; }; }. Array[Int?] x_out_maybes = x_out; }; ```. See https://github.com/broadinstitute/cromwell/issues/2020 for more info of cromwell patches, including a centaur test, that may be updated or removed if this is fixed in wdl4s.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700,3,"['echo', 'error']","['echo', 'error']"
Availability,"@kshakir helpfully notes:; >That error looks like JNI, that I suspect is jython related, thus is probably heterodon. Heterodon was slimmed down to remove everything NOT tested via mac and/or CI. So since we donâ€™t have any :travis: / :jenkins: testing windows I would not expect heterodon to work. Good news (?): we still support shell invoking `cwltool`, but I have zero expectation for that to work on windows either... So this behavior is likely the result of a deliberate and helpful size optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597,2,"['down', 'error']","['down', 'error']"
Availability,@kshakir is this the library that we downgraded the last time we tried to upgrade all of our libraries?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251:37,down,downgraded,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251,1,['down'],['downgraded']
Availability,"@kshakir just a standard config file. I hadn't actually looked at the error yet, but will make a note to come back to this discussion :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397,1,['error'],['error']
Availability,@ktibbett can you comment if this is something you need patched into hotfix? Or something you can tolerate until you're on 0.20+,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661:98,toler,tolerate,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661,1,['toler'],['tolerate']
Availability,"@lbergelson right, these look like transient failures so I'l give the tests a quick nudge...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465,1,['failure'],['failures']
Availability,"@lij41 - do you have any more error information, like a specific failure message? Also, were you creating the AMI from the CloudFormation templates? If so, which version - with or without VPC creation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888,2,"['error', 'failure']","['error', 'failure']"
Availability,"@markjschreiber Thank you for this PR. I had a quick look at it and it looks pretty good. I just had a few questions. > 7. Set up /var/lib/docker/docker to auto-expand as inputs are now read directly into the container. Is this the only documentation on this requirement? Also, are you saying that that directory is now being auto-expanded in the underlying ECS image, or that a client needs to create an AMI to auto-expand that directory instead of `/cromwell_root`? Also, is it `/var/lib/docker/docker` or `/var/lib/docker/containers`? . EDIT:. The README.md references a LaunchTemplate which provides a UserData script to an underlying AMI, but it's not linked anywhere. I think I've tracked it down to the document here: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/aws-genomics-launch-template.template.yaml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076:698,down,down,698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076,1,['down'],['down']
Availability,@markjschreiber running into the same error for both v52 and v53.1. I am using the same CloudFormation @mderan-da mentioned . Appreciate your newer documentation on this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254,1,['error'],['error']
Availability,"@matthewghgriffiths did it work with `\cromwell_mount` or `\cromwell_root`. I'm getting a very similar error, and I've double checked that I had ""cromwell_root"" as listed [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions). I created my AMI today, and I definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['error'],['error']
Availability,"@matthewghgriffiths, for troubleshooting why nothing gets written at any point:. 1. Set the min cpus in your compute environment (the one in use) to at least 1 so that an instance spins up and will stay available for a little bit. This will give you an instance that lives for long enough to test things.; 2. SSH into the instance that spins up (or one already running) in the compute environment.; 3. Try to `aws s3 cp` a file into your cromwell executions bucket. Doing this from the instance simulates the permissions used by the batch job. If you get an error about permissions, then there is likely a policy problem with your instance role.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011:203,avail,available,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011,2,"['avail', 'error']","['available', 'error']"
Availability,"@mcnelsonsema4 - the `SubmitJob` API does not support overriding volumes and mount points to the container. These are set in the Job Definition referenced by `SubmitJob`. The host path for the volume uses the UUID that Cromwell generates for the task to isolate localized inputs and any outputs generated. Since the UUID is regenerated for each task execution, a new Job Definition revision is created with the corresponding new host path for the volume. The API call for creating new Job Definitions and revisions isn't intended for a high volume of requests. The [code involved](https://github.com/broadinstitute/cromwell/blob/90154ed22b2a78dfbb1c5342a8f0d39164aaeac8/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L201-L218) seems to keep retrying the request to the API. It's possible an API response is returned that isn't caught as a error and allowing the use of the ""most recent"" revision, which was a revision submitted by another task by the same name that ran earlier. @cjllanwarne, @mcovarr, @danbills - any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505665761:874,error,error,874,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505665761,1,['error'],['error']
Availability,"@mcovarr , @cjllanwarne . ~~Sorry for the errors still. I want to test locally of course, but that does not work for some reason:~~; EDIT: Nevermind. I found the documentation here: https://cromwell.readthedocs.io/en/stable/developers/Centaur/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138,1,['error'],['errors']
Availability,"@mcovarr - yes. Alteratively, you could use `DescribeImages` and supply an `ImageIds` argument to filter down the results by either `imageTag` or `imageDigest`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486926555:105,down,down,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486926555,1,['down'],['down']
Availability,"@mcovarr @Horneth I have not been able to make tests work when DataAccess is a singleton. If you can diagnose the failures in branch: ""data_access_singleton"" then let me know, but I don't think I'll do it as part of this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075:114,failure,failures,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075,1,['failure'],['failures']
Availability,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:89,failure,failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447,2,['failure'],['failure']
Availability,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:381,failure,failure,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073,2,['failure'],['failure']
Availability,@mcovarr Can you please ping me when you guys are close to revisiting this? I don't have bandwidth right now but would definitely like to go over this before it hits. Do you have a sense of timeline?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827:24,ping,ping,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827,1,['ping'],['ping']
Availability,"@mcovarr Given that these were JES specific failures, I'm going to run Tyburn over them. Presumably the local tests for these are already there and already working?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460,1,['failure'],['failures']
Availability,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:16,echo,echo,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182,4,"['down', 'echo']","['downloading', 'echo']"
Availability,@mcovarr I continue to get the same error after creating a build with the develop branch seems that it is not ready! Thanks for letting me know :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458,1,['error'],['error']
Availability,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:451,down,down,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['down'],['down']
Availability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, Iâ€™m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:419,Error,Error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,@mcovarr Yes. It's ridiculous that we get build failures due to external services failing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166392599:48,failure,failures,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166392599,1,['failure'],['failures']
Availability,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['error'],['error']
Availability,"@mcovarr commented on [Fri Aug 05 2016](https://github.com/broadinstitute/centaur/issues/96). ---. @cjllanwarne commented on [Tue Dec 20 2016](https://github.com/broadinstitute/centaur/issues/96#issuecomment-268376059). @mcovarr what's the context of this ticket?. ---. @ruchim commented on [Tue Dec 20 2016](https://github.com/broadinstitute/centaur/issues/96#issuecomment-268390892). I believe he's referring to the test in Centaur? I notice today the test was set to ignored, even though we offer this wf failure mode. Perhaps its just outdated, I'll try un-ignoring the test. ---. @mcovarr commented on [Mon Jan 30 2017](https://github.com/broadinstitute/centaur/issues/96#issuecomment-276110656). Only seeing this now! Yes I was referring to the test in Centaur, and I just confirmed it still fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2889:508,failure,failure,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2889,1,['failure'],['failure']
Availability,"@mcovarr commented on [Thu Aug 04 2016](https://github.com/broadinstitute/centaur/issues/95). It looks like most of the shards are successful, but one fails and might not be retried:. ```; 2016-08-03 15:20:01,502 cromwell-system-akka.dispatchers.backend-dispatcher-107 INFO - $a [UUID(eaeaa32d)DeliciousFileSpam.StringSpam:215:1]: JesAsyncBackendJobExecutionActor [UUID(eaeaa32d):DeliciousFileSpam.StringSpam:215:1] Status change from Running to Success; 2016-08-03 15:20:01,923 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: Job DeliciousFileSpam.StringSpam:215:1 succeeded!; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionFailedState. Shutting down.; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] done. Shutting down.; 2016-08-03 15:20:03,593 cromwell-system-akka.dispatchers.engine-dispatcher-85 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from ExecutingWorkflowState to FinalizingWorkflowState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationPendingState to FinalizationInProgressState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationInProgressState to FinalizationSucceededState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatche",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2887:966,down,down,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2887,1,['down'],['down']
Availability,"@mcovarr commented on [Thu Sep 07 2017](https://github.com/broadinstitute/wdltool/issues/48). Per the link below, enhance wdltool to be able to detect malformed expressions. Expressions that can't be evaluated are okay and expected due to values not being available, but malformed expressions are not okay. https://gatkforums.broadinstitute.org/wdl/discussion/10311/error-evaluating-output-files-that-serve-as-input-files-for-following-step#latest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2869:256,avail,available,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2869,2,"['avail', 'error']","['available', 'error-evaluating-output-files-that-serve-as-input-files-for-following-step']"
Availability,@mcovarr commented on [Tue Nov 14 2017](https://github.com/broadinstitute/wdltool/issues/51). This is for the benefit of womtool which is going to have to resort to forging inputs as a [stopgap measure](https://github.com/broadinstitute/cromwell/pull/2857). WdlNamespace should continue to make the current API available for production code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2867:311,avail,available,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2867,1,['avail'],['available']
Availability,"@mcovarr does ""needs docs if not a checkpoint"" still apply?. EDIT: I see some docs do exist - is that sufficient to satisfy the PR description comment?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3933#issuecomment-408934534:35,checkpoint,checkpoint,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3933#issuecomment-408934534,1,['checkpoint'],['checkpoint']
Availability,"@mcovarr it does seem to have worked ðŸ˜„; @cjllanwarne @geoffjentry Good point, my main goal was to make it possible for the metadata service to report statsd metrics, which is done through another service. But like you said since it's just making available the service registry actor to the services I don't think it introduces any additional coupling.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969:246,avail,available,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969,1,['avail'],['available']
Availability,"@mcovarr oh I see thanks. Well I only saw 4 failures IIRC, so it's not that bad. In the meantime we could always keep `/bin/bash` as the default and set it to`/bin/sh` for CWL conf tests which would make 117 pass",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191,1,['failure'],['failures']
Availability,@mcovarr that was indeed my theory. I'm trying to create a test case to actually reproduce this error to make sure my fix doesn't have some other weird downstream problems (eg getting 10 lines later then throw some other exception wouldn't be a great outcome),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964,2,"['down', 'error']","['downstream', 'error']"
Availability,"@mcovarr that's true, I think it would fail eventually anyway whenever something tries to access those files (or not if nothing does ?) ? And I thought that one of the assumption in this ""no-copy"" mode is that we expect the files to be relatively immutable anyway. . But I also agree that it would be cleaner to fail the ""caching"" if they don't exist rather than the downstream task failing by itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116:367,down,downstream,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116,1,['down'],['downstream']
Availability,@mcovarr well the title of the pr **does** mention programmer error ðŸ˜›,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-480294525:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-480294525,1,['error'],['error']
Availability,@mcovarr what do you believe is the impact of this change toward managing production failures?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3962#issuecomment-424957788:85,failure,failures,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3962#issuecomment-424957788,1,['failure'],['failures']
Availability,"@mcovarr worth figuring out as any non-trivial downtime is going to be met with a giant ""no""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472133864:47,downtime,downtime,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472133864,1,['downtime'],['downtime']
Availability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:171,error,error,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,1,['error'],['error']
Availability,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601,2,['error'],['error']
Availability,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:482,failure,failure,482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623,1,['failure'],['failure']
Availability,"@meganshand commented on [Fri Apr 29 2016](https://github.com/broadinstitute/wdltool/issues/9). The following task results in an uninformative error message when using `validate`: . ```; task printReads {; File bam; File ref_fasta; File ref_fasta_index; File ref_dict. command {; java -jar /usr/gitc/GenomeAnalysisTK.jar \; -T PrintReads \; -I ${bam} \; -o smaller.bam \; -L chr1 \; -R ${ref_fasta}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud:1.1010_with_gatk3.5""; disks: ""local-disk 400 SSD""; memory: ""10 GB""; }; output {; File smaller = smaller.bam; }; }; ```. results in this error message:. ```; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```. The problem is that there aren't quotes around ""smaller.bam"" in the outputs of the task. It would be great if the error message could tell you which line or object was causing the problem. The error message from cromwell is different, but also uninformative and it would be great if the error message could be clearer there as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:143,error,error,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,5,['error'],['error']
Availability,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:89,error,errors,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,2,"['down', 'error']","['down', 'errors']"
Availability,@notestaff @Horneth What about a line along the lines of heeding typical concerns regarding docker-in-docker situations? I don't really want to go down the path of providing various workarounds (as we already see in this thread they're controversial) but I think it'd satisfy @notestaff 's request to at least make the issue visible,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4100#issuecomment-436723837:147,down,down,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4100#issuecomment-436723837,1,['down'],['down']
Availability,"@orodeh in case you're not able to read the Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:69,failure,failure,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,@patmagee It turned out it was a different issue entirely in our production environment that had the symptoms of abort failures. We've not had success recreating this -- but let us know what you end up observing!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311:119,failure,failures,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311,1,['failure'],['failures']
Availability,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,recover,recover,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714,1,['recover'],['recover']
Availability,"@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:58) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) Encountered unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.generateParseException(TurtleParser.java:1034) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.jj_consume_token(TurtleParser.java:902) org.semanticweb.owlapi.rdf.turtle.parser.TurtleParser.parseDocument(TurtleParser.java:165) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:54) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:3216,ERROR,ERROR,3216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['ERROR'],['ERROR']
Availability,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:350,echo,echo,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"@rebrown1395 ; Yes, it worked. I didn't see this error anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4353#issuecomment-444627372:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4353#issuecomment-444627372,1,['error'],['error']
Availability,"@rhpvorderman oh, I didn't notice your comment about the tests in my re-review. Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code. Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386:198,failure,failure,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386,1,['failure'],['failure']
Availability,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,4,['error'],['error']
Availability,"@ruchim @geoffjentry . Hi, ; Sorry, it looks like I copied the wrong history. I added the correct history at https://gist.github.com/denis-yuen/b3aa8b0e882dee1fe8cb6cab82286e46. The error message is pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - â€˜cromwell-36.jarâ€™ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:182,error,error,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['error'],['error']
Availability,"@ruchim I think we talked about this offline, but it'd be workflow level info. Except I guess for the aforementioned idea of grouping by task failures.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348#issuecomment-383682658:142,failure,failures,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348#issuecomment-383682658,1,['failure'],['failures']
Availability,"@ruchim I updated the description and title, this is not nearly as bad as the previous title made it sound. It's a very weird case found in the centaur test failures that I'm looking at.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611:157,failure,failures,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611,1,['failure'],['failures']
Availability,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:365,echo,echo,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,14,"['echo', 'error']","['echo', 'error']"
Availability,"@ruchim Thank you very much for the help and explanation here, they look great to me! @jishuxu a follow up for one of your the failures you ran into previously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390846376:127,failure,failures,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390846376,1,['failure'],['failures']
Availability,"@ruchim The options part of this might duplicate that bug, but I'm not sure since I don't know the failure mode there. The non-localized paths part of this is maybe related to #1944 or could be original, not sure about that either. ðŸ˜¦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483,1,['failure'],['failure']
Availability,"@ruchim Were you able to get to the bottom of the ""Could not find suitable filesystem among Default to parse gs://..."" errors you mention in the review thread? I'm hitting the same error when running a WDL on Google's wdl_runner, but the same WDL with the same inputs runs fine on the DSDE Methods Cromwell server (both C24 and C25). In my case the error occurs when using read_lines() on a gs:// filepath that points to a text file (list of file paths that I'd rather not include in the json for practical reasons). . Also tagging friendly bug rotator @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066:119,error,errors,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066,3,['error'],"['error', 'errors']"
Availability,"@ruchim commented on [Thu Aug 24 2017](https://github.com/broadinstitute/wdltool/issues/46). When validating the workflow below with wdltool-0.14.jar, the response is simply ""null"". . ```; workflow w {; String s = ""test"". output {; String o =; }; }; ```; It would be great if a more comprehensive error is returned in the case of invalid workflow outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2883:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2883,1,['error'],['error']
Availability,"@ruchim commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29). Given a task:. task myTask {; File f; command {; touch ${f.bam.bai}; }; }. A workflow with such a task validates in wdltool-0.10 but when run on cromwell-26, it fails with an error: java.lang.UnsupportedOperationException: Could not evaluate expression:.... Given a slightly altered version of that previous task:. task myTask {; File f; command {; touch ${f%%.bam.bai}; }; }. This task also validates but fails before the Workflow is about to run with the error: scala.MatchError: null. ---. @geoffjentry commented on [Wed May 24 2017](https://github.com/broadinstitute/wdltool/issues/29#issuecomment-303819742). Is this an artifact of wdltool being out of synch? it happens way too often :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2873:268,error,error,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2873,2,['error'],['error']
Availability,@ruchim is this a parsing error or a runtime error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3051#issuecomment-350848249:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3051#issuecomment-350848249,2,['error'],['error']
Availability,"@salonishah11 for example, I'm running a cromwell container in server mode, bound to port 8000: ; ```; docker run -p 8000:8000 cromwell server; ```; but when I try to ; ```; docker run cromwell submit --host 0.0.0.0:8000 ...; ```; I get: ; ```; Error: Option --host failed when given '0.0.0.0:8000'. no protocol: 0.0.0.0:8000; ```. Some simple docs would help here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124:245,Error,Error,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124,1,['Error'],['Error']
Availability,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:828,avail,available,828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,1,['avail'],['available']
Availability,@samanehsan @geoffjentry -- is there a good message you'd like to see in replacement of the existing error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959,1,['error'],['error']
Availability,@samanehsan this seems like a one-off event of a metadata value (startTime) getting lost. Whats the frequency of such an error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4128#issuecomment-424948652:121,error,error,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128#issuecomment-424948652,1,['error'],['error']
Availability,"@scottfrazer FWIW my vision of the world was a lot closer to the diagram you drew up but I don't have strong feelings on that. In terms of distributed jars what I'd like to see distributed to the world would be:; - cromwell.jar: full fat jar like we have today which also includes all of the supported backends built in; - cromwell-backend.jar: a jar providing the interface stuff which someone can use to build their own backend jar. I'd be totally okay with (and could see value in):; - cromwell-lite.jar (or something like that): a stripped down fat jar w/o any supported backends or maybe local; - foo-backend.jar for each supported backend. My main concern though is that we always make the most obvious download for a naive user of cromwell to be the one with all of the supported (or perhaps a 'very common supported' subset) backends built in so as to minimize the work someone needs to do to get rolling. The other jars are really an artifact of the multi-project model and can be ignored. The side discussion about `core` is exactly why I was picturing the hierarchy stemming from `backend` all along. Despite the name of `supportedBackends` being my request I'll admit I was just looking at the name when I said that, not thinking the whole thing through critically :). IMO `core` should be code which is shared between all components, I'd call the filesystem concept a component, not something in core itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235:544,down,down,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235,2,['down'],"['down', 'download']"
Availability,"@scottfrazer re compile errors, yeah i made a tiny change on my last commit which I wouldn't think would break anything so didn't bother to test it. oops.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423,1,['error'],['errors']
Availability,"@seandavi - implementing the config suggested by @TimurIs and removing the specification of `concurrent-job-limit` I was able to run the following workflow with out issue. ```; task t {; Int id; command { echo ""scatter index = ${id}"" }; runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""512MB""; }; output { String out = read_string(stdout()) }; }. workflow w {; Array[Int] arr = range(1000); scatter(i in arr) { call t { input: id = i } }; output { Array[String] t_out = t.out }; }; ```. Approximate numbers:; * max # jobs observed in ""submitted"" state = 250 - 270; * max # jobs observed in ""running"" state = 20-30",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747:205,echo,echo,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747,1,['echo'],['echo']
Availability,"@seandavi What are you seeing on the quota failures? We should be robust to that and putting things back in the to-retry pool, while our quotas are jacked pretty high we run into this as well for our larger stuff, so that's something we've needed to work around ourselves.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297:43,failure,failures,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297,2,"['failure', 'robust']","['failures', 'robust']"
Availability,"@seandavi Yeah, was just reading the thread. That's something different than what I was picturing when I saw quota failures. I'll look into it a bit.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477:115,failure,failures,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477,1,['failure'],['failures']
Availability,"@slnovak Hi - thanks for this! I just had a couple of quick questions on the Homebrew & maintenance front. ; - What's the plan going forward in terms of keeping it up to date, is this something that you plan on doing? If so will you be tracking the official releases (e.g. now 0.16) or updating on some semi-regular (or even not-so-semi) interval? ; - Is there anything we could do in terms of helping out w/ the homebrew angle?; - I'm not at all familiar w/ the homebrew formula stuff. Where is it calling `sbt assemble`? Is that done elsewhere? If so, how does that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670:88,mainten,maintenance,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670,1,['mainten'],['maintenance']
Availability,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:321,error,error,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241,3,"['error', 'failure']","['error', 'failures']"
Availability,"@tmdefreitas commented on [Tue Mar 22 2016](https://github.com/broadinstitute/wdltool/issues/7). I came across this while helping a colleague debug her WDL file. When this WDL file is validated, wdltool claims an ""Error: finished parsing without consuming all tokens"", even though the error is commented out:. ```; task comment_bug {; #String an_input. command {; echo ""Alternate command""; # The following line has a WDL syntax error, but in a comment!; #echo {an_input} ; }. output {. }; }. workflow test {; call comment_bug; }; ```. EDIT: error message, for completeness:. ```; $ java -jar wdltool.jar validate comment_bug.wdl; ERROR: Finished parsing without consuming all tokens. output {; ^; ```. I can get rid of the error by changing the comment line to `#echo ${an_input}`. ; I think errors in comment lines should probably be ignored by the validator. As an aside, is there a more helpful error message for this? The message sounds like an unused input variable or something, not that the bracket syntax was off, so it was hard to diagnose (The above is a toy example, the real task had a much more complicated command). ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200416975). This is happening because the the `command {...}` section is parsed differently than the rest of WDL. The parser thinks that the closing brace in `#echo {an_input}` is actually trying to close the `command` section. If you use the alternative delimiters (`command <<< ... >>>`) this is another way to get around it. We parse the command as ""opaque strings intermixed with `${...}` blocks"". That means that the `#`-style comments inside a command section are not interpreted as WDL comments, but instead as part of the command. More thought would have to go into figuring out what the right thing to do here is. ---. @tmdefreitas commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200442613). Admittedly",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:214,Error,Error,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,11,"['ERROR', 'Error', 'echo', 'error']","['ERROR', 'Error', 'echo', 'error', 'errors']"
Availability,"@tmdefreitas commented on [Tue Mar 29 2016](https://github.com/broadinstitute/wdltool/issues/8). In the following WDL, **GSEA_v_1_0_fwer_p_val_threshold** was not declared as an input, but the validator didn't raise an error. Cromwell choked when trying to run the task. Is there a reason wdltool shouldn't throw an error here?. ```; task tool_gsea_mrnaseq_subtypes {; String outputprefix; String pheno_from_aggregate_molecular_subtype_clusters; String pheno_name; File tcga_pheno_FileName; File tcga_exp_FileName; File gs_db; String GSEA_v_1_0_reshuffling_type; String GSEA_v_1_0_nperm; String GSEA_v_1_0_weighted_score_type; String GSEA_v_1_0_nom_p_val_threshold; String GSEA_v_1_0_topgs; String GSEA_v_1_0_adjust_FDR_q_val; String GSEA_v_1_0_gs_size_threshold_min; String GSEA_v_1_0_gs_size_threshold_max; String GSEA_v_1_0_reverse_sign; String GSEA_v_1_0_perm_type. command {; /R/RunR.sh -f main /src/Pathway_GSEA.R --libdir/src --disease_type${outputprefix} --pheno.from.Aggregate_Molecular_Subtype_Clusters${pheno_from_aggregate_molecular_subtype_clusters} --pheno.name${pheno_name} --tcga.pheno.FileName${tcga_pheno_FileName} --tcga.exp.FileName${tcga_exp_FileName} --gs.db${gs_db} --GSEA.v.1.0.reshuffling.type${GSEA_v_1_0_reshuffling_type} --GSEA.v.1.0.nperm${GSEA_v_1_0_nperm} --GSEA.v.1.0.weighted.score.type${GSEA_v_1_0_weighted_score_type} --GSEA.v.1.0.nom.p.val.threshold${GSEA_v_1_0_nom_p_val_threshold} --GSEA.v.1.0.fwer.p.val.threshold${GSEA_v_1_0_fwer_p_val_threshold} --GSEA.v.1.0.fdr.q.val.threshold${GSEA_v_1_0_fdr_q_val_threshold} --GSEA.v.1.0.topgs${GSEA_v_1_0_topgs} --GSEA.v.1.0.adjust.FDR.q.val${GSEA_v_1_0_adjust_FDR_q_val} --GSEA.v.1.0.gs.size.threshold.min${GSEA_v_1_0_gs_size_threshold_min} --GSEA.v.1.0.gs.size.threshold.max${GSEA_v_1_0_gs_size_threshold_max} --GSEA.v.1.0.reverse.sign${GSEA_v_1_0_reverse_sign} --GSEA.v.1.0.perm.type${GSEA_v_1_0_perm_type}. zip -r ${outputprefix}.pathway_gsea_mrnaseq_subtypes.zip . ; }. output {; File zip_results=""${outputprefix}.pat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,2,['error'],['error']
Availability,@tom-dyar fyi - I received this boxed error today as well. It eventually cleared when I did an sbt clean and rebuilt the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982,2,['error'],['error']
Availability,"@vdauwera commented on [Sun Jan 08 2017](https://github.com/broadinstitute/wdltool/issues/20). I had a few issues that were fixed by building the latest from source. There's a good chance that will also be the case for the other issues that have been reported. Would be good to cut a release to make it available without requiring people to build from source. . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281030248). Are we responsible for building a new release now that we own this repo? Or am I misremembering?. ---. @ruchim commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281054777). The wdltool release is a part of the Cromwell release process . ---. @knoblett commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281096658). Ah, thank you, that makes sense. @vdauwera do we still need a release? The most recent release was done 3 days prior to the creation of this ticket. ---. @vdauwera commented on [Mon Feb 20 2017](https://github.com/broadinstitute/wdltool/issues/20#issuecomment-281097178). Does the release process include making a packaged jar? I think my problem was that there wasn't a jar that corresponded to the latest releases version available for download, come to think of it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872:303,avail,available,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872,3,"['avail', 'down']","['available', 'download']"
Availability,"@vdauwera you have officially been pinged. ðŸ˜„ . As I mentioned above I don't have strong opinions about where the documentation ends up, but I put it here for now so I could edit it. If the content remains here we should either take down or mark as ""pre-28"" the content that's on the forums.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326:35,ping,pinged,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326,2,"['down', 'ping']","['down', 'pinged']"
Availability,"@vortexing - task input and output data staging is handled by the `ecs-proxy` container that is installed when you create a custom AMI with ""cromwell"" settings. If you are not seeing data move in/out a good place to check for errors is the Cloudwatch log for a task that didn't have it's data staged correctly. Append `-proxy` to the job's cloudwatch log url to get the logging generated by the `ecs-proxy`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845:226,error,errors,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845,1,['error'],['errors']
Availability,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:201,echo,echo,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053,2,['echo'],['echo']
Availability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:216,echo,echo,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,3,['echo'],['echo']
Availability,"@vsoch Actually I'm piping the singularity command to qsub, so I'm actually doing exactly what you're suggesting :stuck_out_tongue: (Notice the `echo ' \` and `' | \` surrounding the singularity command.). For the binds: `/exports` is needed because output is written to this directory and the `${cwd}:${docker_cwd}` is needed because the script under `${script}` (generated by cromwell) uses this path. As for the multiprocessing thing, I get an error: `OSError: [Errno 30] Read-only file system` when I try to run a python script using multiprocessing inside a container. This seems to be caused by the fact that `/dev/shm` is a symlink to `/run/shm` (as in that issue I linked), so it can't be found unless I mount the `/run` directory. So singularity works just fine without it, but this is needed for certain python based programs. This is also specific to the cluster, as on my own pc I don't have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438692924:145,echo,echo,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438692924,2,"['echo', 'error']","['echo', 'error']"
Availability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several ðŸ”¥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. ðŸ¤ž",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:617,down,down,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['down'],['down']
Availability,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:649,robust,robust,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,1,['robust'],['robust']
Availability,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:154,error,errors,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463,1,['error'],['errors']
Availability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1406,failure,failures,1406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['failure'],['failures']
Availability,@wleepang Could you share what change you made to fix the problem? I'm getting the same error on my own CloudFormation template.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-506369596:88,error,error,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-506369596,1,['error'],['error']
Availability,@wresch Looks like you are having DNS resolution issues. \. Can you resolve the endpoint per the error message?; `nslookup https://auth.docker.io/token`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891,1,['error'],['error']
Availability,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,redundant,redundant,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905,1,['redundant'],['redundant']
Availability,"@yfarjoun Yeah, I agree. @vdauwera quickly provided a totally valid use case beyond ""I'm screwing around"". Specifically what I was concerned about here is providing a case where we allow corners to be cuttable for the implementer which then leave loaded guns sitting around for downstream users to shoot themselves with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814:278,down,downstream,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814,1,['down'],['downstream']
Availability,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:293,error,error,293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,1,['error'],['error']
Availability,"A 503 StorageException seems to have failed one of the centaur JES jobs, and hence the workflow. Via the [cromwell.log](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:390,ERROR,ERROR,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,3,"['ERROR', 'Failure', 'recover']","['ERROR', 'Failure', 'recoverWith']"
Availability,A bad file input should create an error on the BCS backend but is currently succeeding. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3522:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522,1,['error'],['error']
Availability,"A bit more info on this. The job mentioned above ran out of disk space. The monitoring.log is full of ""out of space"" errors. However, the job ran to completion and the output directory has an rc file containing 0, so Cromwell considered it a success. But the output files were truncated to zero bytes, presumably due to the disk space issue. Normally we get a hard failure when we run out of disk space but not in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997:117,error,errors,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997,2,"['error', 'failure']","['errors', 'failure']"
Availability,A cleanly shut down Cromwell instance cleans up the workflow store to null out the Cromwell instance ID field for the workflow store entries it was running. When a new Cromwell instance comes up it will consider those workflow store entries to be fair game for pickup because those instance ID fields are null. However an uncleanly shut down Cromwell does not null out the Cromwell instance ID field of its running workflows before it exits. When a new Cromwell instance comes up it will see that those workflow store entries appear to be owned by another Cromwell and will only pick them up if the heartbeats on those rows are older than the workflow heartbeat TTL (default 10 minutes). . The problem here was some vestigial logic for the way restarts used to work that no longer makes sense in the 2/3-implemented horizontal Cromwell system. It is now entirely reasonable to see workflows in Running or Aborted state with or without a heartbeat timestamp depending on whether the Cromwell that was previously running those workflows was shut down cleanly or not.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3675:15,down,down,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3675,6,"['down', 'heartbeat']","['down', 'heartbeat', 'heartbeats']"
Availability,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:47,failure,failures,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432,2,"['failure', 'mainten']","['failures', 'maintenance']"
Availability,"A couple of separate ""belt-and-braces"" fixes to BW-478 which allowed errors in job preparation engine execution to leave workflows stuck in a zombie/Running/""call Starting"" state:. * Stop the engine function itself from throwing an exception; * Put a safety catch in the Job Execution actor to catch any other exceptions thrown by engine functions; * Put a safety catch in the `ErrorOr` `flatMap` function to automatically catch any exception thrown in `for`-comprehension chains.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6161:69,error,errors,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6161,2,"['Error', 'error']","['ErrorOr', 'errors']"
Availability,A couple of tactical centaur reliability tune-ups [BW-484],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6156:29,reliab,reliability,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6156,1,['reliab'],['reliability']
Availability,"A few workflows that we aborted in Cromwell-as-a-Service have the status ""Aborted"" (e.g. ""429e0aaf-c429-4438-a12d-734f1f444801"") but the subworkflow that was running when the parent workflow was aborted is still in the ""running"" status. When trying to abort the subworkflow that is still running (""34074359-f8ed-4402-ba65-c92ab550e999""), I see the error:. ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 34074359-f8ed-4402-ba65-c92ab550e999 because no workflow with that ID is in progress""; }; ```. It looks like Cromwell thinks the workflow is not running, but it's metadata says that it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654,2,['error'],['error']
Availability,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:92,alive,alive,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549,1,['alive'],['alive']
Availability,"A follow-up question (prompted by the `quay.io` outage today), does `singularity exec` check the internet to see if it has the most up to date version of the tag? What happens if I gave it a docker with digest instead, would it still poll the internet?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760:48,outage,outage,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760,1,['outage'],['outage']
Availability,"A key feature for job manager is an ability to 'archive' a job so that it no longer shows in the UI when you don't want to see it anymore. Say it was a failure and you have dealt with the failure. You don't need to see it every time you look at recently failed jobs if you've dealt with it. The idea for implementing this was to have a label key called ""flag"" and have a value ""archived"" that we could apply to jobs a user wants to archive. The part we are struggling with is how to filter those out of view in the UI. If it's not difficult, the best way would be to be able to do queries where we say ""show me all the jobs that match this query and do not have the label `flag:archived` attached to them"". Is this something that would be possible?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3608:152,failure,failure,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3608,2,['failure'],['failure']
Availability,"A partial implementation of the WES 1.0 standard directly embedded in the Cromwell server (modulo auth, but that's another story altogether). Why partial? Because I wanted to keep PR sizes down and didn't want to be rebasing every 3 days. Also this lays the basic infrastructure, so might as well get commentary on that. It's not hurting anything to have a partial implementation. Why status & abort? Because they were the easiest to do.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4425:189,down,down,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4425,1,['down'],['down']
Availability,"A problem (#4364) existed with aliased tasks not being correctly separated on the AWS instance. . The test case relied on a file on the EC2 instance being written to by both tasks and failed if this happened. Unfortunately, the test case only works (ie fails correctly) if the two aliased tasks *happen to* run on the same EC2 instance. Come up with a way to make this test reliably fail if the aliasing is broken. Maybe by forcing centaur tests to always run on exactly one instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4848:374,reliab,reliably,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848,1,['reliab'],['reliably']
Availability,"A quick question: if I run in local my workflow and I parallelize with several sample using scatter, Cromwell, takes all the CPU that we have in our machine. I then used this option in every task in order to take only 6 CPU and let some memory also for my colleagues:. ```; runtime {; docker_user: ""ngs""; cpu: 6; }; ```. But this does not work, and all the CPU are taken at the same time. And I have these errors:. The first is this one: ; `[warn] Local [f8d35e0f]: Key/s [cpu] is/are not supported by backend. Unsupported attributes will not be part of job executions.`. The second this one: ; `[warn] BackgroundConfigAsyncJobExecutionActor [7e5755bcscMeth.mapping:0:1]: Unrecognized runtime attribute keys: cpu`. Is there any explanation for this? These errors do not cause any interruption though.; Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367:406,error,errors,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367,2,['error'],['errors']
Availability,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057,2,['failure'],"['failure', 'failures']"
Availability,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5228:324,echo,echo,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228,5,['echo'],['echo']
Availability,"A significant amount of GotC failures are due to out of memory / disk errors.; Design a mechanism that allow to specify custom retry strategies that can modify runtime parameters based on failure modes. For example, â€œRetry on return code X with double the amount of memory and / or diskâ€. Thoughts:; - `currentAttempt()` wdl function to be used as a variable in a memory / disk formula; - monitor the job (monitoring script ?) to detect disk / memory overflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1847:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1847,3,"['error', 'failure']","['errors', 'failure', 'failures']"
Availability,"A simple rewrite of a CKTS test in Centaur, I'm curious to hear what the waterfowl think before going too far down this path. Some other CTKS tests I've looked at would require adding a minor feature or two to Centaur (e.g., assertions for exclusivity of outputs) but so far nothing major.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4503:110,down,down,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4503,1,['down'],['down']
Availability,"A task in a workflow failed because of the issue:; ```; Gsutil failed: Could not capture docker logs: Unable to capture docker logs exit status 1; ```; We could find all of the expected output files for the failed task in the cromwell-execution bucket, except `stdout.log` and `stderr.log` files. And we can make sure that this task only used nearly 2% of the disk space. Only 1 out of 8000 workflows ran into this issue, and re-run the failed workflow with call caching on didn't run into the same error, so it seems like an intermittent problem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615:499,error,error,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615,1,['error'],['error']
Availability,"A user (@helgridly) was flummoxed when they tried to use `read_json` to read a result back into WDL after running a python task. They had designed a method around the idea of being able to serialize and deserialize to/from JSON and had a bad experience at the last minute when these functions turned out to not be implemented. The less-than-ideal error message did not help matters. Although the general-purpose `read_json` might be dangerous, we could at least support something like `read_json_array`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1825:347,error,error,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825,1,['error'],['error']
Availability,"A user has a workflow where the output of a task is localized to a path ""/some/path/./blah/blah.txt"". A task downstream that goes to utilize that file path as input fails silently because it's trying at some point to convert that path to a RealPath (http://googlecloudplatform.github.io/google-cloud-java/0.7.0/apidocs/com/google/cloud/storage/contrib/nio/CloudStoragePath.html method:`toRealPath`) and it fails to do so as dot-dirs are not allowed. Something throws an exception `IllegalArgumentException - if path contains extra slashes or dot-dirs when permitEmptyPathComponents is false, or if the resulting path is empty.` and it's present in the server logs only and the workflow remains running and the task hangs in notstarted state. It would be great if dot-dirs were handled or atleast reported back as not-supported and fail the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2506:109,down,downstream,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2506,1,['down'],['downstream']
Availability,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698,3,"['error', 'failure']","['error', 'failure']"
Availability,"A user requested this [here](https://gatkforums.broadinstitute.org/firecloud/discussion/10273/error-messages-should-include-the-problematic-input-whenever-possible-disk-strings). This user ran into an error message which stated he had an improperly formatted disk string, but with no further information on how to find out which disk string was incorrect. It would be better if the error message specifically referenced which disk string was improperly formatted, either through a line number, or by showing the specific incorrect disk string referenced. In addition, his improper formatting was `local-disk 0 HDD`, but the error message stated that the format must be `local-disk SIZE TYPE`. If you could add that `SIZE` must be a non-zero value, that would improve the error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2739:94,error,error-messages-should-include-the-problematic-input-whenever-possible-disk-strings,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2739,5,['error'],"['error', 'error-messages-should-include-the-problematic-input-whenever-possible-disk-strings']"
Availability,"A user requested this [here](https://gatkforums.broadinstitute.org/firecloud/discussion/10319/incorrect-error-message-when-task-fails-to-complete). . When a script fails to run, the error that is eventually returned is that Cromwell failed to delocalize the output file, due to the fact that the file is missing because the script failed to run. They would like the error message to be clearer as to the true reason of the failure, that it is due to an invalid return code from the script and (secondarily) a failure to delocalize the missing file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2740:104,error,error-message-when-task-fails-to-complete,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2740,5,"['error', 'failure']","['error', 'error-message-when-task-fails-to-complete', 'failure']"
Availability,"A very simple WDL:; ```; version development. task sfx {; input {; Array[String] x; String docker = ""ubuntu:20.04""; }. command <<<; echo ~{sep="" "" x}; >>>. output {; Array[String] x_sfx = suffix("".sfx"", x); }. runtime {; docker: docker; }; }. workflow sfx {; input {; Array[String] x; }. call sfx {; input:; x = x; }. output {; Array[String] x_sfx = sfx.x_sfx; }; }; ```; Will fail when run with the latest version of Cromwell:; ```; java -jar cromwell-51.jar run sfx.wdl -i sfx.json; ```; With the following error:; ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'suffix'; ```; Yes, the Array[String] [suffix](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#arraystring-suffixstring-arrayx)(String, Array[X]) function is part of the WDL specification. It seems like this should not be difficult to implement as the [prefix](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#arraystring-prefixstring-arrayx) function is already implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5549:132,echo,echo,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5549,2,"['echo', 'error']","['echo', 'error']"
Availability,"A while back we changed things to return gzipped content in the metadata endpoint by default, despite what the client says it can handle. generally the client should be telling the server what it can expect and the server either handles it appropriately or returns a 415. I think that we should switch this to encoding on request. The downside is that there might be users who would be much better off getting a gzipped response but not realizing they have to do something extra (depending on their client).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2419:335,down,downside,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2419,1,['down'],['downside']
Availability,"A while back we had a fairly serious performance bug due to the usage of `mapValues`. It'd gone away for a while but it seems to be used somewhat frequently again. Currently 22 times in Cromwell. A quick skim of them made at least a few of them look to be pretty suspicious looking. Remove these, or at least prove that any remaining invocation can't possibly be a big deal down the road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2434:374,down,down,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2434,1,['down'],['down']
Availability,"A workflow was submitted and the inputs looked like:; ```; ""inputs"": {; ""HaplotypeCallerGvcfScatterWf.ref_dict"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.dict\"""",; ""HaplotypeCallerGvcfScatterWf.MergeVCFs.disk_size"": 10,; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.mem_size"": ""4 GB"",; ""HaplotypeCallerGvcfScatterWf.ref_fasta"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta\"""",; ""HaplotypeCallerGvcfScatterWf.ref_fasta_index"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta.fai\"""",; ""HaplotypeCallerGvcfScatterWf.input_bam_index"": ""gs://gatk-tutorials/workshop_1702/variant_discovery/data/bams/father.bai"",; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.interval_padding"": 100,; ""HaplotypeCallerGvcfScatterWf.MergeVCFs.mem_size"": ""2 GB"",; ""HaplotypeCallerGvcfScatterWf.scattered_calling_intervals_list"": ""gs://gatk-test-data/intervals/b37_wgs_scattered_calling_intervals.txt"",; ""HaplotypeCallerGvcfScatterWf.input_bam"": ""gs://gatk-tutorials/workshop_1702/variant_discovery/data/bams/father.bam"",; ""HaplotypeCallerGvcfScatterWf.HaplotypeCaller.disk_size"": 100; },; ```; Cromwell started jobs but they don't have operations IDs and the jobs/workflow remain in the ""Running"" state forever. Ideally the workflow should fail, and fail gracefully with an error message that makes sense. This ticket can be easily reproduced on 25_hotfix by corrupting a file input path to look like:; ```; ""HaplotypeCallerGvcfScatterWf.ref_fasta_index"": ""\""gs://gatk-legacy-bundles/b37/human_g1k_v37_decoy.fasta.fai\"""",; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2178:1284,error,error,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2178,1,['error'],['error']
Availability,A/C write a unit test that characterizes the performance of heartbeat writing with and without autocommit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416:60,heartbeat,heartbeat,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416,1,['heartbeat'],['heartbeat']
Availability,"AC: Confirm that this is reproducible behavior in the latest version of Cromwell, and given thats the case, confirm that job/workflow failure messages make it over to the workflow logs (not just the server logs)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466:134,failure,failure,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466,1,['failure'],['failure']
Availability,"AC: We require a unit test that recreates a deadlock occurs when workflow heartbeats are de-serialized (#4239). This is important because this is a problem we've seen before in production (prior to the serialization of heartbeats) and that's the *main* scenario we have to solve for as a requirement to be able to scale Cromwell horizontally. Since we've only really seen deadlock behavior in production, it seems running a unit test at high scale could be one possible way to reproduce the deadlocking error. For example:. Start 10K workflows (that just sleep) and configure the heartbeat-interval rate to be the shortest duration possible (or whatever is reasonable). One would have to check the Cromwell server logs to see if there's a SQL exception",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4414:74,heartbeat,heartbeats,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4414,4,"['error', 'heartbeat']","['error', 'heartbeat-interval', 'heartbeats']"
Availability,"AC:; Something like this should run:. JSON:; {; ""Test.testMe"":{""left"":""Left"",""right"":""Right""}; }. WDL:; workflow Test {; 	Pair[String,String] testMe; call echoPair{input: pair=testMe}; }. task echoPair{; Pair[String,String] pair; command{; echo (${pair.left}, ${pair.right); }; output {; String out = read_string(stdout()); }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2260:155,echo,echoPair,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2260,3,['echo'],"['echo', 'echoPair']"
Availability,"ASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permitted; [2020-08-08 19:20:00,49] [error] 6e4fa8a1:main.blaster:46:1: Hash error (Is a directory), disabling call caching for this job.; ```. **contents of the BLASTDB directory:**; ```; /data/blast/blastdb$ ls; nt.00.nhd nt.01.nhd nt.02.nhd nt.03.nhd nt.04.nhd nt.05.nhd nt.06.nhd nt.07.nhd nt.08.nhd nt.09.nhd nt.10.nhd nt.11.nhd nt.12.nhd nt.13.nhd nt.14.nhd nt.15.nhd nt.16.nhd nt.17.nhd nt.18.nhd nt.19.nhd nt.20.nhd nt.21.nhd nt.22.nhd nt.23.nhd nt.24.nhd nt.nal nt.00.nhi nt.01.nhi nt.02.nhi nt.03.nhi nt.04.nhi nt.05.nhi nt.06.nhi nt.07.nhi nt.08.nhi nt.09.nhi nt.10.nhi nt.11.nhi nt.12.nhi nt.13.nhi nt.14.nhi nt.15.nhi nt.16.nhi nt.17.nhi nt.18.nhi nt.19.nhi nt.20.nhi nt.21.nhi nt.22.nhi nt.23.nhi nt.24.nhi nt.ndb nt.00.nhr nt.01.nhr nt.02.nhr nt.03.nhr nt.04.nhr nt.05.nhr nt.06.nhr nt.07.nhr nt.08.nhr nt.09.nhr nt.10.nhr nt.11.nhr nt.12.nhr nt.13.nhr nt.14.nhr nt.15.nhr nt.16.nhr nt.17.nhr nt.18.nhr nt.19.nhr nt.20.nhr nt.21.nhr nt.22.nhr nt.23.nhr nt.24.nhr nt.nos nt.00.nin nt.01.nin nt.02.nin nt.03.nin nt.04.nin nt.05.nin nt.06.nin nt.07.nin nt.08.nin nt.09.nin nt.10.nin nt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:2073,error,error,2073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,1,['error'],['error']
Availability,AWS backend: sleep 60 secs and retry 5 times if download or upload fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4749:48,down,download,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4749,1,['down'],['download']
Availability,AWS hello output parsing failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4373:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4373,1,['failure'],['failure']
Availability,AWS: Error building path,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6233:5,Error,Error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6233,1,['Error'],['Error']
Availability,AWS: Error creating reconfigured-script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106:5,Error,Error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106,1,['Error'],['Error']
Availability,"Able to replicate the break from v36 to v37, I switched my check-alive in my config to use scontrol and it succeeded:; ```; ""check-alive"": ""scontrol show job ${job_id}"",; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252:65,alive,alive,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252,2,['alive'],['alive']
Availability,"Accepts yaml input files during submission.; Whether the input is in yaml or json, it will be written in **json** in the database, as well as metadata.; Also `cats`ify things a little to get better error reporting on incorrect submit request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832:198,error,error,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832,1,['error'],['error']
Availability,"Accommodate ""enhanced"" Requester Pays error messages [CROM-6820]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,1,['error'],['error']
Availability,"According to [the docs](https://dev.mysql.com/downloads/connector/j/) it is ""highly recommended"" that we upgrade to the latest version, even if our MySQL is behind - `5.6.36-google-log` in our case. Reason I started looking into this was https://github.com/broadinstitute/cromwell/issues/4689",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690:46,down,downloads,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690,1,['down'],['downloads']
Availability,"According to https://cromwell.readthedocs.io/en/stable/backends/Google/; The format for the json file should be; ```; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:438,error,error,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,3,"['Down', 'Error', 'error']","['DownField', 'Errors', 'error']"
Availability,"According to of the WDL spec, if an undefined optional value is used in a string interpolation expression, the result of the expression is the empty string. However this doesn't seem to work correctly if there is more than two arguments to `+`, e.g. (x + y + z). ```; version 1.0. workflow Test {; call Tester; }. task Tester {; input {; String? optional; }; command <<<; echo interpolation 1: ~{optional + "" something2""}; echo interpolation 2: ~{""something1 "" + optional}; echo interpolation 3: ~{""something1 "" + optional + "" something2""}; >>>; output {; String out = read_string(stdout()); }; }; ```; When run, example 1 and 2 produces no output but example 3 produces output. I would expect 3 to also produce no output.; ```; $ java -jar cromwell-38.jar run /tmp/test.wdl ; ...; {; ""outputs"": {; ""Test.Tester.out"": ""interpolation 1:\ninterpolation 2:\ninterpolation 3: something2""; },; ""id"": ""8e2523af-b6eb-45d9-901f-8c7e0b6bd726""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4724:372,echo,echo,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4724,3,['echo'],['echo']
Availability,"According to the [list of PAPI error codes](https://cloud.google.com/life-sciences/docs/troubleshooting#unavailable_14), `14` is indeed preemption so I agree that is surprising on a non-preemptible. You can use Cromwell retries to re-run; or reach out to your GCP support venue to better understand what's going on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118,1,['error'],['error']
Availability,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245,2,['error'],['error']
Availability,Actor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:749); 	 at scala.util.Try$.apply(Try.scala:213); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1139); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1131); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at akka.actor.Actor.aroundReceive(Actor.scala:539); 	 at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:7974,robust,robustExecuteOrRecover,7974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['robust'],['robustExecuteOrRecover']
Availability,Actor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132) ; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279:2459,robust,robustExecuteOrRecover,2459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279,1,['robust'],['robustExecuteOrRecover']
Availability,"Actually, the main problem is that the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had; an inkling that it's a missing file problem. so I have to resort to divide; and conquer in order to identify the missing file...and that's a pain. On Sun, Jul 10, 2016 at 10:08 PM, Jeff Gentry notifications@github.com; wrote:. > The former. He was looking for a backend-aware validation type behavior; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0hYeQnaJcsNEVJlnxrz9-tA880TLks5qUaWXgaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585,1,['error'],['error']
Availability,"Adam and Jeff;; Thanks for this. I'm definitely agreed I don't want to break your tests every time we accidentally introduce a bug in bcbio. We have tagged versions of the Docker images (https://quay.io/repository/bcbio/bcbio-vc?tab=tags) so could pin to a specific CWL with a specific Docker tag. Data doesn't change as often in a back-incompatible way but you could also have a copy of that if it becomes an issue. We could generate CWL that ties to a specific Docker build, or just have your tests download the tagged version we've tested on. How aggressive is Cromwell at updating the local Docker version based on what's in the CWL? If it would leave a pre-downloaded and ready to go Docker alone, it seems like a pre-pull from a known tag and the pinned CWL should do most of what we need. Would that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610:501,down,download,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610,2,['down'],"['download', 'downloaded']"
Availability,Adam;; Thanks so much for looking into this. I'm glad the error makes sense and is translatable to a fix for re-reading. We love call caching for job re-use so prefer not to disable it for now. Looking forward to this fix and having cleaner runs going forward. Thanks again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415005318:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415005318,1,['error'],['error']
Availability,Add Recover to Backend Actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/663:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/663,1,['Recover'],['Recover']
Availability,Add a case for 504 error in GCS IoActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344,1,['error'],['error']
Availability,Add a new API endpoint to CromIAM which will allow a user to update the collection name for one or more workflows in a one to many fashion - i.e. one collection name will be applied to 1+ workflows. If Sam says that the user does not have access to this collection name it will be an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2839:284,error,error,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2839,1,['error'],['error']
Availability,Add a workflow with at least 3 calls. For the call which triggers the Cromwell restart make sure that it involves a long sleep or something like that so that we know the job is still considered to be running by Cromwell when we shut down the server.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2353:233,down,down,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2353,1,['down'],['down']
Availability,Add config option to shutdown cromwell when unable to write heartbeats.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:60,heartbeat,heartbeats,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['heartbeat'],['heartbeats']
Availability,Add details in failure when failing to upload auth file for JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1453:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1453,1,['failure'],['failure']
Availability,Add metadata for WorkflowMaterialiserActor Failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/966:43,Failure,Failure,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/966,1,['Failure'],['Failure']
Availability,Add new retryable case: JES error code 2. Message: Instance failed to start due to preemption.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,1,['error'],['error']
Availability,Add non-preemption retryable JES error codes into JES wiring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1926:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1926,1,['error'],['error']
Availability,Add recovery / abort to HtCondorBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/885:4,recover,recovery,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/885,1,['recover'],['recovery']
Availability,Add test to recreate MySQL heartbeat deadlock.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4415:27,heartbeat,heartbeat,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4415,1,['heartbeat'],['heartbeat']
Availability,Add the return code to JES error messages.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2686:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2686,1,['error'],['error']
Availability,Add workflow option to not run new job upon certain call cache errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:63,error,errors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,1,['error'],['errors']
Availability,"Added a test to our CI. It will build docker images for `cromwell`, `womtool`, `cromiam`, and `cromwell-drs-localizer` in a way that is very similar to how we do it in our release process (`chart_update_on_merge.yml`). Build errors should now be caught earlier in the development process.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7151:225,error,errors,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7151,1,['error'],['errors']
Availability,Added heartbeats to swr and refactor kills BA-5983,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5165:6,heartbeat,heartbeats,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5165,1,['heartbeat'],['heartbeats']
Availability,Added index and attempt number to failure messages. Closes #1479,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1580:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1580,1,['failure'],['failure']
Availability,"Added methods for retrieving cwl File/Directory parts like basename.; Added a directory listing stub close to the existing glob listing code.; Updated CWL types to reuse more Polys and to parse ecmascripts with embedded newlines and extra whitespace.; Minor cleanup around throw/ErrorOr/Try conversions especially around Javascript processing.; JsUtil encoding now encodes WomMap/WomArray into instances of JsObject instead of java.util.Map/java.lang.Array.; Hacked JsUtil to support reading in ""structs"" of mixed types.; Replaced all usages of .stripMargin.replaceAll with .stripMargin.replace so that the replacements aren't processed like regexs.; Removed docs/.Rhistory and ./Running empty files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3124:279,Error,ErrorOr,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3124,1,['Error'],['ErrorOr']
Availability,Adding @vdauwera's comment about adding error codes to GATK from [DSDE-docs #1742](https://github.com/broadinstitute/dsde-docs/issues/1742#issuecomment-280304238):; >We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776,2,['error'],['error']
Availability,"Adding `setThrowExceptionOnExecuteError(false)` call causes execution flow to go into the path we want that inspects the error more precisely in `handleGoogleError`. I tested this by deliberately breaking the request so it always gets a `400` back. With the existing code, the log looks just like what we see in prod:. ```; 2023-11-01 19:54:12 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - PAPI request worker had 2 failures making 5 requests: ; 400 Bad Request; POST https://lifesciences.googleapis.com/v2beta/projects/1005074806481/locations/us-central1/operations/6175597626605185257:cancel; {; ""code"": 400,; ""errors"": [; {; ""domain"": ""global"",; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""reason"": ""parseError""; }; ],; ""message"": ""Invalid JSON payload received. Unexpected token.\nasdf\n^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle."",; ""status"": ""INVALID_ARGUMENT""; }; ```. <img width=""1238"" alt=""Screenshot 2023-11-01 at 15 43 59"" src=""https://github.com/broadinstitute/cromwell/assets/1087943/63e4e788-517f-4f1e-a4ff-4075cec3e6d3"">. ---. Changing `throwExceptionOnExecuteError` to `false`, we see that we no longer throw an exception and we get the expected ""no longer running"" message in the log!. ```; 2023-11-01 19:57:48 cromwell-system-akka.dispatchers.backend-dispatcher-162 INFO - PAPI declined to abort job projects/1005074806481/locations/us-central1/operations/5250112889402522122 in workflow b70eafc9-66a7-4b22-b9bc-621c22b5a4ed, most likely because it is no longer running. Marking as finished. Message: Invalid JSON payload received. Unexpected token.; asdf; ^ Payload appears to be compressed. It may either be corrupt or uncompressed data may be too large for the server to handle.; ```. <img width=""1239"" alt=""Screenshot 2023-11-01 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7245:121,error,error,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7245,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,Adding retries and proper failures to the creation of JES runs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/227:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/227,1,['failure'],['failures']
Availability,Additional Failure Metadata Inconstency,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201:11,Failure,Failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201,1,['Failure'],['Failure']
Availability,"Additional info since the time this issue was filed: Alibaba's [Batch Compute service (BCS) is now available in the US](https://www.alibabacloud.com/help/doc-detail/61360.htm?spm=a2c63.l28256.a3.23.194f25719KjP66). This helps test Cromwell-in-the-US-using-DockerHub, but for CN users the above issues still need to be addressed, including figuring out a way for to check hashes from OSS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138:99,avail,available,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138,1,['avail'],['available']
Availability,"Additionally, @rexwangcc has been working on https://cromwell-tools.readthedocs.io, which at least partially seems to implement what you proposed. We're also planning to move other functionality, like parsing of metadata.json for failures, into it over time, to be used together with `miniWDL` for WDL debugging support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501826170:230,failure,failures,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501826170,1,['failure'],['failures']
Availability,"Additionally, in both the above trials, Cromwell still managed to start, but failed with the following error when starting a submission:. ```; 2019-02-25 18:47:23,071 cromwell-system-akka.actor.default-dispatcher-33 ERROR - Error during processing of request: 'Unknown factory null'. Completing with 500 Internal Server Error response. To change default exception handling behavior, provide a custom ExceptionHandler.; java.lang.IllegalStateException: Unknown factory null; 	at akka.http.impl.util.package$.actorSystem(package.scala:34); 	at akka.http.scaladsl.settings.SettingsCompanion.default(SettingsCompanion.scala:20); 	at akka.http.scaladsl.settings.SettingsCompanion.default$(SettingsCompanion.scala:20); 	at akka.http.scaladsl.settings.ParserSettings$.default(ParserSettings.scala:119); 	at cromwell.webservice.CromwellApiService.$anonfun$workflowRoutes$68(CromwellApiService.scala:233); 	at akka.http.scaladsl.server.Directive$.$anonfun$addByNameNullaryApply$2(Directive.scala:134); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1091:82,down,down,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091,1,['down'],['down']
Availability,Address DB write failures for WorkflowStore and JobStore,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1223:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1223,1,['failure'],['failures']
Availability,"Addresses [WX-1282](https://broadworkbench.atlassian.net/browse/WX-1282). PR replaces the INNER JOIN statement against `pg_largeobject` with a `lo_get` statement to avoid ""Permission Denied"" errors that comes from scanning the `pg_largeobject` table (which enforces owner/role permissions for each row which needs to be taken in consideration for the new Workflows App ecosystem). [WX-1282]: https://broadworkbench.atlassian.net/browse/WX-1282?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228:191,error,errors,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228,1,['error'],['errors']
Availability,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306), [WX-1307](https://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:970,error,error,970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Adds a cron trigger for the centaur integration tests that cause the integration tests to run at midnight each weekday and post failures to slack at #cromwell-integration-action,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7089:128,failure,failures,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7089,1,['failure'],['failures']
Availability,Adds an event handler for a backend job that failed with a retry able failure. The engine attempts to restart the failed job again on the same backend for a `max` number of times (where that number is configurable and controlled by the engine),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/795:70,failure,failure,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/795,1,['failure'],['failure']
Availability,"Adds capability to. * Download metadata from GCS using the IoActor; * Fetch workflow labels from the database; * Combine the two into a single response. Currently tested using mock IoActor and ServiceRegistry, and not ""created"" by the main Cromwell process",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5100:22,Down,Download,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5100,1,['Down'],['Download']
Availability,Adjust expected failures for new CWL conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3213:16,failure,failures,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3213,1,['failure'],['failures']
Availability,Adjust the error message for the Call Caching diff endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2406:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2406,1,['error'],['error']
Availability,"After #2952, `missing_input_failure` has a somewhat nicer error: `""Evaluating read_string(wf_hello_input) failed: gs://nonexistingbucket/path/doesnt/exist""`. So, I've ticked that box. Just `missing_optional_output` still to go!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871#issuecomment-348291490:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871#issuecomment-348291490,1,['error'],['error']
Availability,After a little research it seems the default Akka supervision decider and strategy looks [pretty reasonable](https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/FaultHandling.scala#L156) for this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761#issuecomment-397602143:187,Fault,FaultHandling,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761#issuecomment-397602143,1,['Fault'],['FaultHandling']
Availability,"After reviewing cromwell code with Dan, we think that it's OK to downgrade the log level of **WorkflowFailedResponse** event in **WorkflowManagerActor** to INFO so it won't propagate to Sentry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544:65,down,downgrade,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544,1,['down'],['downgrade']
Availability,"After running a job that completed (or so it seems by the files generated ... Cromwell labels it as failed), I tried to retrieve the metadata but I got this error message instead:; ```; {; ""status"": ""error"",; ""message"": ""Metadata for workflow xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx exists in database but cannot be served because row count of 1249471 exceeds configured limit of 1000000.""; }; ```; I have tried to understand what configuration variable holds this 1000000 row limits, but I could not figure it out. :-( I think it would save users valuable time if they were directed to what to do when these type of very specific errors are recognized. I know this is a little bit more work on the developer side but I would certainly be very grateful ... and I would also stop asking questions. :-)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236,3,['error'],"['error', 'errors']"
Availability,"After running a large workflow on GCS with ~2,500 tasks, rather than the workflow transitioning from running to success, I received the following error:; ```; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Workflow is making no progress but has the following unstarted job keys: \nScatterCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1\nConditionalCollectorKey_PortBasedGraphOutputNode_xxx.yyy:-1:1"",; ""causedBy"": []; }; ],; ```. The `xxx.yyy` output variable is from a task being scattered and defined as follows:; ```; task xxx {; ...; output {; ...; File? yyy = if defined(zzz) then ... else None; }; }; ```; With `zzz` not defined. Despite the error, the job seemed to have completed successfully. However the files were not moved into the `final_workflow_outputs_dir` as they were supposed to, causing an unwelcome inconvenience. This [problem](https://support.terra.bio/hc/en-us/community/posts/360073398892-Workflow-failure-Workflow-is-making-no-progress-but-has-the-following-unstarted-job-keys-) has also been reported about six months ago in the Terra forum. The job run with CallCaching activated but no entries in the cache were present before the job started. The only event of notice was that at some point Cromwell crashed due to high memory demand (while trying to retrieve the metadata for the workflow) but, after I restarted it, the workflow proceeded without issues. The workflow is a `version development` WDL, as can be evinced from the use of the `None` keyword.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6238:146,error,error,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6238,4,"['error', 'failure']","['error', 'failure-Workflow-is-making-no-progress-but-has-the-following-unstarted-job-keys', 'failures']"
Availability,"After this morning's discussion of Cromwell's memory usage I poked around the Travis docs and found the container infrastructure on which we were running gave us only 4 GB:. https://docs.travis-ci.com/user/ci-environment. Also on this page is mentioned the new Trusty Tahr beta environment which offers 7.5 GB. I've tried this and have seen no intermittent SlickDataAccess or other failures. As a bonus this is a real VM (not a container) and can run all of our Docker tests. The one gotcha I've found is that these builds don't start as quickly as the container builds, but if this delay remains reasonable it may be worth trading some lag for stability and Docker coverage. There were some additional changes required to the Travis YAML to add MySQL as that's not baked into Trusty, and I also had to pull the ubuntu:latest image in advance to keep the first Docker test from timing out. There was also a bit of weirdness with files not globbing in alphabetical order which broke a test.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/319:382,failure,failures,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/319,1,['failure'],['failures']
Availability,"After workflow succesffuly ran this failed with the below error when trying to copy the final outputs. ```; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log.; java.util.NoSuchElementException: None.get; at scala.None$.get(Option.scala:347) ~[cromwell.jar:0.19]; at scala.None$.get(Option.scala:345) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1$$anonfun$apply$26.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:728) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:727) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:155) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$buildCallFailureTransformer$1.apply(CallMetadataBuilder.scala:153) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:233) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.CallMetadataBuilder$$anonfun$15.apply(CallMetadataBuilder.scala:232) ~[cromwell.jar:0.19]; at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foldLeft(List.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Again 2779. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:493,Robust,RobustClientHelperSpec,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,2,['Robust'],['RobustClientHelperSpec']
Availability,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:29,fault,faulty,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966,1,['fault'],['faulty']
Availability,"Ah yes, let's just claim we fixed one bug then ðŸ˜„ ; My take; use, modify, or discard as desired:; > Fixed a bug that could cause workflows to fail unexpectedly with the error `413 Request Entity Too Large` when accessing Google Cloud Storage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600:168,error,error,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600,1,['error'],['error']
Availability,"Ah, I see now how you intent for it to work. I don't think this will be very practical on any kind of shared SGE HPC without a seperate poll rate. . As mentioned elsewhere; the call rate of the `pollStatus` is geared towards a low-impact filesytem check and not a high-impact call to the SGE queque master (i.e. `check-alive` uses `qstat`). Enabling the `exit-code-timeout` with a somewhat large numbers of tasks will quickly cripple any average SGE HPC. . If you enable this and are an HPC admin; make sure to keep taps on your submission services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492:319,alive,alive,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492,1,['alive'],['alive']
Availability,"Ah, that sounds really nice, but I don't think it's possible: the `actual` is of type `Terminal`, which does not include type information (which makes sense, because once parsing the grammar has broken down you can't make any guarantees that a symbol you find will fit into a defined universe of types)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394447827:202,down,down,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394447827,1,['down'],['down']
Availability,Ah. Now that you say this I'm betting it fails due to how the proxy sidecar works. My quick thought is that the approach in #5064 is likely better but I need to dig into more how the AWS localization is working in the first place. In the meantime I'll try to also ping @elerch and @wleepang in case they have thoughts here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509812726:264,ping,ping,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509812726,1,['ping'],['ping']
Availability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=â€¦` except one is supposed to use [`-Dlogback.configurationFile=â€¦`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,1,['error'],['error']
Availability,"All 3 failures also emit:; ```; - should return pagination metadata only when page and pagesize query params are specified *** FAILED *** (9 seconds, 457 milliseconds); java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3608ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4520#issuecomment-452071787:6,failure,failures,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520#issuecomment-452071787,2,"['avail', 'failure']","['available', 'failures']"
Availability,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:486,Error,Error,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152,3,['Error'],"['Error', 'Errors']"
Availability,"All right so after some investigations here is what I found:; - This problem only seems to appear on MySQL / CloudSQL 5.6; I created 2 CloudSQL instances, 1 on MySQL 5.6 and one on 5.7. I got the error with 5.6 but not 5.7. So if creating a new empty DB is not a problem for this workflow, I think that could be a (temporary) solution.; - About compression, based on the 5.6 instance I created on google, I think the size limit would be closer to 26MB; It's a 5.6.26 version exactly. ```; Your MySQL connection id is 15; Server version: 5.6.26 (Google); ```. And because it's post 5.6.22 the formula to get the max limit is `10% of innodb_log_file_size * innodb_log_files_in_group` (https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_log_file_size) . Which on the 5.6 instance is. ```; | innodb_log_file_size | 134217728 |; | innodb_log_files_in_group | 2 ; ```. => `(134217728 * 2) / 10 = 26843545 B ~= 26 MB`; - One other option could be like Henry said to not store the beginning of the paths and add it back at runtime when needed. This could (maybe) work for `File`s but not for `String`s, which I think is what is needed for this case (manipulate paths as `String`s). @jsotobroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-225966019:196,error,error,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-225966019,1,['error'],['error']
Availability,Allow for the fact that errorMessage might be null,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/408:24,error,errorMessage,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/408,1,['error'],['errorMessage']
Availability,Allow to ask for retries on JES error 13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1849:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849,1,['error'],['error']
Availability,Allows PAPI/GLS jobs to run a background action to upload a checkpoint file to GCS at regular intervals. Notes from demo:. - [x] Remove checkpoint file when task completes; - [x] Change upload interval to 10m,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:60,checkpoint,checkpoint,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,2,['checkpoint'],['checkpoint']
Availability,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['failure'],['failures']
Availability,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,2,['error'],['error']
Availability,"Also @wleepang I was able to do a docker pull on the AMI that I am using:. AMI ID: amzn-ami-2018.03.h-amazon-ecs-optimized (ami-0a0c6574ce16ce87a). `[ec2-user@ip-172-31-29-236 ~]$ docker pull ubuntu:latest; latest: Pulling from library/ubuntu; 473ede7ed136: Pull complete; c46b5fa4d940: Pull complete; 93ae3df89c92: Pull complete; 6b1eed27cade: Pull complete; Digest: sha256:29934af957c53004d7fb6340139880d23fb1952505a15d69a03af0d1418878cb; Status: Downloaded newer image for ubuntu:latest`. Let me know if there is something off here, it seems fine to me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460:449,Down,Downloaded,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460,1,['Down'],['Downloaded']
Availability,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:277,avail,available,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560,1,['avail'],['available']
Availability,"Also as a TOL, maybe consider `failure.toPrettyElidedString` in case the exception content is extremely long?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500:31,failure,failure,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500,1,['failure'],['failure']
Availability,Also cherry pick a couple of Khalid's CI fixes for unrelated failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5035:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5035,1,['failure'],['failures']
Availability,Also creates #852. Following things need to be done (separate stories?) :; - [ ] Add docker support (#884); - [ ] Add recovery and abort (#885); - [x] Add continueOnErrorCode support; - [ ] Find a way to add condor specific runtime attributes to Condor ClassAds (#886). Anything more to support?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/861:118,recover,recovery,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/861,1,['recover'],['recovery']
Availability,"Also have this error, using Cromwell 52, installed using this manual : . https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. logs say : fetch_and_run.is is a directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937,1,['error'],['error']
Availability,"Also is there any way to actually enumerate all the available settings? Probably this would be too many, and modifying some of them would break the system. Still it is nice to have, and I have tried searching for this 'inbuilt' defaults file in vain. I guess it is scattered across many java files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-477831107:52,avail,available,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-477831107,1,['avail'],['available']
Availability,Also removes large chunks of the WDL writer that were only used for very specific error messages.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7382:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7382,1,['error'],['error']
Availability,Also requesting a review by @salonishah11 because I think these resolvers might potentially be useful for downloading content from the `workflowUrl` as well,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-407170726:106,down,downloading,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-407170726,1,['down'],['downloading']
Availability,"Also seen: . ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): running [""docker"" ""pull"" ""quay.io/bcbio/bcbio-vc@sha256:90087824e545df6d3996a28360f2f0fd28dce611a989bbcc79aa8117d341f6ef""]: exit status 1 (standard error: ""Error response from daemon: Get https://quay.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790:268,error,error,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790,2,"['Error', 'error']","['Error', 'error']"
Availability,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:89,echo,echo,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981,4,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:265,echo,echo,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095,2,['echo'],['echo']
Availability,"Also, if I change the type of the output from `Object` to `Map[String, String]` I get a similar error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Map[String, String] name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853,2,"['error', 'failure']","['error', 'failures']"
Availability,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:378,down,download,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,2,"['down', 'error']","['download', 'errors']"
Availability,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> â€”; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1187,error,error,1187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:104,down,downstream,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731,1,['down'],['downstream']
Availability,Alternate command syntax for those investigating: `sbt 'server/run server'`. ~The original caused an error for me.~ EDIT: The original error may have been an unrelated error that pops up sometimes during `sbt clean compile`. I'm not sure of the default dependencies for `sbt */run` but it does appear that [`*/package`](https://www.scala-sbt.org/1.0/docs/Running.html#Common+commands) is being invoked and zipping up the class files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739,3,['error'],['error']
Availability,Alternative title: kicking the SDAS can down the road until 0.21 (when these methods will make sense),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1080:40,down,down,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1080,1,['down'],['down']
Availability,"Alternative to #5588 which completely removes this redundant queuing mechanism which doesn't seem to be doing what it thinks it was doing, and is worse in any case than the existing token distribution safety mechanisms.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:51,redundant,redundant,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redundant'],['redundant']
Availability,Although now that I look the CWL analogy doesn't hold up. They have a max-64K chunk attached to their File object for similar purposes as this (input into expressions) but not to enable further typing like we do. Error it is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917:213,Error,Error,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917,1,['Error'],['Error']
Availability,"Although now that I've said that I know that we *do* have a use case where something like this is being requested. They want a typed set of key/value pairs, but the thing that they really want is to be able to define some boundaries (e.g. ""Foo"" is a number between 1 and 10) and to have the static analysis fail to validate the workflow if one of these are a workflow input and the values are wrong. Now that I type that out, having refinement types in WDL seems like a bad path to be going down. I should verify that's *really* what they want or if I read too much into an example they gave.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059:491,down,down,491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059,1,['down'],['down']
Availability,Am wondering if these limits should be specified in the conf file. They should be generous and static but I could envision a scenario where one would want to override read_json or something. In that scenario a config change would be nice to have available,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107:246,avail,available,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107,1,['avail'],['available']
Availability,"An attempt to document my observation of our general purpose debugging process - will hopefully help the next generation of Cromwell fire troubleshooters. * Moves the release processes under a new ""processes"" banner instead of awkwardly sitting in ""scripts""; * Adds a general-purpose recover process . See the process rendered and [in situ](https://github.com/broadinstitute/cromwell/tree/cjl_all_purpose_mess_remover/processes/troubleshooting). NB: If this gets approval, I'll update our playbook to link to this as our ""general purpose fallback process""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:284,recover,recover,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recover']
Availability,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4013:220,avail,available,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013,2,"['avail', 'down']","['available', 'down']"
Availability,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:28,error,errors,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,4,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:364,ERROR,ERROR,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,An issue to address the user report: http://gatkforums.broadinstitute.org/gatk/discussion/8873/file-not-found-errors-when-using-call-caching/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1929:110,error,errors-when-using-call-caching,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1929,1,['error'],['errors-when-using-call-caching']
Availability,And (for obvious reasons) this does not error out on SGE and local backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844#issuecomment-343271052:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844#issuecomment-343271052,1,['error'],['error']
Availability,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:72,ERROR,ERROR,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['ERROR'],['ERROR']
Availability,And another possible bug: why are we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:155,ERROR,ERROR,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,2,"['ERROR', 'recover']","['ERROR', 'recoverWith']"
Availability,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:157,echo,echo,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558,3,"['echo', 'error']","['echo', 'error']"
Availability,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:42,ERROR,ERROR,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,4,"['ERROR', 'Fault']","['ERROR', 'FaultHandling']"
Availability,And only exec.sh is present in the bucket (no log etc)....; ```#!/bin/bash; tmpDir=$(mktemp -d /cromwell_root/tmp.XXXXXX); chmod 777 $tmpDir; export _JAVA_OPTIONS=-Djava.io.tmpdir=$tmpDir; export TMPDIR=$tmpDir. (; cd /cromwell_root; java -Xmx4g -jar /cromwell_root/broad-dsde-methods/lichtens/test_cnv_validation/gatk.jar ModelSegments \; --denoisedCopyRatios /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-DenoiseReadCountsNormal/G25783.TCGA-55-6986-11A-01D-1945-08.2.denoisedCR.tsv \; --allelicCounts /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-CollectAllelicCountsNormal/attempt-3/G25783.TCGA-55-6986-11A-01D-1945-08.2.allelicCounts.tsv \; \; --maxNumSegmentsPerChromosome 500 \; --minTotalAlleleCount 30 \; --genotypingHomozygousLogRatioThreshold -10.0 \; --genotypingBaseErrorRate 0.05 \; --kernelVarianceCopyRatio 0.0 \; --kernelVarianceAlleleFraction 0.01 \; --kernelScalingAlleleFraction 1.0 \; --kernelApproximationDimension 100 \; --windowSize 8 --windowSize 16 --windowSize 32 --windowSize 64 --windowSize 128 --windowSize 256 \; --numChangepointsPenaltyFactor 1.0 \; --minorAlleleFractionPriorAlpha 25.0 \; --numSamplesCopyRatio 100 \; --numBurnInCopyRatio 50 \; --numSamplesAlleleFraction 100 \; --numBurnInAlleleFraction 50 \; --smoothingThresholdCopyRatio 2.0 \; --smoothingThresholdAlleleFraction 2.0 \; --maxNumSmoothingIterations 10 \; --numSmoothingIterationsPerFit 0 \; --output . \; --outputPrefix G25783.TCGA-55-6986-11A-01D-1945-08.2; ); echo $? > /cromwell_root/ModelSegmentsNormal-rc.txt.tmp; (; cd /cromwell_root. ); sync; mv /cromwell_root/ModelSegmentsNormal-rc.txt.tmp /cromwell_root/ModelSegmentsNormal-rc.txt```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580:1791,echo,echo,1791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580,1,['echo'],['echo']
Availability,And/or include a link to stderr in the failure message?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292,1,['failure'],['failure']
Availability,Another CTKS test down.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4516:18,down,down,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4516,1,['down'],['down']
Availability,"Another error w/ this test:. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2444/testReport/junit/cromwell.core.actor/RobustClientHelperSpec/RobustClientHelper_should_reset_timeout_when_backpressured_is_received/. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,4,"['Robust', 'error']","['RobustClientHelperSpec', 'error']"
Availability,"Another example of a workflow complete failure possibly due to grabbing the hash from google. Workflow 0c7da038-172a-4081-8850-c87fec05f4c1. ```; 2016-04-26 20:52:05,279 cromwell-system-akka.actor.default-dispatcher-28 ERROR - WorkflowActor [UUID(0c7da038)]: Completion work failed for call HaplotypeCaller:46.; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,5,"['ERROR', 'Error', 'error', 'failure']","['ERROR', 'Error', 'errors', 'failure']"
Availability,"Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:498,echo,echo,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,"['echo', 'error']","['echo', 'error']"
Availability,"Another in a long line of @geoffjentry written placeholder issues. While exploring WDL->CWL conversions as well as the potential to replace a strictly WDL object model with something more generic in the Cromwell engine I keep coming back to how to handle WDL expressions. Another ""problem"" (in quotes as it hasn't actually bitten us .... yet) is that expression evaluation is currently happening in an uncontrolled fashion within the engine - it is conceivable that if enough of these triggered at once that it could cause a lot of problems. This has been a nagging concern in the back of my mind for a long time now. Idea: Replace how we evaluate expressions by replacing them with a task upstream of the expression-ed task. These expression tasks shall run on the local backend allowing it to both be fast but also managed by our process throttling. As an example, a JES based `read_string` could involve a `gsutil` call in its `command` block to suck down the file or something similar for a `size` expression. This seems like it'd imply that there needs to be some universal primitives in Cromwell to effectively `read_XYZ` from the local filesystem. I'll admit to not having thought this all the way through. IMO this will improve the robustness/stability of the engine while making a big stride towards de-WDLing the underbelly of the engine. @kcibul note that I'm not asking for this to happen tomorrow but I do think it's an idea we should kick the tires on in the near term to see if it makes enough sense to pursue more deeply at a later point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618:954,down,down,954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618,2,"['down', 'robust']","['down', 'robustness']"
Availability,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329,1,['error'],['error']
Availability,"Another tweak to logging (de)localization errors in PAPI v2.; As you'll notice we blindly retry everything (even a missing file), but that's another issue.. Before:. ```; Attempt 1; Attempt 1; Attempt 1; 2018/11/30 15:53:22 Starting container setup.; gsutil command failed; gsutil command failed; 2018/11/30 15:53:24 Done container setup.; 2018/11/30 15:53:25 Starting localization.; 2018/11/30 15:53:26 Localizing input gs://tjeandet-cromwell-execs/this/does/not/exist -> /cromwell_root/tjeandet-cromwell-execs/this/does/not/exist; Attempt 1; Attempt 2; Attempt 2; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; gsutil command failed; gsutil command failed; Attempt 2; Attempt 3; Attempt 3; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; gsutil command failed; gsutil command failed; Attempt 3; gsutil command failed; CommandException: No URLs matched: gs://tjeandet-cromwell-execs/this/does/not/exist; 2018/11/30 15:53:47 Delocalizing output /cromwell_root/does/not/exist/either -> gs://tjeandet-cromwell-execs/w/bb90765b-c0d0-41f7-ae75-c03ed30a6a4b/call-t/does/not/exist/either; Attempt 1; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; Attempt 1; Attempt 2; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; Attempt 3; gsutil command failed; CommandException: No URLs matched: /cromwell_root/does/not/exist/either; 2018/11/30 15:54:07 Delocalizing output /cromwell_root/stdout -> gs://tjeandet-cromwell-execs/w/bb90765b-c0d0-41f7-ae75-c03ed30a6a4b/call-t/stdout; Attempt 1; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 2; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 3; gsutil command failed; CommandException: No URLs matched: /cromwell_root/stdout; Attempt 1; 2018/11/30 15:54:26 Delocalizi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4445:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4445,1,['error'],['errors']
Availability,"Any details I provide are only definitely true w/ the jes backend, haven't looked at others but I'm assuming they're similar. Also glob processing is backend-specific which leads to further problems I'll also address. Even if other backends aren't similar I think the same problems will manifest in other ways so there should still be similarity of solution. When a glob returns there's a FOFN on JES and that path is stored in a WdlGlobFile. In `JesAsyncBackendJobExecutionActor.postProcess` the function `evaluateOutputs` is called which effectively decomposes the `WdlGlobFile` into a `WdlArray[WdlSingleFile]` and then this array is carried around in memory in perpetuity (for the workflow). Much CPU and memory are spent for this conversion both at creation time and trying to stuff these things into the metadata service (see the very patriotic #1776). . I'm wondering if we could do something sneaky here and maintain the `WdlGlobFile` as-is and evaluate only what's necessary when necessary. Some thoughts as examples, some are contradictory I'm sure. - Allow that `Array[WdlFile]` to have both `WdlGlobFile` and `WdlSingleFile` with the former being dynamically expanded; - When we need the full list of files for downstream tasks perhaps we can stream them somehow instead of holding in memory; - In an example like a scatter/gather perhaps we could perform the collection on the glob files themselves via merging the stored glob file; - Perhaps we could *always* store an Array[WdlFile] as a FOFN on disk?; - Presumably we'll want to always expand to the full list of single files stored in the metadata for output reporting purposes. Could we stream the strings directly (and then remove) into the DB instead of all of the multiple boxings/unwrappings/etc?. Beyond ""this is all a bunch of work"" some issues that pop out:. - As stated not all backends might be handling globs the same. ; - What if Backend 1 generates a `WdlGlobFile` but that gets handed to Backend 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777:1223,down,downstream,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777,1,['down'],['downstream']
Availability,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429,3,['error'],['error']
Availability,"Any thoughts on how a general task retry policy may interact with [#1499](https://github.com/broadinstitute/cromwell/issues/1499) when the job scheduler (or user) kills a job on sge and the server is restarted?. I'm envisioning the ""check-alive"" result showing no job on sge invoking one of the task retries that a user specifies (whereas currently it'd be marked as failed). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515:239,alive,alive,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515,1,['alive'],['alive']
Availability,"Any updates on this front? Weâ€™ve been having issues with occasional spikes in memory usage that donâ€™t abide by a linear model for memory allocation. Currently this requires a lot of â€œbabysittingâ€ for our pipelines (or overprovisioning of memory), reducing their reliability and increasing their cost (on GCP, though the main cost factor is still developersâ€™ time..). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157:262,reliab,reliability,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157,1,['reliab'],['reliability']
Availability,"Anyone can can thumbsup and merge at will, but pinging @geoffjentry if no one else steps up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/572#issuecomment-197445359:47,ping,pinging,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/572#issuecomment-197445359,1,['ping'],['pinging']
Availability,"Anyone have a concrete solution to this, we are also getting the permission denied error with our aws batch setup. We have even included chmod 777 in the cloud init script to ensure that directory is accessible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228,1,['error'],['error']
Availability,ApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2818,recover,recoverWith,2818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,2,['recover'],['recoverWith']
Availability,"Apologies if it's there & i misunderstood what was going on, but would it be feasible to add a test somehow checking that it's working as intended?. I know, I know, Jeff is actually asking for tests? Up is down, cats and dogs living together in sin ....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3464#issuecomment-377058006:206,down,down,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3464#issuecomment-377058006,1,['down'],['down']
Availability,"Apologies if this is the wrong space for this. I've got a workflow written up and running locally in WDL, and I'm getting started with the JES backend. I've done a fair amount of work in google genomics before this, but this is my first use of cromwell/WDL. Some details first. I first noticed this error on 0.19.2, but went back to check 0.19 and HEAD of the develop branch. where it occurs as well. For completeness, here's my WDL file:. ```; cat ~/workflows/hello-jes.wdl ; task jes_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names mus",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,3,"['echo', 'error']","['echo', 'error']"
Availability,"Apologies, but I can not currently access jira. First time user question:. # The problem. When running `cromwell` locally, I get an excessive number of messages (thousands) from `liquibase`. Here is an example:. ```; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Custom SQL executed; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: ChangeSet metadata_changesets/remove_non_summarizable_metadata_from_queue.xml::delete_non_summarizable_metadata_from_queue::mcovarr ran successfully in 1ms; Jan 31, 2022 5:36:07 PM liquibase.changelog; INFO: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_MAS dropped from table WORKFLOW_METADATA_SUMMARY_ENTRY; ```. Is it possible to control these from cromwell? Is this a liquibase issue? ; The standard `-DLOG_LEVEL=WARN` does not seem to effect log messages. - Version: cromwell-74.jar; - Backend: local ; - Java version: 17.0.2+8 (azul). ## Workflow:; ```wdl; version 1.0. task say_hello {; input {; String name; }. command {; set -euxo pipefail; echo ""Hello ~{name}""; echo ""Hello ~{name}"" > greeting.txt; }. output {; File greeting = ""greeting.txt""; }. runtime {; docker: ""debian:bullseye-slim""; }; }. workflow hello {; input {; String name; }. call say_hello {; input: ; name = name; }. output {; File greeting = say_hello.greeting; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6664:985,echo,echo,985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6664,2,['echo'],['echo']
Availability,"Apologies, that metadata appears to have disappeared, but the same issue is referenced here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538:158,error,error-the-local-copy-message-must-have-path-set,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538,1,['error'],['error-the-local-copy-message-must-have-path-set']
Availability,"Apparently there weren't any ""real"" workflow names > 100 chars, only one artificially long one created for test purposes, and Cloud SQL did not take kindly to the attempt to have a 500-char field as part of an index. The test data was downsized and the embiggening commit reverted and the migration ran successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1721:235,down,downsized,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1721,1,['down'],['downsized']
Availability,Are we ok to unilaterally make these changes? Are agora and rawls already building out functionality based on our existing behavior? Will they be resilient to us making these changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-485466032:146,resilien,resilient,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-485466032,1,['resilien'],['resilient']
Availability,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:391,down,down,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957,2,['down'],['down']
Availability,Are you still looking for comments on this or should it be closed for repairs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154397266:70,repair,repairs,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154397266,1,['repair'],['repairs']
Availability,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:302,error,error,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,1,['error'],['error']
Availability,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['error'],['error']
Availability,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436,2,['error'],['error']
Availability,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:160,alive,alive,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['alive'],['alive']
Availability,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398,1,['error'],['error']
Availability,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:120,error,error,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408,1,['error'],['error']
Availability,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:101,down,down,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426,1,['down'],['down']
Availability,"As a Site Reliability Engineer (SRE) , I would like to have Cromwell support Sentry (https://sentry.io) to capture and report exceptions. This will allow me to better support our runtime operations and know when the system is functioning properly. This could be as simple as a document detailing how to deploy cromwell with the appropriate configuration, or it may involve code changes. @ansingh7115 is working on this in workbench at the moment and should be able to provide more information. @davidbernick can also provide configuration details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2120:10,Reliab,Reliability,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2120,1,['Reliab'],['Reliability']
Availability,"As a user I would like:. - [ ] an endpoint (/version) which returns metadata about the current version of cromwell. Currently just the version of the server, but if/when we have WDL versioning it should include that as well; - [ ] the same information available from a command line option (version). --- Original -- ; That returns the versions of Cromwell / WDL / etc. Ultimately we want to bubble this up to users in FC.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1694:252,avail,available,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694,1,['avail'],['available']
Availability,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292:684,error,error,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292,1,['error'],['error']
Availability,"As a user, I have a workflow that runs a scatter over 10 shards with a task that produce a file, then uses writelines to write out the gathered array of files (the fofn), which is used as an input to a downstream step. That downstream step will never call cache because the fofn is different every time because the file in it, while each having the same md5, produce a different fofn because call caching copies the data to new paths on each cache hit. This is painful because I love call caching, and now I have to recompute this step (and all steps downstream of it) every time. @cjllanwarne @jmthibault79 any more details or ideas for addressing this; @katevoss this is the issue we talked about on the phone yesterday. It's slowing down JG, but depending on how hard it is to fix may be too late for this use case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309:202,down,downstream,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309,4,['down'],"['down', 'downstream']"
Availability,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137:191,error,error,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137,2,['error'],['error']
Availability,"As discussed at Workbench office hours with @cjllanwarne. @ruchim This affects @eitanbanks's work on PCAWG. We have been seeing the following error running the Mutect2 wdl on Firecloud. It seemed to start after we switched to an NIO wdl, although that fact may be a red herring. When running the M2 workflow we sometimes (about 5% of the time) get the error:; ```; Failed to import workflow https://api.firecloud.org/ga4gh/v1/tools/gatk:mutect2-gatk4/versions/8/plain-WDL/descriptor Error: Server Error. The server encountered a temporary error. Please try again in 30 seconds.; ```. Salient facts:; * The wdl it's trying to import from the Firecloud methods repository definitely exists.; * The issue always resolves by restarting the job.; * When scattering this wdl eg in the Mutect2 panel of normals workflow, which runs Mutect2 on many samples, the failure occurs on a random set of scatters, and the failing samples are different each time.; * As @cjllanwarne points out, it's odd that a subworkflow import would cause failure only for some scatters -- you would think that it is imported once for the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3637:142,error,error,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637,7,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:497,avail,available,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,1,['avail'],['available']
Availability,"As far as I can tell, the timeline is:. - Shutdown signal received; - The job is aborted in JES but not removed from the JobStore; - On restart, the job is recovered because it remains in the JobStore, but in JES it's already been aborted; - On the console, a ""Job Failed"" message appears.; - The EJEA actor dies in an unexpected way (this concerns me *most*. Why isn't the failure cause recorded!). Not tested on SFS backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:156,recover,recovered,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,2,"['failure', 'recover']","['failure', 'recovered']"
Availability,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,recover,recover,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398,1,['recover'],['recover']
Availability,"As far as I know it's never been tested so I told @takutosato that it probably wasn't working yet, but a quick test with something like. ```; String a = ""hello"". task t {; String i; command {; echo ${i}; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = a }; }; ```. proved me wrong so apparently yes it's live !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310:193,echo,echo,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310,1,['echo'],['echo']
Availability,"As far as I remember, in the Past it was possible to reference global workflow variables inside a task. But now I get wdl validation errors like this:; ```; ERROR: Variable genome does not reference any declaration in the task (line 36, col 27):. curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; ^. Task defined here (line 26, col 6):. task download_genome {; ```; Here is the wdl; ```wdl; workflow indexes {. File genomesFolder; String version #release version; String species #species and also name of the index/. String releaseURL #path to releseas. String transcriptome #relative file name (.fa.gz); String genome #relative file name (.fa.gz); String annotation #relative annotation file name (.gtf). call download_genome {; input:; genomeURL = releaseURL + ""/"" + genome,; transcriptomeURL = releaseURL + ""/"" + transcriptome,; annotationURL = releaseURL + ""/"" + annotation,; folder = genomesFolder + ""/"" + species + ""/"" + version; }. }. task download_genome {. String genomeURL; String transcriptomeURL; String annotationURL; String folder. command {; mkdir -p ${folder}; curl -z ${folder}""/""${genome} --max-time 10 --retry 3 --retry-delay 1 ${genomeURL}; curl -z ${folder}""/""${transcriptome} --max-time 10 --retry 3 --retry-delay 1 ${transcriptomeURL}; curl -z ${folder}""/""${annotation} --max-time 10 --retry 3 --retry-delay 1 ${annotationURL}; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2504:133,error,errors,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2504,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872,1,['error'],['error']
Availability,"As mentioned in [this forum post](http://gatkforums.broadinstitute.org/gatk/discussion/comment/39791), there appears to be a race between cromwell checking stderr and it actually being written/flushed to disk. > WDL seemed to fail with a file not found error always in regard to the stderr file, but when I look up the file manually the file was always there, and the specific task also finished with rc=0, but the main cromwell process failed with return code of 1 already due to the file not found error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378:253,error,error,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378,2,['error'],['error']
Availability,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4069:147,failure,failure,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069,2,['failure'],['failure']
Availability,"As of right now, when using preemptible instances, Google has two types of error messages: 13 & 14. We want to to be able to retry when receiving Error Code 13 in the same way we currently retry for Error Code 14.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744,3,"['Error', 'error']","['Error', 'error']"
Availability,As part of auditing our Codecov secrets leak I am trying to trim down the number of items we maintain in Vault. This changeset allows us to delete `secret/dsde/cromwell/common/cromwell-refresh-token`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6331:65,down,down,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6331,1,['down'],['down']
Availability,"As part of my expression evaluation in `InitialWorkDirRequirement` I happened to accidentally write this:; ```yml; listing:; - entryname: $(script_name); ```; Instead of this:; ```yml; listing:; - entryname: $(inputs.script_name); ```. Instead of the expected runtime workflow failure (static checking of JS is too much), the workflow ran forever, repeatedly printing out the same expression evaluation error. Alas!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3088:277,failure,failure,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3088,2,"['error', 'failure']","['error', 'failure']"
Availability,As per hackathon - this test doesn't reliably fail in the intended circumstances. Closing in lieu of #4848,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484879049:37,reliab,reliably,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484879049,1,['reliab'],['reliably']
Availability,"As tech debt, I maybe would like to see things like `unwrapOutputValues` broken into utility functions. Not going to hold this PR up any longer figuring out this exact refactoring, but looking at current code I'm picturing something along the starting lines of:. ``` scala; // Sort of like Future.sequence, but with; // unwrapOutputValues's Failure(new Throwable(messages.mkString)); def sequenceMessages[T](in: Seq[Try[T]]): Try[T]. // If there are any failures, uses sequenceMessages to create the Failure; def unwrapValues[K,V](in: Map[K,Try[V]]): Try[Map[K,V]]; ```. :+1: For merging for the current code @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449:341,Failure,Failure,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449,3,"['Failure', 'failure']","['Failure', 'failures']"
Availability,As usual I can't really tell what the CI failures mean... they don't seem correlated to anything in particular?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124,1,['failure'],['failures']
Availability,As you see on the screenshot all pairs are highlighted as errors in Intellij while wdltool validates everything without an issue.; ![pair_highlightning_error](https://cloud.githubusercontent.com/assets/842436/25742086/783c9136-3196-11e7-8649-9fd5e1403b70.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246:58,error,errors,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246,1,['error'],['errors']
Availability,"Aside: I don't think having that sort of subtlety in any language, much less wdl, is particularly awesome. Aside part deux: If we go down this path we should think carefully how it is worded in the spec. WDL shouldn't be aware of call caching, so dropping in a feature that's effectively purely for call caching needs some explanation beyond that :) The whole cloud path/not for localization seems like the right official reason. This is of course assuming that this whole scheme gets @kcibul what he needs in the first place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413:133,down,down,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413,1,['down'],['down']
Availability,Asynced the standard backend execute/recover.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1947:37,recover,recover,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1947,1,['recover'],['recover']
Availability,"At least at the level of outputs, in AWS backend the isolation between separate calls of the same task is broken: e.g. running the following wf:; ```; workflow testCase {; call t as t1 { input: str=""t1"" }; call t as t2 { input: str=""t2"" }; output {; Array[String] t1out = t1.out; Array[String] t2out = t2.out; }; }; task t {; String str; command {; echo ${str} >> outfile; }; output {; Array[String] out = read_lines(""outfile""); }; runtime {; docker: ""amazonlinux:latest""; }; }; ```; Results in the output:; ```; ""outputs"": {; ""testCase.t1out"": [; ""t2"",; ""t1""; ],; ""testCase.t2out"": [; ""t2"",; ""t1""; ]; }; ```; i.e. the same file `outfile` is being written to by the two different task calls. I believe this could only be happening (when the same host is used for both containers) because the output file path is being specified by the workflow and task name rather than workflow and task alias.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4364:349,echo,echo,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4364,1,['echo'],['echo']
Availability,"At the moment we see slick errors from excessive database activity. This is working as intended on the part of slick, so now we need to gracefully handle this. There are temporary measures people can enact, but this ticket is about rewiring things to be brave in the face of danger. I see this as requiring heavy tech talk/design discussion prior to being shovel ready. . Some initial thoughts:. - My assumption is via a backpressure mechanism but I don't want to mandate this. But it seems natural to me that the answer to ""it hurts when I do this"" is ""stop doing that for a while"". ; - One area where this gets tricky is the metadata service. The solution should not overfit to the default (currently only) metadata service implementation (i.e. MySQL tied to the same database connection as the rest of the system).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466:27,error,errors,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466,1,['error'],['errors']
Availability,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1355:243,recover,recovering,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355,1,['recover'],['recovering']
Availability,At what interval does check-alive get called?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:28,alive,alive,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,1,['alive'],['alive']
Availability,Atlas 2.1.0 -503 error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,1,['error'],['error']
Availability,"Attached is some documentation that works for v52 and should work for v53. On Wed, Sep 9, 2020 at 9:20 AM mderan-da <notifications@github.com> wrote:. > Hi @markjschreiber <https://github.com/markjschreiber> I'm also running; > into this error. I am using cromwell 53 with a custom cdk stack based on; > the CloudFormation infrastructure described here:; > https://docs.opendata.aws/genomics-workflows/; >; > Are modifications needed for compatibility with newer versions of; > Cromwell? Are these documented somewhere?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EO6WEE4BYYPTX4HZ2LSE56JXANCNFSM4G23FFUQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691326074:238,error,error,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691326074,1,['error'],['error']
Availability,Attempt to fix the repeated 403 auth error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/296:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/296,1,['error'],['error']
Availability,Attempts to unflakify the abort tests by making them fail reliably if they fail > 20% of the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3321:58,reliab,reliably,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3321,1,['reliab'],['reliably']
Availability,Auth before downloading from docker.io then always try to login BT-139 BT-143,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6226:12,down,downloading,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6226,1,['down'],['downloading']
Availability,Automatically increase JES VM boot disk size when docker download fails due to full storage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:57,down,download,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,1,['down'],['download']
Availability,Available system variables accessible from Cromwell configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:0,Avail,Available,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['Avail'],['Available']
Availability,"B""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; }. task gatk_haplotypecaller_task {; String gatk_path = ""/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.7-93-ge9d8068/GenomeAnalysisTK.jar""; String interval; File in_bam; File in_bam_index; String sample_name = ""fumoz_12""; File ? bqsr_table; String ? ploidy; String ? erc; String ? extra_hc_params. File reference_tgz. String out_gvcf_fn = ""${sample_name}.gvcf"". String output_disk_gb; String boot_disk_gb = ""10""; String ram_gb = ""60""; String cpu_cores = ""1""; String preemptible = ""0""; String debug_dump_flag. command {; set -euo pipefail; ln -sT `pwd` /opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:2429,echo,echo,2429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['echo'],['echo']
Availability,BA-5800: Downgrade log level of WorkflowFailedResponse Event to INFO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065:9,Down,Downgrade,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065,1,['Down'],['Downgrade']
Availability,BCS: A bad file inputs should create an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3522:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3522,1,['error'],['error']
Availability,BCS: Delocalizing a file that doesn't exist should error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3523:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523,1,['error'],['error']
Availability,BT-271 Do not cache to calls that are successes by Cromwell standards but failures by Centaur standards.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6335:74,failure,failures,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6335,1,['failure'],['failures']
Availability,BW-1320 Retry unexpectedly transient 400 error that vexes users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6807:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6807,1,['error'],['error']
Availability,Backend Store performing recovery closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1241:25,recover,recovery,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1241,1,['recover'],['recovery']
Availability,Backend should not have any dependency from Engine.; This means that all utils implementation should be moved to backend or removed.; It may require a break down in sub-tasks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/554:157,down,down,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/554,1,['down'],['down']
Availability,"Backend: AWS Batch. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-anothe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:366,error,error,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,5,"['avail', 'down', 'error', 'failure']","['available', 'download', 'error', 'failure']"
Availability,"Backend: AWS Batch; Cromwell version: 45.1; ----; I am building a WDL pipeline using the CloudFormation set up provided in https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/cromwell/cromwell-aio.template.yaml. ; In summary, the set up is a EC2 instance running `java -jar cromwell.jar server` and calling AWS Batch to run WDL workflow using an attached EC2 instance profile. . I have no issue posting workflows and getting results. However, after a certain period of time, I will get `The security token included in the request is expired` error message logged by the cromwell server when I try to post a job. ; - I have checked that `~/.aws` and the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variable don't exist. ; - If I kill the server and restart it again, the server seem to pick up the new security token and I can post workflow again. ; - Checking `cromwell.config` (pasted below), all authentication methods are set to `default` which is documented to mean it is using `DefaultCredentialProvider` in the AWS Java SDK. That should be refreshing the security token? . Is this unexpected behaviour or did I configure something wrongly? . Thanks for your help!. ----. Config file for the cromwell serve:; ```; include required(classpath(""application"")). webservice {; interface = localhost; port = 8000; }. system {; job-rate-control {; jobs = 1; per = 2 second; }; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-southeast-2""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""XXXX""; auth = ""default""; default-runtime-attributes { queueArn = ""XXXXX"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; workflow-options {; workflow-log-dir = ""cromwell-workflow-logs"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162:570,error,error,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162,1,['error'],['error']
Availability,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357:156,Error,Error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357,3,"['Error', 'error']","['Error', 'error']"
Availability,Bad errors reported validating null.wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['error'],['errors']
Availability,Band aid robustification for HealthServiceMonitorActorSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3402:9,robust,robustification,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3402,1,['robust'],['robustification']
Availability,"Based on a conversation with the Epigenomics group that is using Cromwell 34 on PAPI v2 --; it seems that a task can fail, not delocalize an expected output and the workflow will continue to start a downstream task but fail at running the command (as an expected input is missing). It's been confirmed that Cromwell was running in a Fail fast mode. There seem to be two main requirements here:; 1. Cromwell should be evaluating if all expected outputs for a task exist before marking the task as a success.; 2. Cromwell should fail a job if it failed localize a specific input file. ; 3. Cromwell should fail a job if it failed to delocalize any expected output file. AC: Ensure that the PAPI v2 backend fulfills requirements #1-3.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4140:199,down,downstream,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4140,1,['down'],['downstream']
Availability,"Based on a post I made on the openWDL slack; was asked to make a ticket here. ## quick summary; Cromwell handles `/` in strings inconsistently. In some cases, it is dropped without throwing an error, in other cases it will cause an error immediately. If the string is in the WDL file itself, womtool does not detect any issues with it but it will not be handled as expected as runtime. ## use case and how to reproduce; [goleft indexcov ](https://github.com/brentp/goleft/tree/master/indexcov#indexcov) defaults to this value for --excludePattern:; `""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""`. So I set `String excludePattern = ""^chrEBV$|_random$|Un_|^HLA|_alt$|hap$""` in my WDL. That passes miniwdl check and womtool. But... * Terra will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$ `as a variable default or as hardcoded variable, but will handle it incorrectly -- it will not error, but it will be changed into `^chrEBV$|^NC|_random$|Un_|^HLA-|_alt$|hapd$`; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable via JSON; it will fail to import; * Terra will not accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as an input variable if entered manually; it will throw token recognition error in the workflow menu and not allow you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that wa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,2,['error'],['error']
Availability,"Based on a report from @curoli . I can't immediately reason out what the correct behavior is (post-vacation fuzziness...) so this is just a a log of stuff that looked suspicious to me. ---. A workflow like; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}; String s = ""string"". output {; File write_attempt = write_json({""m"": m, ""s"": s}); }; }; ```; validates in Womtool but fails at runtime with error; ```; WorkflowManagerActor Workflow 2a3db889-e126-467b-be60-6abb815ea46e failed (during ExecutingWorkflowState):; java.lang.UnsupportedOperationException:; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types:; Map(; WomString(m) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(a) -> WomString(a), WomString(b) -> WomString(b))),; WomString(s) -> WomString(string); ); ```. ---. The problem is not so simple as heterogeneous types in the map values; the workflow; ```; version 1.0. workflow test {; ; String s = ""string""; Float f = 0.1; File file = ""asdf"". output {; File write_attempt = write_json({""s"": s, ""f"": f, ""file"": file}); }. }; ```; works just fine:; ```; {""s"":""string"",""f"":""0.1"",""file"":""asdf""}; ```. ---. Interestingly, if we take out `String s = ""string""` we do get an error in Womtool, but it's a confusing one - why would we say a map value has to be an `Object` when we clearly used `String`, `Float`, and `File` right above?; ```; version 1.0. workflow test {; ; Map[String, String] m = {""a"": ""a"", ""b"": ""b""}. output {; File write_attempt = write_json({""m"": m}); }; }; ```; yields; ```; womtool validate any_map.wdl ; Failed to process workflow definition 'test' (reason 1 of 1):; Failed to process declaration 'File write_attempt = write_json({ ""m"": m })' (reason 1 of 1):; Failed to process expression 'write_json({ ""m"": m })' (reason 1 of 1):; Invalid parameter 'MapLiteral(Map(StringLiteral(m) -> IdentifierLookup(m)))'. Expected 'Object' but got 'Map[String, Map[String, String]]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4512:430,error,error,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4512,2,['error'],['error']
Availability,"Based on my understanding of the bash, it seems like we might have been retrying with the requester pays flag project regardless of the error. . The change is to enable localization with the project flag only if the requester pays error exists.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3946:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3946,2,['error'],['error']
Availability,"Based on the conversation after standup yesterday, this ticket needs more refinement. Currently, recovering a call for the local backend is the same as executing a fresh call. It's possible there are other ways to wire recovery and that question needs to be answered. @kcibul @geoffjentry I'm returning it to the milestone backlog but hopefully something we can discuss at the prioritization today--seems like a technical/PO type of refinement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645:97,recover,recovering,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645,2,['recover'],"['recovering', 'recovery']"
Availability,"Based on the documentation https://cromwell.readthedocs.io/en/stable/Imports/, we should have the ability to import from any public HTTPS link. I am getting error on import: ; ```; womtool validate wf_test.wdl; Failed to import 'https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl' (reason 1 of 1): Unrecognized token on line 8, column 1:. <!DOCTYPE html>; ^; ```. Using cromwell 78, and running local with version development (also tried with 1.0). Has this feature been disabled? . Running a very simple WDL workflow: ; ```; version development. import ""https://github.com/broadinstitute/cromwell/blob/master/engine/src/main/resources/3step.wdl"" as http_import2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6788:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6788,1,['error'],['error']
Availability,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4058:25,failure,failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058,1,['failure'],['failures']
Availability,"Based on this [user error report](https://support.terra.bio/hc/en-us/community/posts/360043437191-Cromwell-WorkFlow-getting-aborted-intermittently-without-any-exception?page=1#community_comment_360005588172). Investigation is needed but at first glance:. * The job succeeds; * ~The workflow result copy is probably relatively long, given the size of output files~; * ~Cromwell's ""on shutdown"" logic is triggered too soon, and interrupts the result copy, which manifests as a workflow abort~; * The workflow apparently aborts shortly after the job succeeds. EDIT: Running in server mode didn't seem to help, so this is probably unrelated to the shutdown logic triggering too early, and more likely something else - an uncaught exception with the large output file perhaps?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4960:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4960,1,['error'],['error']
Availability,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4081:298,error,error,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081,2,['error'],['error']
Availability,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) â†’ mid-way to completion, Upgrade â†’ ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows â†’ Run them again to see they successfully cached once â†’ upgrade â†’ run them again to ensure theyâ€™re still caching. â€œUpgradeâ€ consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldnâ€™t break:; Log names (detritus files) donâ€™t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4101:657,error,error,657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101,3,['error'],['error']
Availability,"Basic Expectations:; Run a bunch of workflows (call caching turned off) â†’ mid-way to completion, Upgrade â†’; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows â†’ Run them again to see they successfully cached once â†’ upgrade â†’ run them again to ensure theyâ€™re still caching. â€œUpgradeâ€ consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldnâ€™t break:; - ~Log names (detritus files) donâ€™t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4099:686,error,error,686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099,3,['error'],['error']
Availability,"Basically changed the sbt assembly from ""--error"" to ""early(error)"" as they changed the flag. The other stuff is just 1.X cleanup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3597:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3597,2,['error'],['error']
Availability,Be resilient if jobs fail half-way through a very large scatter [BA-6517],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595:3,resilien,resilient,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595,1,['resilien'],['resilient']
Availability,Be resilient if the db has WaitingForQueueSpace statuses [BW-387 fixup],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6131:3,resilien,resilient,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6131,1,['resilien'],['resilient']
Availability,Because sometimes things other than cromwell can cancel jobs. Also might make restarts after aborts a little more resilient in case of unexpected race conditions (not a guarantee TM),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2503:114,resilien,resilient,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2503,1,['resilien'],['resilient']
Availability,"Because writing to the call caching store and the job store is not atomic, the following chain of events is possible and not necessarily desirable:. - A job start; - A cache hit is found; - The outputs are copied; - The hashes / simpletons are written to the DB; - ** Cromwell Stops **: This is after the hashes are written successfully but before the EJEA had a chance to write the outputs to the job store and mark the job as complete.; - Cromwell starts; - The workflow is restarted; - The job is not found in the job store; - At this point the EJEA has a state to check if there are hashes existing for this job already. If there is, it disables call caching (so that the EJEA doesn't try to call cache to himself, and that we don't write to the hash store again - which would fail because of the unique index in the call cache table).; - However since we've disabled call caching we then proceed to try and recover the job, which fails because it was never run (since we found a cache hit the first time), and then falls back to running the job for reals. This is not great because this job already has all the outputs it needs, files have been copied already, but we run the job on top of it, which seems to increase the likelihood of having empty files at least locally when trying to read outputs and cause `cannot create an Int from """"` types of failures. Maybe a better way would be to re-use the outputs that have been written to the cache to make the job succeed and bypass all the rest. Relevant code in the EJEA: https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L153",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3074:912,recover,recover,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3074,2,"['failure', 'recover']","['failures', 'recover']"
Availability,"Before the break I was playing around trying to async-ify Cromwell's IO a bit more. It's not complete and needs clean up / refinements / tests, but considering that one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as po",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:185,reliab,reliability,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,1,['reliab'],['reliability']
Availability,"Better ""no metadata found"" errors for call cache diffs [BA-6106]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:27,error,errors,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,1,['error'],['errors']
Availability,Better JES errors + RC Closes #1848,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,1,['error'],['errors']
Availability,"Better error message for ""Upgrade Config from C26""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2186:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2186,1,['error'],['error']
Availability,Better error message for string member accesses,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3867:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3867,1,['error'],['error']
Availability,Better error messages for cyclic dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4548:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4548,1,['error'],['error']
Availability,Better error messages for incomplete expressions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4175:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4175,1,['error'],['error']
Availability,"Better error messages, esp missing workflow outputs expressions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2907:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2907,1,['error'],['error']
Availability,Better error reporting if an EJEA crashes unexpectedly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1685:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1685,1,['error'],['error']
Availability,Better error reporting when a cromwell task is canceled by google due to 6 day limit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,1,['error'],['error']
Availability,Better import failure messages and importLocalFilesystem resolver set,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3987:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3987,1,['failure'],['failure']
Availability,"Beyond tidiness, do you think this is going to have benefits for test reliability?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425455093:70,reliab,reliability,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425455093,1,['reliab'],['reliability']
Availability,"Beyond tidiness, this PR for @aweng98 to run the unit tests in Jenkins. It won't affect reliability beyond allowing certain tests to pass. Here is a previous failed run without all of these changes: https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/21/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425475944:88,reliab,reliability,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425475944,1,['reliab'],['reliability']
Availability,"Binding"": {; ""prefix"": ""-somatic_min_total""; },; ""default"": 300,; ""id"": ""#purple-2.44.cwl/somatic_min_total""; },; {; ""type"": [; ""null"",; ""float""; ],; ""doc"": ""Proportion of somatic deviation to include in fitted purity score. Default 1.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_penalty_weight""; },; ""default"": 1,; ""id"": ""#purple-2.44.cwl/somatic_penalty_weight""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\nSample name must match tumor parameter. GZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-somatic_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/somatic_vcf""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional location of structural variant vcf for more accurate segmentation.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-structural_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/structural_vcf""; },; {; ""type"": ""File"",; ""doc"": ""Optional location of failing structural variants that may be recovered.\nGZ files supported.\n"",; ""inputBinding"": {; ""prefix"": ""-sv_recovery_vcf""; },; ""secondaryFiles"": [; "".tbi""; ],; ""id"": ""#purple-2.44.cwl/sv_recovery_vcf""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Number of threads\n"",; ""inputBinding"": {; ""prefix"": ""-threads""; },; ""default"": 2,; ""id"": ""#purple-2.44.cwl/threads""; },; {; ""type"": ""string"",; ""doc"": ""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor""; },; ""id"": ""#purple-2.44.cwl/tumor""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Tumor only mode. Disables somatic fitting.\n"",; ""inputBinding"": {; ""prefix"": ""-tumor_only""; },; ""default"": false,; ""id"": ""#purple-2.44.cwl/tumor_only""; }; ],; ""outputs"": [; {; ""type"": ""Directory"",; ""outputBinding"": {; ""glob"": ""$(inputs.output_dir)/""; },; ""id"": ""#purple-2.44.cwl/outdir""; }; ],; ""id"": ""#purple-2.44.cwl""; },; {; ""class"": ""Workflow"",; ""id"": ""#main"",; ""l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:36318,recover,recovered,36318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,Bonus: Robust METADATA_VALUE embiggening.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1648:7,Robust,Robust,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1648,1,['Robust'],['Robust']
Availability,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['failure'],['failures']
Availability,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1443:274,failure,failures,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443,1,['failure'],['failures']
Availability,"Brought this up with Dion, suggested we have a backoff for transient issues like this on their end. Seems very transient, but needed to have it documented. Looks like this:. ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; { ; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }"". timestamp:""2016-05-06T22:39:17.321Z""; jobId:""operations/EOiv78DIKhjQhqv9q_TfliEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903:287,error,errors,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903,3,"['Error', 'error']","['Error', 'errors']"
Availability,Bug Fix: Response error codes in releaseHold endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3918:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918,1,['error'],['error']
Availability,Build 3730. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:188,failure,failure,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['failure'],['failure']
Availability,"Building docker with `docker build .` generates warnings like. `; [warn] Error extracting zip entry 'scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class' to '/cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class': java.io.FileNotFoundException: /cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class (File name too long); `. It appears this is because the max filename under docker is ~242 characters, but the sbt default for max generated class name is ~254/255. See https://github.com/docker/docker/issues/1413. The fix is to reduce this as described here . http://stackoverflow.com/questions/28565837/filename-too-long-sbt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1428:73,Error,Error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428,1,['Error'],['Error']
Availability,Builds for me in IntelliJ (incl clean build) but I can reproduce this error locally with `sbt assembly`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430,1,['error'],['error']
Availability,Bump file read timeout to maybe reduce test failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['failure'],['failures']
Availability,Bump heterodon to version with http download / zlib fix.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4485:36,down,download,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4485,1,['down'],['download']
Availability,But I'm having trouble running even 500 tasks without one of these JES failures. Is the fact that I using pre-emptible instances matter?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663:71,failure,failures,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663,1,['failure'],['failures']
Availability,"But the job ran successfully. Here is the full logs:. [ec2-user@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-43",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:752,heartbeat,heartbeat,752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4078:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078,1,['error'],['error']
Availability,"CI clone of ""Implement recoverAsync for AWS backend"" #5216",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5291:23,recover,recoverAsync,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5291,1,['recover'],['recoverAsync']
Availability,CPU on Cromwell machines pegged and unable to recover without restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4093:46,recover,recover,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093,1,['recover'],['recover']
Availability,CROM-6920 Add option to retry only known errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7456:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7456,1,['error'],['errors']
Availability,CWL + AWS Batch + resource requirements gives error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['error'],['error']
Availability,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2543,heartbeat,heartbeat,2543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,4,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,CWL Parsing Error for large multi-step workflow with no logs to assist debug,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:12,Error,Error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['Error'],['Error']
Availability,"CWL was treating output glob strings as if they were filenames, and thus was not returning the filename that Cromwell expects, namely `glob-${md5(fileName)}.list`. The implementation boils down to `OutputEvaluator` trying to detect whether the output of the expression is a glob. If it is _is_ a glob, it changes the output to be the filename as listed above.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2828:189,down,down,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2828,1,['down'],['down']
Availability,CaaS: Don't try to deserialize SAM HTML errors as JSON,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3622:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622,1,['error'],['errors']
Availability,Cache Hit copy failures with RP,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4771:15,failure,failures,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771,1,['failure'],['failures']
Availability,Cache output copy failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4091:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091,1,['failure'],['failures']
Availability,Call Caching hit 503 Service Unavailable error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1185:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185,1,['error'],['error']
Availability,Call cache no copy fixups and centaur error improvements,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4951:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951,1,['error'],['error']
Availability,Call caching output copy failures handling. Closes #1510,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1559:25,failure,failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1559,1,['failure'],['failures']
Availability,"Call caching works sometimes for me but not all the time. I find it especially strange when working on a scatter job and some of the scatter jobs get a cache hit but others get a cache miss. . I have queried the METADATA_ENTRY table for the two workflows and all the call cache entries look identical. . Here is my process:. 1. I queried METADATA_ENTRY with this WHERE condition: `(WORKFLOW_EXECUTION_UUID ='29791b64-b47a-44ba-aff0-7ab48bc10677' or WORKFLOW_EXECUTION_UUID ='5de042e3-7a03-4c77-8972-f0e4cd010e4b') and CALL_FQN = 'sampleLevelWorkflow_WGS.align' and JOB_SCATTER_INDEX =0`; 2. I sort by METADATA_KEY; 3. Then I go down the list and compare the hashes for the two workflows for each METADATA_KEY. Here is a case where workflow 29791b64 is a restart of 5de042e3. (Workflow 5de042e3 is itself a restart but I don't think that is important here.) I have shown below all the records from METADATA_ENTRY that start with ""callCaching"" and they all look identical, yet it clearly says it is a ""Cache Miss"". **Is there anywhere I can see a log message stating exactly which hashes resulted in the cache miss?** I have tried to enable LOG_LEVEL=DEBUG but couldn't see it there. Thanks in advance for your help!. |WORKFLOW_EXECUTION_UUID|METADATA_KEY|METADATA_VALUE|; |-----------------------|------------|--------------|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:result|Cache Miss|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:result|Cache Miss|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hit|false|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:runtime attribute:failOnStderr|68934A3E9455FA72420237EB05902327|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:runtime attribute:docker|4AD3C387725244C1348F252B031B956D|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:628,down,down,628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['down'],['down']
Availability,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,recover,recoverWith,3365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['recover'],['recoverWith']
Availability,CallableMaker$Ops.toWomCallable(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable$(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$ops$$anon$1.toWomCallable(WomCallableMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:129); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:21); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode$(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$ops$$anon$1.toWomCallNode(WomCallNodeMaker.scala:9); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:68); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:3269,Error,ErrorOr,3269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$4(CompileScope.scala:244); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$2(CompileScope.scala:242); cromwell_1 | 	at flatMap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:1508,error,errors,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['errors']
Availability,"Can be reproduced using the following workflow. ```wdl; version 1.0. task crash {; command <<<; kill -9 $$; >>>; }. workflow crash {; call crash ; }. ```; We use a configuration with the following values:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 120; default-runtime-attributes {; maxRetries: 2; }; }; }; }; }; workflow-options {; workflow-failure-mode = ""ContinueWhilePossible""; }; ```; On Cromwell 37 the workflow will be run. Jobs will be killed and retried.; On Cromwell 39, the retries will not happen any more.; This is very annoying, as our cluster kills jobs that exceed the memory limit, and some java based jobs seem to have random memory spikes. Having only 1 try means basically that a workflow with 50-100 jobs will usually fail, unless we give some jobs an insane memory parameter. This is probably caused by the refactoring in:; https://github.com/broadinstitute/cromwell/pull/4654/files; EDIT: This statement was not meant to put a blame on someone. I understand that code needs to be refactored at times and that bugs can creep in. I will look if I can fix the issue myself but maybe @cjllanwarne can also have a quick look? That would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998:481,failure,failure-mode,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998,1,['failure'],['failure-mode']
Availability,Can that nice error message be removed from its `Option` container?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252,1,['error'],['error']
Availability,Can we also have a test to exercise the multiple errors validations?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812:49,error,errors,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812,1,['error'],['errors']
Availability,"Can you post the WDL, or at least part of it? I've issues like this happen where you declare the path to the file as a `String` instead of a `File`, so it never gets actually downloaded to the local filesystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436144957:175,down,downloaded,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436144957,1,['down'],['downloaded']
Availability,"Can you post your WDL? Searching for `The label in the input is too long` on the 'net suggests that the HTTP resolver is being passed a domain that is too long. I suppose it's possible that you aren't even using HTTP inputs, but when we try all resolvers and the HTTP one fails, the error is not handled correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647:283,error,error,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647,1,['error'],['error']
Availability,Can you put a brief comment with the exception? I've never seen this error before and I'm curious. Besides that :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/275#issuecomment-154549091:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/275#issuecomment-154549091,1,['error'],['error']
Availability,"Can you try with parens?; ```; command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; ```; The parser does seem to be out of spec, but maybe we can give it a nudge in the right direction this way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987:44,echo,echo,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987,1,['echo'],['echo']
Availability,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:760,error,error,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Can-of-wormsy ToL: if `reference.conf` doubles as our config documentation, could we include this there? Otherwise, could we write it down *somewhere*?. I like not cluttering the `reference.conf` but I also don't want to have to rummage through some (I've-already-forgotten-which) class file to find how to change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933,1,['down'],['down']
Availability,Carbonite JSON parsing error checking [BA-6081],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5257:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5257,1,['error'],['error']
Availability,Catch and log workflow log copy failures [BA-4916],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022:32,failure,failures,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022,1,['failure'],['failures']
Availability,Centaur errors to BigQuery.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4072:8,error,errors,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4072,1,['error'],['errors']
Availability,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:1099,heartbeat,heartbeat,1099,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['heartbeat'],['heartbeat']
Availability,Centaur should be more robust to missing files,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2885:23,robust,robust,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2885,1,['robust'],['robust']
Availability,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"Centaur testing appears to fail when too many tests run concurrently due to:; ```; java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); [...]; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: Batch, Status Code: 429, Request ID: 932e695f-5a4b-11e9-abf3-d5638efd51d6); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); [...]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4816:511,Fault,FaultHandling,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4816,1,['Fault'],['FaultHandling']
Availability,Centaur tests should reliably pass even when they don't have a `sleep 2` suffixed to the command block.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1894:21,reliab,reliably,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1894,1,['reliab'],['reliably']
Availability,"Certain error messages that Cromwell receives are longer than the default limit, which is a big pain when debugging. Going from 64 to 1024 characters (1kb) doesn't seem unreasonable and solves this issue. For context, the error message below is 364 characters. . [Relevant Akka Doc](https://doc.akka.io/docs/akka-http/10.0/configuration.html). . Before:; ```; 2024-04-12 14:58:18 cromwell-system-akka.actor.default-dispatcher-26 ERROR - Error in stage [akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse@71a2a20e]: Response reason phrase exceeds the configured limit of 64 characters; akka.http.scaladsl.model.IllegalResponseException: Response reason phrase exceeds the configured limit of 64 characters; 	at akka.http.impl.engine.client.OutgoingConnectionBlueprint$PrepareResponse$$anon$3.onPush(OutgoingConnectionBlueprint.scala:191); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:523); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:409); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:606); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute(ActorGraphInterpreter.scala:47); 	at akka.stream.impl.fusing.ActorGraphInterpreter$SimpleBoundaryEvent.execute$(ActorGraphInterpreter.scala:43); 	at akka.stream.impl.fusing.ActorGraphInterpreter$BatchingActorInputBoundary$OnNext.execute(ActorGraphInterpreter.scala:85); 	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:581); 	at ; ...; ```. After: ; ```; <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN"">; <html><head>; <title>401 Unauthorized</title>; </head><body>; <h1>Unauthorized</h1>; <p>This server could not verify that you; are authorized to access the document; requested. Either you supplied the wrong; credentials (e.g., bad password), or your; browser doesn't understand how to supply; the credentials required.</p>; </body></html>; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7406:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7406,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:33,avail,available,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676,1,['avail'],['available']
Availability,Changing directories in the command block causes workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3116:58,failure,failure,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116,1,['failure'],['failure']
Availability,Chaos monkey Cromwell. Put up a Firewall and deny access to Cromwell. Tear down. See what happened. Block ports. Tables. ; Putting up a block. . The real Chaos Monkey. Much hectic. Simian army. . Henry knows how to spin up a Cromwell test environment. Much magic.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2113:75,down,down,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2113,1,['down'],['down']
Availability,Check RC first and report that status instead of delocalization errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4160:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160,1,['error'],['errors']
Availability,Checkpoint update to wes2cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3932:0,Checkpoint,Checkpoint,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3932,1,['Checkpoint'],['Checkpoint']
Availability,Checksum S3 signed URL downloads during localization [BT-257],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6485:23,down,downloads,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485,1,['down'],['downloads']
Availability,"Chris, thanks for the idea. I tried this and unfortunately had the same issue. From the behavior it looks like http is working in that the files get downloaded, but they don't get proper naming with numerical names instead of the expected file names. This disconnect seems to be what causes issues when passing these on to the CWL tools.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-427443254:149,down,downloaded,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-427443254,1,['down'],['downloaded']
Availability,"Chris;; Thanks for working on this and for the test case to iterate with. This example does work for me in the sense that it generates an md5sum, but also demonstrates the underlying issue I'm having with https inputs. I also get them downloaded and staged into my pipeline, but the file names get mangled into random download number. md5sum is cool with this, but many of my real tasks fail because the expected file extensions and associated secondary file extensions get lost with the random file names. Here's the example output I get from running this that demonstrates the file naming issue:; ```; /usr/bin/md5sum '/home/chapmanb/tmp/cromwell/cromwell_work/cromwell-executions/main-http_inputs.cwl/093e2835-e4cc-4731-9248-88d74dec0977/call-sum/inputs/1515144/1710814112361209342' | cut -c1-32; ```; This input should be called `jamie_the_cromwell_pig.png` but instead gets a long number attached to it. Is it possible to preserve initial file names with https like happens with other filesystem types?. In terms of the test cases, it would be great if it also checked that the file extension and name get preserved. Thanks again for looking at this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999:235,down,downloaded,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999,2,['down'],"['download', 'downloaded']"
Availability,Clarify PAPI Error Code 10 Message 14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855:13,Error,Error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855,1,['Error'],['Error']
Availability,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3722:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722,6,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,Clear cache from optional associated workflow when rerunning after Centaur error [CROM-6807],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654,1,['error'],['error']
Availability,Clearer error message for improperly formatted disk strings,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2739:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2739,1,['error'],['error']
Availability,"Close script done; [2023-02-08 16:24:31,65] [info] dataFileCache commit start; (...); ```. And at the end:; ```; 2023-02-08 16:32:11,54] [info] checkpointClose synched; [2023-02-08 16:32:11,57] [info] checkpointClose script done; [2023-02-08 16:32:11,57] [info] dataFileCache commit start; [2023-02-08 16:32:11,57] [info] dataFileCache commit end; [2023-02-08 16:32:11,69] [info] checkpointClose end; [2023-02-08 16:32:11,69] [info] Checkpoint end - txts: 5342; [2023-02-08 16:32:21,70] [info] Checkpoint start; [2023-02-08 16:32:21,70] [info] checkpointClose start; [2023-02-08 16:32:21,70] [info] checkpointClose synched; [2023-02-08 16:32:21,74] [info] checkpointClose script done; [2023-02-08 16:32:21,74] [info] dataFileCache commit start; [2023-02-08 16:32:21,76] [info] dataFileCache commit end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:3361,down,down,3361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['down'],['down']
Availability,Closes #1649 Closes #1754 Support gcs files with spaces + be robust to workflow log deletion faâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1764:61,robust,robust,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1764,1,['robust'],['robust']
Availability,"Closes #3214. Top level view:; - Totally split `draft2` from a fresh copy of WDL code in `draft3`.; - They now inhabit separate and unconnected sbt projects.; - Any existing code references to `wdl.` are now references to `wdl.draft2.`; - Please review my `build.sbt` file, the rest is just moving, shuffling and fixing up intellij's failures to refactor packages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3229:334,failure,failures,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229,1,['failure'],['failures']
Availability,"Closes #3790 . I went down the ""allow WomAnyType"" values route for now, to solve the general case of:; ```; Object o = read_json(some_file); scatter (x in o.blah) {; ...; }; ```. Ideally directing people to `struct`s will mean this problem goes away, but in the mean time, we'll need to handle scatters over `WomAnyType`s (at least in the static analysis).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3794:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3794,1,['down'],['down']
Availability,Closes #3990. ~Awaiting accompanying OpenWDL PR with the underlying grammar change (after lunch).~. OpenWDL PR: https://github.com/openwdl/wdl/pull/253. ---. Reviewers: is it worth a changelog note to tell people `version draft-3` went away?. I think the change itself is OK because it seems like anyone who signed up to use a draft version is pretty breakage-tolerant.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4135:360,toler,tolerant,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4135,1,['toler'],['tolerant']
Availability,Closes #4023 . I hijacked one of the steps in `cwl_cache_between_workflows.cwl` to test `Long` alongside `Float` - the second commit [fails](https://api.travis-ci.org/v3/job/419242749/log.txt) with the error the user saw.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4033:202,error,error,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4033,1,['error'],['error']
Availability,Closes #4051 . Also checks for and produces clear errors when tabs and spaces are mixed in a cmd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4065:50,error,errors,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4065,1,['error'],['errors']
Availability,"Closes #4557 . - Adds a configurable value `token-log-interval-seconds` in the `hog-safety` stanza.; - Logs when a hog group is at its limit (no more than once per `token-log-interval-seconds` seconds per hog group); - Logs when a backend has used all tokens (no more than once per `token-log-interval-seconds` seconds per backend); - Logs the current status of the Cromwell token queues (no more than once per `token-log-interval-seconds` seconds). Sample log message:; ```; Token Dispenser: The backend Local is starting too many jobs. New jobs are being limited.; ```. Sample queue status output:; ```; ""Token Dispenser state"": {; ""queues"": [{; ""token type"": ""BACKEND=Local/TOKENLIMIT=Some(10)/HOGFACTOR=5"",; ""queue state"": {; ""queue"": [{; ""name"": ""4a458483"",; ""queue size"": 2; }, {; ""name"": ""b106f1f4"",; ""queue size"": 2; }],; ""pool"": {; ""hog groups"": [{; ""hog group"": ""4a458483"",; ""used"": 2,; ""available"": false; }, {; ""hog group"": ""b106f1f4"",; ""used"": 2,; ""available"": false; }],; ""hog limit"": 2,; ""capacity"": 10,; ""leased"": 4; }; }; }],; ""pointer"": 0,; ""total token assignments"": 4; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567:898,avail,available,898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567,2,['avail'],['available']
Availability,"Closes #4824 . I didn't do; >a sort of input_errors map with input names as keys and error(s) as values. because; 1. The underlying WOM creation returns pre-formatted strings (e.g. `Required workflow input 'wf_hello.hello.addressee' not specified`) and changing that interface would be a huge undertaking; 2. There's no neat way to handle extraneous inputs, since they obviously would not match any input name key in the map",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4940:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940,1,['error'],['error']
Availability,"Closes #5460 . Cromwell isn't correctly cascading `string + optional + string` behaviour. It seems that `string + optional` is evaluating to an empty string. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. That is, if `str` resolves to `None`, then this should resolve to `echo `. Instead this is currently resolving to `echo ""` (single quote).; ```; echo ~{'""--prefix"" ""' + str + '""'}; ```. ## Example. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. Without value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl ; # Job quotetest:NA:1 exited with return code -1; # STDERR: unexpected EOF while looking for matching '""'; ```. With value:. ```bash; java -jar cromwell-48.jar run quotetest.wdl -i quotestest-inp.json ; # ""outputs"": {; # ""quotetest.out"": ""--prefix Hello""; # }; ```. ## This PR. Addresses the spec, by:. - Changing type checker: OptionalType<T> + T => Optional<T> (within the current rules).; - Optional + String => String else None",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5464:705,echo,echo,705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464,4,['echo'],['echo']
Availability,Closing - we believe this is fixed and haven't heard this error pop up since. Closing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915,1,['error'],['error']
Availability,Closing as redundant. See #2638.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725:11,redundant,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725,1,['redundant'],['redundant']
Availability,Closing issue: . - In agreement that breaking existing behavior is not a good approach. ; - Need to work out better understanding of cost per sample especially with regards to failure preemption or otherwise. . I'm likely going to soft-fork internally for certain projects and gather some hard numbers.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235:176,failure,failure,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235,1,['failure'],['failure']
Availability,Closing since the error message now contains (a) that a file was missing (b) the appropriate file name,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224,1,['error'],['error']
Availability,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,redundant,redundant,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540,1,['redundant'],['redundant']
Availability,Colon ':' in output filename causes workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919:45,failure,failure,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919,1,['failure'],['failure']
Availability,Combination of read_lines and list of filenames causes erroneous file not found errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:80,error,errors,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,1,['error'],['errors']
Availability,Come up with a way to test aliases reliably,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4848:35,reliab,reliably,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4848,1,['reliab'],['reliably']
Availability,Command field cannot have empty strings errors from Centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3737:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737,1,['error'],['errors']
Availability,Command variable expansion masks bash variable interpretation with ${...},MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3111:27,mask,masks,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3111,1,['mask'],['masks']
Availability,Command-line tool hangs after error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['error'],['error']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl > /dev/null; ```; This produces no output. Running the same command without redirecting STDOUT results in an error message. This message should likely be reported on STDERR, not STDOUT, to be consistent with common standards around command-line tools.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4063:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4063,1,['error'],['error']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:727,ERROR,ERROR,727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['ERROR'],['ERROR']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4061:243,failure,failure,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061,1,['failure'],['failure']
Availability,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:715,heartbeat,heartbeat,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"CommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1647,error,error-keys,1647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,Communications link Failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:20,Failure,Failure,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,1,['Failure'],['Failure']
Availability,"Concoct an SQL table schema which can handle efficiently looking up any information currently provided by the Cromwell endpoints which read from the metadata service. This might get hairy with query and/or metadata, but see what can be done. This schema will be realized in a CloudSQL database. NB: Google PubSub does not guarantee ordering of events. We should already be robust to this via the CRDT structures and such, but take that into account w/ this table structure. I could imagine there being situations where e.g. â€œtimestamp from the payload of last event I processed which led to this stateâ€ might also be good to track or something like that. If you find yourself looking for ideas on how to handle ordering issues, [this Google doc](https://cloud.google.com/pubsub/docs/ordering) provides some examples for all of the situations we have",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3242:373,robust,robust,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3242,1,['robust'],['robust']
Availability,Configurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7835,recover,recover,7835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"Confirmed that this issue has been resolved, a proper error pops up when missing a required refresh token.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248,1,['error'],['error']
Availability,"Confirmed the error is improved in v39:. Looks similar to this. ; ```""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Please check the log file for more details: gs://ss_cromwell_bucket/cromwell-execution/exceed_disk_size/d142f233-f72a-40a6-9f84-8b8a2ead32e7/call-simple_localize_and_fetch_size/simple_localize_and_fetch_size.log.""```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379,2,['error'],['error']
Availability,"Consider the following WDL using the [if then else](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#if-then-else) construct:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if x == 1 then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 20, col 15) when parsing 'e'. Expected then, got """". echo ~{if x == 1 then 1 else 0}; ^. $e = :if $e :then $e :else $e -> TernaryIf( cond=$1, iftrue=$3, iffalse=$5 ); ```. The following equivalent WDL instead:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. Int y	= if x == 1 then 1 else 0; command <<<; echo ~{y}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```. Similarly this equivalent WDL:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if !(x != 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; Success!; ```; It seems like the parser does not accept the `==` operator in the condition of the `TernaryIf` for some reasons, but only in the case it is included in a `command <<< >>>` section.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602:254,echo,echo,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602,5,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,Consistent failure to delocalize file referenced by variable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4901:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4901,1,['failure'],['failure']
Availability,Console error includes debugging information,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['error'],['error']
Availability,"Cool, then you are probably good to go :) Did you want feedback on something in particular? I don't know why you pinged me but I'm glad to see you have a new command!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873:113,ping,pinged,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873,2,['ping'],['pinged']
Availability,Copy workflow logs even upon failure closes #1621,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1720:29,failure,failure,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1720,1,['failure'],['failure']
Availability,Correct HPC tutorial config error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3488:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488,1,['error'],['error']
Availability,"Could not download toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz, giving up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:10,down,download,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['down'],['download']
Availability,"Could you comment on the granularity?. Is it ""cromwell is alive, hooray!"" or some more involved breakdown of various subsystems, e.g. ""can't talk to JES right now, the DB is being a poop, but SGE is going swimmingly!""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/898#issuecomment-222184572:58,alive,alive,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/898#issuecomment-222184572,1,['alive'],['alive']
Availability,"Create a standalone application which will:. - Create/update schema as per #3242 ; - Subscribe to a PubSub topic; - Consume events in the format emitted by the metadata implementation at #3098 ; - For each metadatum, performs any necessary upserts into CloudSQL. This system should pull events off of the message queue exactly as fast as it is writing them to the database, i.e. it shouldn't be backing up and causing Slick* errors but it should also not be dawdling either. * I'm not at all opposed to using something other than Slick. If you want to go this route, let's talk.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3244:425,error,errors,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3244,1,['error'],['errors']
Availability,"Created in collaboration with: @TMiguelT. The OpenWDL spec states when interpolating a string in the command block:; > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. Ie:; - `string + null + string -> null`:. That is, if `str` is not defined (`null`), the following should resolve to null and empty:; ```; ~{'""--prefix"" ""' + str + '""'}; ```. Currently, it's resolving to `""` (a single double-quote). Eg: In the task:. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. A fix to this would reduce our usages of `if defined(name) then """" else """"`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5460:743,echo,echo,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5460,1,['echo'],['echo']
Availability,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1290:365,failure,failure,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290,1,['failure'],['failure']
Availability,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. ðŸ˜› . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:666,echo,echo,666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999,1,['echo'],['echo']
Availability,CromIAM is already more than an IAM service and that will continue. This has already caused some confusion. Perhaps `caas`? . If moved to monorepo I'm thinking a `workbench` subproject would be ideal as we start down this path. . either way these changes will likely need some interaction w/ devops for hte docker magic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3217:212,down,down,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3217,1,['down'],['down']
Availability,Cromiam error with gzip encoding,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4708:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4708,1,['error'],['error']
Availability,"Cromwell (38, in this case) is saturating the available connections to our managed MySQL database. Our DBAs increased the limit and Cromwell proceeded to fill up these slots. How can we limit the number of concurrent connections to a MySQL database? There doesn't seem to be any configuration option for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4777:46,avail,available,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777,1,['avail'],['available']
Availability,Cromwell 0.16 is available now on Macs via `brew install cromwell`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870:17,avail,available,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870,1,['avail'],['available']
Availability,Cromwell 21 fails retrying non-preemptible jobs when getting a 429 error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763,1,['error'],['error']
Availability,"Cromwell 34; Backend: local. `cc28a122-6605-4d53-83d3-245a88f8ad96` is the current workflow and I did not open the log file.; ```; [2018-10-01 10:02:13,69] [error] Failed to delete workflow log; java.nio.file.FileSystemException: /media/sf_broad_oncotator_configs/test_m2_small/cromwell-workflow-logs/workflow.cc28a122-6605-4d53-83d3-245a88f8ad96.log: Text file busy; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); 	at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); 	at java.nio.file.Files.delete(Files.java:1126); 	at better.files.File.delete(File.scala:619); 	at cromwell.core.path.BetterFileMethods.delete(BetterFileMethods.scala:413); 	at cromwell.core.path.BetterFileMethods.delete$(BetterFileMethods.scala:412); 	at cromwell.core.path.DefaultPath.delete(DefaultPathBuilder.scala:55); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186,1,['error'],['error']
Availability,"Cromwell 37 errors when the backend submit configuration contains an expression like:; `${""-l h_vmem="" + memory + ""G""}`: ; <details>; <summary> error message </summary>; <pre><code>; cromwell.core.CromwellFatalException: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,4,"['Error', 'error', 'recover']","['Error', 'error', 'errors', 'recoverWith']"
Availability,Cromwell 503 Service Unavailable Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903:33,Error,Error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903,1,['Error'],['Error']
Availability,Cromwell 55 expected to handle 504 error in GCS but instead WDL pipeline failed,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:35,error,error,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,1,['error'],['error']
Availability,Cromwell 59 download broken,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6500:12,down,download,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6500,1,['down'],['download']
Availability,Cromwell GCP error - The referenced network resource cannot be found,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,1,['error'],['error']
Availability,Cromwell Metadata essentially empty; Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/801:37,Error,Error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/801,1,['Error'],['Error']
Availability,Cromwell Out of Memory Error Joint Genotyping Workflow Large Array of Arrays,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1051:23,Error,Error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1051,1,['Error'],['Error']
Availability,"Cromwell Version: 77. Hi Guys,. Im not sure if this is a bug but I recently noticed this behaviour. Im trying to write a task that registers task S3 outputs at the end of a workflow in a third-party system and copy it to another S3 bucket. This task itself does not generate any files locally but at the end of its execution I expect a new S3 object to appear in the destination bucket. The inputs and outputs are marshalled using a struct with a ""File"" variable for the S3 path of the objects. After the task executes Cromwell throws an error scala.MatchError - it recognises that the output is a `cromwell.filesystems.s3.S3Path` but dosn't appear to know what to do with it. Im wondering if it is because the ""file"" is created outside of the workflow workspace?. My work around is to use the `String` type in place of `File` type for ""path"" and cast back to `File` later in the workflow but this feels inelegant. Input:. {; ""main.bucket"" : ""my_bucket_2"" ,; ""main.file_list"":[; { ""path"": ""s3://my_bucket_1/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""}; ]; }. Expected Output:. [; {; ""id"": ""123""; ""path"": ""s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""; }; ]. Code:. struct file_thing {; String? id; File path; }. task copy_file_list{; input{; String bucket; Array[file_thing] file_list; }. command <<<; copy_files \; --bucket ~{bucket} \; --json_in ~{write_json(file_list)} \; --json_out outputs.json; >>>. output {; Array[file_thing] outputs = read_json(""outputs.json""); }. runtime {; docker: ""my_copy_tool:latest""; }; }. Error:. WorkflowManagerActor: Workflow e7a60e4b-8dc4-471b-aec6-b8cc1481f889 failed (during ExecutingWorkflowState): ; scala.MatchError: s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html (of class ; cromwell.filesystems.s3.S3Path); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath(SharedFileSystem.scala:239); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath$(SharedFileSystem.scala:237); 	at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6716:538,error,error,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6716,1,['error'],['error']
Availability,"Cromwell Version: `""cromwell"": ""33-e90c4de""`. Problem: The workflow has scattered tasks, a few of the shards finished without any errors, but when looking into the actual results of the task, we can only see files with `0B` size. Example workflow: workflow `42e173c6-7fc3-4a3e-93c7-c9d95836f6a5 ` in `https://cromwell.mint-dev.broadinstitute.org/`, specifically, the task: `call-sc/shard-98/SmartSeq2SingleCell/b4ac422c-e5b1-42ed-8dcf-cca51394e08c/call-RSEMExpression`, shard-98. @jishuxu has run into this issue for several times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006:130,error,errors,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006,1,['error'],['errors']
Availability,"Cromwell cyclic dependency error in 30.1, fine in 29",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['error'],['error']
Availability,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1140:697,down,downsampled-bams,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140,1,['down'],['downsampled-bams']
Availability,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117,2,"['error', 'failure']","['error', 'failure']"
Availability,"Cromwell doesn't seem to retry on *some* 503 errors, but does an others.; Might be related to https://github.com/GoogleCloudPlatform/google-cloud-java/issues/1545; The `code` attribute in the exception seems to be 503 in both cases so we could check that explicitly instead of relying on the `isRetryable` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2207:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2207,1,['error'],['errors']
Availability,"Cromwell doesn't support images from ghcr.io. Not sure if there's a workaround besides personally rehosting the image somewhere else. Seeing as a lot of bioinformatics images are hosted there I'd like to see it be supported. ## error returned; [2022-08-11 12:40:55,22] [warn] BackendPreparationActor_for_e48a0b67:Minos.minos_adjudicate:-1:1 [e48a0b67]: Docker lookup failed; java.lang.Exception: Registry ghcr.io is not supported. ## backend; Running Cromwell on a local machine, eventually will be running in Terra. My local machine has Docker installed and already has the required Docker image pulled. ## relevant workflow task; ```; task minos_adjudicate {; 	input {; 		File ref; 		File reads; 		File vcf1; 		File vcf2. 		# runtime attributes; 		Int addldisk = 1; 		Int cpu = 4; 		Int retries = 1; 		Int memory = 8; 		Int preempt = 2; 	}; 	# Estimate disk size required; 	Int ref_size = ceil(size(ref, ""GB"")); 	Int finalDiskSize = 2*ref_size + addldisk. 	String ref_basename = basename(ref). 	command <<<; 		# softlinks don't seem to cut it here; 		set -eux -o pipefail; 		cp ~{ref} .; 		minos adjudicate --reads ~{reads} outdir ~{ref} ~{vcf1} ~{vcf2}; 	>>>; 	; 	runtime {; 		cpu: cpu; 		docker: ""ghcr.io/iqbal-lab-org/minos""; 		disks: ""local-disk "" + finalDiskSize + "" HDD""; 		maxRetries: ""${retries}""; 		memory: ""${memory} GB""; 		preemptibles: ""${preempt}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827,1,['error'],['error']
Availability,Cromwell gave mysterious error of missing files during workflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,1,['error'],['error']
Availability,Cromwell gives wrong error message with unclosed brace,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['error'],['error']
Availability,"Cromwell has a system for adjusting boot disk sizes that is intended to account for large Docker images that would otherwise overflow the boot disk at its default size. However it seems this system is often not preventing boot disks from overflowing: we have received a number of reports recently of users having to explicitly set runtime attributes for boot disk size because this automatic system did not sufficiently increase the boot disk size. Furthermore a review of the ""compression factor"" used for this boot disk size adjustment system revealed that the default compression factor appears to be much larger than actual compression factors seen in Docker images commonly used in Cromwell (cloud-sdk, gatk, etc). i.e. the current model of boot disk size adjustment does not appear to line up well with actual observed boot disk size behavior. This PR changes the way boot disk size adjustment is done to use a more realistic compression factor (3.0 vs 6.0), and instead *adds* the estimated uncompressed Docker image sizes to the default boot disk size. This should yield consistently larger and less failure prone boot disk sizes with scaling behavior that should hopefully better align with real world Docker images.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5790:1108,failure,failure,1108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790,1,['failure'],['failure']
Availability,"Cromwell keeps track of the status of the DAG with an internal map of `Scope`s.; This map should probably now reference `GraphNodes` which are a subset of the `Scope`s and represent more accurately a node in the graph, with associated traversal methods (upstream, downstreams etc...). `Declaration`s are now `Scope`s (and even (`GraphNode`s). This should allow WDL like. ```; ...; workflow wf {; call A; String dec = A.out; call B { input: b_input = dec }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1577:264,down,downstreams,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1577,1,['down'],['downstreams']
Availability,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:385,down,down,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,2,['down'],['down']
Availability,Cromwell metadata servers in FC prod have been brought down by repeated requests for the same workflow metadata before a response has been returned for the first request. It should be fairly easy to implement a requesters map that would allow Cromwell to only assemble metadata once and respond to all requests with that same metadata. The implementation here could be very similar to (though simpler than) the [file hash caching](https://github.com/broadinstitute/cromwell/pull/4143/files) system.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226,1,['down'],['down']
Availability,Cromwell out of memory error large array,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['error'],['error']
Availability,Cromwell parallel to https://github.com/DataBiosphere/cbas/pull/194. #7190 is an example of a cromwell PR that didn't make it to terra-helmfile - [corresponding job failure](https://github.com/broadinstitute/cromwell/actions/runs/5870702268/job/15918276016),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7206:165,failure,failure,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7206,1,['failure'],['failure']
Availability,Cromwell preemption and error retry interaction with GCP Batch built-in retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7476:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7476,1,['error'],['error']
Availability,"Cromwell production goes down, ""too many files open""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716:25,down,down,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716,1,['down'],['down']
Availability,"Cromwell release 36.1 has a docker image available for the release, but nowhere in the docs is the cromwell docker container documented (as far as I can tell).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682:41,avail,available,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682,1,['avail'],['available']
Availability,Cromwell retry file download,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4710:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710,1,['down'],['download']
Availability,"Cromwell seems to erroneously treat private/internal variables with File type as input files and attempts to localize them:. ```; version 1.0. task mytask {; 	input {; 		String mystr = ""Hello World!""; 	}. 	File myfile = ""myfile"". 	command <<<; 		echo ~{mystr} > ~{myfile}; 	>>>. 	output {; 		String file_contents = read_string(myfile); 	}; }. workflow wf {; 	call mytask; }; ```. ```; Could not localize myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile:; 	myfile doesn't exist; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/cromwell-executions/wf/51d2f863-0e91-45b6-9e7b-f2365c259144/call-mytask/inputs/1979661608/myfile -> /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	File not found myfile; 	File not found /home/jared/projects/gambit/data/misc/211031-apollo_illumina_pe-miniwdl/test-cromwell/myfile; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:94); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:90); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:669); 	... 35 common frames omitted; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6562:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6562,1,['echo'],['echo']
Availability,"Cromwell seems to love `hard-links`:; * No matter what configuration I feed it, it will always tries `hard-links` first.; * When this fails. It does not try anything else. Given the config file `test.config`; run with `java -Dconfig.file=test.config -jar cromwell-30.1.jar run -i inputs.json test.wdl` ; ```HOCON; include required(classpath(""application"")). backend {; default=""Local""; providers {; Local {; config {; filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; }; }; }; }; }; }; }; ```. ## Expected behavior:; Crommwell will prefer to use `ln -s`. If that fails it will copy. ## Actual behavior:; Cromwell output:; ```; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_000889155.1_ViralProj51245_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_000889155.1_ViralProj51245_genomic.fna': Invalid cross-device link; ln: failed to create hard link '/home/ruben/IdeaProjects/construct-centrifuge-index/cromwell-executions/ConstructCentrifugeIndex/3e1afa55-3234-4fa9-b7e8-63d143846b9f/call-download/shard-0/execution/glob-0bd9f0edef72448a92c8f9e79babdc8d/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna' => '/tmp/test/centrifuge/data/libraries/53abeac3037f9afe08309700c99f725f/viral/GCF_001343785.1_ViralMultiSegProj274766_genomic.fna': Invalid cross-device link; ```; It tries only `hard-links` if this fails. It just continues, not even trying soft links, failing to record my globbed files and crashing the pipeline down the line.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109:899,down,download,899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109,3,['down'],"['down', 'download']"
Availability,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:46,heartbeat,heartbeats,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028,10,"['down', 'heartbeat']","['down', 'heartbeat', 'heartbeats']"
Availability,Cromwell should be resilient to Docker host outages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,2,"['outage', 'resilien']","['outages', 'resilient']"
Availability,"Cromwell should prevent itself from dying on attempt to read too large workflow metadata, and return read failure instead [BA-6447]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5519:106,failure,failure,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5519,1,['failure'],['failure']
Availability,Cromwell throws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:208,recover,recover,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,4,"['ERROR', 'Error', 'Recover', 'recover']","['ERROR', 'Error', 'Recover', 'recover']"
Availability,"Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6070:154,reliab,reliable,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6070,1,['reliab'],['reliable']
Availability,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855:16,Error,Error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855,8,"['Error', 'error', 'failure']","['Error', 'error', 'failure']"
Availability,Cromwell tries to define undefined variables causing bizarre errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:61,error,errors,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['error'],['errors']
Availability,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:865,error,error,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['error'],['error']
Availability,"Cromwell version: 30-9a7de06. Minimized from one of our WDLs:; ```wdl; workflow Test {; Boolean do; Int n. if (do) {; call Optional; }. scatter (i in range(n)) {; call Scattered; }. call Gather {; input:; # HERE: select_first returns String, and Scattered.out is an Array[String]; ins = if defined(Optional.out) then select_first([Optional.out]) else Scattered.out; }; output {; Gather.out; }; }. task Optional {; command {; echo ""Hey!""; }; output {; String out = read_string(stdout()); }; }. task Scattered {; command {; echo ""Hello!""; }; output {; String out = read_string(stdout()); }; }. task Gather {; Array[String] ins. command {; cat ${write_lines(ins)}; }; output {; String out = read_string(stdout()); }; }; ```. This WDL runs successfully, but in code review I noticed the weird type mismatch between the branches. I asked @cjllanwarne about it and he thought it was an old ""feature"" that had been purged to avoid bugs / confusion. I'd expect something like this to be rejected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3478:425,echo,echo,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3478,2,['echo'],['echo']
Availability,"Cromwell version: 34; Backend: local. Fails in cromwell 34. Worked fine in cromwell 32. Task M2 is scattered. Then FuncotateMaf takes the output value from the first shard. M2 was has not been run by the time this error message appears. Error:; ```; [2018-10-01 10:02:13,68] [error] WorkflowManagerActor Workflow cc28a122-6605-4d53-83d3-245a88f8ad96 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'Mutect2.FuncotateMaf.case_id' (reason 1 of 1): Evaluating M2.tumor_sample[0] failed: Failed to find index Success(WomInteger(0)) on array:. Success([]). 0; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:483); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:480); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:479); 	at scala.util.Either.flatMap(Either.scala:338); ....; java.lang.RuntimeException: Failed to evaluate 'Mutect2.FuncotateMaf.control_id' (reason 1 of 1): Evaluating M2.normal_sample[0] failed: Failed to find index Success(WomInteger(0)) on array:. Success([]). 0; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:483); 	at scala.util.Either.flatMap(Either.scala:338); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185:214,error,error,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185,3,"['Error', 'error']","['Error', 'error']"
Availability,Cromwell's failure is now complete (develop edition). Closes #2201,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2203:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203,1,['failure'],['failure']
Availability,Cromwell's failure is now complete (hotfix edition),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2205:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2205,1,['failure'],['failure']
Availability,"Cromwell's requester pays logic works by trying to perform GCS operations without specifying a project to bill. If the operation is successful, great, all done. If the operation is not successful and the error message looks like a requester pays error, the operation is retried with the project to bill specified. IIRC this system is in place because always specifying the project to bill resulted in the project being billed even if the bucket was not requester pays. It's unfortunate this logic needs to be so clunky when GCS does have the concept of [provisional user projects](https://developers.google.com/resources/api-libraries/documentation/storage/v1/java/latest/com/google/api/services/storage/Storage.Buckets.GetIamPolicy.html#setProvisionalUserProject-java.lang.String-) but this concept is not supported in the Google Storage API used by the GCS filesystem. Anyway the ""is this requester pays"" logic used to look for exact matches to an error message string, i.e. exactly this:; ```; Bucket is requester pays bucket but no user project provided.; ```; However with increasing probability (the `requester_pays_engine_functions` Centaur test fails about 50% of the time with the baseline Cromwell code) we are seeing error messages that actually look like this:; ```; 400 Bad Request; POST https://storage.googleapis.com/upload/storage/v1/b/cromwell_bucket_with_requester_pays/o?projection=full&uploadType=multipart; {; ""error"": {; ""code"": 400,; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""errors"": [; {; ""message"": ""Bucket is requester pays bucket but no user project provided."",; ""domain"": ""global"",; ""reason"": ""required""; }; ]; }; }; ```. The changes here accommodate either version of the error message with a `null`-safe `contains` check courtesy Apache StringUtils.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556:204,error,error,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556,7,['error'],"['error', 'errors']"
Availability,"Cromwell: 36. I had the following config file, missing a brace:. ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:598,alive,alive,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,2,"['alive', 'error']","['alive', 'error']"
Availability,"Cromwell: 36; Backend: Google Cloud. When running a CWL snpEff tool using Cromwell ([CWL is here](https://github.com/broadinstitute/cromwell/files/2743866/snpEff.cwl.txt)), I get the following errors: ([full error log](https://github.com/broadinstitute/cromwell/files/2743887/snpeff_indels-stderr.log)). ```; xargs: invalid option -- 'I'; find: unrecognized: -empty; ```. In other words, Cromwell is generating a script that uses `xargs -I` and `find -empty`, flags which are not compatible with Busybox ([full Cromwell-generated script](https://github.com/broadinstitute/cromwell/files/2743877/script.txt)). The reason this matters is that all [Biocontainers](https://biocontainers.pro/) are based on Busybox, and I would say they represent a majority of the bioinformatics containers, so ensuring compatibility is in everyone's interest. Might it be possible to edit the Cromwell script to use a smaller subset of these command line flags?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536:193,error,errors,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536,2,['error'],"['error', 'errors']"
Availability,"Cromwell: 36; Backend: PAPI/Google Cloud. If an input file to a WDL pipeline contains a space, the script Cromwell generates to run on the pipelines API will fail. For instance, I have an input file like this (truncated):; ```json; {; ""best_practise.ref_fasta"": ""gs://genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta"",; }; ```; This make Cromwell generate a PAPI script containing this line:; ```bash; bash_ref_fasta=/cromwell_root/genovic-test-data/hg19/Reference Genome/default_unzipped/ucsc.hg19.fasta; ```. This then causes the following error in the script:; ```; /cromwell_root/script: line 26: Genome/default_unzipped/ucsc.hg19.fasta: No such file or directory; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393:566,error,error,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393,1,['error'],['error']
Availability,"Cromwell: 36; Mode: Server; Backend: Google Cloud; ***; I've noticed a strange error that only happens on the Google Cloud backend. Cromwell runs the WDL tasks with the correct command line, but somehow the arguments after the initial command aren't being picked up by the binary inside the docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,2,['error'],['error']
Availability,"Cromwell: Development (`37-3b2affa`); Backend: HPC (`ConfigBackendLifecycleActorFactory`). I wanted access to a recently merged pull request (#4437), so I built a development version of Cromwell. However, when I run it with the same configuration file as I used for Cromwell 36, I get this error:; ```; [ERROR] [01/24/2019 11:10:24.126] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/SingleWorkflowRunnerActor] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:290,error,error,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Crossposting here as I was not able to get an answer on WDL forum. Post is here:; https://gatkforums.broadinstitute.org/wdl/discussion/13540/unable-to-do-docker-lookup#latest. Trying to get hello world working on AWS Batch and cromwell. I am able to spin up the servers however it fails in pulling the docker image with the following error message in the cromwell logs:. `; 2018-10-30 15:43:14,845 cromwell-system-akka.dispatchers.engine-dispatcher-9 WARN - BackendPreparationActor_for_7d0c30ad:wf_hello.hello:-1:1 [UUID(7d0c30ad)]: Docker lookup failed java.lang.Exception: Failed to get docker hash for ubuntu:latest Docker hash lookup failed with code 503. The server is currently unavailable (because it is overloaded or down for maintenance). at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:188); `. Here is my wdl:. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell on AWS""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. This is originally from tutorial by @wleepang found here:; https://www.youtube.com/watch?v=jcC3pz_K4gI. Any idea on how to diagnose? I was not able to find a similar issue. Thanks so much for the help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345:334,error,error,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345,4,"['down', 'echo', 'error', 'mainten']","['down', 'echo', 'error', 'maintenance']"
Availability,Cryptic error when missing refresh token,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['error'],['error']
Availability,Ctrl-C: Cromwell Doesn't Shut Down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007:30,Down,Down,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007,1,['Down'],['Down']
Availability,"Current copy simply says, ""wdlDependencies"" ... proposed change to ""wdlDependencies (zip)"". Since uploading a single wdl file in this box will cause an ""invalid zip file error"", but does not specify that the error was in the wdlDependencies field.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1971:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1971,2,['error'],['error']
Availability,"Current known issues (feel free to add/remove/edit):; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. Before running the nightly cron test another script should auto sweep old jobs so they don't have to be manually cleaned up with `bcs dj`. Some mix of bcs/bash/jq/sed/awk/python should work and are all available on Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3555:364,avail,available,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3555,1,['avail'],['available']
Availability,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:144,down,down,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,5,"['avail', 'down', 'failure']","['available', 'down', 'failures']"
Availability,"Current scheme:; - If a workflow option is specified, use that; - If a runtime attribute is specified, use that; - If neither are specified, use the default backend (silently, without error). There are a few things wrong with this.; - If a user specifies a particular backend we should _not_ be silently giving them some other backend. IMO we shouldn't be giving them another backend period.; - The order of workflow option & runtime attribute makes sense in a single backend workflow but in a multi-backend workflow (which apparently _does_ work, at least in some cases) it's wrong. I think the best thing would be to expand workflow option to allow per-call backend specification or something like that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312,1,['error'],['error']
Availability,"Currently -- the Cromwell (and other?) service logs on the alpha env are around for upto 5 days. . It would be great to have their availability extended to a longer life line, if feasible. . Being investigated by David Bernick.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3894:131,avail,availability,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3894,1,['avail'],['availability']
Availability,"Currently Workflow outputs can be copied at the end of a workflow, and this is done by then engine WorkflowFinalizationActor.; All the information this actor has is file paths as `String`s. To be able to copy those files out it needs to create a `Path` from them with the right filesystem / auth, which it currently can't do reliably since it doesn't have any information about which backend produced this output or with which auth. A possible fix to that would be to make `wdl4s.WdlFile` wrap `java.nio.File` instead of `String`, so the filesystem / auth information is not lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1684:325,reliab,reliably,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684,1,['reliab'],['reliably']
Availability,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1504:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504,1,['down'],['down']
Availability,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881:168,avail,available,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:495,down,down,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891,2,['down'],['down']
Availability,"Currently it barfs on aliased call commands yielding the following error:; ```; Exception in thread ""main"" wdl4s.parser.WdlParser$SyntaxError: ERROR: Expression references output on call that doesn't exist (line 1572, col 5):. ValidateCram.*; ^; ; 	at wdl4s.Workflow.$anonfun$expandedWildcardOutputs$5(Workflow.scala:166); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241); 	at scala.collection.immutable.List.foreach(List.scala:378); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:241); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:238); 	at scala.collection.immutable.List.flatMap(List.scala:341); ```. FYI: `ValidateCram` was called as such in the file:; ```; call ValidateSamFile as ValidateCram {; input:; input_bam = ConvertToCram.output_cram,; input_bam_index = ConvertToCram.output_cram_index,; report_filename = sample_name + "".cram.validation_report"",; .....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:191,down,down,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748,1,['down'],['down']
Availability,"Currently the attributes of `WorkflowInputParameter` (secondaryFiles, doc, format etc...) are being ignored and only the type / name are used to create an `ExternalGraphInputNode` that will then get its value from the input yaml file.; Those attributes might contain information that we want to use and should be made available somehow for processing once the value is assigned to the input.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3441:318,avail,available,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3441,1,['avail'],['available']
Availability,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,recover,recovery,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,1,['recover'],['recovery']
Availability,Currently the only trace of a call end status after the workflow has completed is in the metadata.; This makes it impossible for other endpoints (e.g: call caching diff endpoint) to relay this information without reading from metadata. We might want to investigate storing this information somewhere permanent: it is currently available in the jobstore but only for the duration of the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338:327,avail,available,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338,1,['avail'],['available']
Availability,"Currently when a user uses a subworkflow id for anything which takes a workflow id it will fail. This is because CromIAM queries SAM first and SAM only knows about root workflow IDs. . It seems like the right thing to do here will be to get the root workflow ID from Cromwell and then ping Sam, but feel free to do something edifferent if it makes sense to do so",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3459:285,ping,ping,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3459,1,['ping'],['ping']
Availability,"Currently, after a backend job completes successfully, if something goes wrong trying to read the RC file and / or the stderr file size, the previous ""handle"" is sent back which triggers a new status poll and it goes down the same code path again, trying to re-read the RC file / stderr.; If for some reason one or both of those files is permanently damaged and can never be read, the actor will be stuck in an infinite retry loop.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1846:217,down,down,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1846,1,['down'],['down']
Availability,"Currently, as far as I can tell, the paging functionality is pretty hard to use since there is no notation of the total records (given filters) available. Returning page and pagesize or at least starting record would be helpful. Finally, being able to do this relies some kind of reproducible ordering of records; I assume that ordering is deterministic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1793:144,avail,available,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1793,1,['avail'],['available']
Availability,"Currently, it seems that aggregated failures have an identical key within metadata, and thus when metadata is reported with that key, only one of the multiple aggregated errors is reported. . AC: Fix this behavior such that, going forward, all aggregated errors are reported by metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2046:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2046,3,"['error', 'failure']","['errors', 'failures']"
Availability,"Currently, the file systems available to the engine for functions like read_\* are statically defined. GCS, Local, etc. This issue is to make that driven by the config file. The reason this is important is because if you are running a cromwell server and can not disable the ""Local Shared Filesystem"" from the engine... someone could write a WDL that does a read_ on any file that the cromwell server has access to (e.g. read_lines(""./cromwell.conf"")... which is bad",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821:28,avail,available,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821,1,['avail'],['available']
Availability,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:672,avail,available,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,2,['avail'],['available']
Availability,"Currently, there is no library function to flatten an array of array of files (`Array[Array[File]]`). A scatter, where each task call produces an array of files, is a natural way of ending up with such a structure. In order to flatten this array, you can write a task that takes the it as an argument, and manipulate it with python code. However, this task will also download all the files, taking significant time and disk space. To work around this, you can coerce the files into strings (their paths), and manipulate the paths. . You can see an example [here](https://github.com/HumanCellAtlas/skylab/blob/master/10x/count/count.wdl#L195). The `chunk_reads_join` task flattens the `fastq_chunks` file array, which is coerced into an `Array[Array[String]]`. In order to avoid this circuitous implementation, this pull requests implements a generic flatten operation for ragged array types.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2825:367,down,download,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2825,1,['down'],['download']
Availability,"Currently, this is set to default to 128000. This is too small for most practical use, especially given that gs URLs can get quite long. Can the new limit be much higher? We'll need 5GB (no joke!) for some our larger analyses. Or at least a workflow option to temporarily override?. Otherwise, we get an error such as:; ```""Workflow has invalid declarations: Could not evaluate workflow declarations:\nSingleSampleGenotyping.gvcfs_list:\n\tUse of WdlSingleFile(gs://broad-dsde-methods/gauthier/Finnish_FE_WGS.1000samples.gvcf_list) failed because the file was too big (174730 bytes when only files of up to 128000 bytes are permissible""```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768:304,error,error,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768,1,['error'],['error']
Availability,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395:690,error,error,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395,1,['error'],['error']
Availability,Custom scaladoc settings must be appended (++=) instead of overwriting (:=).; Fixed unmoored doc warnings/errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2989:106,error,errors,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2989,1,['error'],['errors']
Availability,Cwl Expressions Support Checkpoint #3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2737:24,Checkpoint,Checkpoint,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2737,1,['Checkpoint'],['Checkpoint']
Availability,"D, referencedTableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,321 INFO - ALTER TABLE ""public"".""WORKFLOW_STORE_ENTRY"" ADD ""HOG_GROUP"" VARCHAR(100); 2019-07-21 23:07:19,322 INFO - Columns HOG_GROUP(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2019-07-21 23:07:19,331 INFO - ChangeSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.vis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34499,ERROR,ERROR,34499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['ERROR'],['ERROR']
Availability,"DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initializ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2266,down,down,2266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['down'],['down']
Availability,"DATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1727,down,down,1727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['down'],['down']
Availability,"DL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3020,Error,Error,3020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Error'],['Error']
Availability,"DL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the language type is; 2. Something is off about the imports flag for 44",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1627,echo,echo-job,1627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,8,['echo'],['echo-job']
Availability,"DO NOT MERGE YET!. Hopefully a final-ish version of the DataAccess API to unblock its implementation in Slick. . RIP StoreActor, SymbolStore, and ExecutionStore. The spirit of ExecutionStore lives on in WorkflowActor, and maybe SymbolStore might find a place there too to rid us of some Await.result()s. StoreActor is dead for real. . Known issues:. FIXED ~~1) The Docker test is broken because the backend-specific initialization was accidentally refactored away. Shouldn't be a big deal to restore that.~~; 2) All other tests pass, but that might just be because the restart test stinks. There are worrisome messages being emitted from that test which need to be tracked down and have assertions put on them. It might also be a good idea to test restarting a workflow more complex than ""Hello World"".; 3) No persistence of BackendInfo yet, shouldn't be tough but that needs a bit of discussion.; 4) More Await.results() in WorkflowActor than I would like or are probably necessary. This could be a separate Tech Debt issue. At least DataAccess is fully async.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/64:673,down,down,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/64,1,['down'],['down']
Availability,DRS ammonite localizer script is gone!! This PR replaces it with a executable jar in docker where the dependencies are no longer complied and downloaded each time you want to localizer DRS input!. JIRA [ticket](https://broadworkbench.atlassian.net/browse/BA-5821),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5120:142,down,downloaded,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5120,1,['down'],['downloaded']
Availability,"DSDEEPB-2026 Fixes Akka logging. The invocation of the error method flipped the arguments such that the exception was thrown away, the other methods don't accept an exception which also effectively threw away the exception. Below is what it looks like on current develop when an exception is thrown away. The Akka formatter realizes it has more substitution arguments than it is able to substitute. WorkflowActor [UUID(8ae8f57a)]: Failed to transition workflow status from Submitted to Failed WARNING arguments left: 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/294:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/294,1,['error'],['error']
Availability,Data migration for restart/recover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1119:27,recover,recover,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119,1,['recover'],['recover']
Availability,Database communications link failures in prod,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4689:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689,1,['failure'],['failures']
Availability,"DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; â€‚â€‚at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 INFOâ€‚â€‚- WorkflowActor [UUID(972b838f)]: persisting status of CollectUnsortedReadgroupBamQualityMetrics:10 to Failed.; 2016-05-10 11:38:08,738 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Read timed out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:8753,ERROR,ERROR,8753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['ERROR'],['ERROR']
Availability,De-serialize the workflow starter and heartbeat writes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4239:38,heartbeat,heartbeat,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4239,1,['heartbeat'],['heartbeat']
Availability,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:458,avail,available,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391,1,['avail'],['available']
Availability,"Dear Cromwell Team,; I am trying to run a workflow written in WDL using Cromwell v.65. The workflow reports the following error in the stdout:; ```[2023-08-11 14:21:11,58] [error] SingleWorkflowRunnerActor received Failure message: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow <UUID> exists in database but cannot be served because row count of 3138431 exceeds configured limit of 1000000.```; This is after having edited the `cromwell.conf` as suggested in [this thread](https://github.com/broadinstitute/cromwell/issues/2519). The configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:122,error,error,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"Dear Cromwell developers,. I am working on SQLite support at the moment. I have been successful in instantiating the database with all the correct tables. In other words the Liquibase migration seems to function fine. Unfortunately, the LiquibaseComparisonSpec keeps failing with a rather non-descript error:; ```; java.lang.NullPointerException was thrown.; java.lang.NullPointerException; 	at liquibase.structure.core.Index.setColumns(Index.java:118); 	at liquibase.snapshot.jvm.PrimaryKeySnapshotGenerator.snapshotObject(PrimaryKeySnapshotGenerator.java:80); 	at liquibase.snapshot.jvm.JdbcSnapshotGenerator.snapshot(JdbcSnapshotGenerator.java:66); ...; ```; If I write the database to a file, all the correct tables seem to be present (I might have missed one, but the database is certainly populated with a schema). Now I have been doing some digging and this is the database object that is created:; ```; database = {SQLiteDatabase@5510} ""null @ jdbc:sqlite::memory:""; systemTables = {HashSet@5517} size = 2; reservedWords = {HashSet@5518} size = 43; defaultCatalogName = null; defaultSchemaName = null; currentDateTimeFunction = ""CURRENT_TIMESTAMP""; sequenceNextValueFunction = null; sequenceCurrentValueFunction = null; dateFunctions = {ArrayList@5520} size = 1; unmodifiableDataTypes = {ArrayList@5521} size = 0; defaultAutoIncrementStartWith = {BigInteger@5522} ""1""; defaultAutoIncrementBy = {BigInteger@5522} ""1""; unquotedObjectsAreUppercased = null; quotingStrategy = {ObjectQuotingStrategy@5523} ""QUOTE_ALL_OBJECTS""; caseSensitive = {Boolean@5524} true; databaseChangeLogTableName = null; databaseChangeLogLockTableName = null; liquibaseTablespaceName = null; liquibaseSchemaName = null; liquibaseCatalogName = null; previousAutoCommit = {Boolean@5524} true; canCacheLiquibaseTableInfo = false; connection = {JdbcConnection@5525} ; outputDefaultSchema = true; outputDefaultCatalog = true; defaultCatalogSet = false; attributes = {HashMap@5526} size = 0; ```. `defaultCatalogName` and `def",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5453:302,error,error,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453,1,['error'],['error']
Availability,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5305:283,error,error,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305,1,['error'],['error']
Availability,"Dear all,. **Issue**; It appears that it is currently not possible to provide task variables in the input json when they are called in a subworkflow. In the example below the goal to overwrite SubTask optional value: overwrite_given_value. **Example**; _For this examples I use womtool 84 and WDL 1.0. I also tried with womtool 53.1 and 83 which showed the same issue_; The following files are used to show case the issue.; The main workflow:; ```; version 1.0. import ""SubWorkflow.wdl"" as SubWorkflow. workflow MainWorkflow {; input {; String value_2_give = ""default value""; }; call MainTask {; input:; given_value = value_2_give; }. call SubWorkflow.SubWorkflow {; input:; value_2_give = value_2_give; }; }. task MainTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. The subworkflow:; ```; version 1.0. workflow SubWorkflow {; input {; String value_2_give = ""default value""; String? overwrite_value_2_give; }; call SubTask {; input:; given_value = select_first([overwrite_value_2_give, value_2_give]); }; }. task SubTask {; input {; String given_value; String? overwrite_given_value; }; command <<<; echo ~{select_first([overwrite_given_value, given_value])};; >>>; }; ```. To be sure I ran the validation mode of womtools:; ```; $ java -jar womtool-84.jar validate MainWorkflow.wdl; Success!; $ java -jar womtool-84.jar validate SubWorkflow.wdl; Success!; ```. After creating these files, I ran womtool with the ""inputs"" option getting the following output:; ```; $ java -jar womtool-84.jar inputs MainWorkflow.wdl; {; ""MainWorkflow.SubWorkflow.overwrite_value_2_give"": ""String? (optional)"",; ""MainWorkflow.MainTask.overwrite_given_value"": ""String? (optional)"",; ""MainWorkflow.value_2_give"": ""String (optional, default = \""default value\"")""; }; ```; This output json shows which variables you can (or must) provide in order to be able to run in this case the main workflow. here we see that",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:803,echo,echo,803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,1,['echo'],['echo']
Availability,"Dear cromwell team,. We are trying to setup a test environment with GATK4 and cromwell for our local users. I tested the helloHaplotypeCaller.wdl in the data bundle but it is giving the following error:. ```; java -jar $cromwell run storage/WDLdata/WDLscripts/helloHaplotypeCaller.wdl -i storage/WDLdata/WDLscripts/helloHaplotypeCaller_inputs.json; [2017-10-04 06:06:48,43] [info] Running with database db.url = jdbc:hsqldb:mem:2812db5e-e9cc-48b0-bc67-62fd2c7887f9;shutdown=false;hsqldb.tx=mvcc; [2017-10-04 06:06:53,48] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-10-04 06:06:53,49] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-10-04 06:06:53,83] [info] Slf4jLogger started; [2017-10-04 06:06:54,01] [info] Metadata summary refreshing every 2 seconds.; [2017-10-04 06:06:54,02] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-10-04 06:06:54,32] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-10-04 06:06:55,29] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-10-04 06:06:55,36] [info] Workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6 submitted.; [2017-10-04 06:06:55,36] [info] SingleWorkflowRunnerActor: Workflow submitted bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,36] [info] 1 new workflows fetched; [2017-10-04 06:06:55,37] [info] WorkflowManagerActor Starting workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,37] [info] WorkflowManagerActor Successfully started WorkflowActor-bf90a37b-6ffa-4122-a12c-24aced32f3b6; [2017-10-04 06:06:55,37] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-10-04 06:06:55,63] [info] MaterializeWorkflowDescriptorActor [bf90a37b]: Call-to-Backend assignments: helloHaplotypeCaller.haplotypeCaller -> Local; [2017-10-04 06:06:56,98] [info] WorkflowExecutionActor-bf90a37b-6ffa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2673:196,error,error,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2673,1,['error'],['error']
Availability,"Dear developers,. During testing I ran into the problem that the `HashPathStrategy` does not include the last modified date of the file. It assumes: ""if the path is there, it is the same file"". This is not necessarily the case. Files can be modified or replaced.Therefore the current `HashPathStrategy` is a big liability when trying to get reproducible results. By adding a ""last modified date"" to the `HashPathStrategy` this will ensure that nothing has happened to the file from the user or system side. This of course is not as safe as the `HashFileStrategy` since it does not protect against filesystem or hardware errors, but it provides a lot more safety compared to the current `HashPathStrategy`. ; This is also how Snakemake checks if files are the same and it works quite well. Alternatively there could be an option in the Configfile that allows you to set this behaviour. Please let me know what you think of this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405:620,error,errors,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405,1,['error'],['errors']
Availability,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999:335,echo,echo,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999,8,['echo'],['echo']
Availability,Defining struct inline with boolean or float value causes no coercion defined from wom value error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:93,error,error,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['error'],['error']
Availability,Delocalizing a file that doesn't exist should create an error on the BCS backend but is currently succeeding. This may or may not require changes to the embedded python worker. A/C:; - Restore the test `bad_file_string` in `testCentaurBcs.sh`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3523:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3523,1,['error'],['error']
Availability,"Depending on what your monitoring script does and how long your command takes to run it's possible that the task finishes before the monitoring script had time to write / flush anything into the monitoring log.; I ran this and got an empty log . ```; task t {; command {; echo ""hey""; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```. but this gave me a non-empty one . ```; task t {; command {; sleep 5; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034:272,echo,echo,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034,1,['echo'],['echo']
Availability,Deprecation errors for DB configs. Closes #2186,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2200:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2200,1,['error'],['errors']
Availability,"Desktop Docker is not the most reliable platform for real work in Cromwell, but it is interesting that a specific version broke it. Do you have time to do a `git bisect` between 55 and current?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044:31,reliab,reliable,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044,1,['reliab'],['reliable']
Availability,Detect error 500 in JES backend and retry job,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1450:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450,1,['error'],['error']
Availability,Determine HTTP error codes to retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1913:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1913,1,['error'],['error']
Availability,"Detritus still errors on hard linking, I think we may not have created the execution directory first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382531175:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382531175,1,['error'],['errors']
Availability,"Did someone say breaking changes? *starts sweating*. Ping is much appreciated. Could we plan to go over the changes together so that you can help us identify what needs to change in our user docs / course materials? . Re: location of docs, @katevoss and I have been discussing this a fair amount, and the latest proto-consensus we came to was that the forum is ultimately not the right place for Cromwell docs proper (as opposed to the end-user/n00b materials that Comms produces) in part because it's too detached from the codebase itself. Based on the audience and technical constraints, we think something like ReadTheDocs will be a more suitable platform. Kate is obviously going to be otherwise occupied for several more weeks, but I might be able to take a stab at prototyping what a ""Read the (Cromwell) Docs"" solution would look and feel like in a week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886:53,Ping,Ping,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886,1,['Ping'],['Ping']
Availability,"Did you test this in real life? Due to the mounting system in containers soft-links may not work at all. This is why they are rightfully banned in docker.; I believe singularity works almost the same. There is no guarantee that the soft-linked target will exist in the container. The filesystem might not be present there, or have a different name.; Just use hard-links, these are much more reliable when working with containers and just as fast.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097:391,reliab,reliable,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097,1,['reliab'],['reliable']
Availability,Did you try the test WDL and json provided by robthompsonweb? Any of my workflows have this same error and they all have essentially that same basic structure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516545076:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516545076,1,['error'],['error']
Availability,"Directory structure:. ```; WDLTesting; -src; --wdl; ---Workflow.wdl; ---WriteTask.wdl; ---Child; ----ChildWF.wdl; ----ChildTask.wdl; ```. Under draft 2 (which is to say, no version specified), Workflow.wdl has the following import statements:; ```; import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write; import ""WDLTesting/src/wdl/Child/ChildWF.wdl"" as Child; ```; ChildWF.wdl has the following import statement:; `import ""WDLTesting/src/wdl/Child/ChildTask.wdl"" as Child`. This works correctly. it works letting things be access from the file system, and it works if we zip up WDLTesting and pass it to Cromwell as --imports. Under version development, this doesn't work. Workflow.wdl's imports still work, but ChildWF.wdl's do not. I must trim it down to ; `import ""Child/ChildTask.wdl"" as Child`; Before it will work. This completely breaks our pipeline, especially since workflows calling files up and down the tree, and even in other projects that start at the same level as ours. Why was this horrible breaking change made, and how do we specify an import that works with respect to the --imports .zip?. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441:746,down,down,746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441,2,['down'],['down']
Availability,Disabled languages produce bad error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3921:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921,1,['error'],['error']
Availability,"Disaggregate ""Unexpected terminal status"" errors in Sentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4003:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4003,1,['error'],['errors']
Availability,Discussed in person but probably not (assuming that the two modes of failure are what I was seeing a couple of weeks ago),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640:69,failure,failure,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640,1,['failure'],['failure']
Availability,Discussed in person w/ @Horneth - we'll keep the implementation specific split for now. It seems likely that in the future it'll boil down to a cloud/not-cloud split but at the moment it's SFS/PAPI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239,1,['down'],['down']
Availability,Display Workflow failure message from /metadata #603,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4875:17,failure,failure,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4875,1,['failure'],['failure']
Availability,Do not log programmer error when Carboniting failed with `TooLargeToArchive` result [BA-6471],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5535:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5535,2,['error'],['error']
Availability,"Do the following:. - [x] Rename the `Preempted` `ExecutionStatus` to be `RetryableFailure`.; - [x] Change the `WorkflowExecutionActor` to blindly honor `JobFailedRetryableResponse`s. Currently it is deciding whether or not to retry when it receives these. Now it will assume the determination was already made.; - [x] Remove the `system.max-retries` config option; - [x] Change the JABJEA to track both preemptions and non-preemption retries via the KV store; - [x] For any failure from JES determine if the job is to be retried, either using the preemption count supplied by the user or a max non-preemption retry count of 2. If the job is retryable ensure a `JobFailedRetryableResponse` finds its way back to the WEA; - [x] Modify `BackendStatus` such that we can see that the job was either a) preempted or b) retried due to ReasonX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1925:474,failure,failure,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1925,1,['failure'],['failure']
Availability,Do they both contain that error string?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429906924:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429906924,1,['error'],['error']
Availability,Do you happen to have the resulting error handy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519#issuecomment-464203099:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519#issuecomment-464203099,1,['error'],['error']
Availability,Do you have an example before & after?. It seems like the output would contain `[First $limitBytes bytes]` from `annotatedContentAsStringWithLimit` which is a pretty strange thing to have in the middle of an error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536,1,['error'],['error']
Availability,Docs HPCSlurmWithLocalScratch.md redundant?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:33,redundant,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['redundant'],['redundant']
Availability,Docs and Error messaging for Pair access,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:9,Error,Error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,1,['Error'],['Error']
Availability,"Docs here: http://doc.akka.io/docs/akka-http/current/scala.html. NB for this ticket these endpoints wouldn't need to do anything useful, just return some hard-coded value (500 server error perhaps?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2147:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2147,1,['error'],['error']
Availability,"Docs:; - Fixed a bunch of broken links; - I think the Scala Steward updates gave us a new version of doc generation that is more strict; - There are more broken links than just these, did not attempt to be comprehensive; - IntelliJ's markdown validation is helpful:. ![Screen Shot 2020-08-19 at 12 15 43 PM](https://user-images.githubusercontent.com/1087943/90661978-d2613d00-e215-11ea-8c1d-5ae4c842213e.png). Error messages:; - Attempted to make them more concise and consistent; - Sometimes we didn't make it obvious that a limit is configurable; - Did not attempt to be comprehensive",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:410,Error,Error,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['Error'],['Error']
Availability,Document a general-purpose recovery process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4991:27,recover,recovery,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4991,1,['recover'],['recovery']
Availability,Document docker stop command to gracefully shut down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562:48,down,down,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562,1,['down'],['down']
Availability,Document that concurrent-job-limit is available on all backends,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751,1,['avail'],['available']
Availability,"Documentation can be downloaded from here; https://cromwell-share-ad485.s3.us-east-2.amazonaws.com/InstallingGenomicsWorkflowCoreWithCromwel52.pdf. On Sun, Sep 13, 2020 at 4:48 PM Yaomin Xu <notifications@github.com> wrote:. > @markjschreiber <https://github.com/markjschreiber> running into the same; > error for both v52 and v53.1. I am using the same CloudFormation; > @mderan-da <https://github.com/mderan-da> mentioned . Appreciate your; > newer documentation on this.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKCM56WST3J6NO5CS3SFUVYLANCNFSM4G23FFUQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-692083983:21,down,downloaded,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-692083983,2,"['down', 'error']","['downloaded', 'error']"
Availability,Documentation error? womtool graph,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5126:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126,1,['error'],['error']
Availability,Does it happen all the time or episodically ? Dockerhub had some downtime in the past few days,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-434835954:65,downtime,downtime,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-434835954,1,['downtime'],['downtime']
Availability,"Does this mean that, because `\\.` is *not* on the list of accepted double-quote uses, it should be an error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412159357:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412159357,1,['error'],['error']
Availability,"Don't know if this is related but I'm seeing this type of failure submitting [this workflow](https://github.com/bcbio/test_bcbio_cwl/blob/master/prealign/prealign-workflow/main-prealign.cwl) by url:. ```; MaterializeWorkflowDescriptorActor [UUID(dfefc8c0)]: Parsing workflow as CWL v1.0; 2018-12-10 13:27:22,372 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/main-prealign.cwl; 2018-12-10 13:31:56,222 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/organize_noalign.cwl; 2018-12-10 13:32:14,196 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples_to_rec.cwl; 2018-12-10 13:32:32,071 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples.cwl; 2018-12-10 13:32:49,793 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl; 2018-12-10 13:33:07,284 cromwell-system-akka.dispatchers.engine-dispatcher-34 ERROR - WorkflowManagerActor Workflow dfefc8c0-c3a1-449c-a747-13147bf8b980 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:58,failure,failure,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939,2,['failure'],['failure']
Availability,Don't log akka debug messages to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3237:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3237,1,['error'],['error']
Availability,Don't log request failures in conformance tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3645:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3645,1,['failure'],['failures']
Availability,Don't log stack traces for known failures Closes #1817,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1824:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1824,1,['failure'],['failures']
Availability,"Don't merge yet, I just discovered another small issue which should take care of the last tyburn failure",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191442121:97,failure,failure,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191442121,1,['failure'],['failure']
Availability,Downgrade liquibase version for unknown mariadb AIOOBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4619:0,Down,Downgrade,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4619,1,['Down'],['Downgrade']
Availability,"Due to ambiguity in the explanation of how to pose questions rather than issues I will post this question (with the same text) here too and see whether it will get removed or answered. This question is the same as [the one I posted a week ago on biostars](https://www.biostars.org/p/448662/). I would like to apologize in advance for any ignorance regarding the documentation that I might have missed. It is not my intention to ask for what I would have known if I had read the documentation better, I am merely trying to grasp the concepts that are abstracted in the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:926,avail,available,926,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['avail'],['available']
Availability,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:910,Error,Error,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"During CromIAM Perf testing, strange behavior occurred where when querying metadata for a workflow right after aborting it yielded a non empty value in the `failures` field, which later disappeared.; Below is an example metadata with the failure:. ```; {; ""calls"": {; ""wf_hello.hello"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:157,failure,failures,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,2,['failure'],"['failure', 'failures']"
Availability,"During code review for #1836 @cjllanwarne noted that `processSource` in what is currently named `WorkflowStoreActor` and most likely `WorfklowStoreSubmitActor` by the time this is acted upon looked suspicious as we had (we think) intended json validation to not happen until later and a workflow ID would always be handed back to the user. Further, the failed Future doesn't appear to be getting handed back to the API at all (I think), which would lead to a timeout response. Further since the sources are being processed monadically it is possible for a user to have multiple borked files but only the first will be reported (if we were reporting). Check into what's up here - either don't perform this check on submission or ensure that appropriate error messages are handed back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882:752,error,error,752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882,1,['error'],['error']
Availability,During our Tech Talk today Thibault pointed out it could be quite bad if one Cromwell instance unexpectedly loses its connection to the database and other Cromwells pick up its workflows and start running them. This sparked a discussion that heartbeat TTL expiration is something that should be highly visible to make sure it isn't happening when not expected. A/C; Log when a workflow is claimed under `INFO` visibility,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4451:242,heartbeat,heartbeat,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4451,1,['heartbeat'],['heartbeat']
Availability,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1344:396,down,down,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344,1,['down'],['down']
Availability,"During the testing hackathon, we discovered a number of problems caused by the eventual consistency of the metadata service. One specific case of this is the granularity of the events. . On one side, we have a publisher who has a whole collection of events that they would like to push. They push them one event at a time to the MD service. Because even things like array elements are pushed one update at a time because of MD format, we run into the situation where a consumer can see half of an array. Taken to the extreme, we could push every char of a string as a separate event. The fundamental problem with these partial updates is that a downstream consumer can not tell if an update is complete. Do they wait? How long? Can they check if the data is done?. While this touches on a larger problem in distributed computing, I think we are shooting ourselves in the foot by making every piece of a single update an async, isolated event. Taken to the extreme, we could push every char of a string as a separate event. The proposal is to extend the PutMetadataAction to take in a Seq/Varargs of MetadataEvents with the contract that these will be made available atomically (e.g. in a single Slick transaction for our implementation). Then in places where we basically unrolling a bundle of events to publish, we should use this API (e.g. WorkflowExecutionActor) to do that atomically. . In theory, this should also help with the scalability as the MD service can persist things with batchinserts in single transaction. For larger workflows, currently this would be hundreds or thousands of transactions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/930:645,down,downstream,645,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/930,2,"['avail', 'down']","['available', 'downstream']"
Availability,"E, file = 93DAD89F707FA490E2A46FFAC924DFFF.; [2023-03-29 12:35:42,07] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,08] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1-20000000024 [b303ae23expanse_figures.CBL_hom_not_SNP_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 12:35:42,08] [info] BT-322 b303ae23:expanse_figures.CBL_hom_not_SNP_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = EA2DED52B795D0B2EA5091B00E8F7A88.; [2023-03-29 12:35:42,08] [info] b303ae23-e1e5-4cde-832b-70114e9efdad-EngineJobExecutionActor-expanse_figures.CBL_hom_not_SNP_assoc:NA:1 [b303ae23]: Call cache hit process had 0 total hit failures before completing successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2006,failure,failures,2006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['failure'],['failures']
Availability,"E.g. (I think). ```; task foo {; File bar; command { ; echo ""contents of ${filename(bar)}:""; cat ${bar}; }; ```. Would generate the script:. ```; echo ""contents of myFile.txt:""; cat /home/chrisl/superexcitingfiles/myFile.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746:55,echo,echo,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746,2,['echo'],['echo']
Availability,"E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorld.txt"" | 2018-11-21 15:09:10.767000 | string |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:2277,echo,echo,2277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['echo'],['echo']
Availability,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:154,error,error,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,2,['error'],['error']
Availability,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461:427,ERROR,ERROR,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,ERROR - Scopes not configured for service account,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:0,ERROR,ERROR,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['ERROR'],['ERROR']
Availability,ERROR: Finished parsing without consuming all tokens.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:0,ERROR,ERROR,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,1,['ERROR'],['ERROR']
Availability,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070:226,avail,available,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070,1,['avail'],['available']
Availability,Emit 'rows deleted' metric on success as well as failure [BW-707],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6358:49,failure,failure,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6358,1,['failure'],['failure']
Availability,Enable/disable checking of task exit code to indicate task failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/185:59,failure,failure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/185,1,['failure'],['failure']
Availability,Enhance Cromwell reporting of Martha errors [WA-340],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5845:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5845,1,['error'],['errors']
Availability,Enhance map errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661,1,['error'],['errors']
Availability,"Ensure GCS file systems use custom configuration.; When an exception/timeout occurs during asyncHashing, report it as a failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2512:120,failure,failure,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2512,1,['failure'],['failure']
Availability,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4026:559,failure,failures,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026,1,['failure'],['failures']
Availability,"Er, what I meant to write was, can you please create this branch in the main Cromwell repo and PR that instead? Having branches in private repos has been causing review and maintenance pain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2777#issuecomment-339033675:173,mainten,maintenance,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2777#issuecomment-339033675,1,['mainten'],['maintenance']
Availability,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:787,alive,alive-or-dead,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,4,['alive'],"['alive', 'alive-or-dead']"
Availability,Erroneous error message creating womgraph,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3284:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284,1,['error'],['error']
Availability,Error 500 in local backend when trying to localize from GCS,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['Error'],['Error']
Availability,Error about trying to head an empty list,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,1,['Error'],['Error']
Availability,Error code 10 take 2 [BA-5932],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5129:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5129,1,['Error'],['Error']
Availability,Error compiling under docker (Class name too long),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1428:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428,1,['Error'],['Error']
Availability,Error handling when Service Registry fails to initialize (e.g. bad config),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/896:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/896,1,['Error'],['Error']
Availability,Error in Cromwell 24 on method that works in Cromwell 21,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,1,['Error'],['Error']
Availability,Error message about reclaiming execution tokens might be important...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['Error'],['Error']
Availability,Error message despite Cromwell completing all its tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618,1,['Error'],['Error']
Availability,Error message for incorrect output string,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2875:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2875,1,['Error'],['Error']
Availability,Error message for inputs pointing to wrong project,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,1,['Error'],['Error']
Availability,Error message from this failed test on PAPI ðŸ¤” . ```Could not copy gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/home/travis/build/broadinstitute/cromwell/common-workflow-language/v1.0/v1.0/gs://centaur-cwl-conformance/cwl-inputs/Hello.java to gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/Hello.java```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137,1,['Error'],['Error']
Availability,Error message should specify which file has the error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933,2,"['Error', 'error']","['Error', 'error']"
Availability,"Error message unclear for invalid options file, Cromwell 24",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2041:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2041,1,['Error'],['Error']
Availability,"Error message: A timeout occurred waiting for a future to complete. Queried 100 times, sleeping 100 milliseconds between each query. tc: ServicesStore should not deadlock. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/566/. Update 10/22; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/708/. Update 10/28:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/831/. Update 11/03:; https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/1003/. Update 11/06:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1076/. Update 11/09:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1166/. Further:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1337; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1422; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1445; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1489; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1525; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1590",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328,1,['Error'],['Error']
Availability,"Error message: The code passed to eventually never returned normally. Attempted 28 times over 20.881773345 seconds. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp). tc: HealthMonitor should fail if any subsystems fail but correctly bin them. Occurred at these times:; 2018-10-10 22:25:49 UTC; 2018-10-12 06:25:28 UTC; 2018-10-13 06:24:31 UTC; 2018-10-13 14:25:16 UTC; 2018-10-12 04:23:43 UTC; 2018-10-15 10:25:31 UTC; 2018-10-13 21:27:58 UTC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259,2,"['Error', 'failure']","['Error', 'failure']"
Availability,Error pulling docker image (tag regex fails),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2589:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2589,1,['Error'],['Error']
Availability,Error reading existing file containing float,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/928:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/928,1,['Error'],['Error']
Availability,"Error running ""docker pull"" when local look-up is used with SHA digest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['Error'],['Error']
Availability,Error running the GATK WDL best practices pipeline,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2064:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2064,1,['Error'],['Error']
Availability,Error when POST and GET are done too close together,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671,1,['Error'],['Error']
Availability,Error with missing files in input JSON,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703,1,['Error'],['Error']
Availability,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364,2,['Error'],['Error']
Availability,Error: Could not load UVM kernel module. Is nvidia-modprobe installed?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,1,['Error'],['Error']
Availability,Escalate service instantiation failures Closes #1266,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1772:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1772,1,['failure'],['failures']
Availability,Esprit d'escalier: Does FC know this is coming? Is it going to slow down their adoption of Cromwell 30?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347207357:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347207357,1,['down'],['down']
Availability,"Even though it is accurate to blame Google, from the userâ€™s point of view itâ€™s Cromwell thatâ€™s behaving unexpectedly. So I agree with Chrisâ€™s sentiment. I also think itâ€™s worth it to mention â€œerror code 10â€, itâ€™s a phrase now widely known and feared in DSP. (Perhaps the right qualification is, â€œthis condition manifests itself as error code 10, but not all code 10s indicate this errorâ€.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403:192,error,error,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403,3,['error'],['error']
Availability,"Example 1 (valid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [""2""]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; And it will run successfully:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; ""main.x"": [""1"", ""2""]; ...; ```. Example 2 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[""1""], [[""2""]]]); }; }; ```; It will validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Success!; ```; But it will error out:; ```; $ java -jar cromwell-55.jar run main.wdl; ...; Failed to evaluate 'main.x' (reason 1 of 1): Evaluating flatten([[""1""], [[""2""]]]) failed: No coercion defined from wom value(s) '[""2""]' of type 'Array[String]' to 'String'.; ...; ```. Example 3 (invalid WDL code):; ```; version 1.0. workflow main {; output {; Array[String] x = flatten([[[""1""]], [[""2""]]]); }; }; ```; It will not validate:; ```; $ java -jar womtool-55.jar validate main.wdl ; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'Array[String] x = flatten([[[""1""]], [[""2""]]])' (reason 1 of 1): Cannot coerce expression of type 'Array[Array[String]+]+' to 'Array[String]'; ```. It is not a big deal, as the offending construct in Example 2 is not widely used, but it seems like a missed opportunity for flagging as an issue during the validation process. And why is it that womtool can flag Example 3 but not Example 2?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6185:559,error,error,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6185,1,['error'],['error']
Availability,"Example WDL taken directly from the spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#workflow-level-resolution; ```; version 1.0. workflow wf {; input {; String s = ""wf_s""; String t = ""t""; }; call my_task {; String s = ""my_task_s""; input: in0 = s+""-suffix"", in1 = t+""-suffix""; }; }; ```; results in; ```; ERROR: Unexpected symbol (line 9, col 5) when parsing 'call_body'. Expected input, got ""String"". String s = ""my_task_s""; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4048:326,ERROR,ERROR,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4048,1,['ERROR'],['ERROR']
Availability,Example error output here: https://travis-ci.com/github/broadinstitute/cromwell/jobs/516477139,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814,1,['error'],['error']
Availability,Excellent! The build failures are due to the MySQL quirks addressed by #160; once that's merged to scatter-gather and this is rebased I expect the build will go green.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384:21,failure,failures,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384,1,['failure'],['failures']
Availability,"ExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.RefreshTokenMode.validateCredential(GoogleAuthMode.scala:127); at cromwell.filesystems.gcs.auth.RefreshTokenMode.credential(GoogleAuthMode.scala:147); at cromwell.filesystems.gcs.GcsPathBuilder.<init>(GcsPathBuilder.scala:57); at cromwell.filesystems.gcs.GcsPathBuilderFactory.withOptions(GcsPathBuilderFactory.scala:37); at cromwell.backend.impl.jes.JesWorkflowPaths.<init>(JesWorkflowPaths.scala:25); at cromwell.backend.impl.jes.JesWorkflowPaths.copy(JesWorkflowPaths.scala:19); at cromwell.backend.impl.jes.JesWorkflowPaths.withDescriptor(JesWorkflowPaths.scala:54); at cromwell.backend.io.WorkflowPaths$class.toJobPaths(WorkflowPaths.scala:51); at cromwell.backend.impl.jes.JesWorkflowPaths.toJobPaths(JesWorkflowPaths.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:1851,Error,Error,1851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,2,"['Error', 'error']","['Error', 'error']"
Availability,Exit with error code 1 on malformed CLI invocation [BA-3094],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5239:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5239,1,['error'],['error']
Availability,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"Explicitly add `503` as a retryable error code on `StorageException`s because the `isRetryable` method seems to not be completely reliable.; Also retry SSL Exception (both top level and as a cause for `StorageException`s, I've seen both happen)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2208:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2208,2,"['error', 'reliab']","['error', 'reliable']"
Availability,Extra failure info for Job Manager in attempt metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4894:6,failure,failure,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894,1,['failure'],['failure']
Availability,FC user have seen their job fail with this error message: https://gatkforums.broadinstitute.org/firecloud/discussion/comment/41300. Could be transient in which case retrying it could be the way to go,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535,1,['error'],['error']
Availability,"FWIW 2: it's also possible to ""fix"" the tests by pushing the flush interval back to ~2s for the tests - thus reducing the chance that the probe messages are sent at the same time as a regular flush message. The downside to that was that the tests were taking 20 seconds each, which didn't feel great (and even though unlikely, there was still a small chance of an accident happening)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038:211,down,downside,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038,1,['down'],['downside']
Availability,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:460,error,error,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"FWIW, in the GATK world we just blanket refuse to support anything Windows. Every now and then we get a question from someone who edited a file manually on a Windows box -- but it happens *maybe* twice a year. I wouldn't advocate for putting a huge amount of effort into this beyond recognizing the error and providing an informative message if possible...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193,1,['error'],['error']
Availability,"FYI - @ruchim I assigned this to you for prioritization. We've seen 6 workflows fail since 5/10, including a couple in the past day. We've also seen it a couple times in the controller test. This is the same total failure rate as https://github.com/broadinstitute/cromwell/issues/738, but is slightly higher priority because it's happened more recently. Whenever we get an issue for ""Error during processing of request HttpRequest"" this will be well below that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-221895948:214,failure,failure,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-221895948,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"FYI @cjllanwarne . FireCloud production is seeing a lot of these errors on a regular basis:; ```; message:2019-02-26 18:12:58 [cromwell-system-akka.actor.default-dispatcher-1160] ; ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: <snip>-EngineJobExecutionActor-CHSworkflow.GenerateAndRunScript:; ```. You can see more of these errors in production Kibana with a simple query on ""Job execution token returned from incorrect"". Current cromwell [version](https://api.firecloud.org/#!/Version/executionEngineVersion):; ```; {; ""cromwell"": ""36-7de344f""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4681:65,error,errors,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"FYI for reviewers. I've added the following additional changes to this commit:. * Added retry handling for a few transient errors I saw during test runs; * Moved stdout/stderr/rc file writes to s3 into the proxy to resolve an intermittent timing issue seen in testing; * Removed the MIME parsing, as this is no longer needed due to the above change. This resolves #3748; * Addressed a TODO in the code regarding a large number of input parameters; * Removed broken code in the proxy (since the shell would continue, this ultimately is a cosmetic change); * Moved the actual command text from the job definition (RegisterJobDefinition) to the job submission (SubmitJob) call. This balances out the payload to allow more work in the job before hitting AWS Batch payload limits: https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html. Additionally it should allow easier consolidation of job definitions in the future. #3750 ; * Fixed the unit tests, which I broke during the initial implementation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4020#issuecomment-415567505:123,error,errors,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4020#issuecomment-415567505,1,['error'],['errors']
Availability,"FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied. Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469:325,failure,failures,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469,1,['failure'],['failures']
Availability,FYI ready for ðŸ‘€ @rsasch and @salonishah11. Regarding the failing dbms test: it's not failing due to this PR's changes and is not a blocker for review. I suspect that failure has something to do with lucky number [PostgreSQL 13](https://news.ycombinator.com/item?id=24578166) coming out yesterday. When @cjllanwarne finishes fighting Prod fires he (or perhaps someone else?) might have a small PR that can fix the issue in develop. When that fix merges I'll just restart the test in this PR and it should pick up the develop changes automagically and pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699:166,failure,failure,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699,1,['failure'],['failure']
Availability,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:351,down,downloads,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['down'],['downloads']
Availability,"FYI, tracked down the bug. It's on the Google side. My guess is that we; won't need the GenomicsScopes at all, and I can just reimplement the; application default credentials with the java object that you guys already; have. Alternately, we can replace that with GenomicsScopes once they work; better. Will mail a PR shortly. On Mon, Jan 4, 2016 at 11:02 AM, kshakir notifications@github.com wrote:. > Abandoning this PR.; > ; > @kcibul https://github.com/kcibul's feature requests should be captured; > in another ticket.; > ; > @tovanadler https://github.com/tovanadler is going to look at; > reimplementing her basic changes in #329; > https://github.com/broadinstitute/cromwell/pull/329 on the latest; > develop, and testing manually to ensure the scopes are all correct.; > ; > â€”; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168716322; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168724071:13,down,down,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168724071,1,['down'],['down']
Availability,"F_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3054,echo,echo,3054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['echo'],['echo']
Availability,Fail workflow on localization failure Closes #922,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1800:30,failure,failure,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1800,1,['failure'],['failure']
Availability,"Failing workflow:. ```wdl; version 1.0. workflow foo {; call bar; output {; Array[File] baz = bar.baz; }; }. task bar {; input {; Array[Array[String]]? baz ; }; command <<<; x=~{ if defined(baz) then write_tsv(baz) else '' }; if [[ -z ""$x"" ]]; then; echo ""no file""; else; cp $x output.tsv; fi; >>>; output {; Array[File] baz = glob(""*.tsv""); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524242:250,echo,echo,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524242,1,['echo'],['echo']
Availability,"Fails the workflow. On Mon, Sep 18, 2017 at 5:21 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/leetl1220> when you get the ""file not; > found"" errors, does it fail the workflow? Or does it still continue?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkxMH5YiFxEFZPeULlAAXomDFkWi2ks5sjt7PgaJpZM4PZZvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330401409:179,error,errors,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330401409,1,['error'],['errors']
Availability,Failure Messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/475:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/475,1,['Failure'],['Failure']
Availability,Failure and RetryableFailure are 2 different messages so in this case I think this is a no-op ? We just never return RetryableFailure messages in Local ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102,1,['Failure'],['Failure']
Availability,Failure in crc32cHash,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738,1,['Failure'],['Failure']
Availability,Failure message for failed workflow should specify the shard number for scattered jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479,1,['Failure'],['Failure']
Availability,Failure messages include unwanted Some,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1893:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1893,1,['Failure'],['Failure']
Availability,Failure metadata migration. Closes #2037. Closes #2039,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2059:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2059,1,['Failure'],['Failure']
Availability,Failure metadata redux,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2055:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2055,1,['Failure'],['Failure']
Availability,"Failure metadata will look something like:. ```; ""failures"": [{; ""failure"": ""Task 874ad75a-e19e-4b53-8225-746df1436c51:ValidateAggregatedSamFile was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3).\nError code 10. Message: Some(14: VM ggp-15150083938845849899 stopped unexpectedly.)"",; ""timestamp"": ""2016-12-18T05:54:07.296Z""; }],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1893:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1893,3,"['Failure', 'failure']","['Failure', 'failure', 'failures']"
Availability,Failure mode 3 from #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1410:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410,1,['Failure'],['Failure']
Availability,Failure to find imported WDLs using relative paths,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515,1,['Failure'],['Failure']
Availability,Failure to localize should fail job immediately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809,1,['Failure'],['Failure']
Availability,Failure to recognize variable declared in task call,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4048:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4048,1,['Failure'],['Failure']
Availability,Failure to transfer String output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2115:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2115,1,['Failure'],['Failure']
Availability,Failure using compound types in runtime attributes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685,1,['Failure'],['Failure']
Availability,"Failure writing to call cache, right truncation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['Failure'],['Failure']
Availability,"Failures are unrelated, merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/188#issuecomment-141186133:0,Failure,Failures,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/188#issuecomment-141186133,1,['Failure'],['Failures']
Availability,Failures during localization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071:0,Failure,Failures,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071,1,['Failure'],['Failures']
Availability,False alarm error in runtime block w/ coercion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643,1,['error'],['error']
Availability,"Fantastic instincts. This fails in the same develop and passes in the same 29:. ```wdl; task two_input_task {; String inA; String inB; command { echo ${inA} and ${inB} }; runtime { docker: ""ubuntu"" }; }. workflow duplicate_scatter_input {; String my_input = ""input""; scatter (i in range(0)) {; call two_input_task { input: inA = my_input, inB = my_input }; }; }; ```. Produces in this error on develop:; ```java; [2017-12-03 14:48:08,10] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-03 14:48:09,17] [error] WorkflowManagerActor Workflow 986bbc71-2b80-4cf1-aade-e93dd77062b0 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2973#issuecomment-348778521:145,echo,echo,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2973#issuecomment-348778521,3,"['echo', 'error']","['echo', 'error']"
Availability,Feature request: Dirt simple error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,2,['error'],['error']
Availability,"Feel free to make this PR redundant ðŸ˜› If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,redundant,redundant,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681,1,['redundant'],['redundant']
Availability,"Figure out how it is possible that travis can not compile the build but returns 0. Incident report:; [travis build](https://travis-ci.org/broadinstitute/cromwell/jobs/292596774); [raw log, excerpted below](https://s3.amazonaws.com/archive.travis-ci.org/jobs/292596774/log.txt?X-Amz-Expires=30&X-Amz-Date=20171025T234339Z&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20171025/us-east-1/s3/aws4_request&X-Amz-SignedHeaders=host&X-Amz-Signature=71b729c64e9fbce0fb7e520c77d2d3da8876b62e8e4f5f87b5084f594b439ee0). When compiling 2.11 `cwl` package, compilation reported an error, yet still returned 0:; ```; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:594,error,error,594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['error'],['error']
Availability,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:98,avail,available,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['avail'],['available']
Availability,FileSystems available to the Engine for expression evaluation should be configurable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/821:12,avail,available,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/821,1,['avail'],['available']
Availability,"Filling in the missing meta and parameter_meta types. This makes Cromwell/WOM support the spec with JSON like structures available for the meta sections. . ```; $parameter_meta = 'parameter_meta' $ws* '{' ($ws* $parameter_meta_kv $ws*)* '}'; $parameter_meta_kv = $identifier $ws* ':' $ws* $meta_value; $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array; $meta_object = '{}' | '{' $parameter_meta_kv (, $parameter_meta_kv)* '}'; $meta_array = '[]' | '[' $meta_value (, $meta_value)* ']'; ```. There is a part I didn't know how to properly code, which is `WorkflowDescription` support ( services/src/main/scala/cromwell/services/womtool/models/MetaValueElementJsonSupport.scala). I could use help there. . Thank you, ; Ohad.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053:121,avail,available,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053,1,['avail'],['available']
Availability,"Finally I figure out one solution, but it is a little bit ugly and still look for an elegant way:. - Global variables WDL as below:. ```; workflow global {; }. task init {; command { }; output {; String version = ""v1.0""; String reference = ""hg19"". }; }; ```. - Pipeline WDL as below:; ```; import ""global.wdl"" as global. workflow pipeline {; # Global variables; call global.init; String version = init.version; String reference = init.reference; # Pipeline variables; String sample_id = ""Sample_001""; call snp { input: version = version, reference = reference, sample_id = sample_id }; }. task snp {; String version; String reference; String sample_id; command { echo ""SNP_${version} for ${sample_id} on ${reference}!"" }. output { String out = read_string(stdout()) }; }. ```; The final result is:; ```; [2018-11-21 18:23:14,32] [info] BackgroundConfigAsyncJobExecutionActor [a225847apipeline.snp:NA:1]: echo ""SNP_v1.0 for Sample_001 on hg19!""; ```. And you can see global variables are passed to the pipeline WDL while there are some workaround such as empty global workflow and helper task of init.; Is there any other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243:663,echo,echo,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243,2,['echo'],['echo']
Availability,"Find the original post [here](http://gatkforums.broadinstitute.org/wdl/discussion/7690/running-in-server-mode-jobs-that-have-localization-error-become-immortal#latest). ---. **User Report**. Running cromwell in server mode, with default configuration in each case, I can reproduce the following behaviour in 0.18, 0.19 and 0.19_hotfix (HEAD):. Submit a workflow that has non-existent file as input to a task, e.g.:. ```; task BillyBob {; File bbInput; command { echo ""done"" }; }; workflow badLocalization {; call BillyBob { input: bbInput=""/foo/bar/baz"" }; }; ```. The server log shows ""Failures during localization"" error (below) - as expected initially, I guess - but then _repeats_ the error every 30 seconds or so, forever, and hitting the API `<workflowId>/status` endpoint shows the job in a ""Running"" state, forever. I would expect this error to cause the task, and then the workflow, to die. example of a single block of the server log error: . ```; 2016-05-27 11:08:57,269 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:138,error,error-become-immortal,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,7,"['Failure', 'echo', 'error']","['Failures', 'echo', 'error', 'error-become-immortal']"
Availability,"First point: Looking at the code again, the only thing that can possibly be sent to metadata after the status of the workflow becomes terminal are workflow failures and end time, due to the ordering of the `onTransition` blocks in the WorkflowActor. I swapped them in the last commit, so from now on nothing should be sent to the metadata after the terminal status event is sent.; I'm happy to add an extra check for robustness but I don't believe it to be necessary. Second point: If the DB is busy writing data, the `WriteMetadataActor` will be in state `WritingToDb`, in which case it will always reply with `HasPendingWrites`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027:156,failure,failures,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027,2,"['failure', 'robust']","['failures', 'robustness']"
Availability,"First test (@dtenenba built the PR 4412 and I tested it):. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; First input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; Second input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. and... drumroll please...... IT WORKED!!!!!!!!!!! ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": true,; ""result"": ""Cache Hit: 98bc2232-f147-419f-9351-49a07daa1720:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"",; ```; And the workflow is ""generating"" the files WAY faster than it should be if it were doing it de novo, so we seem to be getting the correct outputs moved into the new workflow directory as well. . Caveats: ; I did test it with an actual batch and it failed with the job definition error. But as long as PR 4412 was not intended to fix THAT issue as well, I can say it appears on the first pass that call caching with AWS backend might very well be working with an outside test!!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623:1048,error,error,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623,1,['error'],['error']
Availability,"Fix CWL ""failed with null"" error message",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4190:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4190,1,['error'],['error']
Availability,"Fix broken doc links, update error messages [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5779:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5779,1,['error'],['error']
Availability,Fix coercion failure handling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/83:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/83,1,['failure'],['failure']
Availability,Fix copy/paste error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7005:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7005,1,['error'],['error']
Availability,Fix flakey test: RobustClientHelperSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:17,Robust,RobustClientHelperSpec,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,Fix not-so-intermittent failures with WorkflowManagerActorSpec.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224,1,['failure'],['failures']
Availability,Fix occasional errors in WomtoolServiceInCromwellActorSpec due to newâ€¦,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471,1,['error'],['errors']
Availability,"Fix order of args for error method, stringify stack trace for others.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/294:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/294,1,['error'],['error']
Availability,Fix recover on restart [WX-927],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:4,recover,recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,1,['recover'],['recover']
Availability,"Fix the ""non-infinite"" retry bug on transient errors",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/407:46,error,errors,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/407,1,['error'],['errors']
Availability,Fix transient errors on retry + add more tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/407:14,error,errors,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/407,1,['error'],['errors']
Availability,"Fix/update for [WX-1210](https://broadworkbench.atlassian.net/browse/WX-1210). Turns out that the `head_commit` attribute is not available on `pull_request` actions, which is why the JIRA ID check kept failing. I'm opting to use `github.event.pull_request.title` which is accessible on `pull_request`. [WX-1210]: https://broadworkbench.atlassian.net/browse/WX-1210?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7184:129,avail,available,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184,1,['avail'],['available']
Availability,"Fixed Symbol doubly qualifying the symbol names. (thx mcovarr/scottfrazer); Added `WdlType.fromRawString`, with test against respective `WdlValue.toRawString`.; `DummyDataAccess` replaced with using `DataAccess` instances, with cleanup of connections.; When creating in memory databases will create unique `DataAccess` instances, just like Dummy.; TestSlickDatabase now prints a warning, instead of an error, when unable to connect to MySql.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/84:402,error,error,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/84,1,['error'],['error']
Availability,Fixed `programmer error` messages related to race conditions in RootWorkflowFileHashCacheActor [BA-6503],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5580:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5580,1,['error'],['error']
Availability,Fixed a collector failure bug and a slow and unnecessary GCS glob,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/831:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/831,1,['failure'],['failure']
Availability,Fixed error messages for MetadataTooLargeException exceptions and suppressed stacktraces [BA-6456],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5536:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5536,1,['error'],['error']
Availability,Fixed error on attempt to get metadata for workflow with archived status TooLargeToArchive [BA-6475],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5541:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5541,1,['error'],['error']
Availability,"Fixed in #1252. ""walltime"" may now read as a backend specific runtime attribute just like any other. I've called the runtime attribute ""sge_walltime"" below. ```; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String? sge_walltime; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${""-l h rt="" + sge_walltime } \; ${script}; """""". kill = ""qdel ${job_id}"". check-alive = ""qstat -j ${job_id}"". job-id-regex = ""(\\d+)"". }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862:684,alive,alive,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862,1,['alive'],['alive']
Availability,"Fixed the [proof of concept code](https://github.com/broadinstitute/cromwell/compare/develop...rhpvorderman:relativeImports). Now the WOMTOOL is able to handle absolute paths correctly. I can run `java -jar /home/ruben/test/base/womtool-31-1df94fa-SNAP.jar validate /home/ruben/test/base/workflow.wdl ` in any directory on the filesystem and get the same result. However cromwell still uses $PWD to evaluate the base directory. I can see the WOMtool uses the following code to load the WDL file:; ```scala; private[this] def loadWdl(path: String)(f: WdlNamespace => Termination): Termination = {; WdlNamespace.loadUsingPath(Paths.get(path), None, None) match {; case Success(namespace) => f(namespace); case Failure(r: RuntimeException) => throw new RuntimeException(""Unexpected failure mode"", r); case Failure(t) => UnsuccessfulTermination(t.getMessage); }; }; ```; But for cromwell there does not seem to be such a straightforward loading of the wdlfile. Can somebody point me to this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047:708,Failure,Failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047,3,"['Failure', 'failure']","['Failure', 'failure']"
Availability,Fixed timeout error during mysql testing.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['error'],['error']
Availability,"Fixes #3039 . As far as I can tell, expressions that lookup subworkflow outputs sometimes cause an error message to be written which is subsequently ignored. Something in Cromwell 30 caused building that error message to fail because it can no longer find the sub-workflow line to point to in the error message. This hotfix PR lets us build a partial error message with no ""pointing to the error"" line. I think this is ok since we're ignoring the resulting message anyway for subworkflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,5,['error'],['error']
Availability,"Fixes #4084 ; For singularity users it is nice that `dockerRoot` can be set, as they do not have control over which paths are available in the image and `/cromwell-executions` cannot be automatically created. Other paths could be used, BioContainers for example have a `/data` folder meant specifically for these use cases. But this requires dockerRoot to be configurable. The fallback value is still `/cromwell-executions` so nothing changes if the value is not specified in the config.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088,1,['avail'],['available']
Availability,Fixes broken builds:; ```; [error] (wdlDraft3LanguageFactory/*:dockerPush) Failed to push; [error] (wdlDraft2LanguageFactory/*:dockerPush) Failed to push; [error] (cwlV1_0LanguageFactory/*:dockerPush) Failed to push; [error] (languageFactoryCore/*:dockerPush) Failed to push; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3352:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3352,4,['error'],['error']
Availability,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:23,avail,available,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550,1,['avail'],['available']
Availability,Fixes runtime warning introduced in https://github.com/broadinstitute/cromwell/pull/5565. Intentional stowaway: error message improvement for @barkasn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5570:112,error,error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5570,1,['error'],['error']
Availability,"Fixes the false build errors from contributors, such as during https://github.com/broadinstitute/cromwell/pull/3684",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3687#issuecomment-391864246:22,error,errors,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3687#issuecomment-391864246,1,['error'],['errors']
Availability,"Fixes the workaround syntax in #1126, but doesn't completely repair the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582:61,repair,repair,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582,1,['repair'],['repair']
Availability,Fixing '[error] File name too long' on sbt assembly in Docker,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/226:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/226,1,['error'],['error']
Availability,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112:105,alive,alive,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112,1,['alive'],['alive']
Availability,Fixing two bugs in syntax error detection,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109,1,['error'],['error']
Availability,FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.Error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:3674,Robust,RobustClientHelperSpec,3674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,Flatten Failure Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2038:8,Failure,Failure,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2038,1,['Failure'],['Failure']
Availability,"Follow up to [WX-1410](https://github.com/broadinstitute/cromwell/pull/7414) which introduced a bug converted metadata values to strings before database insertion. In addition to fixing the bug, this PR introduces more robust unit testing to confirm that metadata values that should not be modified remain unmodified. [WX-1410]: https://broadworkbench.atlassian.net/browse/WX-1410?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7427:219,robust,robust,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7427,1,['robust'],['robust']
Availability,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Following up on https://github.com/broadinstitute/cromwell/pull/5321, this is another case when GCS IoActor fails with a retryable error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344,1,['error'],['error']
Availability,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1629:340,Ping,Ping,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629,1,['Ping'],['Ping']
Availability,"For Workflow ID: 5541d851-10bb-455d-a6bc-051e85574b74. We're getting nothing back when we try to look at timing or metadata, the metadata curl call returns:. {; ""status"": ""error"",; ""message"": ""None.get""; }. No clue what's going on here, the cromwell logs don't indicate anything unusual for this workflow. Seemingly it ran normally, just some typical preemption messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/801:172,error,error,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/801,1,['error'],['error']
Availability,"For example, test 3 represents:; ```; E1; caused By A1; - E2; caused by E3; - E4; - E5; ```. Being converted into this:; ```; failures: [{; message: ""E1"",; causedBy: [{; message: ""A1"",; causedBy: [{; message: ""E2"",; causedBy: [{; message: ""E3"",; causedBy: []; }]; }, {; message: ""E4"",; causedBy: []; }, {; message: ""E5"",; causedBy: []; }]; }]; }]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2055:126,failure,failures,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2055,1,['failure'],['failures']
Availability,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:147,echo,echo,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541,2,['echo'],['echo']
Availability,"For instance, someone used ""docker.io/<their image>"" path in their runtime block and this led to a failure to call cache since call caching doesn't support docker.io urls. . Right now, since this failed to call cache it reran the job, however this was due to user error and they assumed caching would work. Additionally, there are cases such as when there's a failure to communicate with dockerhub that would also lead to failure to call cache. This request is for a workflow option that worked in conjunction with read_from_cache that set this behavior more strictly - if there is call cache miss, then run a new job, but if there is any other type of failure to call cache (transient, bad url, etc) then fail the job for the reason that the user requested to call cache and call caching failed to determine caching correctly.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2787:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2787,5,"['error', 'failure']","['error', 'failure']"
Availability,"For large output arrays this can get really bogged down as it's processing single threaded. At first glance it looks like it should be monoid-y, perhaps we can farm this out to multiple workers?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776:51,down,down,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776,1,['down'],['down']
Availability,"For our use cases, Iâ€™d say put responsibility on the pipeline developer. If thereâ€™s a collision, which file â€œwinsâ€ may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, weâ€™d like to distinguish â€œpipeline failureâ€ from â€œoutput failureâ€ in an automated way. So if itâ€™s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:272,failure,failure,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467,6,"['Failure', 'error', 'failure']","['Failure', 'error', 'failure']"
Availability,"For some reason, the expression `| cat` was breaking pushing with tags. As a result, the `gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer` repo was filling up with tagless images. When we went to delete-by-tag, the operation failed. Probably you have seen this message on your PRs:. ```; Please wait, building cromwell-drs-localizer into gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-8281552534-papiâ€¦; The push refers to repository [gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer]; [...]; ERROR: (gcloud.container.images.delete) [gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-8281552534-papi] is not found or is not a valid name. Expected tag in the form ""base:tag"" or ""tag"" or digest in the form ""sha256:<digest>""; ```. Fortunately, some time since 2019 Docker added the `--quiet` tag so we can adopt that instead. Once I fixed that problem, we had a new problem where `centaurPapiV2beta`, `centaurPapiV2betaRestart`, and `centaurHoricromtalPapiV2beta` jobs were all pushing images tagged `github-[build number]-papi`. This caused the most recent image to have the tag while the other two were untagged and leaked. I solved it by salting the tag with the job name:. ![Screenshot 2024-03-18 at 20 46 25](https://github.com/broadinstitute/cromwell/assets/1087943/53f2573d-7b42-4470-b3fa-e41f31cb9179). Now we can get rid of the 2.3 TB of old images and they won't come back, saving about $60 per month at current GCS list prices. ![Screenshot 2024-03-18 at 17 43 15](https://github.com/broadinstitute/cromwell/assets/1087943/9276c4c1-9eea-4781-9e97-94000319d658)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7390:517,ERROR,ERROR,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7390,1,['ERROR'],['ERROR']
Availability,"For the following task ; ```; task star_index {. File genomeDir; File genomeFasta; Int threads; Int binBits; Int max_memory = 100000000000. command {; /usr/local/bin/STAR \; --runThreadN ${threads} \; --runMode genomeGenerate \; --genomeDir ${genomeDir} \; --genomeFastaFiles ${genomeFasta} \; --genomeChrBinNbits ${binBits} \; --limitGenomeGenerateRAM=${max_memory}; }. runtime {; docker: ""quay.io/biocontainers/star@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; File out = genomeDir; }. }; ```; I get ; ```; ""Unable to load namespace from workflow: For input string: \""100000000000\""""; ```; error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2744:634,error,error,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2744,1,['error'],['error']
Availability,"For the new /describe endpoint being added for the purposes of Womtool-as-a-Service, implement the ability to validate a given workflow source file. AC: Given a WDL workflow source file (not http url), return whether its a valid WDL workflow, and provide errors if its invalid. ```; {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4433:255,error,errors,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4433,4,['error'],['errors']
Availability,"For the new /describe endpoint developed for the purposes of Womtool-as-a-Service, implement the ability to return inputs for a given workflow source file. AC: Given a WDL workflow source file (not http url), return the optional & required inputs available for this workflow, similar to what would be returned if using the `inputs` function with womtool:. {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; ""inputs"": [ ; { ; ""name"": ""inFile"",; ""valueType"": ""File"",; ""optional"": false; }, {...}; ]; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4452:247,avail,available,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4452,4,"['avail', 'error']","['available', 'errors']"
Availability,"For the new /describe endpoint developed for the purposes of Womtool-as-a-Service, implement the ability to return outputs for a given workflow source file. AC: Given a WDL workflow source file (not http url), return the outputs declared for this workflow:. {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ]; ""outputs"": [; {; ""name"": ""inFile"",; ""valueType"": ""File"",; ""optional"": false; }, {...}; ]; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4461:278,error,errors,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4461,3,['error'],['errors']
Availability,"For users just getting started with Cromwell, the documentation here:. https://github.com/broadinstitute/cromwell#getting-started-with-wdl. Points the user here:. https://github.com/broadinstitute/wdl#getting-started-with-wdl. and when you run your first workflow, you'll see in the output:. ```; [2016-04-14 16:21:12,82] [warn] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredentialFactory.scala:61); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$$anonfun$cromwellAuthenticated$1.apply(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$$anonfun$cromwellAuthenticated$1.apply(StorageFactory.scala:20); at scala.util.Try$.apply(Try.scala:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:441,avail,available,441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,2,['avail'],['available']
Availability,For whatever reason I get a different `failure`/`causedBy` stack when I run these locally vs in travis. I'm probably being stupid ðŸ¤·â€â™‚ï¸. . Reinstate these centaur tests:; - [x] `invalid_runtime_attributes`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2904:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2904,1,['failure'],['failure']
Availability,For workflow-success and workflow-failure type tests:. * Wait for the query results for the workflow to indicate carbonite complete; * Ensure that the metadata is as valid afterwards as it was beforehand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237,1,['failure'],['failure']
Availability,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:237,down,down,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225,4,"['down', 'error']","['down', 'error', 'errors']"
Availability,Format API Error reporting to common json structure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368:11,Error,Error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368,1,['Error'],['Error']
Availability,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4041:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4042:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042,2,"['echo', 'error']","['echo', 'error']"
Availability,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:322,Down,Downloading,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,3,"['Down', 'avail', 'down']","['Downloading', 'available', 'download']"
Availability,"Frequent ""Job execution token returned from incorrect actor"" error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4681:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4681,1,['error'],['error']
Availability,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1150:381,down,downstream,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150,3,['down'],['downstream']
Availability,"From Gitter:; > we're submitting jobs via API to remote cromwell server, and want to submit workflows with all the imports resolved already (client-side) so that querying cromwell metadata submittedFiles.workflow value shows verbatim what's being executed. Is there a way in wdl4s for example to do something effectively like: `val ns = NamespaceWithWorkflow.load(myWorkflow, myResolver); val wfAsString = ns.toWdlSource` i.e. get the string representation of the workflow back again, but with the imports resolved (""expanded"")?. @cjllanwarne your gist is no longer available, do you remember what you wrote?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494:566,avail,available,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494,1,['avail'],['available']
Availability,"From Henry --. My comment is a little different than the commenter above. So I will try to provide some context in order to differentiate. If one looks at Cromwell as being part of a larger application infrastructure/service such as a Genomics Pipeline where sequenced data is constantly being processed through the system or providing a service where users on the Internet can launch workflows ""as a Service"" - whenever they way. In these systems, uptime and availability are critical - at all times. Below are several scenarios where I could see running multiple Cromwells would be very beneficial. High availability:; Having multiple Cromwells running where jobs are ""load balanced"" between them would allow us to continue operate when there are systems issues or failures with one of the Cromwell instances. Zero-downtime deployments:; Supporting multiple Cromwells running different versions of the code, could provide the ability to upgrade Cromwell with little or no user impact. Essentially the new version is deployed to a new server, it is started up and at an appropriate time traffic (via a load balancer or proxy maybe) is directed away from the ""old version"" Cromwell to the new. Similar to item 2: being able to introduce infrastructure changes (host OS, security patches, host resizing,..) more seamlessly:; If I can support multiple instances of Cromwell, I can build a new instance of the host with all the updates and changes I require - deploy cromwell to the new node and cut over. A corollary to this request is also the ability to ""move"" workflows from one Cromwell instance to another. Maybe this is just workflows not in flight or active - but waiting to run. This capability could make it easier to retire older cromwell instances (once multiple cromwell instances support is in place). Some workflows may take days to run, being able to ""relocate"" these workflows to the ""new"" cromwell - allows us to decommission ""old"" cromwells faster.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/691:460,avail,availability,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/691,4,"['avail', 'downtime', 'failure']","['availability', 'downtime', 'failures']"
Availability,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3967:414,down,downloading,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967,1,['down'],['downloading']
Availability,"From `IntToIntArray` in the JG workflow, this was working on 23 but not 24. I chatted with @Horneth and he said it wasn't intentional so let's fix it. Presumably it's a coercion issue but unclear. error was:. `to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout()); wdl4s.util.AggregatedException: Failed to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout())`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1761:197,error,error,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1761,1,['error'],['error']
Availability,From a quick investigation it looks like the EJEA would use the default supervision strategy here. The PAPI BJEA would be restarted and then possibly nothing would happen since the broken disks runtime attribute prevented the job from actually starting. Should be easy enough to reproduce. ```; ERROR akka.actor.OneForOneStrategy - Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:295,ERROR,ERROR,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['ERROR'],['ERROR']
Availability,"From a quick reading of the parent `WorkflowManagerActor` code it appears the default supervision strategy with ""restart on generic Exception"" is being used. Simply restarting a crashed `WorkflowActor` FSM appears to put it back into its initial `WorkflowUnstartedState` where it wouldn't do anything to progress a workflow until it receives a `StartWorkflowCommand` which is not being re-sent. So it looks like this would create a zombie workflow, though it does appear to be abortable.; ```; ERROR akka.actor.OneForOneStrategy - Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; java.lang.RuntimeException: Google credentials are invalid: Error getting access token for service account: 400 Bad Request; {; ""error"": ""invalid_grant"",; ""error_description"": ""Invalid JWT Signature.""; }; 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials(GoogleAuthMode.scala:175); 	at cromwell.cloudsupport.gcp.auth.GoogleAuthMode.validateCredentials$(GoogleAuthMode.scala:173); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.validateCredentials(GoogleAuthMode.scala:237); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:250); 	at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.credentials(GoogleAuthMode.scala:237); 	at cromwell.filesystems.drs.DrsPathBuilderFactory.withOptions(DrsPathBuilderFactory.scala:86); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:494,ERROR,ERROR,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"From looking at the behaviour of cromwell, I think it traverses the execution graph (is that what it's called, the order in which tasks are run?) breadth-first, (e.g. first perform trimming for all samples, then mapping for all samples, then genotyping, etc). For most workflows, different tasks will have different performance characteristics (trimming and mapping is read/write heavy, genotyping is mostly read/CPU intensive). Would it then not make sense to do a depth first traversal of the execution graph? That way, we will have the most diversity in performance characteristics for all running tasks, which should speed up the overall runtime (e.g. no fighting over harddisk time between two trimming tasks). As a secondary bonus, depth first will mean that all different tasks are run as soon as possible, so when there is an error in one of the later tasks this is revealed to the user much more quickly.; The drawback is that cromwell will then also stop running the early tasks that do not give an error, but IIRC that behaviour is configurable in the settings file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736:834,error,error,834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736,2,['error'],['error']
Availability,"From my investigation, those were due to transient failures of ontology parsing, which this should reduce now: https://github.com/broadinstitute/cromwell/pull/4210",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750:51,failure,failures,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750,1,['failure'],['failures']
Availability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:156,error,errors,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,1,['error'],['errors']
Availability,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,2,"['error', 'failure']","['error', 'failures']"
Availability,"Fully caches Genome in a Bottle chm to run in ~2 hours. Still waiting on results from GIAB Joint because that takes much longer and has to be restarted for sporadic failures, but all the calls so far will cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3541:165,failure,failures,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3541,1,['failure'],['failures']
Availability,"Further discussed points:; - The delete endpoint will be implemented by immediately marking a workflow as effectively ""to be deleted""; - An actor will occasionally sweep through these ""to be deleted"" workflows and remove the previously discussed bullet points; - Upon success, the workflow will be gone from the database; - Upon failure, the workflow will be left in a ""failed to delete"" state in the database; - One may re-mark an existing workflow as ""to be deleted"" by re-using the delete endpoint; - If a workflow output is already deleted, we won't try to re-delete it. However--; - As the refresh token will not be available, output files that were generated with refresh tokens will _not_ be able to be deleted. These types of errors, while expected, will require more planning on how to handle these permissions issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448:329,failure,failure,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448,3,"['avail', 'error', 'failure']","['available', 'errors', 'failure']"
Availability,GCP Batch backend failure doesn't block merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497,1,['failure'],['failure']
Availability,GCP Batch backend not recovering workflows on restart,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:22,recover,recovering,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recovering']
Availability,"GCP shut down Genomics on July 9, 2024 and the backend will be removed from Cromwell. Life Sciences is the replacement, for example; ```; endpoint-url = ""https://lifesciences.googleapis.com/""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185:9,down,down,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185,1,['down'],['down']
Availability,GCP: Ignore error if ssh-server fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771,1,['error'],['error']
Availability,GCPBATCH: accessing private gcr.io docker for callcaching raises error: unauthorized,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['error'],['error']
Availability,GCR Docker-Content-Digest v2 not available for all images,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:33,avail,available,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,1,['avail'],['available']
Availability,GCS storage appears to be created redundantly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:34,redundant,redundantly,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,1,['redundant'],['redundantly']
Availability,"GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4988,ERROR,ERROR,4988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['ERROR'],['ERROR']
Availability,"GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14634,ERROR,ERROR,14634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['ERROR'],['ERROR']
Availability,"GOTC keeps track of very workflow failure in a spreadsheet, and summarizes. @bradtaylor has shown it to me before, so he might be the place to start. The task here is to review that spreadsheet, provide a summary of the top failure modes and schedule a tech/design meeting to define the tickets that would have prevented these failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820,3,['failure'],"['failure', 'failures']"
Availability,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763,4,['error'],"['error', 'errors']"
Availability,Gather reliability / workflow failure causes from GOTC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820:7,reliab,reliability,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820,2,"['failure', 'reliab']","['failure', 'reliability']"
Availability,"GitHub ""helpfully"" collapses `JsonEditorSpec` due to the scope of the changes, but actually that does need to be reviewed. ðŸ™‚ . Does:. * Fix `exclude` to only examine workflows and calls; * Support `:` syntax in excludes; * Add `ErrorOr` validation to method signatures; * ""Adjust"" `JsonEditorSpec` to not actively test for incorrect behavior. Does not:. * Fix `include` to only examine workflows and calls; * Support `:` syntax for include; * Add as many real-world or rigorous tests as I would like, mostly because the aforementioned things are still broken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5309:228,Error,ErrorOr,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5309,1,['Error'],['ErrorOr']
Availability,Give cromwell the opportunity to shut down gracefully in Caas,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3674:38,down,down,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3674,1,['down'],['down']
Availability,"Given a Local config like:. ```; runtime-attributes = """"""; String? docker; String? docker_user; String? docker_env; """""". submit-docker = """"""docker run ${docker_env} --rm ${""--user "" + docker_user} -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; ```. Docker environment variables could be passed with a WDL like:; ```; task build_env {; Array[String] kvs = [""k1=v1"", ""k2=v2"", ""k3=v3""]; Array[String] prefixed = prefix(""-e "", kvs); command {; echo ""${sep=' ' prefixed}""; }; output {; String out = read_string(stdout()); }; }. task docker_task {; String docker_env. command {; echo $k1; echo $k2; echo $k3; }. runtime {; docker: ""ubuntu:latest""; docker_env: ""${docker_env}""; }. output {; Array[String] out = read_lines(stdout()); }; }. workflow w {; call build_env; call docker_task { input: docker_env = build_env.out }; output {; Array[String] out = docker_task.out; }; }; ```. Having to use a separate task to stringify an array of String kv environment pairs is a little clunky, but it looks like the way the `${sep=' '...}` expansion works currently requires this to be done in the command section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410:475,echo,echo,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410,4,['echo'],['echo']
Availability,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185,4,"['Failure', 'error']","['Failure', 'error']"
Availability,"Given the eventually consistent scheme in place for deriving workflow status, it's likely that a status check soon after workflow submission will fail with a 404 response. From a UX perspective a bogus 404 error due to eventual consistency feels wrong; a lag in status update is one thing, but Cromwell probably shouldn't disavow the existence of the workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/962:206,error,error,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/962,1,['error'],['error']
Availability,"Given the re-architecting of 0.21+ Cromwell this particular error can no longer occur, but please let us know if you see any other problems. ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869#issuecomment-253919168:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869#issuecomment-253919168,1,['error'],['error']
Availability,"Given this workflow:; ```; workflow w {; 	call t {}; 	output { t.* }; }. task t {; 	; 	command {; 		touch file.txt; 		echo ""file.txt"" > filename.txt; 	}. 	output {; 		File f1 = ""filename.txt""; 		File f2 = read_string(f1)		; 	}. 	runtime {; 	 docker: ""ubuntu""; 	}; }; ```; One would expect 2 files to be marked for delocalization for `call t`, yet only ""filename.txt"" is properly localized, but not ""file.txt"". This behavior is only observed when running against the JES backend, and works as expected on the SharedFileSystem backend. This is a regression introduced in Cromwell v30. When checking Google's operation metadata, the outputs are listed as:; ```; outputs:; filename.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/filename.txt; t-rc.txt: gs://rm-dev/w/6b29d52a-d665-4ce9-b042-8cd4e4374473/call-t/t-rc.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3065:118,echo,echo,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065,1,['echo'],['echo']
Availability,"Glob only appears to work if the globbed files are in the same file system cromwell's exec directory. Example:; ```; task doGlob {; String globExpr. command {; echo ""noop"" > /dev/null; }; output {; Array[File] files = glob(globExpr); }; runtime {; backend: ""Local""; }; }; ```. If `globExpr` points to a different file system from where the `exec` dir is, this fails. The reason seems to be that `glob` only attempts **hard-linking**, and if that fails, it does not attempt symbolic linking. Here's an extract of the generated `script` file:. ```; ( ln -L /tmp/my/path/* /my/cromwell/exec/myWf/.../call-doGlob/execution/glob-64b5dd2db11682a95849e477695e878c 2> /dev/null ) || \; ( ln /tmp/my/path/* /my/cromwell/exec/myWf/.../call-doGlob/execution/glob-64b5dd2db11682a95849e477695e878c ); ```. This part fails with `Invalid cross-device link`. . However, the script itself does not fail - it goes on to list the contents of (empty) `glob-xxxx` directory, and returns an empty array. Running on local backend but I assume a HPC backend would get the same behaviour. For context, the reason why I need a task like this is because putting globs in arguments is still not supported.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395:160,echo,echo,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395,1,['echo'],['echo']
Availability,Global comment: I think based on a problem we saw last week that the `check-alive` command in `application.conf` is wrong for SGE. Should be `qstat -j ${job_id}`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965:76,alive,alive,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965,1,['alive'],['alive']
Availability,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314,1,['failure'],['failures']
Availability,Going with changing Martha to try-and-silence accessUrl generation failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463:67,failure,failures,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463,1,['failure'],['failures']
Availability,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:345,failure,failures-based-on-check-alive,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279,6,"['alive', 'failure']","['alive', 'failure', 'failures-based-on-check-alive']"
Availability,"Good news: the error message is now spot on. Bad news: unmarshalling error. ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.5, Date: Mon, 28 Jan 2019 23:23:57 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: ...""; }),HttpProtocol(HTTP/1.1)); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447,4,['error'],['error']
