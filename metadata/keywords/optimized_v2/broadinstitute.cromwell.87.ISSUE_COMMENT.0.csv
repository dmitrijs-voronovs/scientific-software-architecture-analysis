quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989); 	at com.mysql.jdbc.MysqlIO.<init>(MysqlIO.java:341); 	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2192); 	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2225); 	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2024); 	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:779); 	at com.mysql.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:3907,failure,failure,3907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['failure'],['failure']
Availability," 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2449); 	at com.mysql.jdbc.StatementImpl.executeInternal(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:3478,failure,failures,3478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['failure'],['failures']
Availability," (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colon$plus$colon;@52b558ea,SchemaDefRequirement).; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowD",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4398,error,error,4398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,2,['error'],['error']
Availability," (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the softwa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:3909,down,down,3909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['down'],['down']
Availability," = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1906,error,error,1906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability," = 22009; }. call extract_field as year_of_birth { input :; script_dir = script_dir,; id = 34; }. call extract_field as month_of_birth { input :; script_dir = script_dir,; id = 52; }. call extract_field as date_of_death { input :; script_dir = script_dir,; id = 40000; }. call extract_field as phenotype { input :; script_dir = script_dir,; id = phenotype_id; }. scatter (categorical_covariate_id in categorical_covariate_ids) {; call extract_field as categorical_covariates { input :; script_dir = script_dir,; id = categorical_covariate_id; }; }. call platform_agnostic_workflow.main { input:; script_dir = script_dir,. phenotype_name = phenotype_name,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariates.data,; is_binary = is_binary,; is_zero_one_neg_nan = is_zero_one_neg_nan,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,. fam_file = fam_file, # Could instead create a task for downloading this with ukbgene; withdrawn_sample_list = withdrawn_sample_list,. sc_white_brits = white_brits.data,; sc_ethnicity_self_report = ethnicity_self_report.data,; sc_sex_aneuploidy = sex_aneuploidy.data,; sc_genetic_sex = genetic_sex.data,; sc_reported_sex = reported_sex.data,; sc_kinship_count = kinship_count.data,; sc_assessment_ages = assessment_ages.data,; sc_pcs = pcs.data,; sc_year_of_birth = year_of_birth.data,; sc_month_of_birth = month_of_birth.data,; sc_date_of_death = date_of_death.data,; sc_phenotype = phenotype.data; }. 	output {; 		Array[File] out_sample_lists = main.out_sample_lists; 	}; }; ```. platform_agnostic_workflow.wdl; ```; # platform agnostic workflow. version 1.0. import ""tasks.wdl"". workflow main {. input {; String script_dir. String phenotype_name; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; Boolean is_binary; Boolean is_zero_one_neg_nan; String date_of_most_recent_first_occurrence_update. File fam_file # task for generating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:3390,down,downloading,3390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['down'],['downloading']
Availability," CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2371,failure,failures,2371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['failure'],['failures']
Availability," Denied (Service: S3Client; Status; > Code: 403; Request ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1203,Fault,FaultHandling,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability," Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3262,error,error,3262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['error'],['error']
Availability," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1514,failure,failures,1514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['failure'],['failures']
Availability," [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm mo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:2185,ERROR,ERROR,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability," and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Buc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2166,error,error,2166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability," clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6023,down,down,6023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['down'],['down']
Availability," definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:1310,recover,recoverWith,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['recover'],['recoverWith']
Availability," even create the `.list` file:; ```bash; #!/bin/sh; umask 0000; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution; base_name=$(basename /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam .bam); java -Xmx4g -jar $picard CollectHsMetrics \; I=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/picard_aggregation/DEV-7039/Pond-544027,_Standard_Kapa_LC/v1/Pond-544027,_Standard_Kapa_LC.bam \; O=$base_name.hs_metrics \; PER_TARGET_COVERAGE=$base_name.per_target_coverage \; TI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; BI=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/ice-22.interval_list \; R=/humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/inputs/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; ); echo $? > /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution/rc.tmp; (; cd /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution. ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028:2005,echo,echo,2005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980#issuecomment-280022028,1,['echo'],['echo']
Availability," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4798,error,error,4798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability," is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1071,error,error,1071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,2,"['Error', 'error']","['Error', 'error']"
Availability," line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-workflow-id: cromwell-0344f62e-809d-48d4-8e9a-ede11fe5dd5c; wdl-call-alias: detect-sv; wdl-task-name: detect-sv-cwl; <b>machineType: custom-2-4096</b>; </pre>. @chapmanb I was curious if you've seen this before ? I'm modifying the CWL to ask for a bit more memory but I'm wondering if there's something else that Cromwell is not doing right",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:2339,ERROR,ERROR,2339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,2,"['ERROR', 'avail']","['ERROR', 'available']"
Availability," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1917,Error,Error,1917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability," on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2664,recover,recoverWith,2664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['recover'],['recoverWith']
Availability," on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1740,echo,echo,1740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,4,['echo'],['echo']
Availability," pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1205,down,download,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['down'],['download']
Availability, scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4602,Error,ErrorOr,4602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability," the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a6eeab"" >&2 &; (; cd /cromwell_root. /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R /cromwell_root/required-files/references/b37/human_g1k_v37_decoy.fasta \; -I /cromwell_root/temporary-files/XXXXXX-001/workspace/SingleSampleGenotyping/415cf327-c799-4d1d-a726-272028b4e8c5/call-ubam2bam/from_ubam.to_bam_workflow/36aabe2e-6ff6-456b-a7cf-d76cb7b93173/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/e4a6eeab-c2ff-4940-b1aa-1ae64e63e1ae/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam \; --useOriginalQualities \; -O XXXXXX-001.recal_data.csv \; -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf \; -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf \; -L 12:1+ -L 13:1+; ) > ""$oute4a6eeab"" 2> ""$erre4a6eeab""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync. ); mv /cromwell_root/rc.tmp /cromwell_root/rc; ```. I appreciate the help",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:4040,echo,echo,4040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,1,['echo'],['echo']
Availability," the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1266,alive,alive,1266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['alive'],['alive']
Availability," useful. Thanks. Workflow Id:. `129f0510-5d6b-4c4c-b266-116a9a52f325`. Step meta data:. ```. {; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:1015,failure,failures,1015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1651,error,error,1651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,5,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability," {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1870,error,error,1870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['error'],['error']
Availability,![download](https://user-images.githubusercontent.com/961771/150234480-e61224c2-c7e6-49cd-9bc2-fb721c682eee.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200,1,['down'],['download']
Availability,![download](https://user-images.githubusercontent.com/961771/27764039-f12b841e-5e5d-11e7-9c9e-2d12766fbacd.jpg). 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2408/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2408#issuecomment-312443688,1,['down'],['download']
Availability,![download](https://user-images.githubusercontent.com/961771/37727961-6b031812-2d0f-11e8-8150-cf0f1d2bbc00.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3442#issuecomment-375038040:2,down,download,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3442#issuecomment-375038040,1,['down'],['download']
Availability,"""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvalu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:2158,error,error,2158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['error'],['error']
Availability,"""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 358",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2183,ERROR,ERROR,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:606,avail,available,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495,1,['avail'],['available']
Availability,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@370f3e3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## develop #2657 +/- ##; ==========================================; Coverage ? 64.03% ; ==========================================; Files ? 381 ; Lines ? 8893 ; Branches ? 193 ; ==========================================; Hits ? 5695 ; Misses ? 3198 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...backend/standard/StandardAsyncExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree#diff-YmFja2VuZC9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL3N0YW5kYXJkL1N0YW5kYXJkQXN5bmNFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `67.44% <100%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165,2,['error'],['error-reference']
Availability,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2663?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@839ea1e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## develop #2663 +/- ##; ==========================================; Coverage ? 64.32% ; ==========================================; Files ? 381 ; Lines ? 8892 ; Branches ? 195 ; ==========================================; Hits ? 5720 ; Misses ? 3172 ; Partials ? 0; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801,2,['error'],['error-reference']
Availability,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:237,error,error-reference,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119,2,['error'],['error-reference']
Availability,"## I want to. ## (i) scatter scatters as requested by @skazakoff above (aka ""nested scatter"") and also. ## (ii) re-array and scatter again (what I'm calling criss-cross scatter). As I show in this diagram: . ![img_4460](https://cloud.githubusercontent.com/assets/11543866/18210288/d489f7b4-7105-11e6-9737-59f75d4a6949.JPG). My attempt to ask WDL/Cromwell to do the first scatter-of-scatter above gives the following error:. ```; [2016-09-02 16:30:43,699] [error] WorkflowActor [e1b18d33]: Call failed to initialize: Failed to start call: Nested Scatters are not supported (yet).; ```. It was unclear how I would go about criss-cross scattering. My current solution around this is to run two separate WDL workflows. Between the two workflows, I use a simple python script to organize the files for the second scatter. The first WDL workflow is per sample, while the second WDL workflow acts on all the outputs of the multiple runs of the first workflow (multiple samples). ---. ## Example of workflows that I want to run within a single WDL workflow. These are scripts that run on the cloud that I would like to run within a single WDL script using the nest-scatter and criss-cross-scatter features. Currently, I run the first script per sample (1) for multiple samples, then run a helper script to organize all the outputs (1.5), and finally run the second WDL script across the multi-sample outputs per genomic interval (2). . I would like to be able to scatter the samples, scatter per interval, then run a differently organized (criss-cross) scatter across all the samples per interval. ### (1) First WDL workflow. #### UltimateScatterHaplotypeCaller_cloud_quicktest.wdl. ```; # ScatterHaplotypeCaller.wdl #############################################################; # Must use GATK v3.6, especially with hg38 where contig names have colons,; # as a bug in prior versions of GATK strips this off.; # Each BAM file represents a single different sample.; # Option to include an additional file to H",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995:416,error,error,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995,4,['error'],['error']
Availability,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:724,error,errors,724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527,3,"['Ping', 'error']","['Pinging', 'errors']"
Availability,"#2030 fixes this specific issue. There remains a greater tech debt of actor supervision, where unexpected errors like this shouldn't leave the workflows in a running state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614:106,error,errors,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-282584614,1,['error'],['errors']
Availability,"$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"",",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:2003,error,error,2003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['error'],['error']
Availability,$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1448,Robust,RobustClientHelperSpec,1448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:2070,echo,echo,2070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['echo'],['echo']
Availability,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:368,failure,failure,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503,1,['failure'],['failure']
Availability,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:1244,avail,available,1244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531,1,['avail'],['available']
Availability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = … ++ assemblySettings ++ …`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:1231,down,downstream,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,1,['down'],['downstream']
Availability,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:42,failure,failures,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427,10,"['error', 'failure']","['error', 'failures']"
Availability,"**Was the imports zip the same in each workflow?**; Yes, we were running the same workflow so the imports zip is the same. We get those import files by downloading them from github and adding them to a cache so that we don't have to download them for each workflow. **Were the workflows all the same?**; Yes, all of the workflows were the same . **Do you have any logs from the sender to check that a zip was indeed sent?**; No, we just have logs that a workflow was submitted to Cromwell 😞. **Were they submitted as a series of 999 separate submits or as a single batch submit POST?** ; They were submitted as a series of 999 individual requests in the ""On Hold"" status, and a separate process sent requests to Cromwell to started one of those on-hold workflows every 10 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649:152,down,downloading,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-422854649,2,['down'],"['download', 'downloading']"
Availability,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:165,error,error,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487,2,['error'],['error']
Availability,"+1 on `tmpfs`. Currently, we have to create a directory under `/dev/` and rely on the assumption that that directory gets mounted by default as a `tmpfs` with 1/2 of the available RAM (at least on GCP). This is obviously not ideal. Delocalization of such files is problematic as well. Our use case is exactly the same, to unpack/process tens or hundreds of thousands of small files (in a BCL). Doing so with any ""normal"" disk is much slower than with RAM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155:170,avail,available,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-413995155,1,['avail'],['available']
Availability,+1 on this feature (or one like it) -- it's really helpful for writing robust and cheap workflows,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064:71,robust,robust,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316800064,1,['robust'],['robust']
Availability,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:490,failure,failure,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563,2,['failure'],"['failure', 'failures']"
Availability,"+1 to this. I'm getting the same warning, which looks like a failure but actually seems to work fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434580781:61,failure,failure,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-434580781,1,['failure'],['failure']
Availability,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:359,error,error,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414,4,['error'],['error']
Availability,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016,2,['error'],['error']
Availability,"- If the job itself fails to start, this can be reported back immediately by Cromwell (although please do raise this as a separate issue with a reproducing WDL if that's what you mean).; - If Cromwell can start the job but the bash script will fail immediately, then that's out of Cromwell's hands.; - The overhead of running jobs on JES means that even `echo hello` has a minimum overhead of around 5-10 minutes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761:355,echo,echo,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1358#issuecomment-253926761,1,['echo'],['echo']
Availability,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:364,avail,available,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262,1,['avail'],['available']
Availability,"-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1775,ERROR,ERROR,1775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:5875,down,down,5875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,6,['down'],['down']
Availability,-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traver,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:1150,Robust,RobustClientHelperSpec,1150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,1,['Robust'],['RobustClientHelperSpec']
Availability,-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traver,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1413,Robust,RobustClientHelperSpec,1413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,1,['Robust'],['RobustClientHelperSpec']
Availability,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2823,ERROR,ERROR,2823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,1,['ERROR'],['ERROR']
Availability,"-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as CWL v1.0; [2018-11-09 10:25:19,60] [info] Pre-Processing /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1310,down,download,1310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['down'],['download']
Availability,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2429,rollback,rollback,2429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['rollback'],['rollback']
Availability,".$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1721,ERROR,ERROR,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['ERROR'],['ERROR']
Availability,... but it's still broken. Closing this down,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2834#issuecomment-342886123:40,down,down,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2834#issuecomment-342886123,1,['down'],['down']
Availability,...and if there was an error.. they just retry later,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227521672:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227521672,1,['error'],['error']
Availability,"...so if the 403 comes back with that information, that's great!. On Mon, Jul 11, 2016 at 8:27 AM, Yossi Farjoun farjoun@broadinstitute.org; wrote:. > Indeed. but the problem was that we couldn't tell that a file was missing; > and indeed which file it was.; > ; > On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; > wrote:; > ; > > IIRC the error in question was passing the 403 Forbidden back from Google; > > ; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > > or mute the thread; > > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259:363,error,error,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720259,1,['error'],['error']
Availability,".impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I received yet another error: . ```; 2019-02-25 18:46:46,698 cromwell-system-akka.actor.default-dispatcher-3 ERROR - Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:2431,error,error,2431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['error'],['error']
Availability,".java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. ```; mysql> select * from WORKFLOW_METADATA_SUMMARY_ENTRY;; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | WORKFLOW_METADATA_SUMMARY_ENTRY_ID | WORKFLOW_EXECUTION_UUID | WORKFLOW_NAME | WORKFLOW_STATUS | START_TIMESTAMP | END_TIMESTAMP | SUBMISSION_TIMESTAMP | PARENT_WORKFLOW_EXECUTION_UUID | ROOT_WORKFLOW_EXECUTION_UUID | METADATA_ARCHIVE_STATUS |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | 2 | 796f3949-47e6-497e-9458-59ab53a063c6 | wf_hello | Succeeded | 2020-06-04 21:40:10.924000 | 2020-06-04 21:43:38.055000 | 2020-06-04 21:40:10.726000 | NULL | NULL | TooLargeToArchive |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; ```. But there was a small bug:; ```; 2020-06-04 21:43:43,512 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Programmer Error! The CarboniteWorkerActor cannot convert this into a completion metric: CarboniteWorkflowComplete(796f3949-47e6-497e-9458-59ab53a063c6,TooLargeToArchive); ```; I created a ticket for it: https://broadworkbench.atlassian.net/browse/BA-6471",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:4317,ERROR,ERROR,4317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,".jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4815,failure,failures,4815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['failure'],['failures']
Availability,".scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncEx; > ecutionActor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1360,Fault,FaultHandling,1360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,".withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4366,Error,Error,4366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,['Error'],['Error']
Availability,".xml::call_job_identifiers::rmunshi: Column METADATA_JOURNAL.METADATA_CALL_INDEX renamed to JOB_SCATTER_INDEX; 2018-06-07 12:16:10,762 INFO - changelog.xml: changesets/standardize_column_names.xml::call_job_identifiers::rmunshi: Column METADATA_JOURNAL.METADATA_CALL_ATTEMPT renamed to JOB_RETRY_ATTEMPT; 2018-06-07 12:16:10,762 INFO - changelog.xml: changesets/standardize_column_names.xml::call_job_identifiers::rmunshi: ChangeSet changesets/standardize_column_names.xml::call_job_identifiers::rmunshi ran successfully in 1ms; 2018-06-07 12:16:10,778 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::entry_or_journal_existence_xor::mcovarr: ChangeSet changesets/embiggen_metadata_value.xml::entry_or_journal_existence_xor::mcovarr ran successfully in 15ms; 2018-06-07 12:16:10,789 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_entry::mcovarr: Marking ChangeSet: changesets/embiggen_metadata_value.xml::embiggen_metadata_entry::mcovarr ran despite precondition failure due to onFail='MARK_RAN':; changelog.xml : Table PUBLIC.METADATA_ENTRY does not exist. 2018-06-07 12:16:10,792 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr: METADATA_JOURNAL.METADATA_VALUE datatype was changed to LONGTEXT; 2018-06-07 12:16:10,793 INFO - changelog.xml: changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr: ChangeSet changesets/embiggen_metadata_value.xml::embiggen_metadata_journal::mcovarr ran successfully in 3ms; 2018-06-07 12:16:10,795 INFO - changelog.xml: changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi: Table CALL_CACHING_JOB_DETRITUS created; 2018-06-07 12:16:10,795 INFO - changelog.xml: changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi: ChangeSet changesets/call_caching_job_detritus.xml::call_caching_job_detritus::rmunshi ran successfully in 1ms; 2018-06-07 12:16:10,797 INFO - changelog.xml: changesets/call_caching_job_d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:43916,failure,failure,43916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1533,Fault,FaultHandling,1533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,6,['Fault'],['FaultHandling']
Availability,"02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1232,ERROR,ERROR,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-For: 69.173.127.107, User-Agent: Java/1.8.0, Host: app:8000),Empty,HTTP/1.1); com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3434) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3334) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3774) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Mysq",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:2191,failure,failure,2191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,1,['failure'],['failure']
Availability,"18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2941,echo,echo,2941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['echo'],['echo']
Availability,"19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1574,ERROR,ERROR,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"19:02:19.372170Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Failed to complete master workflow, error code: 1; [2018-11-04T19:02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1129,ERROR,ERROR,1129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"2 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4943,echo,echoHelloWorld,4943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11235,error,errors,11235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['errors']
Availability,"2-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-SortAndFixSampleBam/XXXXXX-001.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O XXXXXX-001.recal_data.csv -knownSites /cromwell_root/required-files/references/broadBundle/dbsnp_138.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/Mills_and_1000G_gold_standard.indels.b37.vcf -knownSites /cromwell_root/required-files/references/broadBundle/1000G_phase1.indels.b37.vcf -L 10:1+; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.bdd4ff39; [Global flags]. (...EXECUTION LOGS...). 16:35:03.610 INFO ProgressMeter - 10:135446829 2.3 3269161 1437710.1; 16:35:03.614 INFO ProgressMeter - Traversal complete. Processed 3269161 total reads in 2.3 minutes.; 16:35:03.819 INFO BaseRecalibrator - Calculating quantized quality scores...; 16:35:03.882 INFO BaseRecalibrator - Writing recalibration report...; 16:35:04.992 INFO BaseRecalibrator - ...done!; 16:35:04.996 INFO BaseRecalibrator - Shutting down engine; [October 31, 2018 4:35:05 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 2.34 minutes.; Runtime.totalMemory()=4054515712; Tool returned:; 3269161; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication. ; Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]...; ServiceException: 401 Requester pays bucket access requires authentication.; ```. The only thing that is not default in bucket is that we have set lifecycle option to delete objects after 5 days. Our workflow takes ~6 hours to end so it should not be a problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126:1992,down,down,1992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435884126,1,['down'],['down']
Availability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:1744,ERROR,ERROR,1744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:182,failure,failure,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['failure'],['failure']
Availability,"4/19 PM Hi Evan, a patch went out to fix this at 10 AM this morning. Can you confirm that you no longer see this?. 4/19 AM Hi Evan - were you signed into the forum when you got this error? Can you send me the url of the page you were on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047:182,error,error,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047,1,['error'],['error']
Availability,"4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 358986",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2526,error,error,2526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['error'],['error']
Availability,"73364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1671,failure,failure,1671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['failure'],['failure']
Availability,"7d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non streaming, blue is streaming. ---; The main takeaway is that when **under memory pressure** (i.e when available memory is insufficient to build the requested metadata), streaming makes a significant difference on relieving the heap usage for medium to large (> 100K) metadata. ### The less good. - Response time is not as good. The use cases above were specifically targeted towards trying to build large to very large metadata.; However when used in a more realistic scenario with lots of small sized metadata and few large ones, the overall response time is increasing significantly.; If Cromwell has sufficient memory to sustain the load then streaming does not give any real improvement.; The graph below shows memory usage with (v1s) and without streaming (v1) when Cromwell has enough memory to build all requests (in MB).; ![memory-v1-v1s](https://user-images.githubusercontent.com/2978948/48013920-818d5c80-e0f3-11e8-9f71-d4dedcbb2ba1.png). The graph below shows the average response time of the metadata endpoint with and without streaming (in ms).; ![metadata-200-v1-v1s](https://user-images.githubusercontent.com/2978948/48013852-53a81800-e0f3-11e8-9152-6c844e896b09.png). A ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:2100,avail,available,2100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['avail'],['available']
Availability,"9-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignments: printHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:2308,down,down,2308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['down'],['down']
Availability,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:2019,ERROR,ERROR,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,1,['ERROR'],['ERROR']
Availability,:+1: (minus the failure),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/57#issuecomment-119694172:16,failure,failure,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/57#issuecomment-119694172,1,['failure'],['failure']
Availability,":+1: It seems there's a potential to reduce some of the boilerplate around instances dealing with `ExpressionElement`. It seems like those instances exist to refine the type down to the ""leaf"" level where the more specific type can do its thing. I would try to eliminate one of these and see if you can parameterize the callers with a `[T]` or `[T <: ExpressionElement]` to save you from the trouble of specializing/refining/narrowing/casting (not sure the right word) the type yourself. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911:174,down,down,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911,1,['down'],['down']
Availability,:+1: Thank you @delocalizer this is great! 😄 Can you please squash down to one commit before we merge?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2049/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732:67,down,down,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-283940732,1,['down'],['down']
Availability,":+1: although I could not agree more with @geoffjentry's sentiments on slimming down `WorkflowDescriptor`, I think that's better done in pluggable backend PRs or maybe completely separate PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-190957501:80,down,down,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-190957501,1,['down'],['down']
Availability,:+1: barring the test failure... I looked into it briefly but I didn't figure anything out. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/573/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609:22,failure,failure,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197986609,1,['failure'],['failure']
Availability,:+1: total failure. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2203/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297187333,1,['failure'],['failure']
Availability,":+1:. Very nice! I need to go back and re-read this after my brain cools down, but the tests give me a lot of confidence it's all working. 🙂 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1969/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986:73,down,down,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1969#issuecomment-279063986,1,['down'],['down']
Availability,:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.run(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:11196,Error,ErrorHandling,11196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['Error'],['ErrorHandling']
Availability,:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4374,Error,ErrorHandling,4374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Error'],['ErrorHandling']
Availability,":52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99474,failure,failure,99474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,"; > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwell.backend.standard.St",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1281,Fault,FaultHandling,1281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7667,down,down,7667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,26,['down'],['down']
Availability,"> * cromwell v27; > * SGE backend; > * server mode; > ; > Cromwell timing diagram displays SGE queued (qw) status as Running. This increases difficulty of debugging (or evaluating) a tool, since we do not have a reliable and easy(!) way to look at timing. @LeeTL1220 hi，Have you solved this problem ？",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657:212,reliab,reliable,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2464#issuecomment-732694657,1,['reliab'],['reliable']
Availability,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:166,failure,failures,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038,1,['failure'],['failures']
Availability,"> > Looks very promising! 😄; > > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788; > ; > Thanks for your review!; > In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok. We have provided a new custom ECS image with local `ubuntu:latest` docker image(`src/ci/bin/test_bcs.inc.sh`). Please run Centaur CI for a new test. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308:281,failure,failures,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-499071308,2,"['avail', 'failure']","['available', 'failures']"
Availability,"> @alartin You're spot on in regards to array jobs. Unfortunately the internal design of Cromwell makes it difficult to do this, it's come up before for HPC backends as well. Something we should address some day but it'd be a fairly major undertaking. Hi @geoffjentry I wonder if there is any guide for developer, especially for the backend impl. Or is there some doc/slide available for that? I am willing to implement backend for other public cloud vendor and curious about how to get it started quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480:374,avail,available,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480,2,['avail'],['available']
Availability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:220,error,errors,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['error'],['errors']
Availability,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119,4,"['avail', 'error']","['available', 'error', 'errored']"
Availability,"> A couple of other observations:; > ; > 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded. I like that a bit less, since in Travis, if one of the subbuilds failed after 1 second, you can independently restart it immediately. But in CircleCI you would need to wait for all subbuilds to finish in order for it to let you restart the failed onces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564:55,failure,failures,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564,1,['failure'],['failures']
Availability,"> Also have this error, using Cromwell 52, installed using this manual :; > ; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > ; > logs say : fetch_and_run.is is a directory. Extra info : cloning job & resubmitting through aws console runs fine. so it seems to be a temporary issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613,1,['error'],['error']
Availability,"> Are there any config properties which you know of that might help with this?. Not that I can think of unfortunately :/ One quick thing we could maybe do before the fixing it ""the right way"" would be to enable retries at the GCS library level. We've disabled it because we have our own retry mechanism which is more reliable and asynchronous (but WDL functions couldn't use it so far, which is why they're not retried). We're about to release Cromwell 30 imminently so I don't think this can make it before then but we could consider hotfixing it if this is really becoming urgent. Edit: actually looking at it more closely, even though we don't supply an ""retry settings"" to the GCS library they have default ones allowing for 6 attempts. However like I said we've found their retries to not always be reliable so it might be that for some particular errors it's not retried at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774:317,reliab,reliable,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774,3,"['error', 'reliab']","['errors', 'reliable']"
Availability,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:517,down,download,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634,2,['down'],['download']
Availability,"> Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory"". Also have this error. Anyone figure out what the issue is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269:126,error,error,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269,1,['error'],['error']
Availability,"> Did we update the akka http library in the last few days?. FYI, the import resolver, that during centaur tries to download the non-existent file from GitHub, [doesn't use akka-http](https://github.com/broadinstitute/cromwell/blob/49/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L191) like most of cromwell/cromiam/centaur/etc. It uses yet another jvm http client called [sttp](https://github.com/softwaremill/sttp#readme). Still, I think what you're running into is that GitHub changed their 404 response. See `curl -i https://raw.githubusercontent.com/broadinstitute/cromwell/develop/my_workflow`. I don't know how stable the change is either... they may change the body of the 404 response again [without notice](https://xkcd.com/1172/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242:116,down,download,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242,1,['down'],['download']
Availability,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:239,down,downloads,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['down'],['downloads']
Availability,"> FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied.; > ; > Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275:327,failure,failures,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275,1,['failure'],['failures']
Availability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:25,avail,available,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,1,['avail'],['available']
Availability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. I built cromwell 88. Then I ran two large preemptible runs, one with cromwell 87 and the other with 88. The e2 with standard machines ran faster, and was pre-empted less, than the n1 with custom machines.; 11% faster, and 424 preemptions vs 276. Many of the 424 were preemptions that happened really early in the run process. So it may be that I just had bad luck with someone kicking off a large non-spot run while my 88 run was going but teh 87 was finished. But I've been advised that custom machine types are much more likely to be preempted than standard ones. Would it be possible for you to default to n1 or n2 standard machines, rather than custom ones?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654:25,avail,available,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654,1,['avail'],['available']
Availability,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:897,error,error,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,1,['error'],['error']
Availability,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:527,alive,alive,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,3,['alive'],['alive']
Availability,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:313,down,down,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,2,['down'],['down']
Availability,"> Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code.; Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry. @cjllanwarne I found the problem. I did not have a `final_workflow_outputs_dir` set in the options.json files for the centaur tests. If this path is not set `use_relative_output_paths` is of course not used... :man_facepalming: That is fixed now and it works as expected. Colliding outputs will return as a workflow failure. Since I got the local testing working I was able to add more advanced tests and make sure these are correct as well. The error message is tested when the outputs are colliding. In order for that test to work I had to make the output order of colliding paths in the error message deterministic. (Otherwise it would fail randomly). Also my colleague @DavyCats showed me some centaur tests were file outputs are tested. I used these as an example to also test for the outputs. ; All the behavior that this PR affects is now properly tested, which means that these tests should be able to discover regressions in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876:121,failure,failure,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876,4,"['error', 'failure']","['error', 'failure']"
Availability,"> How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?. I wish! Centaur only handles `200 OK` responses. This fix returns a `400 Bad Request`, quickly, instead of a timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402:77,error,errors,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318531402,1,['error'],['errors']
Availability,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:329,error,errors,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,2,['error'],"['error', 'errors']"
Availability,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,3,"['error', 'failure']","['error', 'failure']"
Availability,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:567,down,down,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746,1,['down'],['down']
Availability,"> I have tried Float memory_gb = 1.0 as the runtime attribute and ${""-l mem="" + memory_gb + ""GB""} as the submit string but this fails with qsub: Illegal attribute or resource value Resource_List.mem error. `Int memory = 1` is the equivalent of `Int memory_b = 1` and is generating values in **bytes**. A WDL specifying gigs of memory will therefore generate very large values, with 4GB generating the string `-l mem=4294967296""GB""`. If you navigate within the cromwell-executions directory and find the `submit*` files that contain the generated qsub command, you should see something like that. `cd` to the directory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; St",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:199,error,error,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,2,['error'],['error']
Availability,> I wasn't sure how the sbt magic you put in for the API docs worked. Pass the word on. Currently available both in the updated [wiki](https://github.com/broadinstitute/cromwell/wiki/DevZone#generating-a-markdown-document-of-the-swagger-yaml) and the more comprehensive mega doc-on-docs in our [team drive](https://drive.google.com/drive/u/1/folders/0By3wA7o30lk3VmF3aldBNkEzaDg). EDIT: Maybe we can leave a link at the top of the cromwell.yaml?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690:98,avail,available,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690,1,['avail'],['available']
Availability,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1005,error,error-prone,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936,1,['error'],['error-prone']
Availability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:653,error,error,653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['error'],['error']
Availability,"> I'm trying to figure out how to get [travis?] to redo the travis build. Yeah, PRs 1333 and this 1334 currently have the same git hash. Try updating to a new git hash by ""touching"" the commit, and then force pushing:. ``` bash; git checkout jg_haircut_for_testkitspec && \; git commit --amend -C HEAD --date=now && \; git push -f origin HEAD; ```. NOTE: Add `-q` after each `git <command>` to quiet down the output.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156:400,down,down,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242259156,1,['down'],['down']
Availability,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:225,avail,available,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973,4,['avail'],['available']
Availability,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,recover,recovery,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208,1,['recover'],['recovery']
Availability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:153,error,error,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['error'],['error']
Availability,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing 📖 (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:816,avail,available,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755,1,['avail'],['available']
Availability,> Just curious if putting a sleep [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L341) or [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L198) are viable solutions until something more generic in the core of Cromwell can be implemented. Any update and interim solution available? I am curious if there is an alternative like python aiohttp concurrent requests number limit rather than sleep.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911:555,avail,available,555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911,1,['avail'],['available']
Availability,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:104,failure,failure,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921,3,"['error', 'failure']","['errors', 'failure']"
Availability,"> Looks very promising! 😄; > ; > I would ask that all commented out code be removed in the final version of the PR (if it's part of work-in-progress that's totally fine).; > ; > I applied these changes to a branch in the Cromwell repo and ran our Centaur CI. There were several test failures, logs are available here: https://travis-ci.com/broadinstitute/cromwell/jobs/202934788. Thanks for your review!; In this PR, BCS started to use the runtime `docker` which only supports AlibabaCloud Container Registry, so in the failed cases, docker image `ubuntu:latest` from dockerhub is not supported. In order to fix it, we need to provide your test account in us-east-1 region a new ECS image which contains a local `ubuntu:latest` docker image, and next week will be ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732:283,failure,failures,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-497553732,2,"['avail', 'failure']","['available', 'failures']"
Availability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:138,reliab,reliability,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"> OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario. Actually, that work has already been done and merged into develop. The goal of this ticket was to verify that 1.000.000 rows chosen as default limit is a sane choice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679:130,resilien,resilient,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679,1,['resilien'],['resilient']
Availability,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,redundant,redundant,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598,1,['redundant'],['redundant']
Availability,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655,2,['error'],['error']
Availability,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:200,failure,failure,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,1,['failure'],['failure']
Availability,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:282,error,error,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732,1,['error'],['error']
Availability,"> Thanks for the report, we'll check it out.; > ; > It seems that something about running inside Cromwell causes the tool to attempt allocating `18446744073709550532 bytes`, or 18.45 exabytes!. Hi，i also got this problem while running the gmap on Linux shell ,it shows that is need such memory.; Failed attempt to alloc 18446744073709551600 bytes; Exception: Allocation Failed raised at indexdb.c:2886; Segmentation fault; .Could you please tell me how you solve this problem ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-598648632:416,fault,fault,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-598648632,1,['fault'],['fault']
Availability,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:97,avail,available,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140,2,['avail'],['available']
Availability,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:112,error,errors,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,2,['error'],['errors']
Availability,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:251,down,down-featured,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500,2,['down'],['down-featured']
Availability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:1114,rollback,rollbackException,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,3,"['error', 'rollback']","['error', 'rollbackException']"
Availability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:707,error,error,707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,1,['error'],['error']
Availability,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:977,down,down,977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['down'],['down']
Availability,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:358,repair,repair,358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,1,['repair'],['repair']
Availability,"> We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?. Interesting, I'm running a local build from current `develop` and I seem to have the code you've linked above, but I don't see either the `""cromwell-workflow-id""` or `""goog-batch-worker""` labels on my task logs 🤔. . In `.labels` I have `hostname`, `job_uid`, `task_group_name`, and `task_id` keys.; In `.resource.labels` I have `job_id`, `location`, and `resource_container` keys. For the proposed additional labels, with respect to `GcpBatchRequestFactoryImpl#createAllocationPolicy`:. - Root workflow id is in `data.createParameters.jobDescriptor.workflowDescriptor.rootWorkflowId`.; - Everything else is in the `BackendJobDescriptorKey` via `data.createParameters.jobDescriptor.key`:; - task name in `call.identifier.localName` (I think); - shard in `index`; - attempt in `attempt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046:307,avail,available,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046,1,['avail'],['available']
Availability,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978,1,['error'],['error']
Availability,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:328,ERROR,ERROR,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"> Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. For the ""docker pull failed during a very large scatter"" case, retries are pretty much always successful (except when the image actually doesn't exist of course). Would it be reasonable for this to be handled, especially since this isn't a rare failure when scattering more than 300x?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263:91,error,errors,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263,2,"['error', 'failure']","['errors', 'failure']"
Availability,"> every backend should have the option to return one. :+1: . > Add “reason(s) for failure” to database, metadata. :+1:, but happy if storing strings waits for a future ticket",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184756416:82,failure,failure,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184756416,1,['failure'],['failure']
Availability,"> it is trying to resubmit jobs to the local engine. Do you mean jobs that were running when you stopped Cromwell were ""restarted"" on the local backend ?; Or new downstream jobs for the same workflow were then submitted to the local backend ?; Or both ?. I imagine you did not change your configuration in between the stop/start ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539:162,down,downstream,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539,1,['down'],['downstream']
Availability,"> it seems like we would want to continue seeing it; @aednichols; The bug in Life Sci API is that the ssh server is supposed to be disabled on the VM, but in some cases it is not, causing the `address already in use` problem. Since the ssh server is not disabled, ssh access to the VM is in fact possible. The error then becomes meaningless: the dockerized ssh server is unrelated to the wdl workflow, and users can still ssh to the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597:310,error,error,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597,1,['error'],['error']
Availability,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:298,alive,alive,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929,5,"['alive', 'down', 'ping']","['alive', 'down', 'ping']"
Availability,> not sure why this was showing all green with only one review 🤔. PullApprove audits are always available via the `code-review/pullapprove` [Details](https://pullapprove.com/broadinstitute/cromwell/pull-request/3691/) links. In this case the change fell into `groups.one_reviewer` because of `groups.two_reviewers.conditions.files.exclude: centaur/*`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770:96,avail,available,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770,1,['avail'],['available']
Availability,> pinning to a known good version is a good idea regardless. Agreed and I'd even say that embedding the cwltool executable in Cromwell is what we should do. Or whatever makes sense such that when I download the latest Cromwell release and try to run a CWL I don't get an error because I don't have cwltool or I have the wrong version.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263:198,down,download,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263,2,"['down', 'error']","['download', 'error']"
Availability,">Does ""5xx HTTP Status Code"" literally appear in real life instead of the actual 500-511 error codes?. No but I think the idea is to make sure the regex that checks for ""5"" matches ""500"" but not ""5 apples""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876,1,['error'],['error']
Availability,>Merging despite the slurm test failure because:. I have considered and endorse this decision.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719:32,failure,failure,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719,1,['failure'],['failure']
Availability,">This change appears to validate the format of the disk requirements but then do nothing with the actual values? Is that correct?. AWS has auto-sizing and auto-expanding disks, so the concept of specifying a disk size is not applicable in this universe. This PR lets Cromwell ignore everything after `local-disk` instead of issuing an error. >Can we update the test cases which now work? I suspect custom_mount_point at least could be re-enabled?. `custom_mount_point` is not on the excluded list in `testCentaurAws.sh`. Are you requesting new coverage by adding `awsbatch` to the backends for `custom_mount_point.test`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441:335,error,error,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441,1,['error'],['error']
Availability,"@AlexMTX @rhpvorderman Am I able to specify the following in a task using draft-2?; ```; meta {; volatile: ""true""; } ; ```. I found that call-caching still occurred for the task with this volatile: ""true"" included in the meta. Is this only available for version 1.0 or am I misunderstanding your issue Alex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651:240,avail,available,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651,1,['avail'],['available']
Availability,"@DavyCats thank you for the report. Would you mind posting the WDL that you're using to induce the error? (Or if it contains sensitive information, a minimal repro case.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927#issuecomment-407519270:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927#issuecomment-407519270,1,['error'],['error']
Availability,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:64,alive,alive,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['alive'],['alive']
Availability,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:558,alive,alive,558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['alive'],['alive']
Availability,"@EvanTheB `reference.conf` is the configuration file used by Cromwell.; > Also is there any way to actually enumerate all the available settings?. I am not sure how we can do that. But you are right, it might be a lot of effort to restructure/modify those files to be able to enumerate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687,1,['avail'],['available']
Availability,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['error'],['error']
Availability,"@Horneth - thanks for the information - this works. . So I had organized my WDL tasks into folders just for organizational purposes (just like you had posted above). Sorry if you have answered this before, but is this going to be supported down the road? Or should is it always going to be recommended to have all WDL tasks are in one folder at the same level? . Thanks again for getting back to me so quickly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-374289503:240,down,down,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-374289503,1,['down'],['down']
Availability,"@Horneth @patmagee I'm going to have to enable continue on failures, except I am not sure that is viable.; @Horneth Can I tell cromwell to continue even on these failures? These are not non-zero return codes, right?; ; I actually can't just rerun the jobs and expect any progress. Due to the high rate of these failures and that I scatter over quite a few jobs, nothing gets to complete before one job fails. If I were to make my scatter narrower, I'd probably still get failures due, since each job would take substantially longer. . In other words, I can't proceed unless there is a viable ""continue on failure"" option (or a failure in my reasoning above). And even then, I am likely to proceed slowly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728:59,failure,failures,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300171728,6,['failure'],"['failure', 'failures']"
Availability,@Horneth I came here to update the latest failure and noticed your comment. Considering that @kshakir noted Liquibase weirdness in #4320 perhaps these are related in some way?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823,1,['failure'],['failure']
Availability,"@Horneth I don't believe this has been improved any further, is that true? Do you have any sense of how often users encounter this error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329661768,1,['error'],['error']
Availability,"@Horneth I don't think so, although I'm pretty sure what would work would be to add a case similar to the one on 481, but catching `UnsuccessfulRunStatus` w/ the isPreemptible if. I didn't originally go down that road because at the time I thought it wouldn't work, but I misunderstood how things were working and I'd forgotten about it by the end. I'm going to give that a whirl as it'd be a lot easier than digging into the op metadata",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358785007:203,down,down,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358785007,1,['down'],['down']
Availability,@Horneth I have run into the disk full error several times and the workflow appropriately fails,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2054#issuecomment-323150716,1,['error'],['error']
Availability,"@Horneth I just noticed - this is going to try forever, right? What happens if this error is being thrown appropriately?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4272#issuecomment-431714560:84,error,error,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4272#issuecomment-431714560,1,['error'],['error']
Availability,"@Horneth I pruned my number of jobs dramatically.... I am now down to <5k. And I am still getting this error and very slow call caching. @katevoss Can we tell google, if the crashes are on their end?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460:62,down,down,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298733460,2,"['down', 'error']","['down', 'error']"
Availability,"@Horneth I think you're right that most reasonable execution backends should have _some_ notion which could be viewed as a numeric return code. Perhaps ""return code"" isn't the right term there, maybe something like ""error code"" would be more apropos. Pinging @mcovarr @gauravs90 and @francares here in case they have thoughts on the matter.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184752207:216,error,error,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184752207,2,"['Ping', 'error']","['Pinging', 'error']"
Availability,@Horneth I totally missed your message here. It was on the methods cromwell 30 instance. I'm not sure how to find logs. My guess is that if they're produced by default they're still available somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854:182,avail,available,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854,1,['avail'],['available']
Availability,"@Horneth I tried it with call-caching based on file path only, and while it was significantly faster than it used to be, it still got bogged down. I restarted it with call-caching turned off completely, and then things seemed to work quite well. . It's running over 1000 jobs with no ""Cromwell Overhead"" in the timing diagram. So I would say this works when call-caching is turned off, but with call-caching it is still slow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596:141,down,down,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289036596,1,['down'],['down']
Availability,@Horneth Just pinging to see what the status is on this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284:14,ping,pinging,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-231509284,1,['ping'],['pinging']
Availability,@Horneth Out of curiosity has this been addressed. We are using v28.2 and seem to get a lot of these errors when doing `size` lookups,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348286795:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348286795,1,['error'],['errors']
Availability,"@Horneth Per our discussion this morning, it was already a blocking call, this just makes retrieving the return code more robust in case of disconnect.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3411#issuecomment-373500372:122,robust,robust,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3411#issuecomment-373500372,1,['robust'],['robust']
Availability,"@Horneth Thanks for letting me know. It would be great if this was prioritized, we are unable to fully do call caching at the moment, so having 503 errors can get quite expensive for us. Are there any config properties which you know of that might help with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128:148,error,errors,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-348720128,1,['error'],['errors']
Availability,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:205,fault,fault,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,4,"['down', 'error', 'fault', 'toler']","['down', 'error', 'fault', 'tolerance']"
Availability,"@Horneth What, if anything, are the downsides of this change?. IOW is this creating a tradeoff where we're deciding this improvement is overall better than the negative change elsewhere, or is this purely positive?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422898224:36,down,downsides,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422898224,1,['down'],['downsides']
Availability,"@Horneth although we're already mapping something else to preemptible in the spot where i'm doing it now, so this would mean we're mapping ""other"" errors to preemptible in 2 different spots",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358788264:147,error,errors,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3162#issuecomment-358788264,1,['error'],['errors']
Availability,@Horneth and @mcovarr can you review when you get a chance? This should now fix the Tyburn failures on the `develop` branch,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191825712:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191825712,1,['failure'],['failures']
Availability,@Horneth available for review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/238#issuecomment-148163666:9,avail,available,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/238#issuecomment-148163666,1,['avail'],['available']
Availability,"@Horneth check out DSDEEPB-2876. Having thought about this and having seen that new issue in our backlog (Add “reason(s) for failure” to database, metadata) I think this would be better reported as a stringly-typed ""reason for failure"" error message rather than an explicit ""backend return code"" field. It would then be the job of the backend to convert its ""backend error code"" into a useable and sensible message to report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747:125,failure,failure,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747,4,"['error', 'failure']","['error', 'failure']"
Availability,"@Horneth my suggestion for this PR (happy to be overruled - @geoffjentry @kcibul) would be to get the return code being ONLY the return code. And forget the ""backend return code"" entirely for now until that upcoming ticket (maybe enhance that other ticket to add ""JES return codes appear in failure message"" as another AC?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682:291,failure,failure,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682,1,['failure'],['failure']
Availability,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:93,avail,available,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010,2,['avail'],['available']
Availability,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:117,error,error,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,1,['error'],['error']
Availability,"@LeeTL1220 @Horneth I ran into similar issues when running > 400 Tasks Simultaneously. I would occasionally get 403 from the JES backend from various causes; - size built in function would timeout ; - Pulling the docker image from gcr.io would time out; - occasionally pulling the docker image from docker hub would also time out; - I also observed the above error that you were experiencing as well. I was not able to debug any of them, because of the transient nature. Rerunning the workflow generally fixed the problem. I am also using preemptible instances for the majority of the tasks that were being run, however I do not see how that could be contributing to the issue. If anything i would guess that we are bumping into an api quota and are being throttled by google leading to the timeouts",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322:359,error,error,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300167322,1,['error'],['error']
Availability,"@LeeTL1220 @patmagee I'd been paying less attention to this than I should have. I mentioned to @LeeTL1220 yesterday that the general issue of ""sometimes logs fail to upload and it doesn't retry a ton - but Google knows this"". However looking at the frequency and the actual errors makes me think I need to ping Google to make sure they know about the errors themselves. Perhaps there's something more fundamentally wrong going on that they're blissfully unaware of.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858:274,error,errors,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-300186858,3,"['error', 'ping']","['errors', 'ping']"
Availability,@LeeTL1220 Do you still encounter this error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215#issuecomment-396068773,1,['error'],['error']
Availability,"@LeeTL1220 Does the error have the effect on the actual final status of the workflow ? Or does it cause cromwell to exit with a non 0 exit code ? I wasn't able to reproduce the exact same error but I've had similar ones and I've got a branch that should fix it, if you want to try it out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429,2,['error'],['error']
Availability,"@LeeTL1220 I just ran the following WDL against `PAPI` and `Local` it worked for both, could you confirm that it fails against SGE?. ```; task foo {; Int? mem; Int final_mem = select_first([mem, 3]). command {; echo hello world; }. runtime {; memory: final_mem + "" GB""; docker: ""ubuntu:latest""; }. output {; Int five = 5; }; }. workflow bar {; call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2643#issuecomment-330975479:211,echo,echo,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2643#issuecomment-330975479,1,['echo'],['echo']
Availability,"@LeeTL1220 I tried this task below and although it's not the same exact type of syntax as what you used, I was able to use a task input variable inside of my glob: . `task createFileArray {; String dir = ""out""; command <<<; mkdir ${dir}; echo ""hullo"" > ${dir}/hello.txt; echo ""buh-bye"" > ${dir}/ciao.txt; sleep 2; >>>; output {; Array[File] out = glob(""out/*.txt"") ; Array[File] out2 = glob(dir + ""/*.txt""); #Array[File] out3 = glob(""${out}/*.txt""); }; runtime {docker:""ubuntu:latest""}; }`; out and out2 are valid task outputs, but not out3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267175875,2,['echo'],['echo']
Availability,@LeeTL1220 I'm seeing the same log copying failure in our test suite actually. So it very likely isn't your doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648:43,failure,failure,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-299019648,1,['failure'],['failure']
Availability,"@LeeTL1220 I'm still on this, not @ruchim. A couple days ago you asked what I changed to get the workflow to run for success and exit cleanly. I was having problems even getting the workflow to reach success with the given hash `cromwell-23-79f6e12-SNAPSHOT.jar`, as these outputs with spaces were causing errors:. ```; File purity_series_small_amp = ""purity_plots/purity_series_small_Small Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_small_Small Deletions.png""; Array[File] purity_files = glob(""purity_plots/*""); ```. I've been investigating whether this failure is a problem with `glob()`s, or just the `File`s with spaces. . FYI swapping out the three elements for another set of files ""random"" files allowed the workflow to succeed, and cromwell exited cleanly in single workflow mode. ```; # hacked paths to allow downstream calls to still run; File purity_series_small_amp = ""purity_plots/purity_series_Amplifications.png""; File purity_series_small_del = ""purity_plots/purity_series_Deletions.png""; Array[File] purity_files = glob(""purity_plots/purity_series_*.png""); ```. If there's another hash besides `79f6e12` that causes cromwell to run with spaces, _finish successfully_, and then, let me know. **TL;DR I can either get `79f6e12` with the workflow to succeed & exit, or fail-- but not succeed and lock up.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116:306,error,errors,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-262309116,3,"['down', 'error', 'failure']","['downstream', 'errors', 'failure']"
Availability,@LeeTL1220 I'm unable to reproduce the error you saw. Have you seen the same issue with more recent versions of Cromwell? What version were you using when you saw the initial error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298,2,['error'],['error']
Availability,"@LeeTL1220 so I haven't been able to get cromwell to exit with a non 0 exit code, although I do get a bunch of log errors like yours. I created a branch that shuts the system down in a cleaner way, which I believe is the reason for this error : `cromwell-2079`.; If you can test it out and see if it makes any difference that would be very useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542:115,error,errors,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-290824542,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"@LeeTL1220 we have made number of improvements with transient errors in JES, Cromwell (in version 26) will now retry transient errors automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291589871,2,['error'],['errors']
Availability,"@LeeTL1220 when you get the ""file not found"" errors, does it fail the workflow? Or does it still continue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991,1,['error'],['errors']
Availability,@MartonKN This is almost certainly not a real error but rather some annoying/alarming yet harmless Cromwell messages. Other than this does it appear that your workflow successfully completed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613,1,['error'],['error']
Availability,@MartonKN this error should have been addressed in Cromwell v33 -- please reopen this issue if needed. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-402242786:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-402242786,1,['error'],['error']
Availability,@SHuang-Broad Thank you for reporting this. All these execution errors seem to indicate silent copying failures. Have you always seen this percentage of cp style failures or this has been recent?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424945872:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424945872,3,"['error', 'failure']","['errors', 'failures']"
Availability,"@TMiguelT . I separated out the part that is failing into a separate WDL and tried running just that WDL with different inputs, it all failed with the same error message. The WDL is attached so that you can just run to see the error. [cromwell_4356.zip](https://github.com/broadinstitute/cromwell/files/2555124/cromwell_4356.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756,2,['error'],['error']
Availability,"@TMiguelT . The error is happening at `FilterByOrientationBias` stage. Not the `CollectSequencingArtifactMetrics` stage. `CollectSequencingArtifactMetrics` is the previous task where the input file is coming from (`FilterByOrientationBias` uses output files from `CollectSequencingArtifactMetrics` stage) I'm re-pasting the whole error ( as I previously split it out into two) for clarification. ```; [2018-11-02 17:24:33,42] [info] AwsBatchAsyncBackendJobExecutionActor [1651349bSomaticSNVInDel.FilterByOrientationBias:1:1]: gatk FilterByOrientationBias \; \; -V /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,2,['error'],['error']
Availability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:1599,error,error,1599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,3,"['Error', 'error']","['Error', 'error']"
Availability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:305,error,error,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['error'],['error']
Availability,"@TMiguelT I've moved the singularity cache section back down. Hopefully this is all the comments actioned. Sorry @vsoch, I realised I've been resolving your comments as I actioned them, but I probably should've left them open as they're from your review. Also sorry for what was probably a lot of email spam over the past few hours.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558:56,down,down,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465407558,1,['down'],['down']
Availability,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1878,echo,echo,1878,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,4,"['alive', 'echo', 'error']","['alive', 'echo', 'error']"
Availability,"@Xophmeister Ultimately this is going to need changes to the WDL spec and thus a conversation over in the [OpenWDL group](www.openwdl.org), although your issue does start entering the murky grey area between WDL and Cromwell (which admittedly exists due to WDL's heritage as originally being a product of the Cromwell team). You're able to reference values in `runtime` because the WDL spec allows it. However, there's no notion in WDL of `default_runtime_attributes`, that's a purely Cromwell concept. Thus if Cromwell were to allow this, it'd encourage WDLs which would fail elsewhere. . My own $0.02 is that `runtime` is horribly broken and needs to be rebuilt from the ground up (again, over in OpenWDL, not here). Since history has shown that there's always some path to fixing `runtime` which **doesn't** require my burn it all down approach a good first step might be to suggest an official system of defaults over there. I'm going to close this issue as I don't believe there's anything reasonable we can do from the Cromwell side. If, given the context I provided above, you disagree (e.g. perhaps I misunderstood the ask) let me know and we might reopen it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741#issuecomment-472412608:834,down,down,834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741#issuecomment-472412608,1,['down'],['down']
Availability,@adamstruck That test tries to run a Docker image which has a default user which is not root (the test was created based on [this ticket](https://github.com/broadinstitute/cromwell/issues/472)). That error looks like the non-root user in the container doesn't have execute permission on the script that was created by Cromwell running as a different user.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503:200,error,error,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279848503,1,['error'],['error']
Availability,"@aednichols All the removals of the `new Integer(n)` calls are changes I made ""while I was in there"" to silence warnings. What with the other errors, I lost track of my intention to check in on whether it was reasonable to change that stuff. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796:142,error,errors,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796,1,['error'],['errors']
Availability,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:481,down,download,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,1,['down'],['download']
Availability,"@aednichols I want to reopen, because Map[String, MyStruct] also crashes and because if you claim it is about types than the error should appear when I validate with latest womtool and not in a runtime!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464240925,1,['error'],['error']
Availability,"@aednichols Thanks for the new invitation. I did accept it, but when I try to push a branch to this repo I get error 403.; Am I doing something wrong?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424:111,error,error,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424,1,['error'],['error']
Availability,"@aednichols and @rsasch - yes, `Integer.MIN_VALUE` is some huge negative value. I tested with a fetchSize of ""1"" and got the same out of memory errors as when it was 1000. I don't know whether `Integer.MIN_VALUE` is a special sentinel value or any value below 0 would do.... but since it works, I'm inclined to treat it as a magic number, and document it as such.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136:144,error,errors,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136,1,['error'],['errors']
Availability,@aednichols are the Travis CI failures concerning?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754,1,['failure'],['failures']
Availability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,4,['error'],['error']
Availability,"@aednichols thanks for a detailed look into this. With the latest dev branch, and with the commit that contains the fix, I'm getting the below error. I can still run unpacked without error. ```; (p3) [jeremiah@localhost fail_cromwell]$ /usr/lib/jvm/java-11-openjdk/bin/java -Dconfig.file=/home/jeremiah/code/fresh/really/cromwell/cromwell.example.backends/cromwell.examples.conf --illegal-access=warn -jar /home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar run test_wf_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0; [2019-04-18 17:19:09,95] [info] Running with database db.url = jdbc:hsqldb:mem:39c64473-526e-47d6-a015-f9193a0fd4f4;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:17,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-04-18 17:19:17,78] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-04-18 17:19:17,92] [info] Running with database db.url = jdbc:hsqldb:mem:58f8cd7c-3e36-430d-b36a-1620b0333e3e;shutdown=false;hsqldb.tx=mvcc; [2019-04-18 17:19:18,65] [info] Slf4jLogger started; [2019-04-18 17:19:18,79] [info] Pre Processing Workflow...; [2019-04-18 17:19:19,12] [info] Pre-Processing file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl; WARNING: Illegal reflective access by org.python.core.PySystemState (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.io.Console.encoding(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method sun.nio.ch.SelChImpl.getFD(); WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.FileChannelImpl.fd; WARNING: Illegal reflective access by jnr.posix.JavaLibCHelper (file:/home",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:143,error,error,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,2,['error'],['error']
Availability,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:92,down,downstream,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481,1,['down'],['downstream']
Availability,"@alexfrieden - I just ran this successfully. With your first error, the fact that you are getting an HTTP response indicates that it's not necessarily a networking issue on the AWS side, and as @Horneth stated something going on with DockerHub. For debugging in that scenario, I would try launching a t2.micro based on the CustomAMI you created with the same Batch instance profile in the VPC used by the Batch compute environment and try `docker pull <image>`. As for your second issue, did your tasks eventually transition from RUNNABLE? I've noticed that sometimes it takes about 5-10min for the Batch scheduler to ""re-warm"" after scaling down instances that were used for previous tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443,2,"['down', 'error']","['down', 'error']"
Availability,@andy7i Those are FireCloud errors because of the resulting missing status. GAWB-1645 has been opened to be more resilient on our side (and thus make those errors go away); this ticket is for Cromwell to deal with why the statuses are missing in the first place.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976:28,error,errors,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501976,3,"['error', 'resilien']","['errors', 'resilient']"
Availability,@andy7i pinging you as this is also a great perfomance test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397:8,ping,pinging,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3712#issuecomment-393634397,1,['ping'],['pinging']
Availability,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:30,ping,ping,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773,2,['ping'],['ping']
Availability,"@antonkulaga -. 1. If you use read_json the type you get will actually be a WDL `Object`. You should be able to coerce to map but only if the values are all the right type. Also look out for better `object` support with draft 3's `struct`s. 2. you're right, some kind of list comprehension (equivalent to a scala map function) has certainly come up before (they'd need to be spec changes in openWDL, but I'd certainly be happy to see those sorts of PR popping up). 3. for values available outside scatters - it's just like call outputs - we add implicit gatherers for Declarations too. The syntax is ugly but hopefully it works for you until we get some sugar around it in the spec via openwdl 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367483310:479,avail,available,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367483310,1,['avail'],['available']
Availability,"@antonkulaga We always welcome PRs! It's not obligated though, even if you use BioWDL :wink: . On-topic: Yes, I think Cromwell could modify the input folder and its contents to be read-only . But that might have some unforeseen consequences down the line. This would need to be tested.; DISCLAIMER: I am not of the cromwell team. So I will not implement this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438:241,down,down,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438,1,['down'],['down']
Availability,"@bfoster-lbl - An example config with the parameters you highlighted is available [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#cromwell-server). That said, I agree that the tutorial in cromwell.readthedocs.io should match.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509:72,avail,available,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278#issuecomment-435545509,1,['avail'],['available']
Availability,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999,1,['error'],['error']
Availability,"@chapmanb Somatic completed successfully by bumping the memory (I doubled it to 8GB) :); I have another question about the rnaseq pipeline if you don't mind.; I'm hitting this error on the `pipeline_summary` task:. ```; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/cyvcf2/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/loca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['error'],['error']
Availability,"@cjllanwarne - Here goes... This works:; ```; workflow wf {; call tsk {; input: foo=""Hello"", bar=""World""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```; This doesn't:; ```; workflow wf {; call tsk {; input: foo=""Hello""; }; }; task tsk {; String foo; String? bar; command {; echo ""${foo} ${""here comes bar"" + bar}""; }; }; ```. When `bar` is left out, job stays in the running state, and exceptions are continually thrown in the server logs.; The wdl validates fine.; Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981:160,echo,echo,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830#issuecomment-272077981,2,['echo'],['echo']
Availability,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,redundant,redundant,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517,1,['redundant'],['redundant']
Availability,"@cjllanwarne @Horneth I'll just add that we're going to need to start slimming down WorkflowDescriptor and removing its coupling to Backend, so while if it's the only way to do something for now that's one thing but if it's a matter of how one skins the proverbial cat going another pat would be good",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873:79,down,down,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170674873,1,['down'],['down']
Availability,"@cjllanwarne @aednichols Looks like the tests didn’t complete with the same error, I gave merging from develop another crack without success. Hopefully it’s all good though 😬",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203,1,['error'],['error']
Availability,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,redundant,redundant,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492,2,['redundant'],['redundant']
Availability,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:88,error,errors,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961,2,['error'],"['error', 'errors']"
Availability,"@cjllanwarne Checkout out #199, a PR into this PR. It refactors `DataAccess`, `Backend`, and `BackendType` around a bit such that the high level workflow manager actor can pass in its data access instance to the backend, OR the various test suites can keep using separate data access instances. . The problem with ""data_access_singleton"" is that the singleton data access seemingly cannot handle the onslaught of our multi-threaded tests. One of our many thread pools around the database seemed to then start returning uncaught(?) errors. Definitely showed some warts in our non-existent load testing... Take a look, decide what you want to keep or jettison, but I do believe that a new database pool / data access should **NOT** be created for each JES `Run`. Otherwise, this branch looks good to go for merge. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894:531,error,errors,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143000894,1,['error'],['errors']
Availability,"@cjllanwarne Does this only require handling the retryable failure in the WorkflowExecutionActor, by re-running the failed call up to a `max` number of retries? Your previous comment somehow indicates this is perhaps more than this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-216631299:59,failure,failure,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-216631299,1,['failure'],['failure']
Availability,@cjllanwarne I can't find it now (perhaps it has since been closed as a won't fix) but this has come up before. If you're open to trying to track it down close one as a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287:149,down,down,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300800287,1,['down'],['down']
Availability,"@cjllanwarne I considered doing this at first and I think I actually tried to implement it, but there was a fundamental inconsistency that I couldn't figure out how to resolve. It might have had to do with going from the `WomLong` type back to a WDL type, but I hit so many errors along the way that I may be mixing them up. The current implementation is what I eventually settled on as the least invasive (not necessarily the most elegant). If the current approach is a deal-breaker I can take another look, but I suspect the blast radius will be larger no matter what. My hope was that CWL users wouldn't actually notice or care about the underlying JVM type - is there a use case for large integer arrays in Cromwell?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335:274,error,errors,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335,1,['error'],['errors']
Availability,"@cjllanwarne I disagree. IMO officially supported backends should be provided by the downloadable fat jar, we should not require a user to need to download multiple jars if all they want to do is fart around w/ the officially supported stuff.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208423531:85,down,downloadable,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208423531,2,['down'],"['download', 'downloadable']"
Availability,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707,1,['failure'],['failure']
Availability,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:31,error,errors,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015,3,"['down', 'error']","['down', 'errors']"
Availability,"@cjllanwarne I know we've added interpolation for certain instances, is this still relevant? I can add this to our list of doc things to fix, but I agree that adding an error/warning would be good too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219:169,error,error,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2060#issuecomment-329487219,1,['error'],['error']
Availability,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584,1,['error'],['error']
Availability,"@cjllanwarne I'd prefer to see it talking to a single point (currently a single VA, as @Horneth & I discussed, if performance, reliability or complexity call for it, to be morphed into something else). I don't care that much if it happens here, but it's not an onerous change to make on this PR. I _will_ care once something else is talking to a VA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254:127,reliab,reliability,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195440254,1,['reliab'],['reliability']
Availability,@cjllanwarne I'm going to go out on a limb and say that the test failure had nothing to do with this change. Just a guess.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133:65,failure,failure,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212408133,1,['failure'],['failure']
Availability,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:395,down,down,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066,2,['down'],['down']
Availability,"@cjllanwarne I've run into this issue also, while I can work around it currently, it will likely become an issue down the road when trying to build workflows comprising of scatter operations when you want to collect up the results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4538#issuecomment-482190924:113,down,down,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538#issuecomment-482190924,1,['down'],['down']
Availability,"@cjllanwarne Indeed different containers might have different requirements, as it will depend on the container what mount points are available. I would like to point out, however, that you can already define this per task using a custom runtime attribute. For example, in my config I could put something like:; ```; runtime-attributes= """"""; String? docker; String? dockerMountPoint = ""/data""; """"""; dockerRoot = ""${dockerMountPoint}""; ```. EDIT: Hmm, nevermind, looks like that wouldn't work. In the submission command, it would, but you'll just end up with `${dockerMountPoint}` in the execution script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684:133,avail,available,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684,1,['avail'],['available']
Availability,"@cjllanwarne Initially the concern was that I've heard, well, concern from firecloud folks here and there about frivolous metadata migrations on our part as they cause downtime they don't always want to incur. It also ties in to my rapidly increasing sentiment that we should not be migrating a supposedly immutable data store for what are merely cosmetic changes - if we have to in order to fix something we know is causing issues w/ programatic clients you gotta do what you gotta do. When we talked with @abaumann he indicated that a) they wouldn't really care if the old data stayed as-is and b) made a similar point as I did about the event store. @Horneth with all of these taken together I don't think we should be migrating. The key constituency has said that it would be at best superfluous and IMO it's part of a habit we should be getting out of anyways.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322329894:168,downtime,downtime,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2546#issuecomment-322329894,1,['downtime'],['downtime']
Availability,"@cjllanwarne Looking into the WorkflowActor I found this comment . ```; // The assumption for restart is that the call will have been found in state Running. Explicitly; // re-writing the Starting status causes the new start date to be persisted, which is desirable to; // provide a realistic elapsed execution time.; ```. which seems contradictory with DSDEEPB-2127..; This PR doesn't change the current behavior. I found a bug indirectly related to this though where the resumable and non-resumable calls where not processed correctly leading to weird DB errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/449#issuecomment-184922009:557,error,errors,557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/449#issuecomment-184922009,1,['error'],['errors']
Availability,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744,1,['error'],['error']
Availability,@cjllanwarne Perhaps one is spinning down and the next one has already triggered?. @kcibul :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185928699:37,down,down,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185928699,1,['down'],['down']
Availability,"@cjllanwarne Really it's just that they were developed simultaneously, and it's probably my fault for not going back and cleaning the other one up. In retrospect I knew it existed, but probably just lost track of it. I certainly wouldn't disapprove of converting future-based logic in actors into a more actor-y solution but that's because IMO it's easier to reason about multiple actors (and their messages) than composed Futures. My stance isn't one which is universally held, however",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998:92,fault,fault,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998,1,['fault'],['fault']
Availability,"@cjllanwarne Thanks for looking into it! Our current use case is heavily relying on `""additionalQueryResultFields"": [""labels""]`, with this error we have to loop through all workflows and make X thousands requests each time :( And I assume JMUI is relying on `""additionalQueryResultFields"": ""parentWorkflowId""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-455263017:139,error,error,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-455263017,1,['error'],['error']
Availability,"@cjllanwarne Thanks for picking up those two errors, I fixed them. Should be all set now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235,1,['error'],['errors']
Availability,"@cjllanwarne Thanks for the explanation! Exactly right, this is what the script says: ; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp; cd /cromwell_root. echo ""Hello foobar!"" && exit 1; echo $? > job.rc.txt. @pgrosu The exit 1 was the purpose here - I was modifying the basic hello.wdl test, trying to do a lightweight test simulating the failure of the binary being called. Turns out, as @cjllanwarne explains, that exit 1 is not a good way to simulate that, because it defeats Cromwell's return-code monitoring. Here's a better test, which does work as expected: . task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && head nonexistent; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792:212,echo,echo,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792,4,"['echo', 'failure']","['echo', 'failure']"
Availability,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,2,['error'],['error']
Availability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:136,failure,failures,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,11,"['Error', 'error', 'failure', 'reliab']","['Error', 'error', 'errors', 'failure', 'failures', 'reliably']"
Availability,"@cjllanwarne Would it make sense to send a `0L` metric for `workflowArchiveTotalTimeFailureMetricPath` in [Success cases here](https://github.com/broadinstitute/cromwell/blob/80cfe3e4b653c5ab6f2f935c9b306b34cd4287c9/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataSchedulerActor.scala#L77-L99) so that the graph line can come back to 0 upon a success in the below graph? Currently after a failure metric, it stays at the x seconds and never comes back down. ![Screen Shot 2021-04-28 at 12 28 15 PM](https://user-images.githubusercontent.com/16748522/116439527-a757d000-a81d-11eb-8b2e-1d4238aab769.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501:421,failure,failure,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501,2,"['down', 'failure']","['down', 'failure']"
Availability,@cjllanwarne available for review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/237#issuecomment-148164236:13,avail,available,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/237#issuecomment-148164236,1,['avail'],['available']
Availability,@cjllanwarne do you think this was resolved by your improvements to JES transient errors?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959:82,error,errors,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960#issuecomment-291985959,1,['error'],['errors']
Availability,"@cjllanwarne got it, thanks. I did note that you said ""one class"" before commenting :) The belief **is** that those WFs are getting stuck and it's not just a dilation issue, so it'll be interesting to see if the failures dry up after this merges",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341726:212,failure,failures,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486#issuecomment-446341726,1,['failure'],['failures']
Availability,@cjllanwarne not having this consume the imports of a workflow has actually lead us to create our own service for describing workfklows as a json schema. having a JSON schema returned is actually quite practical since we can build some pretty UI's around it with existing tools. Is this still low down on the priority list?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4572#issuecomment-557277899:297,down,down,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4572#issuecomment-557277899,1,['down'],['down']
Availability,"@cjllanwarne note that what I just said in #1777 could be used against my disagreement although I think handling all the different bifurcations here would be trickier than the one over there. At the end of the day the number one priority is that a WDL should always produce the exact same result regardless of where it is run (possibly excepting docker, and that's part of what rubs me the wrong way about runtime attrs). The number two priority should be that a user can't possibly take down a server w/ a rogue wdl. . After that anything can be tunable in terms of how a server achieves the first one whilst not allowing the second one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397:488,down,down,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-268799397,1,['down'],['down']
Availability,"@cjllanwarne regarding testing the ""write to cache"" and ""read from cache""... that feature isn't available to this code, it's on another PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332:96,avail,available,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164577332,1,['avail'],['available']
Availability,"@cjllanwarne that wdl does look right. I was able to reproduce the error successfully using a similar wdl in Papiv2 backend:; ```; task random_words {. String dollar = ""$""; command <<<; cat << 'EOF' > random.txt; This should fail. ${dollar}{Hello} blah blah; EOF; >>>. runtime {; docker: ""ubuntu:latest""; }. output {; String text = read_string(""random.txt""); }; }. workflow read_random_words {; call random_words. output {; String random_text = random_words.text; }; }; ```. The error thrown by Cromwell is:; ```; ERROR - WorkflowManagerActor Workflow a8de1168-8b70-4d54-83f6-aa41adc8f87e failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'read_random_words.random_text' (reason 1 of 1): Evaluating random_words.text failed: key not found: random_words; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:523); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$2(list.scala:66); 	at cats.Eval$.advance(Eval.scala:271); 	at cats.Eval$.loop$1(Eval.scala:350); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:517); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4928#issuecomment-488786242:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4928#issuecomment-488786242,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"@cjllanwarne yes. This won't protect us from returning 40x for workflows that are not in the store that may have legitimately gone terminal, but at least this won't produce errors for workflows that are obviously OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494904639:173,error,errors,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494904639,1,['error'],['errors']
Availability,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:325,ERROR,ERROR,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073,4,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"@cjllanwarne, here is the PR. This is only for workflow definitions, and only for line numbers. I found that it is, as you were saying, hard to extract reliable information from Hermes for column numbers. I *would* like to get the entire extent in the source file covered by an AST. It was slow slog to updates the tests to correctly check line numbers. Let's start with this change, and see how it goes. . Thank you,; Ohad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117:152,reliab,reliable,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117,1,['reliab'],['reliable']
Availability,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:131,failure,failure,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926,1,['failure'],['failure']
Availability,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:108,avail,available,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111,2,['avail'],['available']
Availability,"@coreone Merge at will. If the database isn't updated, the new build will crash with an error containing the SQL that needs to be run. The paths to the scripts have also changed, so even if an old jenkins job tries to run the scripts manually, I suspect it would fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974:88,error,error,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974,1,['error'],['error']
Availability,"@cowmoo is there a way for Cromwell to tell from the result of running the ""begin execution"" command, or from the resulting failure, that the task should be retryable?. I like the idea of retrying certain things, but a blanket ""retry everything n times, regardless of problem"" is probably going to annoy more people than it helps (especially if they're paying for it directly in cloud compute!). One thing we previously considered was a ""retryOnStdoutRegex"" attribute, maybe something like:; ```; command {; # ...; }; runtime {; retryOnStdoutRegex: ""path not found:""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555:124,failure,failure,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-296215555,1,['failure'],['failure']
Availability,"@cpavanrun Your right here, but this can be improved. What can be done here is lower the number of parallel jobs submitted by cromwell. This depends really on the cluster. In our case I did a stress test with 10000 parallel jobs and it still acts fine. Only downside is that the log is getting spammed a bit but it still works like it should. Still in the past (on older hardware) the headnode could not deal with this number of jobs. If this is the case limiting the parallel jobs could be a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773:258,down,downside,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773,1,['down'],['downside']
Availability,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:125,robust,robust,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929,2,['robust'],['robust']
Availability,"@danbills OOC when was this screenshot taken ? Last timed I looked at the requester pays failures, I sampled a few and they were due to the alpine issue IIRC so I'd have expected them to stop happening by now.; Last one I see on develop from sentry was Sept 24th here: https://sentry.io/broad-institute/cromwell/issues/650196050/events/?query=ci_env_branch%3Adevelop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429867335:89,failure,failures,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429867335,1,['failure'],['failures']
Availability,@danbills can you explain more about when the error occurs? Do you have an idea of how much effort it would take to fix?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-332641113,1,['error'],['error']
Availability,"@danbills yes IMO the ""powers of 2"" you have here is the usual and expected form of exponential backoff. The other way is technically exponential but doesn't slow down nearly as quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403:163,down,down,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403,1,['down'],['down']
Availability,"@danxmoran I'm trying to recreate this - could you list the `womtool` version that you're using and a minimal WDL that reproduces the issue?. Note: I tried to recreate on `develop` using this WDL:; ```wdl; version 1.0. import ""not/a/file.wdl"" as oops. workflow foo {; call oops.not_a_thing; }; ```. And received an error message and an exit code of 1:; ```; Failed to import 'not/a/file.wdl' (reason 1 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/cromwell (without escaping Some([...]/cromwell))' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 2 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/bad_import (without escaping None)' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 3 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'not/a/file.wdl' relative to nothing; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176:315,error,error,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176,1,['error'],['error']
Availability,@davidbernick @hjfbynara would you please confirm update script has been run so that I can rule out pingdom/firewall issues?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659:100,ping,pingdom,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659,1,['ping'],['pingdom']
Availability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:76,down,down,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,2,"['down', 'echo']","['down', 'echo']"
Availability,"@delocalizer Any chance you still have your ""hacky non-async"" piece of code still? The original link you posted is no longer available, and it might be useful for @caross73. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581:125,avail,available,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360128581,1,['avail'],['available']
Availability,@delocalizer Thanks for tracking this down ! I'm working on a fix. From what you describe and what I've found out so far it looks like it would not work either even if the value is supplied. Have you hit that case as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047:38,down,down,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765#issuecomment-266057047,1,['down'],['down']
Availability,@delocalizer Yeah we changed this a while back to help w/ submission performance under load. Those synchronous checks were really bogging things downl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962:145,down,downl,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515#issuecomment-279579962,1,['down'],['downl']
Availability,"@dgtester Since @geoffjentry just brought me up to speed on the exciting changes coming down the line with #401 (as well as #413), it would probably make more sense to revisit some of these tests afterwards, if they are still pertinent at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501:88,down,down,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177692501,1,['down'],['down']
Availability,@dianekaplan could you run `gcloud alpha genomics operations describe operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU` and add in anything that looks like an error message or error code?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050,2,['error'],['error']
Availability,@dinvlad In my PR (#5023) the intention was to allow users to be able to interact with BQ from inside their WDL commands. With that in mind I believe what you're suggesting is that nothing untoward would happen unless they did this and their service account didn't have the corresponding permission set. Is that correct?. I still think it's worth testing to be sure but since it looks like we've added scopes before w/o issue I'm less fearful .... but IMO there's still a risk and we should make sure the risk is 0. Denis - it's on my list to poke at this but if you all don't want to wait for me and would like to validate success/failure please feel free to do so,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296:632,failure,failure,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502247296,1,['failure'],['failure']
Availability,"@dinvlad it was indeed a blind hunt! So in that sense, 204 permissions is not that much ... it's a pretty refined subset ;-) Previously I was running Cromwell with the `editor` role set, which likely has even more than 204 permissions. Without the `firebase.developAdmin` role, the only error I get is that the tasks start running, then they fail immediately, and the only thing you find in the logs is: `yyyy/mm/dd hh:mm:ss Starting container setup.`. In any case, I wanted to give an answer here to provide publicly available information to other users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333:287,error,error,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680292333,2,"['avail', 'error']","['available', 'error']"
Availability,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:292,error,error,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972,4,['error'],"['error', 'errors']"
Availability,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:122,error,errors,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531,1,['error'],['errors']
Availability,"@dshiga -- can you help me understand one piece of your request:; ``` We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way.```. Why exactly does pagination make it unreliable to find the last from a list?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417326254:214,reliab,reliably,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-417326254,1,['reliab'],['reliably']
Availability,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:401,error,errors,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,1,['error'],['errors']
Availability,"@dvoet I wondered if you all were overriding that elsewhere. Have you run into issues w/ the loss of number of inserted elements or do you all just not care about that ever?. The function in question (don't have it in front of me) was using `++=` but that's why I was wondering if perhaps there's something else about our slick code which counteracts this. My slick-fu is likely not strong enough, I'll probably need to rely on bigger guns next week. My guess is that this is the culprit. I was looking at the general query log (I've only been using mysql, not cloudsql) and all I saw were individual inserts, never a batch insert. I can't get jprofiler to work reliably running against a JVM on GCS vms (at least not from home) so wasn't even looking at that :). Another thought is that something upstream is actually calling our slick code per-item instead of per-collection but I don't think that's the case. It's at least something I can double check easily.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991:662,reliab,reliably,662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269846991,1,['reliab'],['reliably']
Availability,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,1,['error'],['error']
Availability,@elerch -- I got IntelliJ fired up and can step through to see if I can track down what is happening...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-397402360:78,down,down,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-397402360,1,['down'],['down']
Availability,@ernoc -- just pinging you again to see if theres movement. Thanks,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-440498404:15,ping,pinging,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-440498404,1,['ping'],['pinging']
Availability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:222,error,errors,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['error'],['errors']
Availability,"@ffinfo As I read it:. - If state is `None`, go to `Running` state; - If state is `Running`, the first thing we do is run `isAlive` (which I don't want to ever do!). That means that I have no way to opt-out of ever running `isAlive` (which is the thing I want before approving this PR). ---. What I was suggesting is (but there are many other ways):; - If I set `pollForAliveness: ""1 minute""` in the config file:; - Use `context.system.scheduler.scheduleOnce` to run an `isAlive` 1 minute in the future (completely separate from `pollStatus`).; - If that `isAlive` is true, schedule again another 1 minute in the future; - If not, record the time at which the job was not alive; - `pollStatus` continues on a different schedule:; - If the job is no longer alive, the `pollStatus` switches to `WaitingForReturnCode`; - If a time limit is set for the `WaitingForReturnCode` state, honor it; - If I set `pollForAliveness: false` in the config file:; - Go straight to `WaitingForReturnCode`; - No time limits for `WaitingForReturnCode`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053:672,alive,alive,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053,2,['alive'],['alive']
Availability,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:285,alive,alive,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,3,['alive'],['alive']
Availability,"@francares @cjllanwarne I think the actual issue here is that the ""/tmp"" assertions on lines 260 and 261 always fail on Mac, regardless of whether Docker is available or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582:157,avail,available,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230058582,1,['avail'],['available']
Availability,"@francares Cool. My main concern there was that when i did the akka http conversion that I ""fixed"" it by giving bad results, so as long as the results look good I'll stand down my fretting :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631:172,down,down,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451631,1,['down'],['down']
Availability,"@francares Definitely - the key though is it'll still need to eventually be rebased on top of develop. There are more than one ways to skin this cat but one possibility is that when you're ready you could squash this down to one commit (or some other smaller number) and then rebase those on top of develop, handing the differences there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174601136:217,down,down,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174601136,1,['down'],['down']
Availability,"@francares I'm not saying the engine itself _needs_ to know (although I could envision a scenario where it'd be useful for it to know), I'm saying that the outside world needs to know information particular to the backend. That information needs to be stored somewhere and right now the only somewhere is in the DB which is accessed purely through engine. I see this PR as a transition point - it's against develop which does _not_ have pluggable backends but starts to remove the direct requirements (i.e. the backend specific tables). It's not the final state things will live in but it makes the ultimate changes smaller down the road.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090:624,down,down,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182531090,1,['down'],['down']
Availability,"@francares the result when run on a mac:. ```; ""#!/bin/sh; cd local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello; docker run -w /workingDir -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:/Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T:ro -v /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello:/outputDir --rm ubuntu/latest ; echo /Users/chrisl/IdeaProjects/cromwell/local-cromwell-executions/hello/73af18e0-8a0f-4e4b-b5e6-910694a58abb/call-hello/var/folders/c4/_dbcj9012gl6lbjsvd22_m21hj9ndk/T/testFile977065604273058878.out; echo $? > rc"" did not contain ""/tmp:"" (HtCondorJobExecutionActorSpec.scala:261); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075:654,echo,echo,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1113#issuecomment-230490075,2,['echo'],['echo']
Availability,"@freeseek `firebase.developAdmin` is a pretty wide role (with 204 permissions), so it's not surprising that it gives some permissions that are needed here. What would be helpful is if Google showed the exact permissions in their error messages, though from what it seems, that's not always the case. Then if you have a list of permissions, you can find minimal role(s) that encompass those permissions, rather than through a blind hunt (please correct me if it wasn't entirely blind here..). Btw @freeseek, from my limited experience, GitHub issues here are not often-looked-through, it might be better to create an internal JIRA ticket instead ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141,1,['error'],['error']
Availability,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422,1,['error'],['error']
Availability,"@gauravs90 @geoffjentry ; Re JES Backend - there's a _lot_ of hidden complexity in the JES backend that I think could quite easily end up being more than 5 days work to re-implement under another workflow runner. We currently have a Master branch which lets us run hundreds of Genomes-on-the-Cloud jobs concurrently in JES. If this merge goes ahead before the JES backend is made (and is as robust as it currently exists), we LOSE that ability. I don't think we should underestimate this task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137:391,robust,robust,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174213137,1,['robust'],['robust']
Availability,"@gauravs90 I don't doubt that passing the actorref around everywhere looked ugly. But in terms of moving parts and other such things my bet is that it's worth it. Your description of this scheme sounds a lot more complex (publishing, assuming everything going to metadata will always track w/ state change, having to shoehorn the data side in, etc) than simply passing the metadata down as is typically suggested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687:382,down,down,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687,2,['down'],['down']
Availability,"@gauravs90 a couple of global comments:; - The symbol store and execution store in the old WorkflowActor were not necessarily database-backed. They just stored which calls were completed and which were in flight.; - The graph is a nice idea but currently isn't as eager as it could be. Consider:. ```; A -> B -> C; X -> Y -> Z; ```; - I believe this will run this in pairs, `(A,X)`, `(B,Y)`, `(C,Z)`. But what if A and B are really quick but X and Y are really slow - we're slowed down from executing C because the unrelated tasks X and Y haven't finished yet.; - I think I would prefer the existing method of determining (after every job completes) the set of jobs which have now become runnable. There's already an implicit DAG there. ; - A major reason is, it has already been shown to work with the scatter/gather and other features which update the graph at run-time and I can't see how that would work with this static graph approach?. _NB Sorry all for the repeated almost-identical edits to the above comment. Markdown is hard... :(_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919:481,down,down,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919,1,['down'],['down']
Availability,@gemmalam I tried to create an account but got an error saying I don't have access,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-498771071,1,['error'],['error']
Availability,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:162,error,error,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,4,['error'],['error']
Availability,"@geoffjentry & @Horneth for review please, including the lenthall patch needed to get this cromwell build repaired",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040:106,repair,repaired,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040,1,['repair'],['repaired']
Availability,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:436,error,errors,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897,3,['error'],"['error-caused-by-a-job-submission-failure', 'errors']"
Availability,"@geoffjentry ; **Prerequisite**: We have Spark cluster (3 nodes = 1 master, rest worker including master), Docker on all three nodes, Hdfs file system (3 nodes = 1 Namenode, rest Data nodes including NN) Spark app build locally in Docker repository. This app (spark_hdfs.jar) has two main entry points 1) Word count 2) Vowel count as main class. App consumes input available on Hdfs (assumption pre-loaded) and produces output to Hdfs. . **WDL:**. ```; task spark {; command {; /opt/spark/spark-1.6.1-bin-hadoop2.6/bin/spark-submit --class com.intel.spark.poc.nfs.SparkVowelLine --master spark://10.0.1.22:7077 /app/spark_hdfs.jar hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output; }. output {; File empty = stdout(); }. runtime {; docker: ""sparkapp""; }. }. workflow test {; call spark; }. ```. So the app is available inside the docker container that has base image with Spark environment(i.e. Spark driver + hadoop connector) that will connect to Spark master on host machine within the cluster to submit job, reads input from hdfs file system and turn them into RDDs and distribute the work to workers with the help of master and at the end write output to hdfs. . Following are the arguments to the app ; `hdfs://10.0.1.22:8020/home/himanshuj/test/kinglear.txt hdfs://10.0.1.22:8020/home/himanshuj/output`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660:365,avail,available,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-230938660,2,['avail'],['available']
Availability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:128,fault,fault-tolerant,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,2,['fault'],['fault-tolerant']
Availability,@geoffjentry I didn't dig into to the root cause of the error. I launched two Cromwell servers (via the cfn template on the AWS docs page) against the same AWS Batch setup and tested a hello world wdl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804,1,['error'],['error']
Availability,@geoffjentry I had trouble building this pr:. ```; [error] /work/engine/src/main/scala/cromwell/webservice/SwaggerService.scala:3:35: imported `CromwellApiService' is permanently hidden by definition of object CromwellApiService in package webservice; [error] import cromwell.webservice.routes.CromwellApiService; [error] ^; ```. Any ideas?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866,3,['error'],['error']
Availability,"@geoffjentry I ran this wdl on local backend, call caching off, and I mocked the backend response to immediately return success instead of running the job. So all jobs finish immediately and at the same time. It's not a realistic use case but it's an easy way to push cromwell hard in terms of execution performance for large scatter. ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; runtime {; docker: ""ubuntu""; }; }. workflow wf_hello {; String wf_hello_input = ""world""; Array[Int] s = range(200000); scatter (i in s) {; call hello {input: addressee = wf_hello_input }; }; }; ```. Here are the results:. | Branch | JobStore Writes | ExecutionTime |; |------------|-----------------|-------------------------------------------------------------|; | Develop | On | Still computing runnable calls after 30' - no shard started |; | ThisBranch | On | 8' |; | ThisBranch | Off | 1'30"" |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274:383,echo,echo,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2087#issuecomment-289090274,1,['echo'],['echo']
Availability,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,1,['error'],['error']
Availability,"@geoffjentry In this case I don't think it should be in the ExecutionInfos Table which by my understanding is more of a custom bag of key/values for backend specific info (like JES Run ID and JES status).; If this is general enough to be valid for any backend it should be in the EXECUTION table IMO.; @cjllanwarne Makes sense to have a string yes, which JES returns anyway but for now we only extract the error code from it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184757316:406,error,error,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184757316,1,['error'],['error']
Availability,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:479,error,error,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290,1,['error'],['error']
Availability,"@geoffjentry Not really solved. The pipeline could be terminated by the same error, i just extract the samples that are not processed and run it again. It would be better with local MySQL database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294,1,['error'],['error']
Availability,@geoffjentry Thanks for the quick response. Goal here is to enable rapid failure detection. I'm very open to different approaches to this!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215:73,failure,failure,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-380300215,1,['failure'],['failure']
Availability,"@geoffjentry The main advantage of running these test cases daily vs weekly seems to be that it’s easier to narrow down which change couldve caused this test suite to fail. However, it seems unlikely to me that these tests could find breaking changes everyday that the centaur standard test cases wouldn’t already uncover. I see these tests as a release requirement for Cromwell more than anything else. However, it’s totally upto the team on whatever makes them most comfortable, I don’t have a strong opinion on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368:115,down,down,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368,1,['down'],['down']
Availability,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:83,down,down,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770,3,"['down', 'error']","['down', 'error']"
Availability,"@geoffjentry Yeah so I actually changed the fix because @cjllanwarne suggestion seemed cleaner and I though it would have the same effects, but apparently it doesn't. I can change it back to the first version that worked for you.; On a larger point, I think (hope) this kind of failures will be a lot less happening when we re-factor the test suite infrastructure. IMO it's happening because we keep doing more and more complex tricks in the spec to get it to do what we need but with all the features we keep adding it keeps getting less and less stable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463:278,failure,failures,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/466#issuecomment-188299463,1,['failure'],['failures']
Availability,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:55,error,errors,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,2,"['error', 'failure']","['errors', 'failure-recovery']"
Availability,"@geoffjentry Yeah, I just had to run 14 `cat` commands before I finally found the (transient) issue... `docker: Error response from daemon: device or resource busy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756:112,Error,Error,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202756,1,['Error'],['Error']
Availability,"@geoffjentry actually the constructor is at arity 20 now. I think the point Jeff is driving at is that there aren't built-in Spray marshallers for arities > 22, and I'm actually not sure this won't break down completely for arities > 22 (it is possible to have case class constructors of arity > 22, but functions can't have arity > 22 so something else might still break). So beyond the aesthetic issues here this has become like a game of musical chairs; it might be nice to address this proactively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198329111:204,down,down,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198329111,1,['down'],['down']
Availability,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1218,robust,robust,1218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,2,['robust'],['robust']
Availability,@geoffjentry is it possible to call cache when the original input no longer exists? ; @dheiman have you looked at the [call cache diff endpoint](https://github.com/broadinstitute/cromwell#get-apiworkflowsversioncallcachingdiff)? This is not available in FireCloud but it may have more information about why a workflow cached (or did not).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210:241,avail,available,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335536210,1,['avail'],['available']
Availability,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:314,error,error,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['error'],['error']
Availability,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902,1,['error'],['error']
Availability,@geoffjentry would this fit under the current work with labels that @ruchim is doing? I agree that it's a good idea and it would help make labels even more robust and useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979:156,robust,robust,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2221#issuecomment-299583979,1,['robust'],['robust']
Availability,"@geoffjentry yes, this is turning the knob higher and hoping for the best. The downstream tests succeeded except for the one that depended on cross-talking with the failed test...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458:79,down,downstream,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458,1,['down'],['downstream']
Availability,"@geoffjentry, these were real failures. The jobs were marked by JES as failures, so Cromwell (correctly) did not attempt to retry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645830,2,['failure'],['failures']
Availability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:218,error,errors,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['error'],['errors']
Availability,"@grsterin yes - I [would expect](https://docs.google.com/document/d/1rvLeQYHJATz17VLGJ7xtJjYA0yqrDW_HQ7ja9WCV2-c/edit) we would do a one-time reading of the manifest at start-up time and use that listing to decide whether a file was available on a reference disk or not. We don't know that there will only be one bucket, and in the future people might want to bring their own reference disks containing files from their own buckets. Instead of making the bucket a magic value that's hard-coded into Cromwell I think it's better to have it as part of the reference file path in the manifest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643:233,avail,available,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643,1,['avail'],['available']
Availability,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:308,error,errors,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['error'],['errors']
Availability,"@huangzhibo ; you may want to check if you see this:; ```; MariaDB [(none)]> SELECT @@SQL_MODE;; +-------------------------------------------------------------------------------------------+; | @@SQL_MODE |; +-------------------------------------------------------------------------------------------+; | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |; +-------------------------------------------------------------------------------------------+; 1 row in set (0.000 sec); ```; as this is the default setup for some linux distros. I get your exact error, and also #3346 with the above SQL_MODE. In https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457 it is suggested to try setting; ```; MariaDB [(none)]> SET GLOBAL sql_mode = 'ANSI_QUOTES';; Query OK, 0 rows affected (0.000 sec); ```; When I do that, both errors no longer occur. Note the above change is not permanent. You can alter; `/etc/my.cnf.d/mariadb-server.cnf`; to something like; ```; [mysqld]; datadir=/var/lib/mysql; socket=/var/lib/mysql/mysql.sock; log-error=/var/log/mariadb/mariadb.log; pid-file=/run/mariadb/mariadb.pid; sql_mode=ANSI_QUOTES; ```; and then restart your db server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031:594,error,error,594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438418031,3,['error'],"['error', 'errors']"
Availability,"@huangzhibo Hi huangzhibo, this makes no difference for me (for cwl). I only contain tools called by the workflow in my zipped directory and it still does not recognize this as a valid workflow.; Example:; ```; ./echo_cat_wf.cwl; ./tools/echo.cwl; ./tools/cat.cwl. zip -r tools.zip tools; ```; then run:. `java -jar cromwell-44.jar run -i in.yml -p tools.zip --type cwl echo_cat_wf.cwl`; Gives me error.; Thanks,; Dennis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519981728:238,echo,echo,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519981728,2,"['echo', 'error']","['echo', 'error']"
Availability,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:55,down,downloading,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301,1,['down'],['downloading']
Availability,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:124,failure,failures,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236,1,['failure'],['failures']
Availability,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:582,ping,ping,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,1,['ping'],['ping']
Availability,@illusional I wanted to echo the comments from @vsoch. Thanks to all of you who worked to get this going.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465204072:24,echo,echo,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-465204072,1,['echo'],['echo']
Availability,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:190,down,down,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['down'],['down']
Availability,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:260,error,error,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284,2,['error'],['error']
Availability,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:239,reliab,reliable,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307,2,['reliab'],['reliable']
Availability,"@jainh I think you need to rebase, my suspicion is that the test failures are due to not being quite up to date on develop. Also, not for this PR but I'd suggest looking into porting this backend to the standard backend trait, it might help wiht these divergences in the future",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729:65,failure,failures,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2619#issuecomment-329019729,1,['failure'],['failures']
Availability,"@jainh I would therefore suggest you want to sort the list yourself to make that guarantee robust, rather than assume it's already correctly ordered in the WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731:91,robust,robust,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235961731,1,['robust'],['robust']
Availability,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:10,echo,echo,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992,1,['echo'],['echo']
Availability,"@jainh Yes, there are multiple `--conf` attributes.; If there is no space in the value of `--conf` attribute, single quote is not needed; otherwise, I think it's needed. However the [gatk-launch](https://github.com/broadinstitute/gatk/blob/70edbb6e4caa2b7cf1b8678450443c0c590a2b76/gatk-launch) in GATK beta 4 does not produce the single quote for such case; but if I run the following without single quote, it leads to error:; >Error: Unrecognized option: -Dsamjdk.use_async_io_read_samtools=false. command:; ```; /opt/spark-latest/bin/spark-submit --master spark://localhost:6066 --deploy-mode cluster \; --driver-cores 4 --driver-memory 8g --executor-memory 4g --total-executor-cores 10 \; --conf 'spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false' \; --conf 'spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true' \; ...; ```; Here is a related [post](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) on stackoverflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862:419,error,error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862,2,"['Error', 'error']","['Error', 'error']"
Availability,@jdidion I talked to a site admin and this setting should be updated. Please let me know if you run into this error again and I will add you manually and continue to communicate with our site admin about this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542,1,['error'],['error']
Availability,@jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999:107,error,error,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999,1,['error'],['error']
Availability,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:105,error,error,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,3,"['error', 'failure']","['error', 'failures']"
Availability,"@jgainerdewar thank you, I didn't check the ""Pull Request"" errors carefully enough (thinking it was just the `ssh_access` problem)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930:59,error,errors,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930,1,['error'],['errors']
Availability,@jmthibault79 Adding this to the retry list would retry the user's job. Is that really what you're advocating for?. This is coming from JES. They've actually been requested (by both Firecloud & other users) to tune down the retries that they do on the gsutil up/downloading.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235:215,down,down,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298708235,2,['down'],"['down', 'downloading']"
Availability,"@jsotobroad As scatter/gather seem to be concurrency events I think you're running into the maximum IOPS (Input/Output Operations Per Second) available from Google. The simulatneous Google disk IOPS are as follows, based on the following link and shown in the table below:. https://cloud.google.com/compute/docs/disks/performance#type_comparison. | Read | Write |; | --- | --- |; | 3000 IOPS | 0 IOPS |; | 2250 IOPS | 3750 IOPS |; | 1500 IOPS | 7500 IOPS |; | 750 IOPS | 11250 IOPS |; | 0 IOPS | 15000 IOPS |. There are other ways this might be tackled where the IOPS is throttled, or a few architectural changes would need to be done where it would write locally to the VM and then transferred over in a preemptive way. There is a microservices approach, but that is a major architectural change for Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696:142,avail,available,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237608696,1,['avail'],['available']
Availability,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:354,robust,robust,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,"['alive', 'robust']","['alive', 'robust']"
Availability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:124,down,down,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,2,['down'],"['down', 'download']"
Availability,"@katevoss IIRC the intended behavior is that submitted files are stored as-is no matter what and then when we pick up the workflow we check to see if everything is valid. However @cjllanwarne noticed that we are actually validating one of the input files at actual submission time which led to two issues: a) there was a reason why we didn't want to do that in the first place, b) there was a suspicion that this could lead to timeouts instead of errors anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898:447,error,errors,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328278898,1,['error'],['errors']
Availability,"@katevoss It'd certainly be more robust and nice to have, however it's unlikely to be some huge fire for poeple",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342:33,robust,robust,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1964#issuecomment-345303342,1,['robust'],['robust']
Availability,@katevoss Specifically my concern has always been providing the ability to execute wdl functions in a controlled fashion via a worker pool (perhaps not even in the same JVM as the main engine) for scalability/robustness reasons. As it stands now a cromwell server could get crushed by a ton of these things happening all at once,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912:209,robust,robustness,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288849912,1,['robust'],['robustness']
Availability,@katevoss This issue expects that there is some way to gracefully shut down Cromwell and that stopping the docker process does not do it by default. . #1495 looks to me like an edge case and is not related to this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-325717424:71,down,down,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562#issuecomment-325717424,1,['down'],['down']
Availability,@katevoss error that is blocking the migration,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290506409,1,['error'],['error']
Availability,@katevoss heads up that this was reported and requested in the forums here: https://gatkforums.broadinstitute.org/wdl/discussion/10853/consistent-503-error-for-engine-functions#latest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349130801:150,error,error-for-engine-functions,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349130801,1,['error'],['error-for-engine-functions']
Availability,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133,2,"['error', 'failure']","['error', 'failure']"
Availability,"@katevoss nope, this is just making a single test more reliable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174:55,reliab,reliable,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2908#issuecomment-344984174,1,['reliab'],['reliable']
Availability,"@katevoss seems to be, yeah. This is a specific fix to the problem in #2576 which also comes with extra benefits like throttling and batching to make it generally much more reliable and scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282:173,reliab,reliable,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2612#issuecomment-349133282,1,['reliab'],['reliable']
Availability,"@katevoss, it not being available in FireCloud is my high-level issue - it turns out that since FireCloud currently implements Cromwell 28, [call_caching_placeholder.txt gets placed, even though it is actually cache-by-copy rather than reference](https://gatkforums.broadinstitute.org/firecloud/discussion/10282/confusing-file-left-in-call-cached-execution-directory). . This makes me believe that it should be trivial to leave a file or log entry with details of _why_ a call was cached, which would be quite useful to me, or anyone else trying to troubleshoot an unexpected occurrence like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147:24,avail,available,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2681#issuecomment-335540147,1,['avail'],['available']
Availability,"@kcibul - I was thinking of adding a ""robustness"" (or something like that) label for tickets like this and #1762 as these aren't really about scaling but definitely could impact the health of a cromwell server. Figured I'd leave it up to you as to a) if that's the right thing and b) the exact nomenclature if so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975:38,robust,robustness,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1725#issuecomment-267833975,1,['robust'],['robustness']
Availability,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:72,avail,available,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040,2,['avail'],['available']
Availability,"@kcibul @ruchim FYI - this has failed 62 workflows for us (out of 277 total failures, 22%). It is tied for our largest source of failure. We have not seen it since 6/7",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-228104671:76,failure,failures,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-228104671,2,['failure'],"['failure', 'failures']"
Availability,@kcibul I added a volume entry to mount the mysql data directory onto the host so the data survives a `docker-compose down`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008:118,down,down,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273878008,1,['down'],['down']
Availability,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:89,down,down,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802,1,['down'],['down']
Availability,@kcibul It might not be. I made the same claim at one point and got a lot of hesitant looks from fellow Cromwellians. We're a lot more robust to storing this stuff but at some point it still needs to be gathered into a single array object,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1051#issuecomment-245964356:135,robust,robust,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1051#issuecomment-245964356,1,['robust'],['robust']
Availability,"@kcibul Ok - that was bothering me so I tracked it down. Turns out that I was misremembering how it was working in the first place in terms of what we retry. We'll retry perpetually for any HTTP error we receive except for a 404 using the exponential backoff up to maxPollingInterval. . How does this related to preemption? Well, it shouldn't. Preemption isn't an HTTP error it's an actual operation failure. When we receive an operation failure we process that and among other things look to see if it had a preemption error code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085:51,down,down,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-266798085,6,"['down', 'error', 'failure']","['down', 'error', 'failure']"
Availability,@kcibul This problem doesn't exist in develop (that I can see) as we're not currently supporting restart/recover,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359:105,recover,recover,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225944359,1,['recover'],['recover']
Availability,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669,1,['down'],['down']
Availability,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:62,failure,failure,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012,1,['failure'],['failure']
Availability,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:376,echo,echo,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340,1,['echo'],['echo']
Availability,"@kcibul what's the downside of just splitting every interval in the original set (N=2500) into 4 even pieces? Is it just that we won't be able to ensure that each quarter is equally balanced? Because rather than spending effort balancing shards, I'd rather optimize the GATK code. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016:19,down,downside,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294228016,1,['down'],['downside']
Availability,"@kcibul why not support empty string as null, for docker, in the backend code? Are you worried about discerning an error where the user wants docker, but accidentally specifies empty string? . If my example wdl/json above is correct, I'd rather not have to delete json entries, since this is a bit more difficult to script around, but I can be convinced.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271613347,1,['error'],['error']
Availability,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:273,down,down,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111,2,['down'],['down']
Availability,"@kshakir As you said, this was an error in the input file. I successfully completed a run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646#issuecomment-261067332,1,['error'],['error']
Availability,"@kshakir Can you tell me a bit more about this ticket? Is it still happening, or is Travis not having network errors anymore?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190:110,error,errors,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791#issuecomment-326420190,1,['error'],['errors']
Availability,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:369,alive,alive,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482,8,['alive'],['alive']
Availability,"@kshakir Per https://github.com/broadinstitute/cromwell/pull/2547, I believe that concurrent-job-limit is now available on all backends, so this issue is null and void. Correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556:110,avail,available,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751#issuecomment-326410556,1,['avail'],['available']
Availability,"@kshakir adding `-elocaldockertest` addressed the issue when I ran the tests locally. In Travis CI, the `non_root_default_user` test is failing with:. ```; Status: Downloaded newer image for mcovarr/notroot:v1; bin/bash: /cromwell-executions/woot/148f812b-028b-4264-82bd-ab2f089efe98/call-notroot/execution/script: Permission denied; ```. This test passes when I run it locally. Any ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906:164,Down,Downloaded,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-279779906,1,['Down'],['Downloaded']
Availability,"@kshakir helpfully notes:; >That error looks like JNI, that I suspect is jython related, thus is probably heterodon. Heterodon was slimmed down to remove everything NOT tested via mac and/or CI. So since we don’t have any :travis: / :jenkins: testing windows I would not expect heterodon to work. Good news (?): we still support shell invoking `cwltool`, but I have zero expectation for that to work on windows either... So this behavior is likely the result of a deliberate and helpful size optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-480391597,2,"['down', 'error']","['down', 'error']"
Availability,@kshakir is this the library that we downgraded the last time we tried to upgrade all of our libraries?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251:37,down,downgraded,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251,1,['down'],['downgraded']
Availability,"@kshakir just a standard config file. I hadn't actually looked at the error yet, but will make a note to come back to this discussion :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473123397,1,['error'],['error']
Availability,@ktibbett can you comment if this is something you need patched into hotfix? Or something you can tolerate until you're on 0.20+,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661:98,toler,tolerate,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661,1,['toler'],['tolerate']
Availability,"@lbergelson right, these look like transient failures so I'l give the tests a quick nudge...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465,1,['failure'],['failures']
Availability,"@lij41 - do you have any more error information, like a specific failure message? Also, were you creating the AMI from the CloudFormation templates? If so, which version - with or without VPC creation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888,2,"['error', 'failure']","['error', 'failure']"
Availability,"@markjschreiber Thank you for this PR. I had a quick look at it and it looks pretty good. I just had a few questions. > 7. Set up /var/lib/docker/docker to auto-expand as inputs are now read directly into the container. Is this the only documentation on this requirement? Also, are you saying that that directory is now being auto-expanded in the underlying ECS image, or that a client needs to create an AMI to auto-expand that directory instead of `/cromwell_root`? Also, is it `/var/lib/docker/docker` or `/var/lib/docker/containers`? . EDIT:. The README.md references a LaunchTemplate which provides a UserData script to an underlying AMI, but it's not linked anywhere. I think I've tracked it down to the document here: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/aws-genomics-launch-template.template.yaml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076:698,down,down,698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076,1,['down'],['down']
Availability,@markjschreiber running into the same error for both v52 and v53.1. I am using the same CloudFormation @mderan-da mentioned . Appreciate your newer documentation on this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254,1,['error'],['error']
Availability,"@matthewghgriffiths did it work with `\cromwell_mount` or `\cromwell_root`. I'm getting a very similar error, and I've double checked that I had ""cromwell_root"" as listed [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions). I created my AMI today, and I definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['error'],['error']
Availability,"@matthewghgriffiths, for troubleshooting why nothing gets written at any point:. 1. Set the min cpus in your compute environment (the one in use) to at least 1 so that an instance spins up and will stay available for a little bit. This will give you an instance that lives for long enough to test things.; 2. SSH into the instance that spins up (or one already running) in the compute environment.; 3. Try to `aws s3 cp` a file into your cromwell executions bucket. Doing this from the instance simulates the permissions used by the batch job. If you get an error about permissions, then there is likely a policy problem with your instance role.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011:203,avail,available,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435024011,2,"['avail', 'error']","['available', 'error']"
Availability,"@mcnelsonsema4 - the `SubmitJob` API does not support overriding volumes and mount points to the container. These are set in the Job Definition referenced by `SubmitJob`. The host path for the volume uses the UUID that Cromwell generates for the task to isolate localized inputs and any outputs generated. Since the UUID is regenerated for each task execution, a new Job Definition revision is created with the corresponding new host path for the volume. The API call for creating new Job Definitions and revisions isn't intended for a high volume of requests. The [code involved](https://github.com/broadinstitute/cromwell/blob/90154ed22b2a78dfbb1c5342a8f0d39164aaeac8/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L201-L218) seems to keep retrying the request to the API. It's possible an API response is returned that isn't caught as a error and allowing the use of the ""most recent"" revision, which was a revision submitted by another task by the same name that ran earlier. @cjllanwarne, @mcovarr, @danbills - any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505665761:874,error,error,874,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-505665761,1,['error'],['error']
Availability,"@mcovarr , @cjllanwarne . ~~Sorry for the errors still. I want to test locally of course, but that does not work for some reason:~~; EDIT: Nevermind. I found the documentation here: https://cromwell.readthedocs.io/en/stable/developers/Centaur/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482021138,1,['error'],['errors']
Availability,"@mcovarr - yes. Alteratively, you could use `DescribeImages` and supply an `ImageIds` argument to filter down the results by either `imageTag` or `imageDigest`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486926555:105,down,down,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-486926555,1,['down'],['down']
Availability,"@mcovarr @Horneth I have not been able to make tests work when DataAccess is a singleton. If you can diagnose the failures in branch: ""data_access_singleton"" then let me know, but I don't think I'll do it as part of this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075:114,failure,failures,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-142934075,1,['failure'],['failures']
Availability,"@mcovarr @geoffjentry Like I mentioned at standup I added a second commit after seeing a failure in centaur.; What happened was JES failed the job because it couldn't localize the auth file (not found).; However I didn't see anything in Cromwell suggesting that the upload failed (which we log if it happens). So my guess is JES tried to localize the file when Cromwell was restarting the workflow and hence re-writing the file, which made sense according to the timestamps at least. This commit makes the upload of the auth file fail if the file already exists, unless it's a known restart in which case it ignores the failure and keeps going. I think it makes sense to fail the workflow if there's already an auth file for this workflow and it's the *first* time we run it. It might indicate something is wrong and failing the workflow avoids taking chances with refresh tokens / secrets. If you disagree please voice your concerns :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447:89,failure,failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2490#issuecomment-319093447,2,['failure'],['failure']
Availability,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:381,failure,failure,381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073,2,['failure'],['failure']
Availability,@mcovarr Can you please ping me when you guys are close to revisiting this? I don't have bandwidth right now but would definitely like to go over this before it hits. Do you have a sense of timeline?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827:24,ping,ping,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312978827,1,['ping'],['ping']
Availability,"@mcovarr Given that these were JES specific failures, I'm going to run Tyburn over them. Presumably the local tests for these are already there and already working?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/290#issuecomment-156140460,1,['failure'],['failures']
Availability,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:16,echo,echo,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182,4,"['down', 'echo']","['downloading', 'echo']"
Availability,@mcovarr I continue to get the same error after creating a build with the develop branch seems that it is not ready! Thanks for letting me know :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458,1,['error'],['error']
Availability,"@mcovarr I think we still need to ensure that the submission is correct before sending back a 201 with the workflow ID, which means being sure that everything necessary to start executing the workflow is ready (all DB executions succeeded etc...); @kshakir I see your point, however in this case I don't think having the ask timing out is a problem, if a WorkflowActor takes forever to initialize itself then there is actually some bottleneck further down, and it might even be better to say ""sorry but we're really too busy right now, retry later"", than keeping waiting for WorkflowActors, which is going to trigger a timeout anyway since this comes from the ""submit endpoint"" and spray is not going to wait forever either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376:451,down,down,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161985376,1,['down'],['down']
Availability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:419,Error,Error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,@mcovarr Yes. It's ridiculous that we get build failures due to external services failing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166392599:48,failure,failures,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166392599,1,['failure'],['failures']
Availability,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['error'],['error']
Availability,"@mcovarr does ""needs docs if not a checkpoint"" still apply?. EDIT: I see some docs do exist - is that sufficient to satisfy the PR description comment?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3933#issuecomment-408934534:35,checkpoint,checkpoint,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3933#issuecomment-408934534,1,['checkpoint'],['checkpoint']
Availability,"@mcovarr it does seem to have worked 😄; @cjllanwarne @geoffjentry Good point, my main goal was to make it possible for the metadata service to report statsd metrics, which is done through another service. But like you said since it's just making available the service registry actor to the services I don't think it introduces any additional coupling.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969:246,avail,available,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367025969,1,['avail'],['available']
Availability,"@mcovarr oh I see thanks. Well I only saw 4 failures IIRC, so it's not that bad. In the meantime we could always keep `/bin/bash` as the default and set it to`/bin/sh` for CWL conf tests which would make 117 pass",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392814191,1,['failure'],['failures']
Availability,@mcovarr that was indeed my theory. I'm trying to create a test case to actually reproduce this error to make sure my fix doesn't have some other weird downstream problems (eg getting 10 lines later then throw some other exception wouldn't be a great outcome),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488082964,2,"['down', 'error']","['downstream', 'error']"
Availability,"@mcovarr that's true, I think it would fail eventually anyway whenever something tries to access those files (or not if nothing does ?) ? And I thought that one of the assumption in this ""no-copy"" mode is that we expect the files to be relatively immutable anyway. . But I also agree that it would be cleaner to fail the ""caching"" if they don't exist rather than the downstream task failing by itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116:367,down,downstream,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307422116,1,['down'],['downstream']
Availability,@mcovarr well the title of the pr **does** mention programmer error 😛,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-480294525:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-480294525,1,['error'],['error']
Availability,@mcovarr what do you believe is the impact of this change toward managing production failures?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3962#issuecomment-424957788:85,failure,failures,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3962#issuecomment-424957788,1,['failure'],['failures']
Availability,"@mcovarr worth figuring out as any non-trivial downtime is going to be met with a giant ""no""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472133864:47,downtime,downtime,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472133864,1,['downtime'],['downtime']
Availability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:171,error,error,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,1,['error'],['error']
Availability,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601,2,['error'],['error']
Availability,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:482,failure,failure,482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623,1,['failure'],['failure']
Availability,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:89,error,errors,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,2,"['down', 'error']","['down', 'errors']"
Availability,@notestaff @Horneth What about a line along the lines of heeding typical concerns regarding docker-in-docker situations? I don't really want to go down the path of providing various workarounds (as we already see in this thread they're controversial) but I think it'd satisfy @notestaff 's request to at least make the issue visible,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4100#issuecomment-436723837:147,down,down,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4100#issuecomment-436723837,1,['down'],['down']
Availability,"@orodeh in case you're not able to read the Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:69,failure,failure,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,@patmagee It turned out it was a different issue entirely in our production environment that had the symptoms of abort failures. We've not had success recreating this -- but let us know what you end up observing!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311:119,failure,failures,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-400468311,1,['failure'],['failures']
Availability,"@pgrosu It's in our internal space, however I can give you the gist. We're doing a few things at once ...; - Make workflow submission async. Submitted workflows go into a new store and the WorkflowManagerActor can pull them as necessary. Within the store they'll be marked as either Submitted, Running or Restartable. The latter is a state which is assigned to any workflow in Running state when the system comes online; - The EngineJobExecutionActor (EJEA above) sits between the WorkflowExecutionActor and the BackendJobExecutionActor, and will manage engine-side knowledge in a persisted store. The combination of this and the above will allow us to bring back what we call the 'restart' functionality - i.e. pick up a running workflow from the engine side but not reattach to running backend jobs; - Less hashed out at the moment, if a backend will support 'recover' functionality (attaching to the backend jobs, we'll implement this in as many of our own backends as we can), the backend will need to manage its own information, e.g. using the KV store",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714:862,recover,recover,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230605714,1,['recover'],['recover']
Availability,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:350,echo,echo,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"@rebrown1395 ; Yes, it worked. I didn't see this error anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4353#issuecomment-444627372:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4353#issuecomment-444627372,1,['error'],['error']
Availability,"@rhpvorderman oh, I didn't notice your comment about the tests in my re-review. Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code. Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386:198,failure,failure,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482345386,1,['failure'],['failure']
Availability,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,4,['error'],['error']
Availability,"@ruchim @geoffjentry . Hi, ; Sorry, it looks like I copied the wrong history. I added the correct history at https://gist.github.com/denis-yuen/b3aa8b0e882dee1fe8cb6cab82286e46. The error message is pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:182,error,error,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['error'],['error']
Availability,"@ruchim I think we talked about this offline, but it'd be workflow level info. Except I guess for the aforementioned idea of grouping by task failures.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348#issuecomment-383682658:142,failure,failures,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348#issuecomment-383682658,1,['failure'],['failures']
Availability,"@ruchim I updated the description and title, this is not nearly as bad as the previous title made it sound. It's a very weird case found in the centaur test failures that I'm looking at.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611:157,failure,failures,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611,1,['failure'],['failures']
Availability,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:365,echo,echo,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,14,"['echo', 'error']","['echo', 'error']"
Availability,"@ruchim Thank you very much for the help and explanation here, they look great to me! @jishuxu a follow up for one of your the failures you ran into previously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390846376:127,failure,failures,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390846376,1,['failure'],['failures']
Availability,"@ruchim The options part of this might duplicate that bug, but I'm not sure since I don't know the failure mode there. The non-localized paths part of this is maybe related to #1944 or could be original, not sure about that either. 😦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277049483,1,['failure'],['failure']
Availability,"@ruchim Were you able to get to the bottom of the ""Could not find suitable filesystem among Default to parse gs://..."" errors you mention in the review thread? I'm hitting the same error when running a WDL on Google's wdl_runner, but the same WDL with the same inputs runs fine on the DSDE Methods Cromwell server (both C24 and C25). In my case the error occurs when using read_lines() on a gs:// filepath that points to a text file (list of file paths that I'd rather not include in the json for practical reasons). . Also tagging friendly bug rotator @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066:119,error,errors,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295561066,3,['error'],"['error', 'errors']"
Availability,@ruchim is this a parsing error or a runtime error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3051#issuecomment-350848249:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3051#issuecomment-350848249,2,['error'],['error']
Availability,"@salonishah11 for example, I'm running a cromwell container in server mode, bound to port 8000: ; ```; docker run -p 8000:8000 cromwell server; ```; but when I try to ; ```; docker run cromwell submit --host 0.0.0.0:8000 ...; ```; I get: ; ```; Error: Option --host failed when given '0.0.0.0:8000'. no protocol: 0.0.0.0:8000; ```. Some simple docs would help here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124:245,Error,Error,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124,1,['Error'],['Error']
Availability,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:828,avail,available,828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,1,['avail'],['available']
Availability,@samanehsan @geoffjentry -- is there a good message you'd like to see in replacement of the existing error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959,1,['error'],['error']
Availability,@samanehsan this seems like a one-off event of a metadata value (startTime) getting lost. Whats the frequency of such an error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4128#issuecomment-424948652:121,error,error,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128#issuecomment-424948652,1,['error'],['error']
Availability,"@scottfrazer FWIW my vision of the world was a lot closer to the diagram you drew up but I don't have strong feelings on that. In terms of distributed jars what I'd like to see distributed to the world would be:; - cromwell.jar: full fat jar like we have today which also includes all of the supported backends built in; - cromwell-backend.jar: a jar providing the interface stuff which someone can use to build their own backend jar. I'd be totally okay with (and could see value in):; - cromwell-lite.jar (or something like that): a stripped down fat jar w/o any supported backends or maybe local; - foo-backend.jar for each supported backend. My main concern though is that we always make the most obvious download for a naive user of cromwell to be the one with all of the supported (or perhaps a 'very common supported' subset) backends built in so as to minimize the work someone needs to do to get rolling. The other jars are really an artifact of the multi-project model and can be ignored. The side discussion about `core` is exactly why I was picturing the hierarchy stemming from `backend` all along. Despite the name of `supportedBackends` being my request I'll admit I was just looking at the name when I said that, not thinking the whole thing through critically :). IMO `core` should be code which is shared between all components, I'd call the filesystem concept a component, not something in core itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235:544,down,down,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235,2,['down'],"['down', 'download']"
Availability,"@scottfrazer re compile errors, yeah i made a tiny change on my last commit which I wouldn't think would break anything so didn't bother to test it. oops.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164598423,1,['error'],['errors']
Availability,"@seandavi - implementing the config suggested by @TimurIs and removing the specification of `concurrent-job-limit` I was able to run the following workflow with out issue. ```; task t {; Int id; command { echo ""scatter index = ${id}"" }; runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""512MB""; }; output { String out = read_string(stdout()) }; }. workflow w {; Array[Int] arr = range(1000); scatter(i in arr) { call t { input: id = i } }; output { Array[String] t_out = t.out }; }; ```. Approximate numbers:; * max # jobs observed in ""submitted"" state = 250 - 270; * max # jobs observed in ""running"" state = 20-30",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747:205,echo,echo,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443399747,1,['echo'],['echo']
Availability,"@seandavi What are you seeing on the quota failures? We should be robust to that and putting things back in the to-retry pool, while our quotas are jacked pretty high we run into this as well for our larger stuff, so that's something we've needed to work around ourselves.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297:43,failure,failures,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260644297,2,"['failure', 'robust']","['failures', 'robust']"
Availability,"@seandavi Yeah, was just reading the thread. That's something different than what I was picturing when I saw quota failures. I'll look into it a bit.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477:115,failure,failures,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260646477,1,['failure'],['failures']
Availability,"@slnovak Hi - thanks for this! I just had a couple of quick questions on the Homebrew & maintenance front. ; - What's the plan going forward in terms of keeping it up to date, is this something that you plan on doing? If so will you be tracking the official releases (e.g. now 0.16) or updating on some semi-regular (or even not-so-semi) interval? ; - Is there anything we could do in terms of helping out w/ the homebrew angle?; - I'm not at all familiar w/ the homebrew formula stuff. Where is it calling `sbt assemble`? Is that done elsewhere? If so, how does that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670:88,mainten,maintenance,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670,1,['mainten'],['maintenance']
Availability,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:321,error,error,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241,3,"['error', 'failure']","['error', 'failures']"
Availability,@tom-dyar fyi - I received this boxed error today as well. It eventually cleared when I did an sbt clean and rebuilt the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982,2,['error'],['error']
Availability,"@vdauwera you have officially been pinged. 😄 . As I mentioned above I don't have strong opinions about where the documentation ends up, but I put it here for now so I could edit it. If the content remains here we should either take down or mark as ""pre-28"" the content that's on the forums.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326:35,ping,pinged,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311427326,2,"['down', 'ping']","['down', 'pinged']"
Availability,"@vortexing - task input and output data staging is handled by the `ecs-proxy` container that is installed when you create a custom AMI with ""cromwell"" settings. If you are not seeing data move in/out a good place to check for errors is the Cloudwatch log for a task that didn't have it's data staged correctly. Append `-proxy` to the job's cloudwatch log url to get the logging generated by the `ecs-proxy`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845:226,error,errors,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845,1,['error'],['errors']
Availability,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:201,echo,echo,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053,2,['echo'],['echo']
Availability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:216,echo,echo,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,3,['echo'],['echo']
Availability,"@vsoch Actually I'm piping the singularity command to qsub, so I'm actually doing exactly what you're suggesting :stuck_out_tongue: (Notice the `echo ' \` and `' | \` surrounding the singularity command.). For the binds: `/exports` is needed because output is written to this directory and the `${cwd}:${docker_cwd}` is needed because the script under `${script}` (generated by cromwell) uses this path. As for the multiprocessing thing, I get an error: `OSError: [Errno 30] Read-only file system` when I try to run a python script using multiprocessing inside a container. This seems to be caused by the fact that `/dev/shm` is a symlink to `/run/shm` (as in that issue I linked), so it can't be found unless I mount the `/run` directory. So singularity works just fine without it, but this is needed for certain python based programs. This is also specific to the cluster, as on my own pc I don't have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438692924:145,echo,echo,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438692924,2,"['echo', 'error']","['echo', 'error']"
Availability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several 🔥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:617,down,down,617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['down'],['down']
Availability,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:649,robust,robust,649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498,1,['robust'],['robust']
Availability,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:154,error,errors,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463,1,['error'],['errors']
Availability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1406,failure,failures,1406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['failure'],['failures']
Availability,@wleepang Could you share what change you made to fix the problem? I'm getting the same error on my own CloudFormation template.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-506369596:88,error,error,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-506369596,1,['error'],['error']
Availability,@wresch Looks like you are having DNS resolution issues. \. Can you resolve the endpoint per the error message?; `nslookup https://auth.docker.io/token`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891,1,['error'],['error']
Availability,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,redundant,redundant,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905,1,['redundant'],['redundant']
Availability,"@yfarjoun Yeah, I agree. @vdauwera quickly provided a totally valid use case beyond ""I'm screwing around"". Specifically what I was concerned about here is providing a case where we allow corners to be cuttable for the implementer which then leave loaded guns sitting around for downstream users to shoot themselves with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814:278,down,downstream,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323832814,1,['down'],['downstream']
Availability,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:293,error,error,293,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,1,['error'],['error']
Availability,"A bit more info on this. The job mentioned above ran out of disk space. The monitoring.log is full of ""out of space"" errors. However, the job ran to completion and the output directory has an rc file containing 0, so Cromwell considered it a success. But the output files were truncated to zero bytes, presumably due to the disk space issue. Normally we get a hard failure when we run out of disk space but not in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997:117,error,errors,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997,2,"['error', 'failure']","['errors', 'failure']"
Availability,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:47,failure,failures,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432,2,"['failure', 'mainten']","['failures', 'maintenance']"
Availability,"A fix for this issue would be much appreciated! It is particularly frustrating as the check-alive config parameter sounds like exactly the test I want cromwell to run, but it is actually for different usecase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549:92,alive,alive,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380667549,1,['alive'],['alive']
Availability,"A follow-up question (prompted by the `quay.io` outage today), does `singularity exec` check the internet to see if it has the most up to date version of the tag? What happens if I gave it a docker with digest instead, would it still poll the internet?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760:48,outage,outage,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760,1,['outage'],['outage']
Availability,"A quick question: if I run in local my workflow and I parallelize with several sample using scatter, Cromwell, takes all the CPU that we have in our machine. I then used this option in every task in order to take only 6 CPU and let some memory also for my colleagues:. ```; runtime {; docker_user: ""ngs""; cpu: 6; }; ```. But this does not work, and all the CPU are taken at the same time. And I have these errors:. The first is this one: ; `[warn] Local [f8d35e0f]: Key/s [cpu] is/are not supported by backend. Unsupported attributes will not be part of job executions.`. The second this one: ; `[warn] BackgroundConfigAsyncJobExecutionActor [7e5755bcscMeth.mapping:0:1]: Unrecognized runtime attribute keys: cpu`. Is there any explanation for this? These errors do not cause any interruption though.; Thanks in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367:406,error,errors,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-516913367,2,['error'],['errors']
Availability,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698,3,"['error', 'failure']","['error', 'failure']"
Availability,A/C write a unit test that characterizes the performance of heartbeat writing with and without autocommit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416:60,heartbeat,heartbeat,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-443798416,1,['heartbeat'],['heartbeat']
Availability,"AC: Confirm that this is reproducible behavior in the latest version of Cromwell, and given thats the case, confirm that job/workflow failure messages make it over to the workflow logs (not just the server logs)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466:134,failure,failure,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466,1,['failure'],['failure']
Availability,"Able to replicate the break from v36 to v37, I switched my check-alive in my config to use scontrol and it succeeded:; ```; ""check-alive"": ""scontrol show job ${job_id}"",; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252:65,alive,alive,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252,2,['alive'],['alive']
Availability,"According to the [list of PAPI error codes](https://cloud.google.com/life-sciences/docs/troubleshooting#unavailable_14), `14` is indeed preemption so I agree that is surprising on a non-preemptible. You can use Cromwell retries to re-run; or reach out to your GCP support venue to better understand what's going on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118,1,['error'],['error']
Availability,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245,2,['error'],['error']
Availability,"Actually, the main problem is that the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had; an inkling that it's a missing file problem. so I have to resort to divide; and conquer in order to identify the missing file...and that's a pain. On Sun, Jul 10, 2016 at 10:08 PM, Jeff Gentry notifications@github.com; wrote:. > The former. He was looking for a backend-aware validation type behavior; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231627459,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0hYeQnaJcsNEVJlnxrz9-tA880TLks5qUaWXgaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231711585,1,['error'],['error']
Availability,"Adam and Jeff;; Thanks for this. I'm definitely agreed I don't want to break your tests every time we accidentally introduce a bug in bcbio. We have tagged versions of the Docker images (https://quay.io/repository/bcbio/bcbio-vc?tab=tags) so could pin to a specific CWL with a specific Docker tag. Data doesn't change as often in a back-incompatible way but you could also have a copy of that if it becomes an issue. We could generate CWL that ties to a specific Docker build, or just have your tests download the tagged version we've tested on. How aggressive is Cromwell at updating the local Docker version based on what's in the CWL? If it would leave a pre-downloaded and ready to go Docker alone, it seems like a pre-pull from a known tag and the pinned CWL should do most of what we need. Would that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610:501,down,download,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460720610,2,['down'],"['download', 'downloaded']"
Availability,Adam;; Thanks so much for looking into this. I'm glad the error makes sense and is translatable to a fix for re-reading. We love call caching for job re-use so prefer not to disable it for now. Looking forward to this fix and having cleaner runs going forward. Thanks again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415005318:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415005318,1,['error'],['error']
Availability,Adding @vdauwera's comment about adding error codes to GATK from [DSDE-docs #1742](https://github.com/broadinstitute/dsde-docs/issues/1742#issuecomment-280304238):; >We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280379776,2,['error'],['error']
Availability,"Additional info since the time this issue was filed: Alibaba's [Batch Compute service (BCS) is now available in the US](https://www.alibabacloud.com/help/doc-detail/61360.htm?spm=a2c63.l28256.a3.23.194f25719KjP66). This helps test Cromwell-in-the-US-using-DockerHub, but for CN users the above issues still need to be addressed, including figuring out a way for to check hashes from OSS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138:99,avail,available,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518#issuecomment-399762138,1,['avail'],['available']
Availability,"Additionally, @rexwangcc has been working on https://cromwell-tools.readthedocs.io, which at least partially seems to implement what you proposed. We're also planning to move other functionality, like parsing of metadata.json for failures, into it over time, to be used together with `miniWDL` for WDL debugging support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501826170:230,failure,failures,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501826170,1,['failure'],['failures']
Availability,"Additionally, in both the above trials, Cromwell still managed to start, but failed with the following error when starting a submission:. ```; 2019-02-25 18:47:23,071 cromwell-system-akka.actor.default-dispatcher-33 ERROR - Error during processing of request: 'Unknown factory null'. Completing with 500 Internal Server Error response. To change default exception handling behavior, provide a custom ExceptionHandler.; java.lang.IllegalStateException: Unknown factory null; 	at akka.http.impl.util.package$.actorSystem(package.scala:34); 	at akka.http.scaladsl.settings.SettingsCompanion.default(SettingsCompanion.scala:20); 	at akka.http.scaladsl.settings.SettingsCompanion.default$(SettingsCompanion.scala:20); 	at akka.http.scaladsl.settings.ParserSettings$.default(ParserSettings.scala:119); 	at cromwell.webservice.CromwellApiService.$anonfun$workflowRoutes$68(CromwellApiService.scala:233); 	at akka.http.scaladsl.server.Directive$.$anonfun$addByNameNullaryApply$2(Directive.scala:134); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"After #2952, `missing_input_failure` has a somewhat nicer error: `""Evaluating read_string(wf_hello_input) failed: gs://nonexistingbucket/path/doesnt/exist""`. So, I've ticked that box. Just `missing_optional_output` still to go!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871#issuecomment-348291490:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871#issuecomment-348291490,1,['error'],['error']
Availability,After a little research it seems the default Akka supervision decider and strategy looks [pretty reasonable](https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/FaultHandling.scala#L156) for this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761#issuecomment-397602143:187,Fault,FaultHandling,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761#issuecomment-397602143,1,['Fault'],['FaultHandling']
Availability,"After reviewing cromwell code with Dan, we think that it's OK to downgrade the log level of **WorkflowFailedResponse** event in **WorkflowManagerActor** to INFO so it won't propagate to Sentry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544:65,down,downgrade,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544,1,['down'],['downgrade']
Availability,"Again 2779. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:493,Robust,RobustClientHelperSpec,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,2,['Robust'],['RobustClientHelperSpec']
Availability,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:29,fault,faulty,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966,1,['fault'],['faulty']
Availability,"Ah yes, let's just claim we fixed one bug then 😄 ; My take; use, modify, or discard as desired:; > Fixed a bug that could cause workflows to fail unexpectedly with the error `413 Request Entity Too Large` when accessing Google Cloud Storage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600:168,error,error,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600,1,['error'],['error']
Availability,"Ah, I see now how you intent for it to work. I don't think this will be very practical on any kind of shared SGE HPC without a seperate poll rate. . As mentioned elsewhere; the call rate of the `pollStatus` is geared towards a low-impact filesytem check and not a high-impact call to the SGE queque master (i.e. `check-alive` uses `qstat`). Enabling the `exit-code-timeout` with a somewhat large numbers of tasks will quickly cripple any average SGE HPC. . If you enable this and are an HPC admin; make sure to keep taps on your submission services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492:319,alive,alive,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425025492,1,['alive'],['alive']
Availability,"Ah, that sounds really nice, but I don't think it's possible: the `actual` is of type `Terminal`, which does not include type information (which makes sense, because once parsing the grammar has broken down you can't make any guarantees that a symbol you find will fit into a defined universe of types)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394447827:202,down,down,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394447827,1,['down'],['down']
Availability,Ah. Now that you say this I'm betting it fails due to how the proxy sidecar works. My quick thought is that the approach in #5064 is likely better but I need to dig into more how the AWS localization is working in the first place. In the meantime I'll try to also ping @elerch and @wleepang in case they have thoughts here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509812726:264,ping,ping,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509812726,1,['ping'],['ping']
Availability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=…` except one is supposed to use [`-Dlogback.configurationFile=…`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,1,['error'],['error']
Availability,"All 3 failures also emit:; ```; - should return pagination metadata only when page and pagesize query params are specified *** FAILED *** (9 seconds, 457 milliseconds); java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3608ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4520#issuecomment-452071787:6,failure,failures,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520#issuecomment-452071787,2,"['avail', 'failure']","['available', 'failures']"
Availability,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:486,Error,Error,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152,3,['Error'],"['Error', 'Errors']"
Availability,"All right so after some investigations here is what I found:; - This problem only seems to appear on MySQL / CloudSQL 5.6; I created 2 CloudSQL instances, 1 on MySQL 5.6 and one on 5.7. I got the error with 5.6 but not 5.7. So if creating a new empty DB is not a problem for this workflow, I think that could be a (temporary) solution.; - About compression, based on the 5.6 instance I created on google, I think the size limit would be closer to 26MB; It's a 5.6.26 version exactly. ```; Your MySQL connection id is 15; Server version: 5.6.26 (Google); ```. And because it's post 5.6.22 the formula to get the max limit is `10% of innodb_log_file_size * innodb_log_files_in_group` (https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_log_file_size) . Which on the 5.6 instance is. ```; | innodb_log_file_size | 134217728 |; | innodb_log_files_in_group | 2 ; ```. => `(134217728 * 2) / 10 = 26843545 B ~= 26 MB`; - One other option could be like Henry said to not store the beginning of the paths and add it back at runtime when needed. This could (maybe) work for `File`s but not for `String`s, which I think is what is needed for this case (manipulate paths as `String`s). @jsotobroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-225966019:196,error,error,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-225966019,1,['error'],['error']
Availability,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526,1,['failure'],['failures']
Availability,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,2,['error'],['error']
Availability,"Also @wleepang I was able to do a docker pull on the AMI that I am using:. AMI ID: amzn-ami-2018.03.h-amazon-ecs-optimized (ami-0a0c6574ce16ce87a). `[ec2-user@ip-172-31-29-236 ~]$ docker pull ubuntu:latest; latest: Pulling from library/ubuntu; 473ede7ed136: Pull complete; c46b5fa4d940: Pull complete; 93ae3df89c92: Pull complete; 6b1eed27cade: Pull complete; Digest: sha256:29934af957c53004d7fb6340139880d23fb1952505a15d69a03af0d1418878cb; Status: Downloaded newer image for ubuntu:latest`. Let me know if there is something off here, it seems fine to me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460:449,Down,Downloaded,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435913460,1,['Down'],['Downloaded']
Availability,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:277,avail,available,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560,1,['avail'],['available']
Availability,"Also as a TOL, maybe consider `failure.toPrettyElidedString` in case the exception content is extremely long?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500:31,failure,failure,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500,1,['failure'],['failure']
Availability,"Also have this error, using Cromwell 52, installed using this manual : . https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. logs say : fetch_and_run.is is a directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937,1,['error'],['error']
Availability,"Also is there any way to actually enumerate all the available settings? Probably this would be too many, and modifying some of them would break the system. Still it is nice to have, and I have tried searching for this 'inbuilt' defaults file in vain. I guess it is scattered across many java files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-477831107:52,avail,available,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-477831107,1,['avail'],['available']
Availability,Also requesting a review by @salonishah11 because I think these resolvers might potentially be useful for downloading content from the `workflowUrl` as well,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-407170726:106,down,downloading,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3916#issuecomment-407170726,1,['down'],['downloading']
Availability,"Also seen: . ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): running [""docker"" ""pull"" ""quay.io/bcbio/bcbio-vc@sha256:90087824e545df6d3996a28360f2f0fd28dce611a989bbcc79aa8117d341f6ef""]: exit status 1 (standard error: ""Error response from daemon: Get https://quay.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n""); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790:268,error,error,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438#issuecomment-443429790,2,"['Error', 'error']","['Error', 'error']"
Availability,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:89,echo,echo,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981,4,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:265,echo,echo,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095,2,['echo'],['echo']
Availability,"Also, if I change the type of the output from `Object` to `Map[String, String]` I get a similar error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Map[String, String] name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853:96,error,error,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853,2,"['error', 'failure']","['error', 'failures']"
Availability,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:378,down,download,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,2,"['down', 'error']","['download', 'errors']"
Availability,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1187,error,error,1187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:104,down,downstream,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731,1,['down'],['downstream']
Availability,Alternate command syntax for those investigating: `sbt 'server/run server'`. ~The original caused an error for me.~ EDIT: The original error may have been an unrelated error that pops up sometimes during `sbt clean compile`. I'm not sure of the default dependencies for `sbt */run` but it does appear that [`*/package`](https://www.scala-sbt.org/1.0/docs/Running.html#Common+commands) is being invoked and zipping up the class files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739,3,['error'],['error']
Availability,Although now that I look the CWL analogy doesn't hold up. They have a max-64K chunk attached to their File object for similar purposes as this (input into expressions) but not to enable further typing like we do. Error it is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917:213,Error,Error,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294311917,1,['Error'],['Error']
Availability,"Although now that I've said that I know that we *do* have a use case where something like this is being requested. They want a typed set of key/value pairs, but the thing that they really want is to be able to define some boundaries (e.g. ""Foo"" is a number between 1 and 10) and to have the static analysis fail to validate the workflow if one of these are a workflow input and the values are wrong. Now that I type that out, having refinement types in WDL seems like a bad path to be going down. I should verify that's *really* what they want or if I read too much into an example they gave.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059:491,down,down,491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330315059,1,['down'],['down']
Availability,Am wondering if these limits should be specified in the conf file. They should be generous and static but I could envision a scenario where one would want to override read_json or something. In that scenario a config change would be nice to have available,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107:246,avail,available,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303433107,1,['avail'],['available']
Availability,And (for obvious reasons) this does not error out on SGE and local backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844#issuecomment-343271052:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844#issuecomment-343271052,1,['error'],['error']
Availability,"And again on a new run, with something that looks very similar... ```; [ERROR] [05/01/2017 21:06:38.897] [cromwell-system-akka.dispatchers.engine-dispatcher-86] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:72,ERROR,ERROR,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['ERROR'],['ERROR']
Availability,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:157,echo,echo,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558,3,"['echo', 'error']","['echo', 'error']"
Availability,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:42,ERROR,ERROR,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,4,"['ERROR', 'Fault']","['ERROR', 'FaultHandling']"
Availability,And only exec.sh is present in the bucket (no log etc)....; ```#!/bin/bash; tmpDir=$(mktemp -d /cromwell_root/tmp.XXXXXX); chmod 777 $tmpDir; export _JAVA_OPTIONS=-Djava.io.tmpdir=$tmpDir; export TMPDIR=$tmpDir. (; cd /cromwell_root; java -Xmx4g -jar /cromwell_root/broad-dsde-methods/lichtens/test_cnv_validation/gatk.jar ModelSegments \; --denoisedCopyRatios /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-DenoiseReadCountsNormal/G25783.TCGA-55-6986-11A-01D-1945-08.2.denoisedCR.tsv \; --allelicCounts /cromwell_root/broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/CNVValidation/6246d2fa-a8e5-49c0-ac38-8ae33867f394/call-cnvPair/CNVSomaticPairWorkflow/c8a063ce-0309-45a1-a6cf-25fdcfe4245c/call-CollectAllelicCountsNormal/attempt-3/G25783.TCGA-55-6986-11A-01D-1945-08.2.allelicCounts.tsv \; \; --maxNumSegmentsPerChromosome 500 \; --minTotalAlleleCount 30 \; --genotypingHomozygousLogRatioThreshold -10.0 \; --genotypingBaseErrorRate 0.05 \; --kernelVarianceCopyRatio 0.0 \; --kernelVarianceAlleleFraction 0.01 \; --kernelScalingAlleleFraction 1.0 \; --kernelApproximationDimension 100 \; --windowSize 8 --windowSize 16 --windowSize 32 --windowSize 64 --windowSize 128 --windowSize 256 \; --numChangepointsPenaltyFactor 1.0 \; --minorAlleleFractionPriorAlpha 25.0 \; --numSamplesCopyRatio 100 \; --numBurnInCopyRatio 50 \; --numSamplesAlleleFraction 100 \; --numBurnInAlleleFraction 50 \; --smoothingThresholdCopyRatio 2.0 \; --smoothingThresholdAlleleFraction 2.0 \; --maxNumSmoothingIterations 10 \; --numSmoothingIterationsPerFit 0 \; --output . \; --outputPrefix G25783.TCGA-55-6986-11A-01D-1945-08.2; ); echo $? > /cromwell_root/ModelSegmentsNormal-rc.txt.tmp; (; cd /cromwell_root. ); sync; mv /cromwell_root/ModelSegmentsNormal-rc.txt.tmp /cromwell_root/ModelSegmentsNormal-rc.txt```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580:1791,echo,echo,1791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342955580,1,['echo'],['echo']
Availability,And/or include a link to stderr in the failure message?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292,1,['failure'],['failure']
Availability,"Another error w/ this test:. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2444/testReport/junit/cromwell.core.actor/RobustClientHelperSpec/RobustClientHelper_should_reset_timeout_when_backpressured_is_received/. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,4,"['Robust', 'error']","['RobustClientHelperSpec', 'error']"
Availability,"Another example of a workflow complete failure possibly due to grabbing the hash from google. Workflow 0c7da038-172a-4081-8850-c87fec05f4c1. ```; 2016-04-26 20:52:05,279 cromwell-system-akka.actor.default-dispatcher-28 ERROR - WorkflowActor [UUID(0c7da038)]: Completion work failed for call HaplotypeCaller:46.; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:39,failure,failure,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,5,"['ERROR', 'Error', 'error', 'failure']","['ERROR', 'Error', 'errors', 'failure']"
Availability,"Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:498,echo,echo,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,"['echo', 'error']","['echo', 'error']"
Availability,"Another note to add that while i can't reproduce Yossi's error, he & I had previously identified `Scope.fullyQualifiedName` as a possible culprit. I decided to look at why MWDA is so much slower and that *is* being gated by the same function, to the tune of (at the time of this writing) 84.6% (and rising) of the total runtime so far.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114329,1,['error'],['error']
Availability,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429,3,['error'],['error']
Availability,"Any thoughts on how a general task retry policy may interact with [#1499](https://github.com/broadinstitute/cromwell/issues/1499) when the job scheduler (or user) kills a job on sge and the server is restarted?. I'm envisioning the ""check-alive"" result showing no job on sge invoking one of the task retries that a user specifies (whereas currently it'd be marked as failed). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515:239,alive,alive,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515,1,['alive'],['alive']
Availability,"Any updates on this front? We’ve been having issues with occasional spikes in memory usage that don’t abide by a linear model for memory allocation. Currently this requires a lot of “babysitting” for our pipelines (or overprovisioning of memory), reducing their reliability and increasing their cost (on GCP, though the main cost factor is still developers’ time..). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157:262,reliab,reliability,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157,1,['reliab'],['reliability']
Availability,"Anyone can can thumbsup and merge at will, but pinging @geoffjentry if no one else steps up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/572#issuecomment-197445359:47,ping,pinging,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/572#issuecomment-197445359,1,['ping'],['pinging']
Availability,"Anyone have a concrete solution to this, we are also getting the permission denied error with our aws batch setup. We have even included chmod 777 in the cloud init script to ensure that directory is accessible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-531922228,1,['error'],['error']
Availability,"Apologies if it's there & i misunderstood what was going on, but would it be feasible to add a test somehow checking that it's working as intended?. I know, I know, Jeff is actually asking for tests? Up is down, cats and dogs living together in sin ....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3464#issuecomment-377058006:206,down,down,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3464#issuecomment-377058006,1,['down'],['down']
Availability,"Apologies, that metadata appears to have disappeared, but the same issue is referenced here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538:158,error,error-the-local-copy-message-must-have-path-set,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538,1,['error'],['error-the-local-copy-message-must-have-path-set']
Availability,Are we ok to unilaterally make these changes? Are agora and rawls already building out functionality based on our existing behavior? Will they be resilient to us making these changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-485466032:146,resilien,resilient,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-485466032,1,['resilien'],['resilient']
Availability,"Are we using single inserts or batch inserts when writing to the DB? That can cause huge performance gains due the reduction in chatter. > On Dec 27, 2016, at 1:08 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the WriteMetadataActor, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...); > ; > It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957:391,down,down,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269315957,2,['down'],['down']
Availability,Are you still looking for comments on this or should it be closed for repairs?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154397266:70,repair,repairs,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/269#issuecomment-154397266,1,['repair'],['repairs']
Availability,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:302,error,error,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,1,['error'],['error']
Availability,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['error'],['error']
Availability,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436,2,['error'],['error']
Availability,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:160,alive,alive,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['alive'],['alive']
Availability,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:82,error,error,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398,1,['error'],['error']
Availability,"As a **workflow runner**, I want **Cromwell to automatically retry my workflow with increased memory/disk/on a specific error code, etc**, so that I can **get my workflow to complete without having to manually intervene**.; - Effort: **?** @geoffjentry ; - Risk: **Medium**; - if users are unaware that they have retries set in ways that would cost them a lot the 2nd or tertiary run, i.e to double their memory, they could end up paying for a much more expensive VM when a smaller one would do; - Business value: **Large**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408:120,error,error,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-327935408,1,['error'],['error']
Availability,"As a **workflow runner**, I want **use Command+C to abort running jobs**, so that I can **fully shut down Cromwell using a standard command shortcut**.; - Effort: **?** @geoffjentry ; - Risk: **?**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426:101,down,down,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328200426,1,['down'],['down']
Availability,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,recover,recover,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398,1,['recover'],['recover']
Availability,"As far as I know it's never been tested so I told @takutosato that it probably wasn't working yet, but a quick test with something like. ```; String a = ""hello"". task t {; String i; command {; echo ${i}; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = a }; }; ```. proved me wrong so apparently yes it's live !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310:193,echo,echo,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689#issuecomment-261650310,1,['echo'],['echo']
Availability,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872,1,['error'],['error']
Availability,As per hackathon - this test doesn't reliably fail in the intended circumstances. Closing in lieu of #4848,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484879049:37,reliab,reliably,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484879049,1,['reliab'],['reliably']
Availability,"As tech debt, I maybe would like to see things like `unwrapOutputValues` broken into utility functions. Not going to hold this PR up any longer figuring out this exact refactoring, but looking at current code I'm picturing something along the starting lines of:. ``` scala; // Sort of like Future.sequence, but with; // unwrapOutputValues's Failure(new Throwable(messages.mkString)); def sequenceMessages[T](in: Seq[Try[T]]): Try[T]. // If there are any failures, uses sequenceMessages to create the Failure; def unwrapValues[K,V](in: Map[K,Try[V]]): Try[Map[K,V]]; ```. :+1: For merging for the current code @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449:341,Failure,Failure,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449,3,"['Failure', 'failure']","['Failure', 'failures']"
Availability,As usual I can't really tell what the CI failures mean... they don't seem correlated to anything in particular?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124,1,['failure'],['failures']
Availability,"Aside: I don't think having that sort of subtlety in any language, much less wdl, is particularly awesome. Aside part deux: If we go down this path we should think carefully how it is worded in the spec. WDL shouldn't be aware of call caching, so dropping in a feature that's effectively purely for call caching needs some explanation beyond that :) The whole cloud path/not for localization seems like the right official reason. This is of course assuming that this whole scheme gets @kcibul what he needs in the first place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413:133,down,down,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305992413,1,['down'],['down']
Availability,"Attached is some documentation that works for v52 and should work for v53. On Wed, Sep 9, 2020 at 9:20 AM mderan-da <notifications@github.com> wrote:. > Hi @markjschreiber <https://github.com/markjschreiber> I'm also running; > into this error. I am using cromwell 53 with a custom cdk stack based on; > the CloudFormation infrastructure described here:; > https://docs.opendata.aws/genomics-workflows/; >; > Are modifications needed for compatibility with newer versions of; > Cromwell? Are these documented somewhere?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EO6WEE4BYYPTX4HZ2LSE56JXANCNFSM4G23FFUQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691326074:238,error,error,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691326074,1,['error'],['error']
Availability,"Based on the conversation after standup yesterday, this ticket needs more refinement. Currently, recovering a call for the local backend is the same as executing a fresh call. It's possible there are other ways to wire recovery and that question needs to be answered. @kcibul @geoffjentry I'm returning it to the milestone backlog but hopefully something we can discuss at the prioritization today--seems like a technical/PO type of refinement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645:97,recover,recovering,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666#issuecomment-235909645,2,['recover'],"['recovering', 'recovery']"
Availability,"Beyond tidiness, do you think this is going to have benefits for test reliability?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425455093:70,reliab,reliability,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425455093,1,['reliab'],['reliability']
Availability,"Beyond tidiness, this PR for @aweng98 to run the unit tests in Jenkins. It won't affect reliability beyond allowing certain tests to pass. Here is a previous failed run without all of these changes: https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/21/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425475944:88,reliab,reliability,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169#issuecomment-425475944,1,['reliab'],['reliability']
Availability,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['failure'],['failures']
Availability,Build 3730. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:188,failure,failure,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['failure'],['failure']
Availability,Builds for me in IntelliJ (incl clean build) but I can reproduce this error locally with `sbt assembly`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430,1,['error'],['error']
Availability,But I'm having trouble running even 500 tasks without one of these JES failures. Is the fact that I using pre-emptible instances matter?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663:71,failure,failures,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298917663,1,['failure'],['failures']
Availability,"But the job ran successfully. Here is the full logs:. [ec2-user@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-43",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:752,heartbeat,heartbeat,752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2543,heartbeat,heartbeat,2543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,4,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3365,recover,recoverWith,3365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['recover'],['recoverWith']
Availability,CallableMaker$Ops.toWomCallable(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable$(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$ops$$anon$1.toWomCallable(WomCallableMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:129); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:21); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode$(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$ops$$anon$1.toWomCallNode(WomCallNodeMaker.scala:9); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:68); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:3269,Error,ErrorOr,3269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,Can that nice error message be removed from its `Option` container?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252,1,['error'],['error']
Availability,Can we also have a test to exercise the multiple errors validations?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812:49,error,errors,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/562#issuecomment-197047812,1,['error'],['errors']
Availability,"Can you post the WDL, or at least part of it? I've issues like this happen where you declare the path to the file as a `String` instead of a `File`, so it never gets actually downloaded to the local filesystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436144957:175,down,downloaded,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436144957,1,['down'],['downloaded']
Availability,"Can you post your WDL? Searching for `The label in the input is too long` on the 'net suggests that the HTTP resolver is being passed a domain that is too long. I suppose it's possible that you aren't even using HTTP inputs, but when we try all resolvers and the HTTP one fails, the error is not handled correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647:283,error,error,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647,1,['error'],['error']
Availability,Can you put a brief comment with the exception? I've never seen this error before and I'm curious. Besides that :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/275#issuecomment-154549091:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/275#issuecomment-154549091,1,['error'],['error']
Availability,"Can you try with parens?; ```; command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; ```; The parser does seem to be out of spec, but maybe we can give it a nudge in the right direction this way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987:44,echo,echo,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987,1,['echo'],['echo']
Availability,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:760,error,error,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Can-of-wormsy ToL: if `reference.conf` doubles as our config documentation, could we include this there? Otherwise, could we write it down *somewhere*?. I like not cluttering the `reference.conf` but I also don't want to have to rummage through some (I've-already-forgotten-which) class file to find how to change this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316724933,1,['down'],['down']
Availability,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:33,avail,available,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676,1,['avail'],['available']
Availability,"Chris, thanks for the idea. I tried this and unfortunately had the same issue. From the behavior it looks like http is working in that the files get downloaded, but they don't get proper naming with numerical names instead of the expected file names. This disconnect seems to be what causes issues when passing these on to the CWL tools.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-427443254:149,down,downloaded,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-427443254,1,['down'],['downloaded']
Availability,"Chris;; Thanks for working on this and for the test case to iterate with. This example does work for me in the sense that it generates an md5sum, but also demonstrates the underlying issue I'm having with https inputs. I also get them downloaded and staged into my pipeline, but the file names get mangled into random download number. md5sum is cool with this, but many of my real tasks fail because the expected file extensions and associated secondary file extensions get lost with the random file names. Here's the example output I get from running this that demonstrates the file naming issue:; ```; /usr/bin/md5sum '/home/chapmanb/tmp/cromwell/cromwell_work/cromwell-executions/main-http_inputs.cwl/093e2835-e4cc-4731-9248-88d74dec0977/call-sum/inputs/1515144/1710814112361209342' | cut -c1-32; ```; This input should be called `jamie_the_cromwell_pig.png` but instead gets a long number attached to it. Is it possible to preserve initial file names with https like happens with other filesystem types?. In terms of the test cases, it would be great if it also checked that the file extension and name get preserved. Thanks again for looking at this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999:235,down,downloaded,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999,2,['down'],"['download', 'downloaded']"
Availability,Closing - we believe this is fixed and haven't heard this error pop up since. Closing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-519643915,1,['error'],['error']
Availability,Closing as redundant. See #2638.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725:11,redundant,redundant,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2569#issuecomment-331458725,1,['redundant'],['redundant']
Availability,Closing issue: . - In agreement that breaking existing behavior is not a good approach. ; - Need to work out better understanding of cost per sample especially with regards to failure preemption or otherwise. . I'm likely going to soft-fork internally for certain projects and gather some hard numbers.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235:176,failure,failure,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235,1,['failure'],['failure']
Availability,Closing since the error message now contains (a) that a file was missing (b) the appropriate file name,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224,1,['error'],['error']
Availability,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,redundant,redundant,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540,1,['redundant'],['redundant']
Availability,"Confirmed that this issue has been resolved, a proper error pops up when missing a required refresh token.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156#issuecomment-252258248,1,['error'],['error']
Availability,"Confirmed the error is improved in v39:. Looks similar to this. ; ```""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Please check the log file for more details: gs://ss_cromwell_bucket/cromwell-execution/exceed_disk_size/d142f233-f72a-40a6-9f84-8b8a2ead32e7/call-simple_localize_and_fetch_size/simple_localize_and_fetch_size.log.""```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379,2,['error'],['error']
Availability,"Cool, then you are probably good to go :) Did you want feedback on something in particular? I don't know why you pinged me but I'm glad to see you have a new command!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873:113,ping,pinged,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873,2,['ping'],['pinged']
Availability,"Could you comment on the granularity?. Is it ""cromwell is alive, hooray!"" or some more involved breakdown of various subsystems, e.g. ""can't talk to JES right now, the DB is being a poop, but SGE is going swimmingly!""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/898#issuecomment-222184572:58,alive,alive,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/898#issuecomment-222184572,1,['alive'],['alive']
Availability,"Crom support went back to redteam; here is the content from the dsde-docs issue:. ----. There's documentation in the CHANGELOG but nothing in the README, though what's in the CHANGELOG might suffice for the README. I don't know any more than this anyway, @Horneth is the expert. 😛 . * Add support for Google Private IPs through `noAddress` runtime attribute. If set to true, the VM will NOT be provided with a public IP address.; *Important*: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If it's not whitelisted and you set this attribute to true, the task will hang.; Defaults to `false`.; e.g:; ```; task {; command {; echo ""I'm private !""; }. runtime {; docker: ""ubuntu:latest""; noAddress: true; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999:666,echo,echo,666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1922#issuecomment-375075999,1,['echo'],['echo']
Availability,Cromwell 0.16 is available now on Macs via `brew install cromwell`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870:17,avail,available,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870,1,['avail'],['available']
Availability,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117,2,"['error', 'failure']","['error', 'failure']"
Availability,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:385,down,down,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,2,['down'],['down']
Availability,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:46,heartbeat,heartbeats,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028,10,"['down', 'heartbeat']","['down', 'heartbeat', 'heartbeats']"
Availability,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:495,down,down,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891,2,['down'],['down']
Availability,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:191,down,down,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748,1,['down'],['down']
Availability,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,recover,recovery,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345,1,['recover'],['recovery']
Availability,"DATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1727,down,down,1727,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['down'],['down']
Availability,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:458,avail,available,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391,1,['avail'],['available']
Availability,"Depending on what your monitoring script does and how long your command takes to run it's possible that the task finishes before the monitoring script had time to write / flush anything into the monitoring log.; I ran this and got an empty log . ```; task t {; command {; echo ""hey""; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```. but this gave me a non-empty one . ```; task t {; command {; sleep 5; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034:272,echo,echo,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034,1,['echo'],['echo']
Availability,"Desktop Docker is not the most reliable platform for real work in Cromwell, but it is interesting that a specific version broke it. Do you have time to do a `git bisect` between 55 and current?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044:31,reliab,reliable,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044,1,['reliab'],['reliable']
Availability,"Detritus still errors on hard linking, I think we may not have created the execution directory first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382531175:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3532#issuecomment-382531175,1,['error'],['errors']
Availability,"Did someone say breaking changes? *starts sweating*. Ping is much appreciated. Could we plan to go over the changes together so that you can help us identify what needs to change in our user docs / course materials? . Re: location of docs, @katevoss and I have been discussing this a fair amount, and the latest proto-consensus we came to was that the forum is ultimately not the right place for Cromwell docs proper (as opposed to the end-user/n00b materials that Comms produces) in part because it's too detached from the codebase itself. Based on the audience and technical constraints, we think something like ReadTheDocs will be a more suitable platform. Kate is obviously going to be otherwise occupied for several more weeks, but I might be able to take a stab at prototyping what a ""Read the (Cromwell) Docs"" solution would look and feel like in a week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886:53,Ping,Ping,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311445886,1,['Ping'],['Ping']
Availability,"Did you test this in real life? Due to the mounting system in containers soft-links may not work at all. This is why they are rightfully banned in docker.; I believe singularity works almost the same. There is no guarantee that the soft-linked target will exist in the container. The filesystem might not be present there, or have a different name.; Just use hard-links, these are much more reliable when working with containers and just as fast.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097:391,reliab,reliable,391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097,1,['reliab'],['reliable']
Availability,Did you try the test WDL and json provided by robthompsonweb? Any of my workflows have this same error and they all have essentially that same basic structure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516545076:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-516545076,1,['error'],['error']
Availability,Discussed in person but probably not (assuming that the two modes of failure are what I was seeing a couple of weeks ago),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640:69,failure,failure,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094#issuecomment-235613640,1,['failure'],['failure']
Availability,Discussed in person w/ @Horneth - we'll keep the implementation specific split for now. It seems likely that in the future it'll boil down to a cloud/not-cloud split but at the moment it's SFS/PAPI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2330#issuecomment-306282239,1,['down'],['down']
Availability,Do they both contain that error string?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429906924:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4247#issuecomment-429906924,1,['error'],['error']
Availability,Do you happen to have the resulting error handy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519#issuecomment-464203099:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519#issuecomment-464203099,1,['error'],['error']
Availability,Do you have an example before & after?. It seems like the output would contain `[First $limitBytes bytes]` from `annotatedContentAsStringWithLimit` which is a pretty strange thing to have in the middle of an error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536,1,['error'],['error']
Availability,"Documentation can be downloaded from here; https://cromwell-share-ad485.s3.us-east-2.amazonaws.com/InstallingGenomicsWorkflowCoreWithCromwel52.pdf. On Sun, Sep 13, 2020 at 4:48 PM Yaomin Xu <notifications@github.com> wrote:. > @markjschreiber <https://github.com/markjschreiber> running into the same; > error for both v52 and v53.1. I am using the same CloudFormation; > @mderan-da <https://github.com/mderan-da> mentioned . Appreciate your; > newer documentation on this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-691723254>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKCM56WST3J6NO5CS3SFUVYLANCNFSM4G23FFUQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-692083983:21,down,downloaded,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-692083983,2,"['down', 'error']","['downloaded', 'error']"
Availability,Does it happen all the time or episodically ? Dockerhub had some downtime in the past few days,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-434835954:65,downtime,downtime,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-434835954,1,['downtime'],['downtime']
Availability,"Does this mean that, because `\\.` is *not* on the list of accepted double-quote uses, it should be an error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412159357:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412159357,1,['error'],['error']
Availability,"Don't know if this is related but I'm seeing this type of failure submitting [this workflow](https://github.com/bcbio/test_bcbio_cwl/blob/master/prealign/prealign-workflow/main-prealign.cwl) by url:. ```; MaterializeWorkflowDescriptorActor [UUID(dfefc8c0)]: Parsing workflow as CWL v1.0; 2018-12-10 13:27:22,372 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/main-prealign.cwl; 2018-12-10 13:31:56,222 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/organize_noalign.cwl; 2018-12-10 13:32:14,196 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples_to_rec.cwl; 2018-12-10 13:32:32,071 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples.cwl; 2018-12-10 13:32:49,793 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl; 2018-12-10 13:33:07,284 cromwell-system-akka.dispatchers.engine-dispatcher-34 ERROR - WorkflowManagerActor Workflow dfefc8c0-c3a1-449c-a747-13147bf8b980 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:58,failure,failure,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939,2,['failure'],['failure']
Availability,"Don't merge yet, I just discovered another small issue which should take care of the last tyburn failure",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191442121:97,failure,failure,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/499#issuecomment-191442121,1,['failure'],['failure']
Availability,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:910,Error,Error,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"E.g. (I think). ```; task foo {; File bar; command { ; echo ""contents of ${filename(bar)}:""; cat ${bar}; }; ```. Would generate the script:. ```; echo ""contents of myFile.txt:""; cat /home/chrisl/superexcitingfiles/myFile.txt; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746:55,echo,echo,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255440746,2,['echo'],['echo']
Availability,"E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorld.txt"" | 2018-11-21 15:09:10.767000 | string |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:2277,echo,echo,2277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['echo'],['echo']
Availability,"Er, what I meant to write was, can you please create this branch in the main Cromwell repo and PR that instead? Having branches in private repos has been causing review and maintenance pain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2777#issuecomment-339033675:173,mainten,maintenance,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2777#issuecomment-339033675,1,['mainten'],['maintenance']
Availability,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:787,alive,alive-or-dead,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,4,['alive'],"['alive', 'alive-or-dead']"
Availability,Error message from this failed test on PAPI 🤔 . ```Could not copy gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/home/travis/build/broadinstitute/cromwell/common-workflow-language/v1.0/v1.0/gs://centaur-cwl-conformance/cwl-inputs/Hello.java to gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/Hello.java```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137,1,['Error'],['Error']
Availability,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:0,Error,Error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364,2,['Error'],['Error']
Availability,Esprit d'escalier: Does FC know this is coming? Is it going to slow down their adoption of Cromwell 30?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347207357:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347207357,1,['down'],['down']
Availability,"Even though it is accurate to blame Google, from the user’s point of view it’s Cromwell that’s behaving unexpectedly. So I agree with Chris’s sentiment. I also think it’s worth it to mention “error code 10”, it’s a phrase now widely known and feared in DSP. (Perhaps the right qualification is, “this condition manifests itself as error code 10, but not all code 10s indicate this error”.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403:192,error,error,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403,3,['error'],['error']
Availability,Example error output here: https://travis-ci.com/github/broadinstitute/cromwell/jobs/516477139,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814,1,['error'],['error']
Availability,Excellent! The build failures are due to the MySQL quirks addressed by #160; once that's merged to scatter-gather and this is rebased I expect the build will go green.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384:21,failure,failures,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384,1,['failure'],['failures']
Availability,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"FWIW 2: it's also possible to ""fix"" the tests by pushing the flush interval back to ~2s for the tests - thus reducing the chance that the probe messages are sent at the same time as a regular flush message. The downside to that was that the tests were taking 20 seconds each, which didn't feel great (and even though unlikely, there was still a small chance of an accident happening)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038:211,down,downside,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038,1,['down'],['downside']
Availability,"FWIW [the documentation](https://gsaweb.broadinstitute.org/wdl/devzone/) says `read_json` will do what I expect! Ctrl+F ""Array deserialization using read_json()"". Here's the WDL task I was trying to validate:. ```; # reverses a json array; task reverseArray {; Array[Int] intArray; ; command {; python -c ""import sys; print(list(map(int, sys.argv[-1:0:-1])))"" ${sep=' ' intArray}; }; ; output {; Array[Int] outArr = read_json(stdout()); }; }; ```. And got the error:. ```$ java -jar wdltool-0.4.jar validate json_things.wdl; ERROR: Could not determine type of declaration outArr:; Array[Int] outArr = read_json(stdout()); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754:460,error,error,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1825#issuecomment-271403754,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"FWIW, in the GATK world we just blanket refuse to support anything Windows. Every now and then we get a question from someone who edited a file manually on a Windows box -- but it happens *maybe* twice a year. I wouldn't advocate for putting a huge amount of effort into this beyond recognizing the error and providing an informative message if possible...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193,1,['error'],['error']
Availability,"FYI - @ruchim I assigned this to you for prioritization. We've seen 6 workflows fail since 5/10, including a couple in the past day. We've also seen it a couple times in the controller test. This is the same total failure rate as https://github.com/broadinstitute/cromwell/issues/738, but is slightly higher priority because it's happened more recently. Whenever we get an issue for ""Error during processing of request HttpRequest"" this will be well below that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-221895948:214,failure,failure,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-221895948,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"FYI for reviewers. I've added the following additional changes to this commit:. * Added retry handling for a few transient errors I saw during test runs; * Moved stdout/stderr/rc file writes to s3 into the proxy to resolve an intermittent timing issue seen in testing; * Removed the MIME parsing, as this is no longer needed due to the above change. This resolves #3748; * Addressed a TODO in the code regarding a large number of input parameters; * Removed broken code in the proxy (since the shell would continue, this ultimately is a cosmetic change); * Moved the actual command text from the job definition (RegisterJobDefinition) to the job submission (SubmitJob) call. This balances out the payload to allow more work in the job before hitting AWS Batch payload limits: https://docs.aws.amazon.com/batch/latest/userguide/service_limits.html. Additionally it should allow easier consolidation of job definitions in the future. #3750 ; * Fixed the unit tests, which I broke during the initial implementation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4020#issuecomment-415567505:123,error,errors,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4020#issuecomment-415567505,1,['error'],['errors']
Availability,"FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied. Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469:325,failure,failures,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469,1,['failure'],['failures']
Availability,FYI ready for 👀 @rsasch and @salonishah11. Regarding the failing dbms test: it's not failing due to this PR's changes and is not a blocker for review. I suspect that failure has something to do with lucky number [PostgreSQL 13](https://news.ycombinator.com/item?id=24578166) coming out yesterday. When @cjllanwarne finishes fighting Prod fires he (or perhaps someone else?) might have a small PR that can fix the issue in develop. When that fix merges I'll just restart the test in this PR and it should pick up the develop changes automagically and pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699:166,failure,failure,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699,1,['failure'],['failure']
Availability,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:351,down,downloads,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['down'],['downloads']
Availability,"FYI, tracked down the bug. It's on the Google side. My guess is that we; won't need the GenomicsScopes at all, and I can just reimplement the; application default credentials with the java object that you guys already; have. Alternately, we can replace that with GenomicsScopes once they work; better. Will mail a PR shortly. On Mon, Jan 4, 2016 at 11:02 AM, kshakir notifications@github.com wrote:. > Abandoning this PR.; > ; > @kcibul https://github.com/kcibul's feature requests should be captured; > in another ticket.; > ; > @tovanadler https://github.com/tovanadler is going to look at; > reimplementing her basic changes in #329; > https://github.com/broadinstitute/cromwell/pull/329 on the latest; > develop, and testing manually to ensure the scopes are all correct.; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168716322; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168724071:13,down,down,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-168724071,1,['down'],['down']
Availability,"Failing workflow:. ```wdl; version 1.0. workflow foo {; call bar; output {; Array[File] baz = bar.baz; }; }. task bar {; input {; Array[Array[String]]? baz ; }; command <<<; x=~{ if defined(baz) then write_tsv(baz) else '' }; if [[ -z ""$x"" ]]; then; echo ""no file""; else; cp $x output.tsv; fi; >>>; output {; Array[File] baz = glob(""*.tsv""); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524242:250,echo,echo,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524242,1,['echo'],['echo']
Availability,"Fails the workflow. On Mon, Sep 18, 2017 at 5:21 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/leetl1220> when you get the ""file not; > found"" errors, does it fail the workflow? Or does it still continue?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330359991>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkxMH5YiFxEFZPeULlAAXomDFkWi2ks5sjt7PgaJpZM4PZZvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330401409:179,error,errors,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632#issuecomment-330401409,1,['error'],['errors']
Availability,Failure and RetryableFailure are 2 different messages so in this case I think this is a no-op ? We just never return RetryableFailure messages in Local ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102:0,Failure,Failure,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102,1,['Failure'],['Failure']
Availability,"Failures are unrelated, merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/188#issuecomment-141186133:0,Failure,Failures,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/188#issuecomment-141186133,1,['Failure'],['Failures']
Availability,"Fantastic instincts. This fails in the same develop and passes in the same 29:. ```wdl; task two_input_task {; String inA; String inB; command { echo ${inA} and ${inB} }; runtime { docker: ""ubuntu"" }; }. workflow duplicate_scatter_input {; String my_input = ""input""; scatter (i in range(0)) {; call two_input_task { input: inA = my_input, inB = my_input }; }; }; ```. Produces in this error on develop:; ```java; [2017-12-03 14:48:08,10] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-03 14:48:09,17] [error] WorkflowManagerActor Workflow 986bbc71-2b80-4cf1-aade-e93dd77062b0 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2973#issuecomment-348778521:145,echo,echo,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2973#issuecomment-348778521,3,"['echo', 'error']","['echo', 'error']"
Availability,"Feel free to make this PR redundant 😛 If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,redundant,redundant,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681,1,['redundant'],['redundant']
Availability,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:98,avail,available,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['avail'],['available']
Availability,"Finally I figure out one solution, but it is a little bit ugly and still look for an elegant way:. - Global variables WDL as below:. ```; workflow global {; }. task init {; command { }; output {; String version = ""v1.0""; String reference = ""hg19"". }; }; ```. - Pipeline WDL as below:; ```; import ""global.wdl"" as global. workflow pipeline {; # Global variables; call global.init; String version = init.version; String reference = init.reference; # Pipeline variables; String sample_id = ""Sample_001""; call snp { input: version = version, reference = reference, sample_id = sample_id }; }. task snp {; String version; String reference; String sample_id; command { echo ""SNP_${version} for ${sample_id} on ${reference}!"" }. output { String out = read_string(stdout()) }; }. ```; The final result is:; ```; [2018-11-21 18:23:14,32] [info] BackgroundConfigAsyncJobExecutionActor [a225847apipeline.snp:NA:1]: echo ""SNP_v1.0 for Sample_001 on hg19!""; ```. And you can see global variables are passed to the pipeline WDL while there are some workaround such as empty global workflow and helper task of init.; Is there any other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243:663,echo,echo,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243,2,['echo'],['echo']
Availability,"First point: Looking at the code again, the only thing that can possibly be sent to metadata after the status of the workflow becomes terminal are workflow failures and end time, due to the ordering of the `onTransition` blocks in the WorkflowActor. I swapped them in the last commit, so from now on nothing should be sent to the metadata after the terminal status event is sent.; I'm happy to add an extra check for robustness but I don't believe it to be necessary. Second point: If the DB is busy writing data, the `WriteMetadataActor` will be in state `WritingToDb`, in which case it will always reply with `HasPendingWrites`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027:156,failure,failures,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289810027,2,"['failure', 'robust']","['failures', 'robustness']"
Availability,"First test (@dtenenba built the PR 4412 and I tested it):. Workflow: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; First input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; Second input json: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. and... drumroll please...... IT WORKED!!!!!!!!!!! ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": true,; ""result"": ""Cache Hit: 98bc2232-f147-419f-9351-49a07daa1720:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"",; ```; And the workflow is ""generating"" the files WAY faster than it should be if it were doing it de novo, so we seem to be getting the correct outputs moved into the new workflow directory as well. . Caveats: ; I did test it with an actual batch and it failed with the job definition error. But as long as PR 4412 was not intended to fix THAT issue as well, I can say it appears on the first pass that call caching with AWS backend might very well be working with an outside test!!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623:1048,error,error,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-480313623,1,['error'],['error']
Availability,"Fixed in #1252. ""walltime"" may now read as a backend specific runtime attribute just like any other. I've called the runtime attribute ""sge_walltime"" below. ```; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; String? sge_walltime; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out} \; -e ${err} \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${""-l h rt="" + sge_walltime } \; ${script}; """""". kill = ""qdel ${job_id}"". check-alive = ""qstat -j ${job_id}"". job-id-regex = ""(\\d+)"". }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862:684,alive,alive,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-242567862,1,['alive'],['alive']
Availability,"Fixed the [proof of concept code](https://github.com/broadinstitute/cromwell/compare/develop...rhpvorderman:relativeImports). Now the WOMTOOL is able to handle absolute paths correctly. I can run `java -jar /home/ruben/test/base/womtool-31-1df94fa-SNAP.jar validate /home/ruben/test/base/workflow.wdl ` in any directory on the filesystem and get the same result. However cromwell still uses $PWD to evaluate the base directory. I can see the WOMtool uses the following code to load the WDL file:; ```scala; private[this] def loadWdl(path: String)(f: WdlNamespace => Termination): Termination = {; WdlNamespace.loadUsingPath(Paths.get(path), None, None) match {; case Success(namespace) => f(namespace); case Failure(r: RuntimeException) => throw new RuntimeException(""Unexpected failure mode"", r); case Failure(t) => UnsuccessfulTermination(t.getMessage); }; }; ```; But for cromwell there does not seem to be such a straightforward loading of the wdlfile. Can somebody point me to this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047:708,Failure,Failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-369579047,3,"['Failure', 'failure']","['Failure', 'failure']"
Availability,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:23,avail,available,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550,1,['avail'],['available']
Availability,"Fixes the false build errors from contributors, such as during https://github.com/broadinstitute/cromwell/pull/3684",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3687#issuecomment-391864246:22,error,errors,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3687#issuecomment-391864246,1,['error'],['errors']
Availability,"Fixes the workaround syntax in #1126, but doesn't completely repair the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582:61,repair,repair,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282571582,1,['repair'],['repair']
Availability,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:147,echo,echo,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541,2,['echo'],['echo']
Availability,"For our use cases, I’d say put responsibility on the pipeline developer. If there’s a collision, which file “wins” may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, we’d like to distinguish “pipeline failure” from “output failure” in an automated way. So if it’s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:272,failure,failure,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467,6,"['Failure', 'error', 'failure']","['Failure', 'error', 'failure']"
Availability,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:237,down,down,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225,4,"['down', 'error']","['down', 'error', 'errors']"
Availability,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:322,Down,Downloading,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,3,"['Down', 'avail', 'down']","['Downloading', 'available', 'download']"
Availability,"From Gitter:; > we're submitting jobs via API to remote cromwell server, and want to submit workflows with all the imports resolved already (client-side) so that querying cromwell metadata submittedFiles.workflow value shows verbatim what's being executed. Is there a way in wdl4s for example to do something effectively like: `val ns = NamespaceWithWorkflow.load(myWorkflow, myResolver); val wfAsString = ns.toWdlSource` i.e. get the string representation of the workflow back again, but with the imports resolved (""expanded"")?. @cjllanwarne your gist is no longer available, do you remember what you wrote?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494:566,avail,available,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2691#issuecomment-335849494,1,['avail'],['available']
Availability,"From my investigation, those were due to transient failures of ontology parsing, which this should reduce now: https://github.com/broadinstitute/cromwell/pull/4210",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750:51,failure,failures,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750,1,['failure'],['failures']
Availability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:156,error,errors,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,1,['error'],['errors']
Availability,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,2,"['error', 'failure']","['error', 'failures']"
Availability,"Further discussed points:; - The delete endpoint will be implemented by immediately marking a workflow as effectively ""to be deleted""; - An actor will occasionally sweep through these ""to be deleted"" workflows and remove the previously discussed bullet points; - Upon success, the workflow will be gone from the database; - Upon failure, the workflow will be left in a ""failed to delete"" state in the database; - One may re-mark an existing workflow as ""to be deleted"" by re-using the delete endpoint; - If a workflow output is already deleted, we won't try to re-delete it. However--; - As the refresh token will not be available, output files that were generated with refresh tokens will _not_ be able to be deleted. These types of errors, while expected, will require more planning on how to handle these permissions issues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448:329,failure,failure,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-257604448,3,"['avail', 'error', 'failure']","['available', 'errors', 'failure']"
Availability,GCP Batch backend failure doesn't block merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497,1,['failure'],['failure']
Availability,"GCP shut down Genomics on July 9, 2024 and the backend will be removed from Cromwell. Life Sciences is the replacement, for example; ```; endpoint-url = ""https://lifesciences.googleapis.com/""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185:9,down,down,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185,1,['down'],['down']
Availability,"Given a Local config like:. ```; runtime-attributes = """"""; String? docker; String? docker_user; String? docker_env; """""". submit-docker = """"""docker run ${docker_env} --rm ${""--user "" + docker_user} -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; ```. Docker environment variables could be passed with a WDL like:; ```; task build_env {; Array[String] kvs = [""k1=v1"", ""k2=v2"", ""k3=v3""]; Array[String] prefixed = prefix(""-e "", kvs); command {; echo ""${sep=' ' prefixed}""; }; output {; String out = read_string(stdout()); }; }. task docker_task {; String docker_env. command {; echo $k1; echo $k2; echo $k3; }. runtime {; docker: ""ubuntu:latest""; docker_env: ""${docker_env}""; }. output {; Array[String] out = read_lines(stdout()); }; }. workflow w {; call build_env; call docker_task { input: docker_env = build_env.out }; output {; Array[String] out = docker_task.out; }; }; ```. Having to use a separate task to stringify an array of String kv environment pairs is a little clunky, but it looks like the way the `${sep=' '...}` expansion works currently requires this to be done in the command section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410:475,echo,echo,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-273916410,4,['echo'],['echo']
Availability,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185,4,"['Failure', 'error']","['Failure', 'error']"
Availability,"Given the re-architecting of 0.21+ Cromwell this particular error can no longer occur, but please let us know if you see any other problems. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869#issuecomment-253919168:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869#issuecomment-253919168,1,['error'],['error']
Availability,Global comment: I think based on a problem we saw last week that the `check-alive` command in `application.conf` is wrong for SGE. Should be `qstat -j ${job_id}`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965:76,alive,alive,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319#issuecomment-241748965,1,['alive'],['alive']
Availability,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314,1,['failure'],['failures']
Availability,Going with changing Martha to try-and-silence accessUrl generation failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463:67,failure,failures,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463,1,['failure'],['failures']
Availability,"Good catch! I closed #2281. . Transferring conversation here:; From @MatthewMah ; > When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive. . From @geoffjentry ; > @katevoss if I managed to correlate this correctly w/ the previous issue I was discussing w/ @kshakir it sounded like it isn't a huge deal, just that there's some nuance to it. From @cjllanwarne:; > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279:345,failure,failures-based-on-check-alive,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328126279,6,"['alive', 'failure']","['alive', 'failure', 'failures-based-on-check-alive']"
Availability,"Good news: the error message is now spot on. Bad news: unmarshalling error. ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.5, Date: Mon, 28 Jan 2019 23:23:57 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: ...""; }),HttpProtocol(HTTP/1.1)); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447,4,['error'],['error']
Availability,"Good question! In the context of the docker://uri, pull and build are almost identical. Both will retrieve layers from Docker Hub (or a registry you target), tar.gz files, and dump them into a binary image. So these two commands do the same thing:. ```bash; singularity build docker://<container>; singularity pull docker://<container>; ```; Build is different if you do it in an environment where you can use sudo, because you can target a build recipe instead, e.g.,. ```bash; sudo singularity build <container>.sif Singularity; ```; and pull is different too if you target some non-docker pull source:. ```bash; singularity pull shub://vsoch/singularity-images; ```; You can look at the codebase to confirm this - a pull of a docker uri [comes down to calling build](https://github.com/sylabs/singularity/blob/03072a88e108966d50fd61b0e6a51e6dbc62ff20/internal/pkg/libexec/pull.go#L42). ```bash; func PullOciImage(path, uri string, opts types.Options) {; 	b, err := build.NewBuild(uri, path, ""sif"", """", """", opts); 	if err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}. 	if err := b.Full(); err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}; }; ```; so choose whichever word you like better :) In the context of docker, without sudo, they do the same thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569:747,down,down,747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569,1,['down'],['down']
Availability,Got the same error in https://github.com/aws-samples/aws-refarch-wordpress,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-489183672:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-489183672,1,['error'],['error']
Availability,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:36,down,downloading,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681,1,['down'],['downloading']
Availability,"Guys, you should update mysql connector! Most of my workflow failures were because of mysql connection loss, it is such a pain to have a pipeline running for >1 day and having stuff crashed because ""cromwell lost connection to mysql""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807,1,['failure'],['failures']
Availability,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:1019,down,downside,1019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151,1,['down'],['downside']
Availability,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:602,down,down,602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,2,['down'],['down']
Availability,"Hah, mea culpa! That *is* Yossi's error!. I misread the logs earlier and had been unwittingly sitting at the same step that he's on. It just takes a lot wider of a scatter on my machine than the one he's on apparently. We have reason to believe that that value can't be fully static, but it might be able to be locked in after construction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757,1,['error'],['error']
Availability,"Happened again last night:. ```; The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; ```. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:140,failure,failure,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['failure'],['failure']
Availability,"Happy for a future PR to propose catching SOE, or even other `Error`s. In past lives I've been indoctrinated that `java.lang.Error` means ""stop and shutdown, something is horribly wrong"". For those who want to read debates on the issue:; https://www.google.com/search?q=java+catch+stack+overflow+error+site:stackoverflow.com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221:62,Error,Error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:207,failure,failures,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439,1,['failure'],['failures']
Availability,"Hello @notestaff . This sounds like a good feature request, but not something that has been addressed yet and it might be a few months before we get to it. For now, is it possible that the output files can be bundled into a zipped file and passed onto a downstream task possibly? As an attempt to consolidate the outputs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-444615940:254,down,downstream,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-444615940,1,['down'],['downstream']
Availability,"Hello again,. Just an update: error persists even when I set the `project` name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802,1,['error'],['error']
Availability,"Hello and thanks for your question. We do not have any current plans to support SQLite (though I personally think it's a fantastic product!). If you can help us understand the scenario in which you're running Cromwell we may be able to offer some advice. In particular, it's surprising that a MySQL database is ""not available or difficult to get""; the typical scenario we envision for running Cromwell with persistence is a Cromwell Docker plus a MySQL Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355:316,avail,available,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355,1,['avail'],['available']
Availability,"Hello! We seem to be running into a similar issue on cromwell `34-bda9485`. After humming along without a problem for a while, we all of a sudden stopped being able to run workflows with zipped WDL imports. Looking at the metadata, we get:; ```json; ""failures"": [; {; ""message"": ""Workflow input processing failed"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""pipelines%2Fdna_seq%2FUnmappedBamToAlignedBam.wdl: Name or service not known""; },; {; ""causedBy"": [],; ""message"": ""java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)""; },...; ```; Is this the same issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244:251,failure,failures,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244,1,['failure'],['failures']
Availability,"Hello, I am new to WDL and have met the same problem recently. I defined a `struct` like this:; ```; struct Fastp {; File reportHtml; File reportJson; Array[File]+ fqs; }; ```; and try to return it as the output in a `task`:; ```; output {; Fastp out = {; ""reportHtml"": ""QC/fastp.html"",; ""reportJson"": ""QC/fastp.json"",; ""fqs"": if flag then [""QC/clean_1.fq.gz"",""QC/clean_2.fq.gz""] else [""QC/clean_1.fq.gz""]; }; }; ```; `womtool validate` was applied to check the language specification and everything went fine, but finally got the error when trying to run my workflow using Cromwell. Here is part of the error report:; ```; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(QC/fastp.html), WomString(QC/fastp.json), [""QC/clean_1.fq.gz"", ""QC/clean_2.fq.gz""]]; ``` ; Any solution to this problem now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093,2,['error'],['error']
Availability,"Hello, I posted this bug because the validator does NOT do a check. IE I run the validator on every WDL I submit, but the validator did not catch this error. The error happens, as you said, after many tasks have run already.; [wot.wdl.txt](https://github.com/broadinstitute/cromwell/files/2168605/wot.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702:151,error,error,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702,2,['error'],['error']
Availability,"Hello, Im running it on SLURM HPC and i got the same error. [2024-01-20 12:06:05,87] [[38;5;220mwarn[0m] SLURM [[38;5;2mfdf21dfc[0m]: Key/s [memory, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1902056409:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1902056409,1,['error'],['error']
Availability,"Hello,. I can better assist you with errors like this over on the [WDL website](https://software.broadinstitute.org/wdl/index.php), specifically the [Ask the WDL team](http://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) section. Oftentimes, you can find a solution to your error, or perhaps you will find documents that answer your question on the website. As such, we prefer to answer questions, or file bug reports (if needed) from the forum. In your particular case, I would suggest asking a new question in the Ask the WDL team forum. Let me know if you need anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974,2,['error'],"['error', 'errors']"
Availability,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:269,down,download,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921,4,"['down', 'echo']","['download', 'echo', 'echoed']"
Availability,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"Here is another one (only appeared once), but the workflow keeps going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChann",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:863,error,error,863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,1,['error'],['error']
Availability,"Here is the size optimization that deletes untested jython files:. https://github.com/broadinstitute/heterodon/blob/b54010d4f1fe9395f854ab62e4b66c203bf3f45d/build.sh#L82-L84. If we come up with a CI regression case for Windows (x64?) then the appropriate files could be excluded from the filter and tested. Re: the borked Cromwell, we could catch-and-box the thrown `java.lang.Error` into a `java.lang.Exception` and Cromwell would handle this particular error more gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720:377,Error,Error,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720,2,"['Error', 'error']","['Error', 'error']"
Availability,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:553,error,error,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046,2,"['error', 'ping']","['error', 'pinging']"
Availability,"Here is the wdl file used for the test:. ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous_task_out = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous_task_out = print_nach_nachman.out; }. output{; File out = print_nach_nachman_meuman.out; }; }. task print_nach{. String output_file = ""task1.out""; command{; echo ""nach"" > ${output_file}; }; output{; File out = output_file; }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; File previous_task_out; Array[String] previous_content = read_lines(previous_task_out); String output_file = ""task2.out""; command{; echo ${sep=' ' previous_content} "" nachman"" > ${output_file}; }; output{; File out = output_file; }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; File previous_task_out; Array[String] previous_content = read_lines(previous_task_out); String output_file = ""three_task_sequence.out""; command{; echo ${sep=' ' previous_content} "" meuman"" > ${output_file}; }; output{; File out = output_file; }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-468214328:375,echo,echo,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-468214328,3,['echo'],['echo']
Availability,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,recover,recovered,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953,1,['recover'],['recovered']
Availability,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:153,error,error,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599,1,['error'],['error']
Availability,"Here's the full metadata @danbills:. ```; {; ""submittedFiles"": {; ""workflow"": ""import \""ss2_single_sample.wdl\"" as ss2\nimport \""submit.wdl\"" as submit_wdl\n\n\ntask GetInputs {\n String bundle_uuid\n String bundle_version\n String dss_url\n Int retry_seconds\n Int timeout_seconds\n\n command <<<\n python <<CODE\n from pipeline_tools import utils\n\n # Get bundle manifest\n uuid = '${bundle_uuid}'\n version = '${bundle_version}'\n dss_url = '${dss_url}'\n retry_seconds = ${retry_seconds}\n timeout_seconds = ${timeout_seconds}\n print('Getting bundle manifest for id {0}, version {1}'.format(uuid, version))\n manifest_files = utils.get_manifest_files(uuid, version, dss_url, timeout_seconds, retry_seconds)\n\n print('Downloading assay.json')\n assay_json_uuid = manifest_files['name_to_meta']['assay.json']['uuid']\n assay_json = utils.get_file_by_uuid(assay_json_uuid, dss_url)\n\n sample_id = assay_json['sample_id']\n fastq_1_name = assay_json['seq']['lanes'][0]['r1']\n fastq_2_name = assay_json['seq']['lanes'][0]['r2']\n fastq_1_url = manifest_files['name_to_meta'][fastq_1_name]['url']\n fastq_2_url = manifest_files['name_to_meta'][fastq_2_name]['url']\n\n print('Creating input map')\n with open('inputs.tsv', 'w') as f:\n f.write('fastq_1\\tfastq_2\\tsample_id\\n')\n f.write('{0}\\t{1}\\t{2}\\n'.format(fastq_1_url, fastq_2_url, sample_id))\n print('Wrote input map')\n CODE\n >>>\n runtime {\n docker: \""humancellatlas/pipeline-tools:0.1.4\""\n }\n output {\n Object inputs = read_object(\""inputs.tsv\"")\n }\n}\n\nworkflow AdapterSs2RsemSingleSample {\n String bundle_uuid\n String bundle_version\n\n File gtf\n File ref_fasta\n File rrna_interval\n File ref_flat\n String star_genome\n String rsem_genome\n String reference_bundle\n\n # Submission\n File format_map\n String dss_url\n String submit_url\n String method\n String schema_version\n String run_type\n Int retry_seconds\n Int timeout_seconds\n\n # Set runtime environment such as \""dev\"" or \""staging\"" or \""prod\"" so sub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:724,Down,Downloading,724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,1,['Down'],['Downloading']
Availability,"Hey @DivyaThottappilly do you still have this issue? I'm trying to get up and running a basic Hello World but keeps getting an S3Exception null error (301). . It seems like you've already past that stage and if you don't mind, could you help me setup this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379,1,['error'],['error']
Availability,"Hey @OgnjenMilicevic, looks like there is an error in that page. I believe it's supposed to be `${docker_script}` instead of `${script}`. (Edit, I've opened a PR to address this). For context, here is my config I use for Slurm + Singularity: https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660,1,['error'],['error']
Availability,"Hey @ParkvilleData, just make sure you're formatting your code with three backticks, then starting the code on the next line, like:. ````; ```; <code starts here/>; ```; ````. The single backticks `` ` `` are for in-line entries, eg: `` `code` `` -> `code`. Can you post the actual error you're seeing, or can you confirm none of your tasks are returning a valid value for ""docker"" in the runtime section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851:282,error,error,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851,1,['error'],['error']
Availability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:499,error,error,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,2,['error'],['error']
Availability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,2,"['error', 'ping']","['error', 'ping']"
Availability,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:109,error,errors,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247,3,['error'],"['error', 'errors']"
Availability,"Hey @antonkulaga, are you running this in Cromwell 30?. The good news: this was indeed a known issue for a long time but I believe it's finally been fixed as of https://github.com/broadinstitute/cromwell/pull/3175. ; The bad news: that won't be available until the next Cromwell release. If you're comfortable building from develop you're welcome to do that and try it out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100:245,avail,available,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100,1,['avail'],['available']
Availability,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:301,down,down,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,2,['down'],['down']
Availability,"Hey @gdlex4015 . We expect IO failures from time to time. There's a place in the Cromwell config to increate the number of retries on IO actions, so possibly bumping that up will help. . https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L721. Closing this for now but please re-open if you see this at a high frequency. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579,1,['failure'],['failures']
Availability,"Hey @geoffjentry I think this is good to go now. The coveralls went down (I think) because I'm now testing against my mock actor rather than the actual actor, so the coverage in that went down. I left the CromwellServer stuff alone in this PR so we could talk about how we wanted to consolidate that, Boot and Main into one(?) thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/22#issuecomment-104481650:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/22#issuecomment-104481650,2,['down'],['down']
Availability,"Hey @geoffjentry. The formula downloads the .jar from the Github releases page and [creates a little wrapper script](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L11) that's used to make `cromwell` available as a command-line tool. There's no need to compile from source. In order to update the formula for future releases, you can just submit a PR to Homebrew by updating the [url](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L4), [SHA](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L5), and [install steps](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L9-L12). I'd be happy to do this in the future for future releases -- just include this step in whatever release checklist you may use. Homebrew is a pretty well-established community, so there's not much to contribute on that end. Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076:30,down,downloads,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076,2,"['avail', 'down']","['available', 'downloads']"
Availability,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:89,fault,fault,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182,4,"['error', 'fault']","['error', 'fault']"
Availability,"Hey @lmtani . Thanks for reporting this. Another [issue](https://github.com/broadinstitute/cromwell/issues/4640) is designed to fix the transient failure mode you're describing. When that issue is closed, you should see this failure mode drop. Closing this as it's a duplicate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-466138620:146,failure,failure,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-466138620,2,['failure'],['failure']
Availability,"Hey @mr-c - I'm looking to update things. But this raises another question - since CWL 1.0.X is by definition backwards compatible, why would our reliance on an older version of `schema-salad` be an error on the tester? It shouldn't matter? . For instance we've talked in the past about embedding a particular version of the python code in our JAR just to ensure stability, but if I'm understanding what's causing issues on the CI (and I'm not sure I **am**, but it's my current theory at least) then if we did such a thing this would come up regardless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386:199,error,error,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386,1,['error'],['error']
Availability,"Hey @nvanaja-. Can you rebase this PR? Hopefully, there aren't any major conflicts since the main branch has continued to diverge. Sorry, we have another set of (currently internal) CI tests that are failing on your branch. The team has [(also internally) discussed](https://broadinstitute.slack.com/archives/C1EH66VCM/p1612377943138400?thread_ts=1612377834.138300&cid=C1EH66VCM) the cryptic docker errors with `mysql-client`, and no one on the team has any quick concrete suggestions at this second other than rebasing. 🤞. Thanks again for your patience while we juggle your PR with our other work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271:399,error,errors,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271,1,['error'],['errors']
Availability,"Hey @rexwangcc -- a requirement for any task to be considered a ""success"" is the generation of stdout/stderr log file. From my experience, I've seen in the past that when there's an intermittent docker issues, the ""docker logs"" (which include stdout/stderr) aren't copied out -- so even if some of the outputs produced by your task exist, the fact that stdout/stderr don't exist is considered a failure. I'm not sure we want to change this behavior as not being able to capture docker logs means not all task outputs were created, thus a failure. In cases like this, what will be helpful is an option to retry transient task failures (such as what you describe here), which is work in review [here](https://github.com/broadinstitute/cromwell/pull/3596). Let me know if there's something I'm misunderstanding here!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390845973:395,failure,failure,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390845973,3,['failure'],"['failure', 'failures']"
Availability,"Hey @sona1111, I took your workflow and was able to approximately get your error. Your issue is that you have trailing whitespace after the first line: `voila tsv \ `:. ![image](https://user-images.githubusercontent.com/22381693/71853318-17c17600-312f-11ea-9e6b-5b85ae692ce2.png). This is a bash thing. You can replicate this problem by just running the following command inside your container:. ```; voila tsv \ f1; ```. This returned me the error:. ```; voila tsv: error: argument files: cannot find ""/cromwell-executions/myWorkflow1/591dc02e-f9e4-48c6-8498-df17242fe217/call-task_voila_tsv/execution/ ""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815,3,['error'],['error']
Availability,"Hey @vsoch, in our submit script, we're opting to pull and build our docker images on the head node as @TMiguelT suggested. If the build exists, singularity will ask to overwrite the existing build. Just wondering if there's an elegant way to skip the build if it exists (and it's up to date) or whether we should be forcing a rebuild every time. I could skip it like this, but it's not as friendly./; ```; echo 'n' | singularity build --sandbox $IMAGE docker://${docker}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596:407,echo,echo,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596,1,['echo'],['echo']
Availability,"Hey @ylipacbio, I'm not from Cromwell but wanted to throw out a comment. For Cromwell to pull files, a [_Filesystem_](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/) needs to be implemented in Scala. . As far as I'm aware, Amazon's EFS is **NOT** implemented, and cannot be configured to work with Cromwell. The available filesystems are ftp, s3, demo-dos, gcs, oss, http (and unix). If you know some scala, [this](https://github.com/broadinstitute/cromwell/tree/develop/filesystems) might be good place to start on how to implement one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319:332,avail,available,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319,1,['avail'],['available']
Availability,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,recover,recovery,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,1,['recover'],['recovery']
Availability,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:416,error,errors,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673,2,['error'],['errors']
Availability,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:105,avail,available,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870,4,['avail'],['available']
Availability,"Hey, did you ever manage to get a workaround for this error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514934007:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514934007,1,['error'],['error']
Availability,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:156,avail,available,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,2,"['avail', 'error']","['available', 'errors']"
Availability,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:126,echo,echo,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201,2,['echo'],['echo']
Availability,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:324,avail,available,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825,2,['avail'],['available']
Availability,"Hi - yes, I can confirm this. Cromwell has had WDL 1.0 support for not quite a year now. You're right that this should be updated (pinging @cjllanwarne )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777:131,ping,pinging,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777,1,['ping'],['pinging']
Availability,"Hi @DSLituiev, thanks for your question and welcome to our repo!. I think I know the problem – the inputs in a call do not need the type declarations (like `File` and `String`). Give this a try:; ```; call touBam.unMap {; input:; mapped_bam=mapped_bam,; unmapped_base=bam_base; }; ```; Unfortunately the error is pretty confusing because when a workflow doesn't conform to the grammar, the parser has a very hard time describing what's wrong in meaningful terms (it just kind of freaks out).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348:304,error,error,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348,1,['error'],['error']
Availability,Hi @DadongZ - I can't replicate this. . I run: `java -jar cromwell-33.1.jar server` (downloaded from ; In my browser I go to `http://localhost:8000/swagger/index.html?url=/swagger/cromwell.yaml` and it works fine. Are you doing something differently than this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403133874:85,down,downloaded,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403133874,1,['down'],['downloaded']
Availability,"Hi @EvanTheB could you check something for me - you should be seeing a message like `Cromwell will watch for an rc file *and* double-check every {} seconds to make sure this job is still alive` when you start your job? (assuming `INFO` level logging is enabled). Then, with that background polling ongoing throughout the job run, if a full iteration of `exit-poll-timeout` has passed since the job stopped running, Cromwell will then mark the job as failed. If that gives you enough to put something more helpful into the docs that would be awesome! If not, I can maybe clarify a bit more? Otherwise we should hopefully be able to cycle round to improving this documentation _eventually_ (though unfortunately I can't make any stronger promises on an ETA than that!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172:187,alive,alive,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172,1,['alive'],['alive']
Availability,"Hi @aednichols !. > Is it possible to add `500` to `AdditionalRetryableHttpCodes`?. It is possible, but it won't give the result we need.; These codes are used only for `StorageException`s, since other exceptions don't have `getCode` method. Therefore, if we add `500` to `AdditionalRetryableHttpCodes`, Cromwell won't retry IOException caused by `500 Internal Server Error`. > It's also possible that by all 500 errors @cjllanwarne means 5xx. We did not think about it :) I think you're right, but just in case we will wait for an answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399:368,Error,Error,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi @antonkulaga - as far as I know there is no provision in the WDL spec to subset a `Directory` like this (nor do I believe this was intended behavior). As such, I'm closing this issue because the error you describe indicates Cromwell is correctly implementing the WDL spec. . If I'm incorrect and there is indeed something in the spec allowing this please reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351:198,error,error,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351,1,['error'],['error']
Availability,"Hi @chapmanb - CWL support was out of scope for the group who did the AWS Batch backend support. Some things might work, but others will not. In particular things like `cwl.inputs.json` and `cwl.output.json` with special control files definitely won't work as that requires special wiring on the part of a backend (i.e. not at the Cromwell engine/WOM layer) in order to be successful. We'll get to this eventually but is not on our immediate roadmap. We'd certainly welcome contributions if other groups were interested in more robust AWS/CWL support in Cromwell (that's more of a general comment to any potentially interested parties who see this)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457697601:528,robust,robust,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457697601,1,['robust'],['robust']
Availability,"Hi @cjllanwarne - I'm now chunking my queries, sending small batches of workflow IDs instead of querying for the status of 8k workflow IDs in the same query and I confirm that this resolves the issue. Thanks!; I'd still be in favour of catching this error or even before trying to interpret the query, counting how many terms are there in the json structure and reject it if too large. That way it wouldn't be so easy to bring down the server either intentionally or by mistake :); Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226,2,"['down', 'error']","['down', 'error']"
Availability,"Hi @cjllanwarne ; I'm not sure that I'm fully understood what you are wanting. > leave any other new IOExceptions un-retried. Do you mean that we should leave the existing case from PR #4272 as is? Instead of changing it, we should add a case for any throwable to check whether it contains ""500 Internal Server Error"", right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694:311,Error,Error,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694,1,['Error'],['Error']
Availability,"Hi @cjllanwarne ; You are right, the file is indeed read twice. There is a problem with reading it only once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:331,error,error,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809,3,['error'],['error']
Availability,"Hi @cjllanwarne, thanks for the response! Actually, my examples passed validation by `wdltool`, but I justed tested them with the current version of `womtool` and they did not pass the validation, and the errors are meaningful. I think it's safe to dismiss the bug label now.; ```Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'```. ```Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function `length`. `length` takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))```. ```Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))))```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555:205,error,errors,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555,1,['error'],['errors']
Availability,Hi @cowmoo - this appears to be retrying across all backends no matter what the error was. Is that what you intended for this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178,1,['error'],['error']
Availability,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:235,failure,failures,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956,2,"['down', 'failure']","['down', 'failures']"
Availability,"Hi @drkennetz ; Thank you for the info. The error you get and the usage scenario are very similar to issue #5085. I'm not sure, but I think that your problem has the same reason.; I already made a PR #5104 that should fix this. ; Although I'm not sure that Cromwell’s team will accept it, you can try to use it. If you know how to assemble a project in a .jar file from source code, then do it from the branch of this PR and run your workflow. I think it should run normally.; Alternatively, you can try to run Cromwell (the one that you have) in a server mode and submit your workflow using REST API or Swagger (it provides nice GUI).; If you'll make any of this, let me know if that helped. If it didn't help, please give me some examples of what you're running so I can reproduce it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519947699:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519947699,1,['error'],['error']
Availability,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:631,failure,failures,631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['failure'],['failures']
Availability,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:355,down,down,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894,2,['down'],['down']
Availability,"Hi @ernoc - this appears to be caused by the number of constraints you're specifying in your workflow query. - Short term, could you try making fewer constraints in your queries? We believe this is happening because slick is recursing per constraint, and thus hitting stack overflow when you have too many? Alternatively you could increase the JVM stack-overflow limit when running Cromwell to accommodate the number of constraints you need.; - Medium term, we should catch this (either in advance or literally catch the exception) and return ""unsupported operation"", rather than allowing this to bring down the entire Cromwell server.; - Longer term, we could try to restructure the query to support this level of querying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827:603,down,down,603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827,1,['down'],['down']
Availability,"Hi @ffinfo - thanks for contributing this!. My only concern as-is is that this has the potential to overload HPC clusters (see the discussions in https://github.com/broadinstitute/cromwell/issues/1499). In your case, since you're aware of the dangers and willing to go ahead regardless so I don't see why we shouldn't let you - but I don't think this should be the default behavior for unsuspecting users. . Would you be willing to make this a configurable option in the backend config (something like `check-alive-all-jobs`)?. Longer term if you start hitting HPC limits perhaps we can circle back round to the batching solutions hinted at in #1499",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292:509,alive,alive-all-jobs,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292,1,['alive'],['alive-all-jobs']
Availability,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR 😄).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:284,down,down,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,1,['down'],['down']
Availability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,recover,recover,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,12,"['alive', 'recover']","['alive', 'recover']"
Availability,"Hi @geoffjentry . I'm currently trying to figure out how to fix this issue, but I'm just starting to understand this project. Therefore, I want to clarify a few things.; As far as I understand, the problem is that some special (ad hoc) files are placed in ""/cromwell_root/path/to/input_file.txt"", while Cromwell expects them to be in ""/cromwell_root/ad_hoc_file.txt"" in order to execute them.; But what exactly is wrong here? It seems like either adhoc files are placed in the wrong directory and must be moved to ""/cromwell_root"" or their location is correct and it's Cromwell's fault that it tries to find them in the wrong directory. Which of these options is correct?; Also, I see you assigned yourself to this issue. Does this mean that help is no longer needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-506510217:580,fault,fault,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-506510217,1,['fault'],['fault']
Availability,Hi @geoffjentry Thanks for your response. I was trying to walk throught `[Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/)`. On the `Start the job` section I wasn't able to choose files. I downloaded 33.1 from [(https://github.com/broadinstitute/cromwell/releases)]. Here is the system version:; ```; $ lsb_release -a; No LSB modules are available.; Distributor ID:	Ubuntu; Description:	Ubuntu 16.04.4 LTS; Release:	16.04; Codename:	xenial; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308:218,down,downloaded,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308,2,"['avail', 'down']","['available', 'downloaded']"
Availability,"Hi @geoffjentry, got an example of a fork in the liquibase scripts? One workaround that I'm already using in a couple of places is having a separate `changeSet` specific to postgres - for example `addAutoIncrement` does not really work the way it does in MySQL. But mostly I'm just enforcing the existing table/column name case conventions (Postgres wants to convert them all to lower-case). The main problem right now is the `IMPORTS_ZIP` column in `WORKFLOW_STORE_ENTRY` - the migrations are working but as soon as I try to run a workflow I get this:; ```; ERROR: column ""IMPORTS_ZIP"" is of type bytea but expression is of type bigint at character 335; ```; Typing this as a `Blob` in Slick appears to be the root of the problem, but I'll keep poking at it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-474911248:559,ERROR,ERROR,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-474911248,1,['ERROR'],['ERROR']
Availability,"Hi @hkeward, would you mind updating your branch from our latest `develop`? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372:104,reliab,reliability,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372,1,['reliab'],['reliability']
Availability,"Hi @likeanowl, would you mind updating your branch from our latest develop? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216:104,reliab,reliability,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216,1,['reliab'],['reliability']
Availability,Hi @markjschreiber I'm also running into this error. I am using cromwell 53 with a custom cdk stack based on the CloudFormation infrastructure described here: https://docs.opendata.aws/genomics-workflows/. Are modifications needed for compatibility with newer versions of Cromwell? Are these documented somewhere?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662,1,['error'],['error']
Availability,"Hi @meganshand, I just re-looked at the WDL and it looks like you're using string interpolation where it isn't currently supported. Right now it's only available in `command` blocks. I don't know whether that'll solve the issue you're seeing but it's almost certainly causing problems!. E.g. These `${...}` won't be expanded and will probably be passed in verbatim to bash, where who knows how they'll be interpreted: ; ```; Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam""]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768,1,['avail'],['available']
Availability,"Hi @mepowers, thanks for updating this for us. Two quick comments:. 1. It looks like this is now a match to the example in [cromwell.example.backends/slurm.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/slurm.conf) but if you could double check that they really match, that'd be very helpful.; 2. I believe that `mem-per-cpu` is a slurm-instance-specific option (ie it's not necessarily globally available)? If so, I wonder if there's any way to indicate that alongside the examples?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667:437,avail,available,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667,1,['avail'],['available']
Availability,"Hi @myazinn, I have had trouble doing the same thing with cromwell 44 for CWL. I cannot import tools.zip to run workflows. . `java -jar ~/bin/cromwell-44.jar run echo_cat_wf.cwl -i in.yml -p tools.zip` gives me error:. ```; Workflow input processing failed:; Invalid workflow reference: echo_cat_wf.cwl; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519931357:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519931357,1,['error'],['error']
Availability,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:323,down,download,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045,1,['down'],['download']
Availability,"Hi @natechols – this is not a request I've heard before (not to say that's conclusive in any way) but it seems reasonable. We don't have any current plans for work in this area so a PR would be helpful. In particular, it looks like a natural place could be the existing list of `export`s in the generated run script. ```; #!/bin/bash. cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; tmpDir=$(mkdir -p ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324"" && echo ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:723,echo,echo,723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838,1,['echo'],['echo']
Availability,"Hi @notestaff and @huangzhibo -; I'm currently investigating this issue and trying to reproduce the problem.; Since I'm not familiar with WDL, I took workflow examples from [here](https://cromwell.readthedocs.io/en/stable/SubWorkflows/) and put `sub_wdl.wdl` in a subdirectory.; Therefore, the `main.wdl` looks like ; ```; import ""sub_dir/sub_wdl.wdl"" as sub. workflow main_workflow {; call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }; # call myTask { input: hello_and_goodbye.hello_output }; output {; String main_output = hello_and_goodbye.hello_output; }; }; ```; The body of `sub_wdl.wdl` wasn't changed; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }; task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }; workflow hello_and_goodbye {; String hello_and_goodbye_input; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```; I put `sub_wdl.wdl` into a subdirectory; ```; mkdir sub_dir; mv sub_wdl.wdl sub_dir/sub_wdl.wdl; ```; zipped this subdirectory; ```; zip -r sub_dir.zip sub_dir/; ```; and ran a workflow with the following commad; ```; java -jar /home/path/to/cromwell/cromwell.jar run /home/path/to/files/main.wdl --imports /home/path/to/files/sub_dir.zip; ```; The workflow succeeded without any problem.; Maybe my workflow is not reproducing this issue because I'm doing something wrong. I tried to run this workflow using Cromwell 41 and 45. Workflows succeeded in both cases. Can you tell me what should be changed in order to reproduce your problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519025813:684,echo,echo,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519025813,2,['echo'],['echo']
Availability,"Hi @ruchim !. 1. Do you see anything in your logs that indicate db errors?. No. I see that the SGE job completed (`Status change from Running to Done`). 2. What does your db config look like?; ```; database {; db.url = ""jdbc:mysql://.../${CROMWELL_DB}?useSSL=false&rewriteBatchedStatements=true""; db.user = ...; db.password = ...; db.driver = ""com.mysql.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; }; ```; backed by a MariaDB instance. 3. When you report the REST endpoint shows the workflow as 'Running', what about the executionStatus key in the metadata? Are some jobs marked as 'Running' as well?. The SGE job reported as running as well. I manually query the database, and I see no changes in the `JOB_STORE_ENTRY` table when the SGE job completes and the corresponding entry appears in the Cromwell logs (although not entirely sure I should see something). 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. I've only observed this behaviour with large scatters AND a file-of-file-names approach. I don't know exactly what combination of WDL features or what threshold of scatter width triggers it .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979:67,error,errors,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979,1,['error'],['errors']
Availability,"Hi @ruchim ,; Thanks for asking.; For example, normally, in the alignment, we need to provide the big fasta files as input.; So, it will download from s3 for each single job.; We have all the reference files in our EFS. For our own usage, we mount the EFS into every job definition. So the batch job can access the EFS directly. They don't need to download every time from S3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027:137,down,download,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027,2,['down'],['download']
Availability,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:551,Error,Error,551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,3,"['Error', 'avail', 'error']","['Error', 'available', 'error']"
Availability,"Hi @ruchim,. Personally, the reason I'm asking for Mesos support is because we have a Mesos cluster available, but no yarn. It's awesome cromwell already supports yarn, but it'd be great to have even more options. Cheers; M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-416991555:100,avail,available,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-416991555,1,['avail'],['available']
Availability,"Hi @seandavi - That does seem like it should work. Thinking back in my past I've definitely encountered tools which expect TMPDIR to exist and aren't smart enough to create it themselves. Also in JES there shouldn't be any issues with permissions, etc. We'd certainly welcome a PR if you're game for it, either (or both) against `0.19_hotfix` or `develop`. On that note, I should point out that a new release (currently `develop`) is imminent and for all but one use case (call caching) we beliee it to be more robust/stable that 0.19. I'd personally recommend people who don't need call caching work with the new system, but I understand that some people aren't comfortable working with code which isn't yet released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458:511,robust,robust,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,1,['robust'],['robust']
Availability,"Hi @tAndreani ,. Were you able to fix this error? I'm actually having a similar issue where I'm providing the files in a singularity container as an input to the workflow and localization via hard link and copy fails. I would appreciate your help. Thanks,; Chetana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224,1,['error'],['error']
Availability,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:254,down,down,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720,1,['down'],['down']
Availability,"Hi @vsoch - the first problem to solve is how to represent the usage of singularity in one's WDL (not sure how CWL does it, will need to look). This is being discussed in the [OpenWDL group](https://github.com/openwdl/wdl/pull/237) so if you have thoughts here that'd be very welcome. . For instance, is there a way to express ""run this container"" but not be locking a downstream WDL user into Singularity vs Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103:369,down,downstream,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103,1,['down'],['downstream']
Availability,"Hi @vsoch,. Lot's of good stuff here on first glance. I'll dive deeper over the weekend. For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the `.circleci/config.yml` into a script, or multiple scripts if necessary?. On a related note, based on your expertise I may want to pick your brain to go over our [existing CI scripts](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L38-L39) too as we move to Circle, or perhaps something even ~shinier~ [newer](https://news.ycombinator.com/item?id=17602838). Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228:141,reliab,reliability,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"Hi @wleepang , I looked at the `-proxy` logs for one of the jobs that failed with this error and saw (among other things):. ```; download failed: s3://fh-ctr-public-reference-data/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam to ../cromwell_root/fh-ctr-public-reference-data/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam [Errno 28] No space left on device; ```. So it seems like maybe this is a scratch space issue? I thought that Cromwell/AWS batch just automatically created more scratch space when it was needed, but that seems to not be happening. Any suggestions for troubleshooting the problem?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468377844:87,error,error,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468377844,2,"['down', 'error']","['download', 'error']"
Availability,Hi @zhilizheng - Please post the output or error logs. We will review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802,1,['error'],['error']
Availability,"Hi All, just checking in on this issue, to see if it is still alive. Would love to see ecr supported for hash-lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511:62,alive,alive,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511,1,['alive'],['alive']
Availability,"Hi Brad, thanks. I'm looking at the error and the paths seem to have the same pathological repetition of the root (`bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295`) that I was hoping to have fixed in that PR I mentioned before. Can you double check you're running from the latest develop ?; I'm re-running it right now as well with a longer root path to see if I can reproduce it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445918021:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445918021,1,['error'],['error']
Availability,"Hi Brad,. Thanks for reporting this. This error definitely should not occur but I'm surprised that it's causing Cromwell to freeze. Have you seen any other error in the log further down ?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386320045:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386320045,3,"['down', 'error']","['down', 'error']"
Availability,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:543,error,error,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402,1,['error'],['error']
Availability,"Hi Chris, thanks for the reply. Both of these workarounds are good, and it's nice to have them written down. Having said this I think the issue is actually that Cromwell isn't correctly implementing the CWL spec, because the H3ABio workflow is technically correct to use relative imports. I think it would be good for us to have a way to solve this without having to change the CWL itself (which is why my two suggestions would only involve changing the Cromwell submission but not the CWL).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449248258:103,down,down,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449248258,1,['down'],['down']
Availability,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:901,down,down,901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['down'],['down']
Availability,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:494,avail,available,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,2,['avail'],['available']
Availability,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1167,down,down,1167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,1,['down'],['down']
Availability,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:315,down,down,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,2,['down'],['down']
Availability,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:226,failure,failure,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165,3,['failure'],['failure']
Availability,"Hi Ruchi,. 1. I have the metadata from a later run which failed in the same way, it's [here](https://gist.github.com/dtenenba/87ba6eaad666071625da6cfe98db9f61). 2. Yes, it seems like most of the BaseRecalibrator tasks transitioned from running to Done.; I see this in the logs, right before the error:. ```; 2018-12-17 17:11:25,368 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:5:1]: Status change from Running to Succeeded; 2018-12-17 17:11:25,437 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:10:1]: Status change from Running to Succeeded; 2018-12-17 17:11:27,559 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Status change from Initializing to Running; 2018-12-17 17:11:36,844 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:9:1]: Status change from Initializing to Running; 2018-12-17 17:11:51,970 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:15:1]: Status change from Initializing to Running; 2018-12-17 17:11:53,801 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:11:1]: Status change from Running to Succeeded; 2018-12-17 17:11:55,351 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:7:1]: Status change ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-447976105:295,error,error,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-447976105,1,['error'],['error']
Availability,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,3,"['echo', 'error']","['echo', 'error']"
Availability,"Hi Will,. Try it without the `&& exit 1` portion, as I feel you are exiting before it can complete the workflow. For instance I ran the following code without the Docker portion on a cluster, and it worked. Below are all the steps for it:. 1) The wdl file:. ``` Bash; $ cat error_continue.wdl; task hello {. String addressee. command {; echo ""Hello ${addressee}!""; }. output {; String salutation = read_string(stdout()); }. }. workflow w {; call hello; }; ```. 2) The json inputs file:. ``` Bash; $ java -jar cromwell.jar inputs error_continue.wdl; {; ""w.hello.addressee"": ""String""; }; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; $; ```. 3) And then I ran it:. ``` Bash; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-25 18:25:31,77] [info] RUN sub-command; [2016-01-25 18:25:31,91] [info] WDL file: error_continue.wdl; [2016-01-25 18:25:31,92] [info] Inputs: error_continue.json; [2016-01-25 18:25:31,562] [info] Slf4jLogger started; [2016-01-25 18:25:31,649] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-25 18:25:32,777] [info] Running with database db.url = jdbc:hsqldb:mem:65a527dd-bc31-462e-bca9-05a545fea48a;shutdown=false;hsqldb.tx=mvcc; [2016-01-25 18:25:33,796] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:33,812] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-25 18:25:34,730] [info] WorkflowActor [9cdf23a5]: Start message received; [2016-01-25 18:25:34,997] [info] SingleWorkflowRunnerActor: workflow ID 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:34,999] [info] WorkflowActor [9cdf23a5]: ExecutionStoreCreated(Start) message received; [2016-01-25 18:25:35,11] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-9cdf23a5-1eaa-420a-8fae-ea3e4623d4db#-1942530845],Submitted); [2016-01-25 18:25:35,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363:337,echo,echo,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363,1,['echo'],['echo']
Availability,"Hi everyone, Cromwell on Azure is now available here:. https://github.com/microsoft/CromwellOnAzure. Please let me know if you have any questions or issues. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157,1,['avail'],['available']
Availability,"Hi is this implemented in the latest version of cromwell? ; I am getting the following error for files > 5G with the latest version . 2019-10-31 04:31:17,243 cromwell-system-akka.dispatchers.engine-dispatcher-32 WARN - 85d92e7d-3017-4e8d-adac-551ebcd50165-EngineJobExecutionActor-jgi_meta.bbcms:NA:1 [UUID(85d92e7d)]: Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_jgi_meta.bbcms:-1:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: 1272B7BFF87110E8)), invalidating cache entry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241:87,error,error,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241,1,['error'],['error']
Availability,"Hi, . I downloaded the jar file from the GitHub release page:. > wget https://github.com/broadinstitute/cromwell/releases/download/34/cromwell-34.jar. sha256sum results in the hash mentioned by @Horneth. Actually I have two java versions on my system:. ```; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```; and. ```; openjdk version ""1.8.0_141""; OpenJDK Runtime Environment (build 1.8.0_141-b16); OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode); ```. It fails with the first one, however I can launch the server with the openjdk version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300:8,down,downloaded,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300,2,['down'],"['download', 'downloaded']"
Availability,"Hi, I this is might be a little late, but I am having this issue too when running using Batch. I configured my core environment on my own (without using the CF templates). I have a bucket that is located in `us-west-2` and the instance running Cromwell (v59), and the Job Queue are located in `us-east-2`. When I run a job, I get the same error that @illusional was getting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699:339,error,error,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699,1,['error'],['error']
Availability,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:227,error,error,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094,10,"['down', 'error']","['down', 'error']"
Availability,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['error'],['error']
Availability,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:171,avail,available,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410,4,['avail'],['available']
Availability,"Hi,. I recently come across the same issue when using Cromwell with AWS Batch. However, it seems none of the reference links in this thread is available, except for the most recent link to `cromwell-aio.template.yaml` which also failed during the creation. So may I know where I should get help regarding this issue? Many thanks!. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922097668:143,avail,available,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922097668,1,['avail'],['available']
Availability,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:855,Error,Error,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Hi,. This looks like a bug in the generation of the script. Any chance you can; share the WDL file with me?. On Mon, Nov 30, 2020 at 7:35 AM henriqueribeiro <notifications@github.com>; wrote:. > I'm running gatk-sv workflows with AWS backend and I'm facing some issues; > on scatter tasks. It seems that for some of the tasks, the; > reconfigured-script is bad constructed. Below is an excerpt from the script:; >; > #!/bin/bash; >; > {echo '*** LOCALIZING INPUTS ***'if [ ! -d /tmp/scratch ]; then mkdir /tmp/scratch && chmod 777 /tmp/scratch; ficd /tmp/scratch; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:436,echo,echo,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857,1,['echo'],['echo']
Availability,"Hi,. sorry for the late response. It seemed that this caused the error. Best,; Flo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-427867227:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-427867227,1,['error'],['error']
Availability,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:713,degraded,degraded,713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,1,['degraded'],['degraded']
Availability,"Hmm I don't know, I don't think so. Those failures started popping up recently I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140:42,failure,failures,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140,1,['failure'],['failures']
Availability,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. 😕",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:69,error,errors,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894,1,['error'],['errors']
Availability,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:192,echo,echo,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035,2,['echo'],['echo']
Availability,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1000,mask,masking,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,2,['mask'],['masking']
Availability,"Hmmm, I don't even know how I would grant it at the project level. I pretty much used this:; ```; for role in lifesciences.workflowsRunner iam.serviceAccountUser storage.objectAdmin; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:MY-NUMBER-compute@developer.gserviceaccount.com --role roles/$role; done; ```; Maybe if `iam.serviceAccounts.actAs` is granted only once I might have missed it as I was not able to download the whole log file. Do you know why occasionally `storage.buckets.get` is requested and what actually happens to Cromwell if it is not granted to the service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686096916:449,down,download,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686096916,1,['down'],['download']
Availability,"Hmmm, still stuck on this - any updates from your guys' end? I tried cloning and resubmitting, still getting the same error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050:118,error,error,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050,1,['error'],['error']
Availability,"Honestly I didn't go very far down that road once I realized that Travis and Jenkins tests finished if I set it to ONLY ""copy"" mode. I don't think there's any technical reason why we couldn't at least test symlink and copy mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703490:30,down,down,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703490,1,['down'],['down']
Availability,"Honestly I'd really like it if that validation were removed from wdl4s. Even though it ""makes sense"" for it to be there because ""the spec says memory should be formatted like this so we should validate it down at the wdl4s level"". I still would rather change the spec to be a suggestion rather than something that's mandatory so we don't have this one outlier which actually makes the code a lot harder to write. If we do change that now, I'll shed a tear because I have to rebase those changes on my super long lived branch that we were going to get to once PBE was finished.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388:205,down,down,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388,1,['down'],['down']
Availability,Hoping @geoffjentry can provide some insight here as to how to get this pack up and running. Perhaps @jacarey can be second reviewer if available.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-144851621:136,avail,available,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-144851621,1,['avail'],['available']
Availability,How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237:75,error,errors,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237,1,['error'],['errors']
Availability,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:79,failure,failures,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717,2,"['Error', 'failure']","['Error', 'failures']"
Availability,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:590,down,down,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,2,['down'],['down']
Availability,Huh. Did not realise the return-code file checking rate is also bound by the exit-code-checking poll rate. That slows down the development cycle somewhat; we have it set pretty long (~120 minutes) to prevent overloading the queque systems.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488996990:118,down,down,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488996990,1,['down'],['down']
Availability,"I *think* that it errored out while I was still testing local backend, so I; never tested JES. On Thu, Dec 15, 2016 at 11:28 AM, Ruchi <notifications@github.com> wrote:. > @LeeTL1220-- was your example WDL run locally or on JES?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267372631>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7xuRrOQ7Sac2mRulJkYLAIMPnD_ks5rIWqQgaJpZM4JmxQ5>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928:18,error,errored,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928,1,['error'],['errored']
Availability,"I actually was not able to reproduce the failure with newer GATK versions, but the test does confirm that we now choose a larger boot disk size than before the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370:41,failure,failure,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370,1,['failure'],['failure']
Availability,I agree that metrics on checksum failures would be nice but that does seem to be beyond the scope of the ticket as currently written; perhaps a follow-on ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488,1,['failure'],['failures']
Availability,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:173,down,down,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178,1,['down'],['down']
Availability,I agree with not worrying about this CircleCI failure because we still have that coverage from Travis CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054:46,failure,failure,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054,1,['failure'],['failure']
Availability,"I agree with what Chris said above about making this ""opt-in"" and adding a Centaur test. It also looks like those Travis `sbt` failures are real so those tests would need to be fixed as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-439111507:127,failure,failures,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-439111507,1,['failure'],['failures']
Availability,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:572,error,error,572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185,1,['error'],['error']
Availability,I also agree with not worrying about this CircleCI failure for now since the same tests are still running in Travis so the coverage is unchanged.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405:51,failure,failure,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405,1,['failure'],['failure']
Availability,I also encountered the same error. Could anyone make some comments?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458,1,['error'],['error']
Availability,I also hit this while experimenting with using singularity with cromwell. Just replicating https://github.com/kundajelab/atac-seq-pipeline/blob/master/docs/tutorial_local_singularity.md locally. Get the error with cromwell-37 and with a fresh build of develop branch. Works with cromwell-36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989:203,error,error,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989,1,['error'],['error']
Availability,"I also met this error, can you tell me how to solve it? please",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-1895225269:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-1895225269,1,['error'],['error']
Availability,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:360,down,down,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,8,"['Failure', 'down', 'error']","['Failure', 'down', 'error', 'errors']"
Availability,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:204,Error,Error,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,4,"['Error', 'error']","['Error', 'error']"
Availability,"I also want to bind FSx in ECS container.Like @hurchu ,I hava refrence in FSx, I don't want to download every time from S3.; This is my vision, EC2 has two additional filesystems, /fsx /cromwell_root(ebs autoscaling), Container on EC2 mount this two filesystems in itself.Now there is only cromwell_root(ebs filesystem) in container.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-765076130:95,down,download,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-765076130,1,['down'],['download']
Availability,"I am also having this issue. . I don't understand what I am going wrong because it seems to be I am following the basic use case explained in the cromwell manual. I have basicaly took the published paired-fastq-to-unmapped-bam.wdl. can have added inputs and parsing to that each umapped bam file created get processed first by ; processing-for-variant-discovery-local-gatk4.wdl; and then the processed bam file gets processed by ; haplotypecaller-gvcf-gatk4.wdl. So I import the 2 workflows on the top. ```; import ""processing-for-variant-discovery-local-gatk4.wdl"" as process_bam; import ""haplotypecaller-gvcf-gatk4_no_docker.wdl"" as haplotype_caller; ```. and then use them on the ubam outputs. When I run it through validate I get the same error. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.HaplotypeCallerGvcf_GATK4.output_vcf; ^. java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.output_vcf; ^; ```. I thought it might be that 2 subworkflows was not supported so I took out the last one and the error become about the first:. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline_no_variant_calling.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'sub' in workflow (line 110):. Array[File] ProcessedBams_duplication_metrics = sub.duplication_metrics; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736:743,error,error,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736,5,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am experiencing an issue that may also be related to this, using WDL draft-2 spec and cromwell-39.; Here is a dummy example I created off a real error I received, it is minimal but hopefully descriptive enough:. ```WDL; task example {; Map[String, File] sample_files; Array[Array[String]]? tax_id_and_name; String? summary_report_name. String default_summary_report = select_first([summary_report_name, 'summary_report.txt']). command <<<; set -ex; example_command \; -o ${default_summary_report} \; -i ${write_json(sample_files)} \; ${ if defined(tax_id_and_name) then '-t ' + write_tsv(tax_id_and_name) else '' }; >>>; runtime {; docker: ""<local or private image name with the custom `example_command` installed>""; }; output {; File summary_report = ""${default_summary_report}""; }; }; ```; The run fails and the offending log output from Cromwell says:. ```commandline; example_command -o summary_report.txt -i /cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_json_b428b2ef25b3a99656256ecf58545736.tmp -t /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp; E Unable to open file /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp for reading (No such file or directory). Stopped at /usr/bin/example_command line 192.; ```. `write_json()` has no issue creating a path within the container, while `write_tsv()` returns a host path which is not found within the container.; I am able to workaround this at the moment by using `basename(write_tsv())` since the file is still in the execution directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411:147,error,error,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411,1,['error'],['error']
Availability,"I am facing the same problem, because I am using alpine images and bash is not available. Could it be possible that this is fix in the near future?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-364071889:79,avail,available,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-364071889,1,['avail'],['available']
Availability,"I am having an issue when trying to use `--imports tasks.zip` with Cromwell v31. I assume it is related to this issue. I can't seem to make it work with imports. I assume the parameter `--workflow-root` may be an alternative to using `imports`? However, instead of using zip imports, I tried using `--workflow-root` and still did not work. Thanks in advance for any help. Details of my scenario:. In my workflow I have:. ```; import ""tasks/task-1.wdl"" as task1; import ""tasks/task-2.wdl"" as task2; ```. my zip file looks like this:. ```; c4b301bb01ef:Desktop gonzalezma$ unzip -l tasks.zip ; Archive: tasks.zip; Length Date Time Name; -------- ---- ---- ----; 128 03-17-18 11:57 tasks/task-1.wdl; 119 03-17-18 11:57 tasks/task-2.wdl; -------- -------; 247 2 files; ```. Cromwell fails and says it can't find the task wdl files. How do I make this work? . The detailed error is as follows:. ```; [2018-03-17 14:23:42,03] [error] WorkflowManagerActor Workflow 6d61108d-3a3c-4850-8bd9-2862660f953c failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284:868,error,error,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284,1,['error'],['error']
Availability,"I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610,3,"['error', 'failure']","['error', 'failure']"
Availability,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700,1,['error'],['error']
Availability,"I am not familiar with that error message. From a bit of Googling it looks like [this](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9) may be relevant. Assuming `cloud-lifesciences` is Google's project hosting the image that Cloud Life Sciences is trying to use to spin up the worker VM, you may need to add `projects/cloud-lifesciences` to your organization's [trusted image projects](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9#:~:text=the%20trusted%20image-,projects,-.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292,1,['error'],['error']
Availability,"I am running into the same bug on Terra. Not sure what version of Cromwell it is using. My wdl validates however, I get the a run time error. ```; Failed to evaluate input 'fastq1' (reason 1 of 1): No coercion defined from wom value(s) '""gs://fc-secure-46b3886a-473a-49ef-8073-022230a526ac/6463b025-27cf-4649-b6d0-59f860bdf18b/bam2FastQStarAlignWorkflow/a4a0d2f2-cc8b-41d8-a5b5-61cf6c2d0bd4/call-bamToFastq/cacheCopy/GTEX-1192X-0011-R10a-SM-DO941.1.fastq.gz""' of type 'File' to 'Array[File]'.; ```. adding '[' and ']' resolved the run time issue; ```; call starWorkflow.star_fastq_list {; input:; star_index = starIndex,; fastq1 = [ bamToFastq.firstEndFastq ],; fastq2 = [ bamToFastq.secondEndFastq ],; prefix = sampleId; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607,1,['error'],['error']
Availability,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479,23,['error'],"['error', 'errors']"
Availability,"I am trying to use this. I switch to the ""aws_backend"" branch, checked it out, figured out how to build it, and now I have run into:. [2018-06-04 06:34:20,69] [error] java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732:160,error,error,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732,1,['error'],['error']
Availability,"I am trying to use udocker to locally run the gatk4-germline-snps-indels, using the wdl and json file offered by its GitHub page: https://github.com/gatk-workflows/gatk4-germline-snps-indels.; I am locally running it with docker, until now seems working well, no fault report.; I read udocker intro, it said I can use it to pull or run docker image (maybe my understand is wrong). ; What should I do to use udocker replace docker for this task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352:263,fault,fault,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352,1,['fault'],['fault']
Availability,"I am using exactly the wdl and json offered by gatk GitHub page for gatk4-germline-snps-indels, locally, I got this error, intervals-hg38.even.handcurated.20k.intervals is larger than 128000 Bytes. Maximum read limits can be adjusted in the configuration under system.input-read-limits.; I tried to change it via type this in command line: java -Dsystem.input-read-limits=500000 -jar /cromwell-34.jar ; Didn't work.; Who can tell me how to fix it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960,1,['error'],['error']
Availability,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,redundant,redundant,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088,1,['redundant'],['redundant']
Availability,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,recover,recover,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,2,['recover'],['recover']
Availability,"I assigned this to myself 12 (!) days ago but haven't started, I am taking this as an indication I am busy and should make it available for others to pick up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-486406119:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-486406119,1,['avail'],['available']
Availability,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:53,redundant,redundant,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['redundant'],['redundant']
Availability,"I believe that I am running into this problem with a batch of workflows.; I have a Cromwell instance running on GCE (launched via `docker-compose ... up`). Cromwell had gotten stuck accepting new workflow requests, so I shut it down and it didn't go down cleanly. After restarting Cromwell, I see:. ```; cromwell_1 | 2018-05-24 16:03:29,668 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; cromwell_1 | common.exception.AggregatedMessageException: Error(s):; cromwell_1 | Workflow a07583dd-f571-44bf-abb7-5bf281dfd249 in state Running and restarted = false cannot be started and should not have been fetched.; ```. with a lengthy list of workflows listed with the same error message. All of these workflows came to a stop. In fact, querying cromwell, I saw:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":0,""jobs"":0}; ```. I have been able to now restart cromwell and submit new workflows and get them running, but these other workflows were fairly well along. I would like to get them started again. What is the best way to do this?. ```; $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""32-c07d8d9-SNAP""}; ```. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784:228,down,down,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784,6,"['ERROR', 'Error', 'down', 'error']","['ERROR', 'Error', 'down', 'error']"
Availability,"I believe the one Travis failure is unrelated, an intermittent failure in SprayDockerRegistryApiClientSpec.scala that might be a GCR hiccup?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168850405:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168850405,2,['failure'],['failure']
Availability,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:35,failure,failure,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111,1,['failure'],['failure']
Availability,I believe this might be the intended behavior. Since we treat subworkflow calls the same as a task call -- the outputs of a call can't be used until the call completes and only then can downstream calls continue. I can see the advantages of doing things differently for a subworkflow call -- @geoffjentry @danbills thoughts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400446797:186,down,downstream,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400446797,1,['down'],['downstream']
Availability,"I came here to make a comment but as I started typing that made me wonder if what I was about to suggest might have been old behavior and explain why (we think) things went from fail slow to fail fast. Or, alternatively it was _always_ fail fast w/o us meaning to do so and this is all bollocks ... At one point startRunnableCalls was called findRunnableCalls, and then it'd kick everything off. I was going to suggest that one thing you could do is find runnable calls, and if the set is empty you know that either the workflow has failed or succeeded based on if the state of all of the calls (all success = success, any failure = failure, some not run ... uh oh!). Now that I go back and look at your code this seems more or less what you're doing, at least in spirit. I was mainly just excited that this might explain the slow -> fast transition (again, assuming it ever actually happened in the first place!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/436#issuecomment-182178955:623,failure,failure,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/436#issuecomment-182178955,2,['failure'],['failure']
Availability,"I can reproduce this on my system. If I do not include `--type cwl` it fails to recognize echo.cwl as a valid workflow input, if I do include it the tool succeeds. Furthermore, if I run an actual workflow with the tools inside the directory (so no import) it succeeds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595:90,echo,echo,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595,1,['echo'],['echo']
Availability,"I can see these workflows failing in the Cromwell logs, but I do not have the permissions to find out more from within the application (tried via Swagger). ```; [anichols@caas-cromwell-prod101 ~]$ docker logs caas_cromwell_1 | grep 7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-05 21:05:08,784 cromwell-system-akka.dispatchers.api-dispatcher-378 INFO - Unspecified type (Unspecified version) workflow 7541e4e2-f74c-43f7-82af-8df891a27520 submitted; 2018-11-06 04:55:58,066 cromwell-system-akka.dispatchers.engine-dispatcher-98 INFO - Status changed to 'Submitted' for 7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-06 04:56:01,019 cromwell-system-akka.dispatchers.engine-dispatcher-123 INFO - WorkflowManagerActor Starting workflow UUID(7541e4e2-f74c-43f7-82af-8df891a27520); 2018-11-06 04:56:01,019 cromwell-system-akka.dispatchers.engine-dispatcher-123 INFO - WorkflowManagerActor Successfully started WorkflowActor-7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-06 04:56:01,068 cromwell-system-akka.dispatchers.engine-dispatcher-49 ERROR - WorkflowManagerActor Workflow 7541e4e2-f74c-43f7-82af-8df891a27520 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; 2018-11-06 04:56:01,068 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor WorkflowActor-7541e4e2-f74c-43f7-82af-8df891a27520 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437481659:1033,ERROR,ERROR,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437481659,1,['ERROR'],['ERROR']
Availability,"I can't fault the code, but as you say, at the moment it doesn't complete the ticket. Can we add another ticket to actually wire this up?. Otherwise, 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/834/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219721904:8,fault,fault,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219721904,1,['fault'],['fault']
Availability,"I can't reproduce this error with hash 437d1b592ca606cdd96276a1cf85bf84594c31eb on develop, though I do still see the TMPDIR bug. I'll work on fixing that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110,1,['error'],['error']
Availability,"I can't reproduce this. I ran the following WDL without issues:. ```; task test {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow workflow_test {; call test as aliased_test; output {; aliased_test.*; }; }; ```. @yfarjoun do you happen to have the full WDL ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096:94,echo,echo,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096,1,['echo'],['echo']
Availability,"I can't run anything from the command line on this branch:. ```; [error] /Users/sfrazer/projects/cromwell/src/main/scala/cromwell/Main.scala:19: value BackendyString is not a member of object cromwell.engine.backend.CromwellBackend; [error] import cromwell.engine.backend.CromwellBackend.BackendyString; [error] ^; [error] /Users/sfrazer/projects/cromwell/src/main/scala/cromwell/Main.scala:352: value toBackendType is not a member of String; [error] val backendType = args(1).toBackendType; [error] ^; [error] two errors found; [error] (compile:compileIncremental) Compilation failed; [error] Total time: 21 s, completed Dec 14, 2015 5:10:50 PM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164576757:66,error,error,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164576757,10,['error'],"['error', 'errors']"
Availability,I can't tell from this fragment what the problem you're seeing is. This workflow worked as expected for me. I tried running it using `miniwdl run` and `java -jar cromwell.jar run`.; ```; version 1.0. task T {; command {; echo hello world; >&2 echo another world; }; output {; File out = stdout(); File err = stderr(); }; }. workflow W {; call T; output {; File out = T.out; File err = T.err; }; }; ```; Another common form is `String s = read_string(stdout())` which puts the command block `stdout` in a string result. Sometimes this is easier to use than opening a file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482:221,echo,echo,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482,2,['echo'],['echo']
Availability,"I confirmed with EB that the file used here does not need to be protected, so that should make things easier. Whoever takes this ticket... if you find it takes too long to run still let me know and we can work together to slim down the use case even further I think (but it may be just fine the way it is)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/820#issuecomment-218557707:227,down,down,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/820#issuecomment-218557707,1,['down'],['down']
Availability,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:298,down,download,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756,2,"['down', 'echo']","['download', 'echo']"
Availability,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:501,down,down,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,2,['down'],['down']
Availability,"I did that and began encountering the following error:; ```; 2019-02-25 18:17:52,693 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:48,error,error,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I didn't drill in but that is a lot of build failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109,1,['failure'],['failures']
Availability,I do this all the time and never have a problem. It's possible that there's some pathological state things can be in which causes problem but we'd need a reproducible example to track it down and we don't have one. I vote to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375:187,down,down,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375,1,['down'],['down']
Availability,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line — it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` — but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,redundant,redundant,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047,1,['redundant'],['redundant']
Availability,I don't disagree in spirit but I think this is a dangerous road to start going down and we'd need to manage it carefully. There are a *lot* of potential variations in the structure of xSV files (example: http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html) and there aren't too many people who manage to handle it gracefully - I don't have faith that we'll be one of the few who find the holy grail there.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656:79,down,down,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656,1,['down'],['down']
Availability,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:252,down,down,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275,4,"['down', 'fault']","['down', 'fault']"
Availability,"I don't know what's going on with the labels. Ruchi, Gemma and I discussed this when I was Acting Delivery Czar for a day in your absence. It definitely was in the sprint at one point. A/C: at a minimum emulate the exception above and confirm that a running WorkflowActor does not crash and hang forever without making progress. Some very nice-to-haves would be understanding what actually is going on here; could we be failing faster with conspicuous broken credentials? Also de-scary any error messages a la the other `OneForOneStrategy` tickets as appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562:490,error,error,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562,1,['error'],['error']
Availability,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:214,down,download,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,1,['down'],['download']
Availability,"I don't see any error logging associated with this cromwell hash. However, I did see this:. ```; 2016-05-03 10:14:45,314 cromwell-system-akka.actor.default-dispatcher-17 ERROR - BackendCallExecutionActor [UUID(643d3c46):CollectUnsortedReadgroupBamQualityMetrics:22]: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,8,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error', 'errors']"
Availability,I don't think it closes anything no. It should be enough for the release though ? We'll see if/where we need more retries in the logs if this error pops up again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850:142,error,error,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850,1,['error'],['error']
Availability,I don't think it's been improved no. I have no idea how often users encounter this. It could be added as a low hanging fruit for User improvement though as it's not a big deal to make the error message more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391:188,error,error,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391,1,['error'],['error']
Availability,I don't think so but it's the first layer of the multi-failures test I was trying to fix. The rest is not ready yet but I though I'd PR that already in case other tests fail because of it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3457#issuecomment-376602036:55,failure,failures,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3457#issuecomment-376602036,1,['failure'],['failures']
Availability,"I don't think the errors above correlate with DB backup times, when the database would be unavailable, but here are the times. (At least I think I have the correct terraformed `sfmt6…` db). <img width=""664"" alt=""terraform-sfmt6-backups"" src=""https://user-images.githubusercontent.com/791985/48174854-a04c4880-e2d7-11e8-9ea5-48a455567f65.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360#issuecomment-436855808:18,error,errors,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360#issuecomment-436855808,1,['error'],['errors']
Availability,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:113,reliab,reliability,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164,1,['reliab'],['reliability']
Availability,I don't understand the Travis CI failure - is this unrelated? Is there a way for me to re-run the test?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505582077:33,failure,failure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505582077,1,['failure'],['failure']
Availability,"I don't understand the travis build failure, it looks to be something unrelated to my changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493515502:36,failure,failure,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493515502,1,['failure'],['failure']
Availability,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:115,ERROR,ERROR,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I dug up the source for my statement about autocommit:. >If autocommit mode is enabled, each SQL statement forms a single transaction on its own. https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467:211,rollback,rollback,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467,1,['rollback'],['rollback']
Availability,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:248,resilien,resilient,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941,1,['resilien'],['resilient']
Availability,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:242,error,error,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846,1,['error'],['error']
Availability,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:186,down,download,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,2,['down'],"['download', 'downloaded']"
Availability,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:374,error,error,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['error'],['error']
Availability,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546,3,['error'],['error']
Availability,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:62,fault,fault,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,2,"['error', 'fault']","['error', 'fault']"
Availability,"I get the same error message when e.g. renaming an arbitrary `.txt` file to `.zip` - that's not necessarily what's happening, but a clue that the zip itself may be bad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859,1,['error'],['error']
Availability,I got the same error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409,1,['error'],['error']
Availability,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:93,down,downloads,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867,1,['down'],['downloads']
Availability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,5,"['Error', 'error']","['Error', 'error']"
Availability,"I had the same problem. The code that generates `stdout` and `stderr` is included in [StandardAsyncExecutionActor.scala](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:584,error,errorOrDirectoryOutputs,584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,2,['error'],"['errorOrDirectoryOutputs', 'errorOrGlobFiles']"
Availability,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:715,echo,echo,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,2,['echo'],['echo']
Availability,"I have already changed that setting, as it was causing a different error. my aws.conf:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://concr-genomics-results/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<##########>:job-queue/GenomicsDefaultQueue-<###########>"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997,1,['error'],['error']
Availability,I have also sneakily added another commit that seems to fix persistent near-immediate failures to build Centaur TES.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3504#issuecomment-380422757:86,failure,failures,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3504#issuecomment-380422757,1,['failure'],['failures']
Availability,"I have confirmed and worked-around by changing the configuration -o and -e parameters to ; ```; -o ${out}.cromwell; -e ${err}.cromwell; ```; identical duplicated files are now written to the work directory. (Identical except in the case of error, which is when I need these files anyway). My request is to remove the >(tee) lines, but I understand they probably exist to serve some other backend. The ability to turn them off would be appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752:240,error,error,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752,1,['error'],['error']
Availability,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:100,error,error,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782,2,['error'],['error']
Availability,"I have encountered the same problem, either local mode or server mode. version : comwell41; wdl: version 1. - error info: . ```; ""submission"": ""2019-06-28T08:36:56.384Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""/tmp/imports_workflow_63e53e21-b200-46b2-9653-db79983d6c1d_3805297415647673436.zip4786068963842572955/bcftools-task/bcftoolsView.wdl""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ```. - my wdl header. ```; version 1.0. import ""bcftools-task/bcftoolsView.wdl"" as select; import ""beagle-task/prephasing.wdl"" as prephasing. ```. - unzip -v impute_human_beagle_v1.zip. ```; Archive: impute_human_beagle_v1.zip; Length Method Size Cmpr Date Time CRC-32 Name; -------- ------ ------- ---- ---------- ----- -------- ----; 655 Defl:N 319 51% 06-28-2019 15:52 d52896d1 bcftools-task/bcftoolsView.wdl; 694 Defl:N 384 45% 06-28-2019 14:24 552eeedb beagle-task/prephasing.wdl; -------- ------- --- -------; 1349 703 48% 2 files; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137,2,"['error', 'failure']","['error', 'failures']"
Availability,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:199,error,errors,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,1,['error'],['errors']
Availability,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:105,error,error,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['Failure', 'error']","['Failure', 'error']"
Availability,"I have not reproduced the issue, I will try when I get a chance here. At the time I was modifying my backend's 'runtime-attributes'. I made all those attributes optional. I also removed all the 'runtime' stanzas in the wdl file I was testing. I believe there was an error in the 'submit' syntax of my backend config. . Is it possible a config file that fails to parse will cause backends to default to the Local backend? I know parts of the config are not parsed until a job is submitted. I will try to isolate the problem. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043:266,error,error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043,1,['error'],['error']
Availability,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:584,down,downloaded,584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,1,['down'],['downloaded']
Availability,I have to admit I'm among the guilty here - we make the `run` mode available to PacBio customers (via a Python wrapper that provides a friendlier CLI) who prefer to use the command line. Are there drawbacks to this from a black-box user perspective?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473:67,avail,available,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473,1,['avail'],['available']
Availability,I haven't been able to reproduce this on 34_hotfix or develop. Standard output and error filenames are `stdout` and `stderr` in both the metadata and IRL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171148:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171148,1,['error'],['error']
Availability,I heard chatter about a 30.2 release ...is there any chance this change can make it in that release? It's mostly for FC users as the current failure logs are sent to the server logs and basically the user never sees call caching fail even though the job succeeds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283:141,failure,failure,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283,1,['failure'],['failure']
Availability,I highly recommend using the dsde-toolbox method as it gives you access to the `vault-edit` command which is much less error prone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198:119,error,error,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198,1,['error'],['error']
Availability,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:171,error,error,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,3,"['avail', 'error']","['available', 'error']"
Availability,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:7,down,downloaded,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136,1,['down'],['downloaded']
Availability,"I just encountered this error, as well. I narrowed it down to this commit. https://github.com/broadinstitute/cromwell/commit/448067fd72fc1bde89c0e4291d8790a65ff5968f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492444503:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492444503,2,"['down', 'error']","['down', 'error']"
Availability,"I just gave this a try in my facility. It still fails. My current work around is to use cromwell version 0.32 (via conda package). Cheers. On Tue, 19 Feb 2019 at 15:53, Michael Franklin <notifications@github.com>; wrote:. > Able to replicate the break from v36 to v37, I switched my check-alive in; > my config to use scontrol and it succeeded:; >; > ""check-alive"": ""scontrol show job ${job_id}"",; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAdrHHALXNpvY-NDgAF3uYIRY7EyP3zqks5vO4M_gaJpZM4aEezy>; > .; >. -- ; Nicholas Yue; Graphics - Arnold, Alembic, RenderMan, OpenGL, HDF5; Custom Dev - C++ porting, OSX, Linux, Windows; http://au.linkedin.com/in/nicholasyue; https://vimeo.com/channels/naiadtools",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573:289,alive,alive,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573,2,['alive'],['alive']
Availability,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,redundant,redundant,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893,1,['redundant'],['redundant']
Availability,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:144,alive,alive,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161,1,['alive'],['alive']
Availability,"I just ran into this as well--cromwell does not work on OSes without `/bin/bash`. `/usr/bin/env bash` is more portable and IMHO the better option. The following fails to execute for me:. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. Unfortunately this precludes me from using Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087:257,echo,echo,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087,1,['echo'],['echo']
Availability,"I just realized that we essentially blocked ourselves because of the comment above, where we now have so many subworkflows that we can't stand up a job manager on that cromwell because it can't filter them out quickly enough on the JMUI server side to render the website. Our own fault, but we'll be happy to see this functionality go in!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3240#issuecomment-382108058:280,fault,fault,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3240#issuecomment-382108058,1,['fault'],['fault']
Availability,"I just stood up a server locally. Submitted this workflow: https://raw.githubusercontent.com/bcbio/bcbio_validation_workflows/master/NA12878-chr20/NA12878-platinum-chr20-workflow/main-NA12878-platinum-chr20.cwl. Used this as inputs json; ```; {; ""checkerWorkflow.expectedMd5sum"": ""00579a00e3e7fa0674428ac7049423e2"",; ""checkerWorkflow.inputFile"": ""md5sum.input""; }; ```. Received this output:. ```; 2018-10-12 16:33:43,542 INFO - Pre-Processing /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl; 2018-10-12 16:33:58,870 cromwell-system-akka.dispatchers.engine-dispatcher-43 ERROR - WorkflowManagerActor Workflow 7193cfe7-2342-48cc-8d7c-bf7d3434c57f failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl failed with Traceback (most recent call last):; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574:659,ERROR,ERROR,659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574,1,['ERROR'],['ERROR']
Availability,"I just witnessed that too. I think it might be a problem with the docker container transiently not being able to authenticate calls to the google API, which results in failures to localize/delocalize. I'll bring it up to Google but for now the only workaround is to start the workflow again I'm afraid",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991:168,failure,failures,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991,1,['failure'],['failures']
Availability,"I know we've talked about it at length, but I don't know if it's written down anywhere - could you (either in the PR description or the ticket) lay out the WDL versions supported by Cromwell as of this PR, and what user-facing changes this PR introduces?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146:73,down,down,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146,1,['down'],['down']
Availability,I let Aaron know about this dockerhub failure and he'll add it to the ones being retried. I think we can close this for now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429909434:38,failure,failure,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429909434,1,['failure'],['failure']
Availability,"I like the idea of a centralized rate limiter (proxy, supervisor, whatever) to more rationally avoid QPS issues, but these changes are more generally making the vassals robust to any sort of transient problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613:169,robust,robust,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613,1,['robust'],['robust']
Availability,"I love the failure type but since you've done so much, would it be another load of work to use `sttp` + cats-effect backend vs the source of our Future woes, akka?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136,1,['failure'],['failure']
Availability,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:102,down,down,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['down'],['down']
Availability,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:14,down,down,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513,1,['down'],['down']
Availability,"I need a red thumb because of some changes in WOM - specifically I needed some way to indicate that a not-really-a subworkflow call should not get its subworkflow name prepended. I'm also curious whether the ""make up a random UUID for the not-really-a-subworkflow name"" is going to cause problems down the road (eg does restart rely on stable subworkflow names?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-371860973:297,down,down,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-371860973,1,['down'],['down']
Availability,"I need this ability to label Google VMs for resource tracking, but have been thus far unable to have a VM labelled correctly. The jobs submit and run, but the labels do not show up. . From the documentation here (https://cromwell.readthedocs.io/en/develop/wf_options/Google/), it's not clear where the google-specific options are added, so I tried the following: ; ```; {; ""default_runtime_attributes"":{; ""zones"":""us-east1-b"", ; ""google_labels"": {""custom-label"":""custom-value""}; }; }; ```; I submit (Cromwell v42) with:; ```; curl -X POST ""<CROMWELL URL>/api/workflows/v1"" \; -H ""accept: application/json"" \; -H ""Content-Type: multipart/form-data"" \; -F ""workflowSource=@main.wdl"" ; -F ""workflowInputs=@inputs.json"" \; -F ""workflowOptions=@options.json"" \; -F ""workflowType=WDL"" \; -F ""workflowTypeVersion=draft-2""; ```. That submits/runs fine, but when I check the VM that spins up, I only see the two labels of `cromwell-workflow-id` and `wdl-task-name`. If I change the options JSON to anything else, e.g.; ```; {; ""default_runtime_attributes"":{""zones"":""us-east1-b""},; ""google_labels"": {""custom-label"":""custom-value""}; }; ```; then it fails to submit, returning:; ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Invalid workflow options provided: Unsupported key/value pair in WorkflowOptions: google_labels -> {\""custom-label\"":\""custom-value\""}""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533:1206,Error,Error,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533,1,['Error'],['Error']
Availability,"I need to put this down for the moment to finish https://github.com/broadinstitute/cromwell/pull/7439 which is currently affecting users, hope to get back to it tomorrow morning.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826:19,down,down,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826,1,['down'],['down']
Availability,"I now notice that both checking for the RC and the 'check-alive' checks are controlled by this setting. This seems a bit strange as the rc checking is comparatively very cheap compared to 'check-alive'. Initially the point of not running check-alive all the time was that the cost was high compared to rc file checking. But now the solution has been to slow down rc checking to the speed of the costly check-alive! I am a bit muddled on this, so am not sure if I am getting it. . Without exit-code-timeout-seconds at what interval is the rc file checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941:58,alive,alive,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941,5,"['alive', 'down']","['alive', 'down']"
Availability,"I ran a couple workflows, against 0.19 and against develop (considering the workflow id wrapping issue is resolved). Here are some differences I found, there might be other that those workflow didn't catch.; - The ""Collector"" of a scatter is present in the metadata in develop, and wasn't in 0.19. Oddly it doesn't contain its output though : . ```; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ```; - `stdout` and `stderr` are missing (**Local only**); - `runtimeAttributes` is missing ; Completely missing on local.; On JES, only attributes in the WDL show up, those for which the default value was used are missing.; - `executionEvents` is missing (even if there is none, there is an attribute with an empty list in 0.19); - `cache` is missing (**Local only**); e.g. ```; ""cache"": {; ""allowResultReuse"": true; }; ```; - `inputs` at the call level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `inputs` at the workflow level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `outputs` at the workflow level is missing if there are no inputs (`""outputs"" : {}` was present in 0.19); - Scatter keys are shown in develop's metadata as a normal call:. ```; ""w.$scatter_0"": [; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ]; ```; - `submission` is missing; - In 0.19, all ""first level"" (non-shards) calls would appear in the metadata right away, with a `NotStarted` status and some basic available information:. ```; ""example.gatherUltimateAnalysis"": [; {; ""executionStatus"": ""NotStarted"",; ""shardIndex"": -1,; ""outputs"": {},; ""runtimeAttributes"": {},; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""array"": ""ultimateAnalysis.out""; },; ""backend"": ""JES"",; ""attempt"": 1,; ""executionEvents"": []; }; ]; ```. In develop, a call appears in the metadata only at runtime; - `backendStatus` has been renamed to `jesOperationStatus` (JES status)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341:1482,avail,available,1482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341,1,['avail'],['available']
Availability,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:320,error,error,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,1,['error'],['error']
Availability,"I ran into this again today, unless the server stdout is captured the errors don't make their way to workflow logs so it's particularly annoying to debug failed workflows. As a user, I would expect that either all logs (both pertaining to the workflow, or the server's execution of the workflow) are placed in the workflow log, or there is a separately configurable server log file for that stream of information.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172,1,['error'],['errors']
Availability,I realized that downstream libs won't automagically get the updated cats library so no need to relax this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698:16,down,downstream,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698,1,['down'],['downstream']
Availability,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:133,failure,failure,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726,1,['failure'],['failure']
Availability,"I remember we added this for certain JES failures so the backend plumbing might already be there (although IIRC we called it ""unexpected failure"" and it was hard coded to 3)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358781998:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358781998,2,['failure'],"['failure', 'failures']"
Availability,"I reproduced the problem locally. The problem is our default health monitor `StandardHealthMonitorServiceActor` which periodically tries to ping Docker Hub. I'm not sure what changed between 36 and 37 that this now manifests when previously it did not. There is a `cromwell.services.healthmonitor.impl.noop.NoopHealthMonitorServiceActor` that one should be able to swap in to turn off health monitoring, but unfortunately there's a bug in the 37 version of this class that causes it to crash during initialization. As you've noticed the workflow still runs successfully albeit with a lot of noise, we'll try to come up with a more satisfactory resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697:140,ping,ping,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697,1,['ping'],['ping']
Availability,"I reviewed my script and I realized that the scatter is done in a more complicated way. Here is a toy-example of what I am doing:. ```wdl; workflow TestWorkflow {; Array[Int] my_array; ; call GenerateMap {; input:; i = length(my_array); }; scatter (entry in GenerateMap.map_output) {; call CopyFile {; input:; file = entry.right; }; }; ; output {; Array[Pair[Int, Array[File]]] final_out = zip(my_array, CopyFile.out); }; }. task GenerateMap {; Int i; command <<<; for n in `seq 1 ${i}`; do \; touch $n.txt; \; echo -e ""$n\t$n.txt"" >> my_map.txt; \; done; >>>. output {; Map[Int, File] map_output = read_map(""my_map.txt""); }; }. task CopyFile {; File file. String copy_file = basename(file) + "".copy""; command <<<; cp ${file} ${copy_file}; >>>. output {; Array[File] out = [""${file}"", ""${copy_file}""]; }; }; ```. And my outputs after running with the input array with `[1,2,3,4,5,6,7,8,9]` are the following:. ```json; {; ""outputs"": {; ""TestWorkflow.final_out"": [{; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/10.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-0/execution/10.txt.copy""],; ""left"": 1; }, {; ""left"": 2,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/4.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-1/execution/4.txt.copy""]; }, {; ""left"": 3,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/6.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:511,echo,echo,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,1,['echo'],['echo']
Availability,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760,1,['failure'],['failure']
Availability,"I see, so is the VM tasked through PAPI to pull the docker and then run `gcs_localization.sh` and `script`? Is it the internal PAPI code that failed to properly handle the 403 error when pulling the docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059,1,['error'],['error']
Availability,"I see. My usual ""workaround"" for such fail (but continue) is like this:. ```; task foo {; 	command {; 		(echo foo; false) || (echo 1>&2 MSG; true); 	}; }. workflow test {; 	call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301:105,echo,echo,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301,2,['echo'],['echo']
Availability,I should have made this explicit in the previous comment: . I don't currently see a way we can support underscores in bucket names as long as we're using Google's GCS NIO filesystem. But I do think Cromwell can and should fail with useful and timely error messages when presented with bucket names that will not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174,1,['error'],['error']
Availability,"I solved it by myself. This issue is caused by region. I use default setting for region in aws.conf.; When I change to region accordingly (In my case, it is set to ap-northeast-2), it works well. I think the error message about this needs improvement a little more precisely. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-northeast-2""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965,1,['error'],['error']
Availability,"I spoke too soon, seeing a lot of `23:47:24.836 [centaur-acting-like-a-system-akka.actor.default-dispatcher-9] ERROR centaur.api.CentaurCromwellClient$ - Submitting invalid_inputs_json_object returned 400 Bad Request`. . Maybe only log as an error (and retry...) if the return code is not 4xx?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4956#issuecomment-491359116:111,ERROR,ERROR,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4956#issuecomment-491359116,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I started a branch [here](https://github.com/broadinstitute/cromwell/tree/aen_arm64_build) but run into this error, which seems to be widespread on the 'net.; ```; docker exporter does not currently support exporting manifest lists; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703:109,error,error,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703,1,['error'],['error']
Availability,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:173,failure,failure,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106,1,['failure'],['failure']
Availability,"I submitted the regular single_sample.wdl with the VIR_1923 .JSON that's; there...I guess I don't have permissions or something to access the files; that are listed. On Thu, Apr 14, 2016 at 9:01 AM, meganshand notifications@github.com; wrote:. > Not sure if this is a separate issue or not, but when @knoblett; > https://github.com/knoblett and I were submitting a workflow yesterday; > we got the exact same error message (submitted with Swagger). The issue for; > her was that there was an input that was specified to be a File type, but; > in reality it was just a String (so I'm guessing the issue was similar in; > that it couldn't find the ""file""). Unfortunately, it validated just fine,; > but we weren't able to submit it.; > ; > I'd be happy to provide the WDL and JSON files (both the broken version; > and the fixed version) but they won't attach in a github comment.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921:409,error,error,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921,1,['error'],['error']
Availability,I suspect that making outputs and inputs a toggle-able option will kick this particular can significantly down the road.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067,1,['down'],['down']
Availability,"I take it all back, I think we need this functionality. The metadata based REST queries will go through the MetadataBuilderActor which in turn pings the MetadataService. Having the MetadataService implies having the summarizer. Pinging @Horneth just to double check me on this as I believe he's the one who put that stuff together.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145:143,ping,pings,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145,2,"['Ping', 'ping']","['Pinging', 'pings']"
Availability,I tested @TimurIs workaround yesterday. It has the desired effect of throttling API requests. There were no API request errors reported in the logs. I would recommend this as the solution for this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442969906:120,error,errors,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442969906,1,['error'],['errors']
Availability,"I think I broke the tests, I needed to rebase on job_avoidance branch. I'm; working on fixing them now. On Tue, Dec 15, 2015 at 9:36 AM, Chris Llanwarne notifications@github.com; wrote:. > Oh, looks good then. I've restarted the Travis build but assuming the; > failures were just temporary, [image: :+1:]; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164786288:262,failure,failures,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164786288,1,['failure'],['failures']
Availability,"I think I found the one you are talking about. ![screen shot 2018-10-15 at 11 28 47 am](https://user-images.githubusercontent.com/2978948/46960909-92efc580-d06d-11e8-97fe-d81ef63da81a.png). The failure reason is . ```; Task requester_pays_engine_functions.functions:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```. which is not related to the requester pays feature but rather yet another dockerhub flaky response that we should ask google to retry IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811:194,failure,failure,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811,4,"['error', 'failure']","['error', 'failure']"
Availability,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:842,avail,available,842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,3,"['avail', 'error']","['available', 'errors']"
Availability,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:48,down,down,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855,1,['down'],['down']
Availability,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,redundant,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957,1,['redundant'],['redundant']
Availability,"I think that this is related with https://github.com/docker/machine/issues/2517, but I believe that cromwell can be more robust to a container still running but detached due to timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240:121,robust,robust,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240,1,['robust'],['robust']
Availability,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:60,avail,available,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390,1,['avail'],['available']
Availability,"I think the bug is actually in Cromwell's WDL draft-2 support. Submitting a draft-2 workflow with PCRE pattern `\.gz$` yields an `Unrecognized token` error on the backslash. A work-around is to double-escape with `\\.gz$`, thus becoming non-standard and unexpectedly failing when you moved to 1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412088680:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412088680,1,['error'],['error']
Availability,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803,4,['error'],['error']
Availability,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:419,resilien,resiliency,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,2,"['down', 'resilien']","['down', 'resiliency']"
Availability,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:348,echo,echo,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592,2,['echo'],['echo']
Availability,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:314,down,downstream,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648,1,['down'],['downstream']
Availability,I think the other one is redundant,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571:25,redundant,redundant,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571,1,['redundant'],['redundant']
Availability,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:441,failure,failures,441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933,2,['failure'],['failures']
Availability,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:206,failure,failure,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788,5,"['failure', 'resilien', 'robust']","['failure', 'failures', 'resilient', 'robust']"
Availability,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:254,error,error,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146,3,['error'],['error']
Availability,"I think this is actually a separate problem w.r.t. how cromwell constructs its script file. If you look in your GCS folder for the exec.sh, you'll probably see something like:. ```; echo hello && exit 1; cat $? > rc.txt; ```. Although there's no error message returned via the REST api, if you look in the server logs I suspect you'll see something along the lines of ""rc file not found""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109:182,echo,echo,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109,2,"['echo', 'error']","['echo', 'error']"
Availability,"I think this is expected behavior since `array[@]` is not a valid WDL expression and so cannot be used between `${ }` in a command string. Since WDL doesn't have a way to escape the sequence, I suggest inserting the `$` in manually in a way that doesn't trigger WDL string interpolation:; ```; String dollar = ""$""; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${dollar}{array[@]}; do; echo $i; done; >>>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663:328,echo,echo,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663,2,['echo'],['echo']
Availability,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:736,avail,available,736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308,1,['avail'],['available']
Availability,"I think this should be fairly high priority. It causes very confusing errors with the PAPI backend, made even more confusing by the fact that the same code will work on the local executor. Are there any workarounds at the moment?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955,1,['error'],['errors']
Availability,"I think we're good, I got this wdl passing which I think cover all cases we should support for now :. ```; task B {; String B_in; command {; python -c ""print('${B_in}')""; }; output {; String B_out = read_string(stdout()); }; }. task D {; String D_in; command {; python -c ""print(len('${D_in}'))""; }; output {; Int D_out = read_int(stdout()); }; }. task C {; Array[String] C_in; command {; echo ${sep = ' ' C_in}; }; output {; String C_out = read_string(stdout()); }; }. workflow w {; Array[String] an_array = [""a"", ""ab"", ""abc""]. scatter (item in an_array) {; call B {input: B_in = item }; call D {input: D_in = B.B_out }; }. call C {input: C_in = B.B_out }; call C as E {input: C_in = D.D_out }. scatter (item in D.D_out) {; call B as F {input: B_in = item }; }; }; ```. Output:. ```; {; ""w.F.B_out"": [""1"", ""2"", ""3""],; ""w.C.C_out"": ""a ab abc"",; ""w.E.C_out"": ""1 2 3"",; ""w.D.D_out"": [1, 2, 3],; ""w.B.B_out"": [""a"", ""ab"", ""abc""]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135907461:389,echo,echo,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135907461,1,['echo'],['echo']
Availability,"I think we're looking to freeze GCP changes for now due the imminent migration to GCP Batch. We're also not sure if reference disks are staying around, they are maintenance-intensive in Terra.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421:161,mainten,maintenance-intensive,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421,1,['mainten'],['maintenance-intensive']
Availability,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:678,ping,pinging,678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733,1,['ping'],['pinging']
Availability,"I thought it might be a good idea to run Centaur against 0.19_hotfix for comparison. It turns out that this particular Centaur test ""passes"" on 0.19_hotfix since the workflow fails consistently. However this failure appears to be due to botched call path construction when pulling down call logs and not for any reason related to write_lines:. ```; java.io.FileNotFoundException: Item not found: cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/a2f-stdout.log; at com.google.cloud.hadoop.gcsio.GoogleCloudStorageExceptions.getFileNotFoundException(GoogleCloudStorageExceptions.java:42) ~[gcsio-1.4.4.jar:na]; at com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.getMetadata(GoogleCloudStorageReadChannel.java:579) ~[gcsio-1.4.4.jar:na]; .; .; .; 2016-06-07 10:39:03,194 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(1b7836f6)]: persisting status of a2f to Failed.; 2016-06-07 10:39:03,194 cromwell-system-akka.actor.default-dispatcher-10 ERROR - WorkflowActor [UUID(1b7836f6)]: Item not found: cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/a2f-stdout.log; 2016-06-07 10:39:03,207 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(1b7836f6)]: Beginning transition from Running to Failed.; 2016-06-07 10:39:05,018 cromwell-system-akka.actor.default-dispatcher-12 INFO - WorkflowActor [UUID(1b7836f6)]: transitioning from Running to Failed.; ```. So AFAICT this workflow should be passing on develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224306256:208,failure,failure,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224306256,3,"['ERROR', 'down', 'failure']","['ERROR', 'down', 'failure']"
Availability,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:890,failure,failures,890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532,1,['failure'],['failures']
Availability,"I thought we were going to try @aednichols's idea for autocommitting the heartbeat writes (still batched, just not wrapped in one big transaction) to avoid having to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369:73,heartbeat,heartbeat,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369,1,['heartbeat'],['heartbeat']
Availability,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:532,ERROR,ERROR,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,1,['ERROR'],['ERROR']
Availability,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:605,alive,alive,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['alive'],['alive']
Availability,"I verified this workaround using the following WDL file:; ```; task dollarInterpolation {. 	String dollar = ""$""; 	command <<<; 	 array=(one two three); 	 for i in ${dollar}{array[@]}; do; 	 echo $i; 	 done; 	>>>. 	output {; 		String s = read_string(stdout()); 	}; }. workflow main {; 	call dollarInterpolation; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445:190,echo,echo,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445,1,['echo'],['echo']
Availability,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,4,['error'],['error']
Availability,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231,3,['error'],"['error', 'errors']"
Availability,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:298,alive,alive,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082,2,['alive'],['alive']
Availability,"I was using 0.19 proper. I compiled the 0.19_hotfix branch, and I got the same error.; I complied the develop branch, and this problem appears resolved... Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361,1,['error'],['error']
Availability,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:67,down,download,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158,1,['down'],['download']
Availability,"I was using before the in-memory database, and I use the `--metadata-output` to dump a JSON with the metadata of the workflow. What I am experimenting at the moment is to use the call-caching and the database to run the workflow to account for random failures of docker containers (see https://github.com/broadinstitute/cromwell/issues/3370) . It is unclear for me how the database is setup and which information stores, so I am not sure what can be clean and what is important/usuful for later use. That's why I think that it will be nice to have a documentation page showing how the database can be managed to grab information and clean the unnecessary data from time to time. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3415#issuecomment-373394497:251,failure,failures,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415#issuecomment-373394497,1,['failure'],['failures']
Availability,"I wonder if we could peek at the workflow store in the GET case and if the workflow is just-submitted, return a more helpful error than “not found”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-476351327:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-476351327,1,['error'],['error']
Availability,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1027,echo,echo,1027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372,5,['echo'],['echo']
Availability,"I would suggest the `wdl` tag on Stack Overflow, which is available everywhere, permanent, and easy to search. https://bioinformatics.stackexchange.com/questions/tagged/wdl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447:58,avail,available,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447,1,['avail'],['available']
Availability,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:147,error,error,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637,2,['error'],['error']
Availability,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:263,down,down,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,1,['down'],['down']
Availability,"I'd be okay with this, except; 1. I don't currently know how to have the test watch messages go from WorkflowActor to CallActor without logging.; 2. The test should not assume the second start message would be sent from the WorkflowActor before the first CallActor picks up its start message and begins running, and possibly even completes. . Log scraping is supposed to be easy with TestKit, something like:. ```; EventFilter.error(message = ""some message"", occurrences = 1) intercept {; // do something which should trigger such a log message; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394:427,error,error,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394,1,['error'],['error']
Availability,"I'd be surprised if a one-line gitignore addition would cause all those errors, which from an n of 1, seem mostly like bad travis weather... 🤞 a restart will fix these",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302:72,error,errors,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302,1,['error'],['errors']
Availability,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:138,failure,failure,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387,1,['error'],['error']
Availability,"I'll post here the full error message of the failed task, ok? The full Workflow is huge and probably has sensitive information on other tasks. ```python; failures: [{; causedBy: [{; causedBy: [{; causedBy: [],; message: ""Task MakeAnalysisReadyBam.BaseRecalibrator:9:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 19: unexpected exit status 1 was not ignored [Delocalization] Unexpected exit status 1 while running "" / bin / sh - c retry() { for i in `seq 3`; do gsutil - h\ ""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/st",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,4,"['error', 'failure']","['error', 'failures']"
Availability,"I'll start working with development, then. We don't have anything in; ""production"" that we cannot break. Thanks for the direction. On Aug 14, 2016 14:05, ""Jeff Gentry"" notifications@github.com wrote:. > Hi @seandavi https://github.com/seandavi - That does seem like it; > should work. Thinking back in my past I've definitely encountered tools; > which expect TMPDIR to exist and aren't smart enough to create it; > themselves. Also in JES there shouldn't be any issues with permissions, etc.; > ; > We'd certainly welcome a PR if you're game for it, either (or both); > against 0.19_hotfix or develop. On that note, I should point out that a; > new release (currently develop) is imminent and for all but one use case; > (call caching) we beliee it to be more robust/stable that 0.19. I'd; > personally recommend people who don't need call caching work with the new; > system, but I understand that some people aren't comfortable working with; > code which isn't yet released.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AAFpE17tmdKE5bY42PWUnqMW6YV6rifAks5qf1jugaJpZM4INzbb; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994:761,robust,robust,761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994,1,['robust'],['robust']
Availability,"I'm confused, I fixed the compile failure and merged with develop again and now there are more build failures - which do not appear to be consistent with each other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727,2,['failure'],"['failure', 'failures']"
Availability,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270,2,['error'],['error']
Availability,"I'm experiencing the issue, running the Broad ""gatk4-data-processing"" pipeline on their sample data, on Google Cloud. Repository with their code: https://github.com/gatk-workflows/gatk4-data-processing. The only change I made to the .wdl was setting Pre-emption to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:325,error,error,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,2,['error'],['error']
Availability,"I'm fine with whatever regarding scalaz, but would appreciate a pull request with `UnionTypes` replacing `Either`, if someone doesn't mind contributing, or a pointer to example code. I understand the theory of right bias for map/flatmap/etc., but am fresh off another exploration down a rabbit hole and feeling unadventurous regarding set theory at this second. Regarding ""Voetize the name"", what's the full definition-- is that just fixing the negative, or is it expanding the abbreviation ""rc""? I changed the flag name to ""continueOnRc"" in the latest push, leaving ""rc"" consistent with the rest of the code. Will also add docs to the README.md once we settle on the name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142706999:280,down,down,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142706999,1,['down'],['down']
Availability,"I'm getting the same error when using; ```; output {; Array[File] files = glob(""*.txt""); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103,1,['error'],['error']
Availability,"I'm getting the same error with this wdl: [mutect2-replicate-validation.zip](https://github.com/broadinstitute/cromwell/files/2242528/mutect2-replicate-validation.zip). And here is the dependency file: [wdl-dependencies.zip](https://github.com/broadinstitute/cromwell/files/2242529/wdl-dependencies.zip). This happens in versions 30 and up. When I run this in v29, cromwell doesn't throw an error but it hangs after completing the first task. . tsato@gsa5:novaseq: java -jar $wom validate mutect2-replicate-validation.wdl; Exception in thread ""main"" java.lang.RuntimeException: This workflow contains a cyclic dependency on m2.Mutect2.filtered_vcf; 	at wdl.draft2.model.Scope.childGraphNodesSorted(Scope.scala:53); 	at wdl.draft2.model.Scope.childGraphNodesSorted$(Scope.scala:44); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:46); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:46); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:97); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:14); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:10); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition$(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$ops$$anon$1.toWomWorkflowDefinition(WomWorkflowDefinitionMak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,2,['error'],['error']
Availability,I'm getting this error almost certainly when I run workflows where more samples (e.g. 96) than usual are scattered.; Cromwell version: 60-6048d0e-SNAP. Is there a workaround to this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-1244579032:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-1244579032,1,['error'],['error']
Availability,I'm going to close this since I found out that it's possible to drill down into the exception types and get a sorted frequency list within the Sentry UI itself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4003#issuecomment-412942971:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4003#issuecomment-412942971,1,['down'],['down']
Availability,"I'm going to take a quick stab at grabbing the WFID, a thought I had last night is it'd be badass to suck down the centaur log and emit it directly. If it is indeed a pain I'll just merge this for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401,1,['down'],['down']
Availability,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:1079,failure,failures,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['failure'],['failures']
Availability,"I'm having the same error, but only when I change the location to anything other than us-central1; Even with a bucket located in europe-west4 and zones set to europe-west4, it only works with location set to us-central1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965,1,['error'],['error']
Availability,"I'm just reading a bit [more](https://linux.die.net/man/1/flock) about `flock` - so with `-x` the idea is that you are creating a write lock - this means that this particular script will be run by many workers, and the first that gets to creating the lock will get it, continue, and do the build. ```; -s, --shared; Obtain a shared lock, sometimes called a read lock.; -x, -e, --exclusive; Obtain an exclusive lock, sometimes called a write lock. This is the default.; ```; I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.) . Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085:697,avail,available,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085,1,['avail'],['available']
Availability,"I'm leery but not necessarily against it. I'm also generally the one with the most conservative opinion in terms of adding WDL syntax, so view that take as a lower bound of acceptance :). @patmagee what were you thinking in terms of the syntax?. Pinging @vdauwera so she's abreast of this convo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732:246,Ping,Pinging,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732,1,['Ping'],['Pinging']
Availability,"I'm not 100% convinced that scaling is completely wired. Have you confirmed that this increase is also affecting your test timeouts? The linked error from #4223 says:. > Attempted 13 times over 5.126574189 seconds. That's *way* too short for something that should have already been scaled 12x. I'm in the process of [adding some println-debugging](https://github.com/broadinstitute/cromwell/commit/29d2f35662d6a81a4de383ad54df4ee0242611a4) on a similar issue. In my case I suspect one of the many, many timeouts wasn't scaled for an akka `.ask`, but will have to wait until the docker network issues are resolved to find out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625,1,['error'],['error']
Availability,"I'm not a specialist of GATK but this error doesn't seem like a big deal, it looks like it's defaulting to another logging mode. Maybe @vdauwera would know more ?; If you `ps -elf | grep java` is there anything still running in this docker container ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367368011:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367368011,1,['error'],['error']
Availability,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:623,error,error,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,5,"['Error', 'error']","['Error', 'error']"
Availability,"I'm not sure the exact scenario people would want to use this in, but it seems to replace the previous behavior ""retry a few times for known flakinesses, otherwise fail"" - with a less tuned ""retry a custom number of times for all codes"". * Is there any possibility that users would want to continue retrying on known flakinesses (hint: I really think they do) but not blindly restart on other error codes (which could be expensive for typos!)?; ---; One other thought:; * This seems to only catch JES telling us the task has failed. If the task on JES ""succeeds"" but Cromwell later finds out that it cannot download or evaluate a result, does this still retry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281:393,error,error,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281,2,"['down', 'error']","['download', 'error']"
Availability,"I'm really not a big fan of copy/pasting huge chunks of the swagger doc over and checking it into the repo separately. Is there an option in codegen to target only a subset of the endpoints? If not, is there a way to derive this cut-down swagger from the main one before running codegen over it? (eg programmatically parse the yaml, select only the womtool sections and re-write just that section to a file?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505027777:233,down,down,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505027777,1,['down'],['down']
Availability,"I'm seeing the `detect_sv` tool in the somatic workflow fail with this error (from stderr):. ```; [2018-11-04T19:02:19.372170Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Failed to complete master workflow, error code: 1; [2018-11-04T19:02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,10,"['ERROR', 'error']","['ERROR', 'error', 'errorMessage']"
Availability,I'm seeing this error on my builds too so I don't think it's related to your changes. I'll keep investigating.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472,1,['error'],['error']
Availability,"I'm still seeing the same error even when I add the `docker.io/` prefix. Confirming the correct command in `gcloud batch job describe`:. ```; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ --entrypoint\\=/bin/bash\\ docker.io/broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4\\ /mnt/disks/cromwell_root/script""; ```. Also it would be good not to require a `docker.io/` prefix as our users would need to edit all of their WDLs referencing Docker Hub images to be able to run on GCP Batch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035,1,['error'],['error']
Availability,"I'm still slowly investigating, but it looks like the scale factor environment variable isn't `export`ed, thus isn't available to `sbt`. Been playing around with the println's in a433a7f20a74faf70a1ec851545f0e9ec6836ce4 ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/555/consoleFull)) and 7fc56d3b73ce537b47aaecc6bf9cd0f1c020646f ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/557/consoleFull)).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596:117,avail,available,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596,1,['avail'],['available']
Availability,"I'm thinking perhaps this should be address instead through the use of docker User Namespaces and described in the documentation (a critical part of the solution!). When docker runs a container as root, it isolates the user/groups from the host definitions and instead remaps them. This could be important to a user for two reasons. Once, files written as root in the container will instead be written as this remapped user. Second, since IO on the underlying host VM is currently done as root, the container can read files as root. So if you mounted in /etc/passwd you could read/write on top of that. For a good tutorial see:. E.g. http://blog.aquasec.com/docker-1.10-user-namespace. For more details see:. https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-user-namespace-options. Also pinging @davidbernick for more thoughts from a security perspective",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367:807,ping,pinging,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367,1,['ping'],['pinging']
Availability,"I'm trying it with 16g right now. It still slows down abruptly after completing the first task inside of the scatter. The first task is fast, but then the second task (which depends on the first) is slower.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289:49,down,down,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289,1,['down'],['down']
Availability,"I'm uping this as we've seen CloudSQL connection failures recently resulting in a variety of inconsistencies in workflow statuses, metadata and generally DB state.; This is going to be even more important as we move towards CaaS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426:49,failure,failures,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426,1,['failure'],['failures']
Availability,"I'm using `tar` to create `*.zip` like below on a Mac OS; ```; tar -czvf workflow.zip sub_wf.wdl; ```. Also, if you are running a local server, I'm assuming there's a place where zip file is downloaded and unzip to? Do you know where on local machine that is running server does that zip file get unzipped to?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451260898:191,down,downloaded,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451260898,1,['down'],['downloaded']
Availability,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:27,down,downloaded,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750,1,['down'],['downloaded']
Availability,"I'm working on the [Workbench counterpart](https://broadinstitute.atlassian.net/browse/GAWB-1704) to this. I can reproduce an alternate version of this error on Production FireCloud, using Cromwell 25. This is what seems to be happening:; * write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; * file (a) gets localized; * file (b) does not get localized; * the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783,1,['error'],['error']
Availability,"I've always known deep down that I was born in the wrong century, wait millennium !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/471#issuecomment-188452106:23,down,down,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/471#issuecomment-188452106,1,['down'],['down']
Availability,I've been getting some failures on an unrelated workflow in the jenkins test. Looking to see if this change somehow caused a regression or if we're just in for a world of hurt once our nightly kicks off tonight,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4972#issuecomment-492804573:23,failure,failures,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4972#issuecomment-492804573,1,['failure'],['failures']
Availability,"I've been playing around with different settings and testing this out a lot. I like it!. I think I might have found a bug with writing to GCS buckets. I'm using this as my options file:. ```; {; ""workflow_log_dir"": ""gs://sfrazer-dev/foobar"",; ""call_logs_dir"": ""gs://sfrazer-dev/foobar/calls""; }; ```. And I ran a workflow both locally and with JES. In both cases it seemed to write the call logs just fine. However, it seems the workflow log got lost somewhere. It did, however, create a file `gs://sfrazer-dev/foobar` with the contents of the file being `foobar`. I also got this odd error message. Indeed the stderr file that was uploaded was zero bytes. However, it is supposed to be zero bytes! I'll try again and send something else to stderr. Might just be a spurious error when your files happen to be zero bytes... ```; [2016-03-04 10:02:32,132] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log; [2016-03-04 10:02:34,490] [info] Got 'range not satisfiable' for reading gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log at position 0; assuming empty.; [2016-03-04 10:02:34,692] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886:585,error,error,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886,2,['error'],['error']
Availability,"I've been seeing Travis failures in `MainSpec` ""run reading options"" with maybe 30% consistency while working on this, but that's testing an exception being thrown before the WorkflowActor is even created and I don't believe would have been perturbed by my changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-154524208:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-154524208,1,['failure'],['failures']
Availability,"I've been trying to shepherd these tests through. The previous failure was fixed by CROM-6890 work, and we're now getting a new failure that looks like CROM-6872.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003:63,failure,failure,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003,2,['failure'],['failure']
Availability,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:769,down,download,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,2,['down'],['download']
Availability,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:5,down,downloaded,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,1,['down'],['downloaded']
Availability,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:708,failure,failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,2,"['error', 'failure']","['error', 'failure']"
Availability,"I've found that this issue happens only when I use my custom reference files for assembly in BWA (mouse reference downloaded from Sanger ftp). I don't know if this is an issue with Cromwell or just the files I'm using. When I use the Broad's reference files downloaded from the google cloud storage, everything behaves as normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650:114,down,downloaded,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650,2,['down'],['downloaded']
Availability,"I've gone through and updated the dependency graph of updating sttp, you can view diff at https://github.com/delagoya/cromwell/tree/update-depversions . Still one more set of errors to fix: ; ```; root(update-depversions)> | 31>; [info] Compiling 4 Scala sources to $HOME/src/cromwell/womtool/target/scala-2.12/classes...; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:46: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val thisLevelNodesAndLinks: NodesAndLinks = callsAndDeclarations foldMap { graphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:56: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val subGraphNodesAndLinks: NodesAndLinks = subGraphs foldMap { wdlGraphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:26: private val clusterCount in object GraphPrint is never used; [error] private val clusterCount: AtomicInteger = new AtomicInteger(0); [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:40: local default argument in method listAllGraphNodes is never used; [error] def upstreamLinks(wdlGraphNode: WdlGraphNode, graphNodeName: String, suffix: String = """"): Set[String] = wdlGraphNode.upstream collect {; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:54: value foldMap is not a member of Set[wom.graph.GraphNode]; [error] graph.nodes foldMap nodesAndLinks _; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:89: private method nodesAndLinks in class WomGraph is never used; [error] private def nodesAndLinks(graphNode: GraphNode): NodesAndLinks = {; [error] ^; [error] 6 errors found; [error] (womtool/compile:compileIncremental) Compilation failed; [error] Total time: 4 s, completed Apr 16, 2018 9:00:54 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626:175,error,errors,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626,23,['error'],"['error', 'errors']"
Availability,"I've noticed the return code is only being recorded for the final command in a sequence (LocalBackend line 136). I think if I were a WDL writer and I specified failOnRc, I'd want any command failure anywhere in the sequence to trigger a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143250619:191,failure,failure,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143250619,2,['failure'],['failure']
Availability,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:73,Reboot,Rebooting,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509,1,['Reboot'],['Rebooting']
Availability,"I've started a new PR #4635 for some documentation, and @TMiguelT will hopefully be able to contribute to that. If I've got questions, I'll ping you on there to keep it a little more isolated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462535223:140,ping,ping,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462535223,1,['ping'],['ping']
Availability,"ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1253,Fault,FaultHandling,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,IIRC the error in question was passing the 403 Forbidden back from Google,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,1,['error'],['error']
Availability,"Ideally size would be checked prior to doing anything. . For instance: If there's a 1T file sitting on GCS and we're trying to `read_bool()` on it, it'd be nice to know that we shouldn't even bother downloading the file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889:199,down,downloading,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889,1,['down'],['downloading']
Availability,"If I understand this correctly, this will allow all scalatest tests to try twice - with the first failure being reported to a triage system in case it works the second time and the tests failing as usual if the same test fails twice in a row?. That sounds awesome!. The comments on adding this to the scalacheck tests seem like they could be part of a second pass since (a) there's not many of them and (b) they don't fail very often, so leaving them in the current ""must pass first time"" seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658#issuecomment-398533395:98,failure,failure,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658#issuecomment-398533395,1,['failure'],['failure']
Availability,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:219,error,error-keys,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092,1,['error'],['error-keys']
Availability,"If i run cromwell with `-Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible""` it does not work. Also using `-Dconfig.file=application.conf` does not work. Only **workflow_failure_mode** option in JSON config works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780:52,failure,failure-mode,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780,1,['failure'],['failure-mode']
Availability,If the AWS backend uses ioActor this may already be covered in configuration?; ```; system {; io {; # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # the quota availble on the GCS API; #number-of-requests = 100000; #per = 100 seconds. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }; }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778:194,avail,availble,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778,1,['avail'],['availble']
Availability,"If this error happened in production, the Cromwell process would terminate quickly and... presumably restarted by k8s or something. With these changes, the Cromwell process will sleep instead of terminating. Will this negatively impact startup time? I guess it would depend on how long a cold start takes to start initializing backends; I don't know how long that takes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756,1,['error'],['error']
Availability,"If this ever does get fixed - the last version that didn't throw `No getWomBundle method implemented in CWL v1` (`31.1`) threw an error for me when I ran on https://github.com/NCI-GDC/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:130,error,error,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['error'],['error']
Availability,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:188,avail,available,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864,1,['avail'],['available']
Availability,"If this ticket is about what I think it is (picking up jobs that were running when Cromwell is restarted), I think this ticket should be ""Recover Support"". I believe ""retry"" describes Cromwell's resilience to transient failures with Google Cloud services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703:138,Recover,Recover,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703,3,"['Recover', 'failure', 'resilien']","['Recover', 'failures', 'resilience']"
Availability,"If this works and removes the inconsistent test failures then I fully support turning test tragedies towards tip-top triumphs through taming the trusty tahr. In other words, :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/319#issuecomment-164066862:48,failure,failures,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/319#issuecomment-164066862,1,['failure'],['failures']
Availability,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:680,echo,echo,680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314,2,['echo'],['echo']
Availability,If you haven't already it's worth pinging @vdauwera to let her know of the incoming change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862:34,ping,pinging,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862,1,['ping'],['pinging']
Availability,If you rebase onto the tip of `develop` you should be able to fix that test failure. We had to do some fixes to those docker tests thanks to some changes in Travis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-490123112:76,failure,failure,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-490123112,1,['failure'],['failure']
Availability,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,redundant,redundant,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['redundant'],['redundant']
Availability,Ignoring failures of `should successfully run drs_usa_hca`:; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770806; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770807; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770814; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770815. because it is currently a known failure (due to the test data being deleted) and because this is an urgent fix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851,2,['failure'],"['failure', 'failures']"
Availability,Ignoring the PAPI2 failures resulting from their API change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3772#issuecomment-397102286:19,failure,failures,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772#issuecomment-397102286,1,['failure'],['failures']
Availability,"In Google Compute Engine, one can create custom networks and even delete the default network.; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; Confirm that Cromwell honors using a non-default network when specified via the config.; If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462:745,error,error,745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462,1,['error'],['error']
Availability,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:794,echo,echo,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827,2,['echo'],['echo']
Availability,"In a more recent scale test we observed this error on 37 out of 4000 workflows submitted. @geoffjentry @ruchim @danbills . ETA: Sorry, just saw that Rex already commented that (hadn't reloaded the page)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437068102:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437068102,1,['error'],['error']
Availability,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:446,down,down,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,2,"['down', 'error']","['down', 'errors']"
Availability,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,2,['error'],['error']
Availability,"In addition to the commit comments. Specifically for ""the docker"":; - InputParameters weren't wired in for secondary files. Now they are, with Polys and a bit of borrowing from the OutputParameters. The common code was refactored into FileParameter.; - CWL spec says InputArraySchema's can't have secondary files, and that they should be specified on the InputBinding. The CWL via ""the docker"" wants them on the IAS anyway. The IAS secondary files are now wired in through a Poly.; - ""The docker"" was using strings like `$(""foo"")/$(""bar"")` that was tripping up our various expression regex patterns. This PR replaces them with the (latest as of Friday) copy-port of cwltool's state machine. The state machine works on _any_ string, even those without `""$(""`. But the spec for secondary files says that only expressions should be returned-as-rendered, while plain strings should be instead appended instead of used as literals. It turns out this discrimination is done in cwltool `""$("" in expr` (see numerous links in the scala comments). So the presence of `""$(""` is now used for our `ECMAScriptExpression` to similarly guess if a string is a interpolated string or a regular string. `ECMAScriptExpression` and `InterpolatedString` were pretty much the same thing and were merged back together. There was also a bit of code where if a malicious (or errant) CWL was submitted a `java.lang.Error` would be thrown possibly exiting the JVM. The code around this section was refactored to use a compile-time-checked Poly instead of hope-we-got-them-all pattern matching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979:1388,Error,Error,1388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979,1,['Error'],['Error']
Availability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,3,['error'],['error']
Availability,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:330,avail,available,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,1,['avail'],['available']
Availability,"In my FireCloud workflow, call caching is turned on by default. ; <img width=""1153"" alt=""screen shot 2018-06-07 at 8 57 27 am"" src=""https://user-images.githubusercontent.com/10040800/41101026-f1183d66-6a30-11e8-9fc0-75486f87e18f.png"">. and I also just realized that within the same workflow, some early tasks have the cache results recognized (Hit), for example:; <img width=""509"" alt=""screen shot 2018-06-07 at 9 03 24 am"" src=""https://user-images.githubusercontent.com/10040800/41101331-debd6938-6a31-11e8-903a-c6bf9c6e85e6.png"">; However, once it reaches to one task (M2), which splits the job based on scatter count (in my case, it is 50, basically, each subjob will only take care of a fraction of genome), I think the fraction of genome each job takes care of in different runs should be the same because no parameter has changed. But quite unexpectedly, the subjob cant recognize previous run (Miss). for example:; <img width=""905"" alt=""screen shot 2018-06-07 at 9 19 38 am"" src=""https://user-images.githubusercontent.com/10040800/41102077-f3d8fde4-6a33-11e8-8e24-0c13ada5865a.png"">. If I can't copy whatever successfully finished in previous subjobs, i have to start the whole 50 subjobs every time, it will dramatically increase my cost and time and there is no guarantee that new job will finish successfully because of those transient error. . Maybe there is something I am missing to set up call caching correctly, but as a newbie, I can't figure out myself. . Thanks all in advance",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531:1346,error,error,1346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531,1,['error'],['error']
Availability,"In other PC I got another error "" docker: command not found"". Probably it is because it tries to run docker inside cromwell docker container.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669,1,['error'],['error']
Availability,In that case I’d side with more and dialing down instead of less and dialing up,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352072779:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352072779,1,['down'],['down']
Availability,"In the documentations to struct the syntax that is mentioned is:; ```; Person a = {""name"": ""John"",""age"": 30}; ```; So I expect that ; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; should be treated in a similar way. If in a development version you changed the syntax the error should be thrown at compiletime and it should be explained in the documentation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464251330:338,error,error,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464251330,1,['error'],['error']
Availability,"In the spirit of ""everything will be different in a month"" I'm less concerned about some of the weird stuff I pointed out above and I realized it'd be good to get this functionality in 0.21 as some downstream constituents would like this behavior",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101:198,down,downstream,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101,1,['down'],['downstream']
Availability,"In this case `df` produces space-separated columns, but `read_object` is looking for tab separators. So this particular failure is to be expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/615#issuecomment-270210508:120,failure,failure,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/615#issuecomment-270210508,1,['failure'],['failure']
Availability,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:159,error,error,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502,2,"['error', 'failure']","['error', 'failure']"
Availability,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:330,avail,available,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153,1,['avail'],['available']
Availability,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:40,echo,echo,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,2,['echo'],['echo']
Availability,"Indeed. but the problem was that we couldn't tell that a file was missing; and indeed which file it was. On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; wrote:. > IIRC the error in question was passing the 403 Forbidden back from Google; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082:195,error,error,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082,1,['error'],['error']
Availability,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:240,failure,failures,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781,1,['failure'],['failures']
Availability,"Interesting! I may have found another - deterministic - way, based on how it's done in [gopsutil](https://github.com/shirou/gopsutil/blob/e4ec7b275ada47ca32799106c2dba142d96aaf93/disk/disk_linux.go#L271):. 1. Find the `st_dev` device attribute for the mount point in `/proc/self/mountinfo` file,; which is ""the most authoritative source to check your mounts"" [1],; and is always present in modern Linux kernels [2]. Per [3], `st_dev`; > Identifies the device containing the file. The st_ino and st_dev, taken together, uniquely identify the file. The st_dev value is not necessarily consistent across reboots or system crashes, however. The format of `mountinfo`, according to [2]:. 3.5	/proc/<pid>/mountinfo - Information about mounts; --------------------------------------------------------. This file contains lines of the form:. 36 35 98:0 /mnt1 /mnt2 rw,noatime master:1 - ext3 /dev/root rw,errors=continue; (1)(2)(3) (4) (5) (6) (7) (8) (9) (10) (11). (1) mount ID: unique identifier of the mount (may be reused after umount); (2) parent ID: ID of parent (or of self for the top of the mount tree); (3) major:minor: value of st_dev for files on filesystem; (4) root: root of the mount within the filesystem; (5) mount point: mount point relative to the process's root; (6) mount options: per mount options; (7) optional fields: zero or more fields of the form ""tag[:value]""; (8) separator: marks the end of the optional fields; (9) filesystem type: name of filesystem of the form ""type[.subtype]""; (10) mount source: filesystem specific information or ""none""; (11) super options: per super block options. So for example, inside my task; ```; grep cromwell_root /proc/self/mountinfo. 904 885 8:16 / /cromwell_root rw,relatime master:325 - ext4 /dev/disk/by-id/google-local-disk rw; ```; `8:16` here is `st_dev`, with; > (3) major:minor: value of st_dev for files on filesystem. 2. Now we look up `major minor` in `/proc/diskstats` [4]:. The /proc/diskstats file displays the I/O statistics; of b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529613084:601,reboot,reboots,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529613084,2,"['error', 'reboot']","['errors', 'reboots']"
Availability,"Interesting,. The above command gives me ; ```; $ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports .; [2018-11-13 10:26:04,67] [info] Running with database db.url = jdbc:hsqldb:mem:5089344a-b618-4a55-bf07-de1c4d6a9a1e;shutdown=false;hsqldb.tx=mvcc; [2018-11-13 10:26:09,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-13 10:26:09,78] [info] [RenameWorkflowOptionsInMetadata] 100% ; [2018-11-13 10:26:09,89] [info] Running with database db.url = jdbc:hsqldb:mem:740605a3-c63f-4491-b41a-58b162626bcb;shutdown=false;hsqldb.tx=mvcc; [2018-11-13 10:26:10,18] [info] Slf4jLogger started ; Exception in thread ""main"" java.io.IOException: Is a directory ; at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ; at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46) ; at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ; at sun.nio.ch.IOUtil.read(IOUtil.java:197) ; ```. The same error occurs with a full path like ; ```; java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports `pwd`; ```. It does give me a workaround since the following does lead to success!; ```; $ zip -r foo.zip .; $ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports foo.zip; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438308547:1032,error,error,1032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438308547,1,['error'],['error']
Availability,"Interestingly, I can get it to build for a single arch at once just fine [0] and the error is caused by lack of support for multiarch images somewhere in the local toolchain. [0] `platforms = List(""linux/arm64"")`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775,1,['error'],['error']
Availability,"IntervalList to Done.; 2016-05-16 23:42:51,977 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Starting.; 2016-05-16 23:42:55,593 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Done.; 2016-05-16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-For: 69.173.127.107, User-Agent: Java/1.8.0, Host: app:8000),Empty,HTTP/1.1); com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:1743,ERROR,ERROR,1743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,3,"['Alive', 'ERROR', 'Error']","['Alive', 'ERROR', 'Error']"
Availability,Is that not a real failure?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164844157:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164844157,1,['failure'],['failure']
Availability,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640,2,['error'],['error']
Availability,Is this issue related to problem with deadlocks in `ServicesStoreSpec`?; If this task is still actual I'll appreciate more information about it (if any available).; @gemmalam @danbills,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518659520:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518659520,1,['avail'],['available']
Availability,Is this kind of redundant to the token system?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042:16,redundant,redundant,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042,1,['redundant'],['redundant']
Availability,Is this scheme robust to large files?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/272#issuecomment-154187746:15,robust,robust,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/272#issuecomment-154187746,1,['robust'],['robust']
Availability,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:708,failure,failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457,1,['failure'],['failure']
Availability,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:598,error,error,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,1,['error'],['error']
Availability,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:362,error,error,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['error'],['error']
Availability,"It appears that this is the same error being referenced in #1782, closing that one but leaving the breadcrumb in case someone wants a stacktrace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829,1,['error'],['error']
Availability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:408,down,downstream,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['down'],['downstream']
Availability,"It looks like it does now shut down, but the exit code is 0, which is potentially deceiving to people or programs that interact with it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2645#issuecomment-394727569:31,down,down,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2645#issuecomment-394727569,1,['down'],['down']
Availability,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:29,failure,failure,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715,1,['failure'],['failure']
Availability,"It looks like the link OP posted is now a 404, but I'd also like to voice an interest in this option. I have written a few workflows by now and have found that the output of Cromwell is very difficult to navigate through. It's tricky to quickly find errors, especially if the output is piped through a file with no color coding or any situation involving automated tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-1092107938:250,error,errors,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-1092107938,1,['error'],['errors']
Availability,"It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄 . > Behavior is undefined when groups are tied, whatever happens by default is fine",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292,1,['error'],['error']
Availability,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:234,down,downloading,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,3,"['down', 'outage']","['downloading', 'outages']"
Availability,"It seems that the input to the missing-in-action scatter, the indexed array that it scatters on, is empty and that is the root cause of the failure....; IMO it would help to see in the log/report that the scatter has actually been observed but it simply had 0 elements which quite often may indicate that something went wrong (perhaps it should have a warning icon next to it in the report (e.g. a yellow ! triangle) .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377:140,failure,failure,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377,2,['failure'],['failure']
Availability,"It should be fairly easy to add yes, we can also drill down deep in the caused by chain or not depending on how wide/specific we want to be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139,1,['down'],['down']
Availability,"It still exists and no one ever determined if it was appropriate, so it is still potentially relevant. It's just not how we typically handle errors that occur in API calls so it looked weird. May or may not actually be an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094:141,error,errors,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094,1,['error'],['errors']
Availability,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:710,error,error,710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178,1,['error'],['error']
Availability,"It turns out the syntax change; https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather. I wonder if the documentation could be improved?. all the examples I found by googling things like 'wdl array iteration' found links to version 1 example. eventually, I stumbled on the idea of searching for wdl gather. Gather is not a standard term in computer science. Iterating over arrays does not require a scatter task. . I understand it is hard to write front ends with good error messages. I wonder if there is a way to write a something that checks for wdl version incompatibilities. I belive my womtool reported my wdl was valid. . Kind regards. Andy. Also there is a type in the code example https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather . ; ```; call sum {input: ints = inc.increment}; ```. should be; ```; call sum {input: ints = inc.incremented}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519:496,error,error,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519,1,['error'],['error']
Availability,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:424,echo,echoQuick,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,3,['echo'],['echoQuick']
Availability,"It would be great if the error was more descriptive, including what the error is and what the user can do about it (if anything).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375,2,['error'],['error']
Availability,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778,2,['error'],['error']
Availability,"It would be nice to have this information available from the command line too, i.e. a -version flag that dumps the version/hash and exits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405,1,['avail'],['available']
Availability,"It's a bit confusing, you've posted the error log from a stage called `CollectSequencingArtifactMetrics`, but posted the WDL for a task called `FilterByOrientationBias`. Could you please send the WDL for the same task that's failing?. A case where this bug can happen is when you do doing string manipulation with `File` type variables. For example, if you `sub()` a `File` object to generate a new filename, it will use the S3 URL, as the input to `sub`. However, by the S3 URL, when interpreted by Bash as a path, doesn't exist on the right disk. I wrote an issue about it [here](https://github.com/openwdl/wdl/issues/260). Tell me if it's this same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106,1,['error'],['error']
Availability,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193,2,['error'],['error']
Availability,"It's a real error. I know what it is, how to resolve might end up being tricky. Will see when I'm back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164846710:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164846710,1,['error'],['error']
Availability,It's also possible that by; >all 500 errors. @cjllanwarne means 5xx,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448,1,['error'],['errors']
Availability,It's happening again. Last time this was the result of our LB blocking new pingdom servers. @hjfbynara could you please run update script?. I don't see any added in december here: https://help.pingdom.com/hc/en-us/community/posts/208953545-Pingdom-Probe-Servers-Pingdom-IPs.; But it seems too coincidental that the symptoms are exactly the same as they were last time. ![image](https://user-images.githubusercontent.com/165320/50653143-8f762700-0f56-11e9-9b2d-76da416c47f2.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835:75,ping,pingdom,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835,3,"['Ping', 'ping']","['Pingdom-Probe-Servers-Pingdom-IPs', 'pingdom']"
Availability,It's very cool that we get the timing breakdown for the preempted ones. As far as I know all of the other failures don't get any events...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185924247:106,failure,failures,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185924247,1,['failure'],['failures']
Availability,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" …`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:462,failure,failures,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026,1,['failure'],['failures']
Availability,Jeff and all;; This is bcbio CWL triggering the problem. We use 0 for disk when we have operations that don't need to download/manage the inputs and are just manipulating things in memory. If changing turns out to be a major issue I'm happy to re-explore how we handle it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4150#issuecomment-424730864:118,down,download,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4150#issuecomment-424730864,1,['down'],['download']
Availability,"Jeff;; Thanks so much for looking at this. I went back through my runs and realized it is only happening on a single filesystem, the `/n/groups` filesystem on orchestra2 at Harvard Medical School. The folks there were nice enough to let me know the details about the setup:. > /n/groups is NFSv3 being served from an EMC/Isilon fileserver:; > ; > alb15@login03:~$ mount | grep groups; > orchestra2-p05.med.harvard.edu:/ifs/systems/Orchestra/groups on /n/groups type; > nfs (rw,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,hard,proto=; > tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.120.41.64,mountvers=3,mountport=; > 300,mountproto=udp,local_lock=none,addr=10.120.41.64). Does anything in there help provide some clues to dig into it? Apologies, I'm fishing around in the dark with trying to provide useful information since I'm ignorant of both the magic of shared filesystem peculiarities and how that would impact Hsqldb here. All of my searches on this error have to do with database setups and older version of hsqldb so my Google debugging is not helping. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-388827013:966,error,error,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-388827013,1,['error'],['error']
Availability,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4933,alive,alive,4933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['alive'],['alive']
Availability,"Just a though --what if we added a ""retryOn"" attribute that took a boolean expression. Then we add a function like grep(pattern, var/file, mode) <see R for example function> which would probably be similar work and much more reusable. Then Jose could write something like. retryOn = grep(""(foo|bar"", $stderr, ""any""). But could also check any other element. Syntax is lousy typing from tiny keyboard. > On Feb 16, 2017, at 11:23 AM, Kate Voss <notifications@github.com> wrote:; > ; > Adding @vdauwera's comment about adding error codes to GATK from DSDE-docs #1742:; > ; > We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314:523,error,error,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314,2,['error'],['error']
Availability,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004,1,['error'],['error']
Availability,"Just as a safety measure, maybe we should consider add a config option called something like ""allowCrossFSLocalisation"" - just to avoid accidentally downloading a 500GB file from GCS and the costs involved?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494:149,down,downloading,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494,1,['down'],['downloading']
Availability,"Just curious, why are you giving the qsub command to singularity? You might consider launching a job that runs singularity, not the other way around. If the above works for you then ok, but it seems likely to lead to error to me. If you use this method you will likely find you don't need all of those binds. The need for multiprocessing with python was also a requirement that was pre-3.0 - after 3.0 python is totally removed. So if you are adding extra binds for the benefit of the Singularity python, you definitely don't need to do this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438681841:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438681841,1,['error'],['error']
Availability,"Just in time for thumbs to come in, I'm realizing that I rather blindly added `55` to expected failures when in fact there is a chance I broke it. Investigating...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-482727793:95,failure,failures,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-482727793,1,['failure'],['failures']
Availability,"Just noticed, this PR uses different hashes for conformance tests for Local / PapiV1 / PapiV2. I'm assuming that was not intentional. I have an incoming PR (as soon as PRs quiet down + I get travis to pass for once) that refactors this into reusable includes. That will hopefully help making CI changes in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660:178,down,down,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660,1,['down'],['down']
Availability,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516,2,['error'],['error']
Availability,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,recover,recovered,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771,1,['recover'],['recovered']
Availability,"Just to comment that this feature would be really useful for AWS/Cfncluster based work, since cfncluster has a known issue where compute nodes could be scaled down even with jobs still running, leaving no rc file and no way for cromwell to know that the job is already dead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358:159,down,down,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358,1,['down'],['down']
Availability,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:362,avail,available,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,4,"['avail', 'failure']","['available', 'failures']"
Availability,"Lastly,; ```; 2019-01-31 20:30:56,569 INFO - changelog.xml: changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne: ChangeSet changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne ran successfully in 8ms; 2019-01-31 20:30:56,593 ERROR - changelog.xml: changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Change Set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne failed. Error: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSER",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:270,ERROR,ERROR,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,3,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failures']"
Availability,"Latest on aws_backend branch. ________________________________; From: mcovarr <notifications@github.com>; Sent: Thursday, June 7, 2018 6:47:04 AM; To: broadinstitute/cromwell; Cc: Thomas Dyar (EXTERNAL); Author; Subject: Re: [broadinstitute/cromwell] Strange ""Boxed Error"", probably authorization / config (#3736). Also what version of Cromwell is this?. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_broadinstitute_cromwell_issues_3736-23issuecomment-2D395377185&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=U0Ofhj4NWKhfebpsRfeTCvMxBZRUhJ44bevIpm6SR-E&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AIfLudmaJuhICx-5FxkNqusXZWh8pJ14zvks5t6QSogaJpZM4UdPeJ&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=qPDfKyTsVifxuNzZbVjE9HCwrHl6ANQrTo9wh-9YTJE&e=>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027:266,Error,Error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027,1,['Error'],['Error']
Availability,"Learned that a change in Pipelines API omitted the ""preemptible"" key from the operations metadata, and that change introduced a null pointer in the Cromwell code. . AC: As a way to address this, it would be great if we could modify the Cromwell code so that when its parsing operation metadata, that if certain keys are missing (such as Preemptible) -- we use the defaults where possible, else fail gracefully with an error that states which information couldn't be parsed, and that caused the workflow to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804:418,error,error,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804,1,['error'],['error']
Availability,"Let's talk after standup. I think we're going to need it - IIRC there's already something incorrectly reading/writing from metadata, and recovery was the use case which informed it in the first place & that's incoming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284:137,recover,recovery,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284,1,['recover'],['recovery']
Availability,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:9872,Error,ErrorHandling,9872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,4,['Error'],['ErrorHandling']
Availability,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:285,failure,failures,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228,4,"['FAILURE', 'failure']","['FAILURES', 'failures']"
Availability,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:120,ERROR,ERROR,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:79,ERROR,ERROR,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,"Looked at this quickly w/ @salonishah11 and I suspect the problem is due to our ""artisanal"" friend, `cromwell.api.CromwellClient`, which as we discovered earlier this week just gleefully eats errors from Cromwell and returns them in an insane way",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406643905:192,error,errors,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406643905,1,['error'],['errors']
Availability,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:517,down,downstream,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621,1,['down'],['downstream']
Availability,"Looking at the gitter convo, the problem looks more like a case of overzealous logging - could we just drop it from `info` down to `debug`? I'm curious why it was `info` in the first place tbh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302:123,down,down,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302,1,['down'],['down']
Availability,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:57,alive,alive,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['alive'],['alive']
Availability,"Looking into a real test failure here, works for me in IntelliJ but failing in GHA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459,1,['failure'],['failure']
Availability,Looks good to me once those Failures are added. Happy for the array expression outputs/preevaluation stuff to be put off until later.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/118#issuecomment-125236175:28,Failure,Failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/118#issuecomment-125236175,1,['Failure'],['Failures']
Availability,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,16,['error'],"['error', 'errors']"
Availability,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:137,error,errors,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:301,failure,failures,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,"Looks like read_json in current trunk still has some issues.; When I give a json like this:; ```json; {; ""Homo sapiens"": {; ""transcriptome"" : ""/pipelines/indexes/HUMAN/27/gencode.v27.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/HUMAN/27/"",; ""salmon"": ""/pipelines/indexes/HUMAN/27/salmon""; },; ""Mus musculus"": {; ""transcriptome"" : ""/pipelines/indexes/MOUSE/M16/gencode.vM16.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/MOUSE/M16/gencode.vM16.annotation.gtf"",; ""salmon"": ""/pipelines/indexes/MOUSE/M16/salmon""; }; }; ```; with wdl like this; ```; Map[String, Map[String, String]] indexes = read_json(references) ; ```; I get:; ```; Workflow input processing failed; WorkflowFailure(ERROR: indexes is declared as a Map[String, Map[String, String]] but the expression evaluates to a Object: Map[String, Map[String, String]] indexes = read_json(references) ^ ,List()); ```; I do not get what is wrong there, I've tried different type combinations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127:679,ERROR,ERROR,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127,1,['ERROR'],['ERROR']
Availability,"Looks like the data in mysql will be lost when the docker compose is shut 'down'. Is that true? If so, it would be good to document (or make it so) to mount in a volume in the mysql docker compose so the database survives a restart",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187:75,down,down,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187,1,['down'],['down']
Availability,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:59,FAILURE,FAILURE,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499,4,['FAILURE'],"['FAILURE', 'FAILURES']"
Availability,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:646,Failure,Failure,646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971,1,['Failure'],['Failure']
Availability,Looks like this error was specific to my runtime+configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282,1,['error'],['error']
Availability,"Looks like this logic assumes that the array has at least one element:; https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomObject.scala#L83. So we just need to add a case for when it's empty. EDIT: The error is thrown earlier, and tells us that it can't even recognize `WomCompositeType` as `WomObjectType` in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595#issuecomment-458252514:240,error,error,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595#issuecomment-458252514,1,['error'],['error']
Availability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['error'],['error']
Availability,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517,1,['error'],['errors']
Availability,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:65,Ping,Pinging,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078,2,"['Ping', 'down']","['Pinging', 'down']"
Availability,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,redundant,redundant,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692,1,['redundant'],['redundant']
Availability,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:40,error,errorred,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,2,"['alive', 'error']","['alive', 'errorred']"
Availability,"Maybe I'm being melodramatic. Probably if something needs 128GB of memory then something has gone wrong or we need a new method. It's worth a failure to go take a look. Yeah, I could be on board with two parameters like the Java xmx/xms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499246858:142,failure,failure,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499246858,1,['failure'],['failure']
Availability,Merging despite AWS test errors because:; * The AWS backend test suite has known issues relating to dockerhub pull limits; * We are not running AWS in Terra production; * The changes here are **extremely** unlikely to have had any impact on the AWS backend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511:25,error,errors,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511,1,['error'],['errors']
Availability,Merging despite known sbt failure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3390#issuecomment-372422393:26,failure,failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3390#issuecomment-372422393,1,['failure'],['failure']
Availability,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:31,failure,failure,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120,5,"['error', 'failure']","['error', 'failure']"
Availability,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:46,failure,failures,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141,2,['failure'],['failures']
Availability,"Might this eventually expand to be a more general purpose ""allow certain error codes to be retried""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648,1,['error'],['error']
Availability,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1415,alive,alive,1415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453,1,['alive'],['alive']
Availability,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:410,avail,available,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488,1,['avail'],['available']
Availability,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:251,down,down,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937,1,['down'],['down']
Availability,"Must have forgotten this info... 1) develop (cromwell-23-79f6e12-SNAPSHOT.jar); 2) No, cannot believe I forgot again...; 3) No. On Fri, Nov 4, 2016 at 9:43 AM, Chris Llanwarne notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220; > - Is this .21, .22 or develop?; > - Did you capture the 'CMD \' thread dump from the JVM?; > - Was there an error message from Cromwell before it got stuck?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk5vBVr6UFscUsg3sazpo1H9pyVgMks5q6zaXgaJpZM4Ko1_r; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488:363,error,error,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488,1,['error'],['error']
Availability,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:259,recover,recover,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['recover'],['recover']
Availability,"My JSON file:. {; ""CNVSomaticPairWorkflow.common_sites"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/common_snps_sample-chr20.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""8f0ef5140437"",; ""CNVSomaticPairWorkflow.intervals"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/chr20.interval_list"",; ""CNVSomaticPairWorkflow.normal_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam.bai"",; ""CNVSomaticPairWorkflow.bin_length"": ""10000"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/wgs-no-gc.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsample",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590:600,down,downsampled,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590,2,['down'],['downsampled']
Availability,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,3,"['Error', 'error']","['Error', 'error']"
Availability,"My task are not using docker. Also I see no attempts at all to copy or softlink the files. Not in the log, and not in the cromwell-executions folder.; Also hard-linking seems to persist using the `SGE` backend. Even though the localization has the same configuration as above. So the error is not backend specific. Fortunately, all the other values in the config are used. Which makes me think that either my configuration file has some error (keys in wrong place). But I have checked this over and over again already with the example files and it seems to be correct (though I am not infallible of course).; Or the backend just ignores the values due to a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440:284,error,error,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440,2,['error'],['error']
Availability,"My view of difficulty has only increased. You could narrow it down to a subset of things - . - Nothing in the backend; - Nothing in the service registry; - In the future, nothing involving filesystems. And it'd be easy to do. But most of the stuff people will be fiddling with are in those blocks. One could set up maybe some service (not necessarily ServiceRegistry service)where things which care about config register themselves and then process htings that way but that seems like a giant pain in the ass for not enough gain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308:62,down,down,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308,1,['down'],['down']
Availability,My workflows are running without errors now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477128999:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477128999,1,['error'],['errors']
Availability,NB also check how Cromwell reports errors in imported WDL files,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524:35,error,errors,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524,1,['error'],['errors']
Availability,"NB the failures in `travis/pr` `centaurJes` were caused by the `customLabels` change merging into develop before I rebased. . I didn't want to wait 30 minutes to rerun the tests, since they were already passing on the other backends, and in `travis/push`...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2941#issuecomment-347637020:7,failure,failures,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2941#issuecomment-347637020,1,['failure'],['failures']
Availability,NOT MERGED closing temporarily for repairs as there are at least a couple of legit broken PAPI Centaur tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3668#issuecomment-391168787:35,repair,repairs,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668#issuecomment-391168787,1,['repair'],['repairs']
Availability,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:189,error,errors,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030,6,['error'],"['error', 'errorHandler', 'errors']"
Availability,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:62,mainten,maintenance,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867,1,['mainten'],['maintenance']
Availability,Need to investigate the Travis failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/220#issuecomment-145907163:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/220#issuecomment-145907163,1,['failure'],['failures']
Availability,"Net improvement to tests 👍 will merge. Failures are known entities, `Application Default Credentials are not available` and `cromwell-drs-localizer`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033:39,Failure,Failures,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033,2,"['Failure', 'avail']","['Failures', 'available']"
Availability,"Nevermind again, testing now but I don't think we need to change anything here. `test.inc.sh` is already set up to use `VAULT_ROLE_ID` and `VAULT_SECRET_ID` env vars for auth if they're available, so all that's needed is to add those and remove `VAULT_TOKEN`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728,1,['avail'],['available']
Availability,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['failure'],['failures']
Availability,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:489,error,error,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095,2,"['Error', 'error']","['Error', 'error']"
Availability,"New theory: I believe this is seen just before every ""deadlock"" failure. ```java; Exception in thread ""db-2"" java.lang.IllegalArgumentException: requirement failed: count cannot be increased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$increaseInUseCount$1(ManagedArrayBlockingQueue.scala:43); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201); at slick.util.ManagedArrayBlockingQueue.increaseInUseCount(ManagedArrayBlockingQueue.scala:42); at slick.util.AsyncExecutor$$anon$2$$anon$1.beforeExecute(AsyncExecutor.scala:117); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592:64,failure,failure,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592,1,['failure'],['failure']
Availability,"New thoughts/additional info. . I just tried a workflow that evaluates all the way to the end successfully (and had been run before on the same data and no, call caching didn't happen, as is expected with AWS backend), with the exception of adding workflow options to specify the output and logs directories for the final results. . Interestingly enough the new prefixes were generated but files were not transferred over EXCEPT for the log. The workflow log transferred just fine. In the log there appears to be no errors or indication that the intended outputs were not successfully transferred over. . I'm looking at the workflow status, and while all the files were made correctly (so all tasks completed successfully), but the workflow as a whole failed b/c it knows it failed to transfer over the output data. However again, there are no errors indicated in the metadata indicating why no files were copied. . I'm wondering if this too would be expected to be a hashing failure? Are the identities of the files created that are intended as outputs defined by the hashing? Would this behavior be expected given the current issues with call caching? Or is this a new issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084:516,error,errors,516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084,3,"['error', 'failure']","['errors', 'failure']"
Availability,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:89,failure,failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466,2,['failure'],['failure']
Availability,"Nice, handy to have this written down for future Google searchers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919:33,down,down,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919,1,['down'],['down']
Availability,"No effect on final status. Just exits with non-zero. On Mon, Mar 27, 2017 at 9:58 AM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Does the error have the effect; > on the actual final status of the workflow ? Or does it cause cromwell to; > exit with a non 0 exit code ? I wasn't able to reproduce the exact same; > error but I've had similar ones and I've got a branch that should fix it,; > if you want to try it out.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2yfUNo1FrM2Y7ftpxQSHOsOwCc3ks5rp8BpgaJpZM4Mi6Mp>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493:179,error,error,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493,2,['error'],['error']
Availability,"No fatal error. so, close.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-449074013:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-449074013,1,['error'],['error']
Availability,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:129,error,errors,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991,4,['error'],['errors']
Availability,"No, not yet. A bunch of batching code was put into `JesPollingActor`, but that logic is not available in a generic way. If anyone with cycles does come across this ticket, that might be a good first place to start. The logic would need to be slightly different. ~JES~ PAPI allows batching _any_ jobs together, while a batch of GridEngine jobs have to use the same resource requirements, aka runtime attributes. Still partial-grouping-for-similar-jobs would still be an improvement over the current single-submit-per-job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917:92,avail,available,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917,1,['avail'],['available']
Availability,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:541,error,error,541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,1,['error'],['error']
Availability,"Not entirely sure I understand the gcsFilesystem in WorkflowDescriptor, but if everyone agrees that it's fine I'll pipe down. So, :+1: from me!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170681519:120,down,down,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170681519,1,['down'],['down']
Availability,"Not expecting the AWS build to pass, that failure is addressed in #6547.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974,1,['failure'],['failure']
Availability,"Not expecting the PAPI builds to pass, those failures are addressed in #6546.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731,1,['failure'],['failures']
Availability,"Not for this PR, but I noticed that the WorkflowActor is more than 1000 lines now, we might need to think of a way to break things down a little bit more in the future, because I have the feeling it's going to keep growing as we add more features..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/310#issuecomment-161990464:131,down,down,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/310#issuecomment-161990464,1,['down'],['down']
Availability,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:290,avail,available,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['avail'],['available']
Availability,"Not really for this PR I think, but looking at the `validateInputs` method, it only validates the inputs if a file was passed (makes sense), but if no input file is specified and the WDL does need an input, validation will return success, which is weird IMO.; @geoffjentry My 2 cents are I personally prefer having small short-lived actors instead of a singleton actor that handle all the requests. I think it's more robust, faster, and less leading towards godlike actors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365:417,robust,robust,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365,1,['robust'],['robust']
Availability,"Not really, I have a work-around (if /sys/block/sdb/ is a directory and /dev/sdb is mounted in mtab, use /sys/block/sdb/); ```; function findBlockDevice() {; MOUNT_POINT=$1; FILESYSTEM=$(grep -E ""$MOUNT_POINT\s"" /proc/self/mounts \; | awk '{print $1}'); DEVICE_NAME=$(basename ""$FILESYSTEM""); FS_IN_BLOCK=$(find -L /sys/block/ -mindepth 2 -maxdepth 2 -type d \; -name ""$DEVICE_NAME""); if [ -n ""$FS_IN_BLOCK"" ]; then; # found path to the filesystem in the block devices. get the; # block device as the parent dir; dirname ""$FS_IN_BLOCK""; elif [ -d ""/sys/block/$DEVICE_NAME"" ]; then; # the device is itself a block device; echo ""/sys/block/$DEVICE_NAME""; else; # couldn't find, possibly mounted by mapper.; # look for block device that is just the name of the symlinked; # original file. if not found, echo empty string (no device found); BLOCK_DEVICE=$(ls -l ""$FILESYSTEM"" 2>/dev/null \; | cut -d'>' -f2 \; | xargs basename 2>/dev/null \; || echo); if [[ -z ""$BLOCK_DEVICE"" ]]; then; 1>&2 echo ""Unable to find block device for filesystem $FILESYSTEM.""; if [[ -d /sys/block/sdb ]] && ! grep -qE ""^/dev/sdb"" /etc/mtab; then; 1>&2 echo ""Guessing present but unused sdb is the correct block device.""; echo ""/sys/block/sdb""; else ; 1>&2 echo ""Disk IO will not be monitored.""; fi; fi; fi; }; ```. I am not sure if this is a google VM problem, a docker problem, or a problem with how cromwell specifies volumes to docker; but I took their response to be ""we don't care and won't fix it"". Fortunately for me the work-around nearly always works for cromwell jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322:621,echo,echo,621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322,7,['echo'],['echo']
Availability,"Not really. The problem is that then the command has to be written very; carefully so that errors in intermediate steps propagate...we already have; this problem, but this will make it worse. On Fri, Jan 27, 2017 at 4:54 PM, Chris Llanwarne <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Is this good enough as an interim?; >; > task foo {; > command {; > # do some stuff...; > # verify and write results to verification.txt; > }; > output {; > # normal outputs; > # Boolean verification = read_bool(verification.txt); > }; > }; >; > task fail {; > command {; > # something guaranteed to fail; > }; > }; >; > workflow bar {; > call foo; > if (!foo.verification) { call fail }; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275785501>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qC2URNFfJfnNoDOm6_RCQtKt4p_ks5rWmejgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429:91,error,errors,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429,1,['error'],['errors']
Availability,"Not sure how much UI we do within the cromwell code base. Agree with Jeff that perhaps this might be a better contribution to another UI team, but I'm not the ticket master. Also, I don't really have the ramp up to review HTML/JS for code style conventions / quality myself. Tried it out locally. As long as the jobs don't run too fast, the endpoint seems to work pretty well!. Btw, when _all_ calls run with start/stop _inside_ a particular second, the page renders with a cryptic red box that says:. > Cannot read property 'v' of undefined. Probably not a big deal unless we expect lots of folks running `ThreeStep.wdl` like I did. ``` javascript; // At least one row must span a second 00. // Ok; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.000-05:00""),; new Date(""2015-11-02T18:00:00.001-05:00""),; null , 100, null];. // or also ok; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.999-05:00""),; new Date(""2015-11-02T18:00:01.000-05:00""),; null , 100, null];. // But if all rows within a second, then error!. // Red box of death; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.001-05:00""),; new Date(""2015-11-02T18:00:00.999-05:00""),; null , 100, null];. // Also an error too; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.001-05:00""),; new Date(""2015-11-02T18:00:00.002-05:00""),; null , 100, null];; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153182822:1066,error,error,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153182822,2,['error'],['error']
Availability,"Not sure if this is a separate issue or not, but when @knoblett and I were submitting a workflow yesterday we got the exact same error message (submitted with Swagger). The issue for her was that there was an input that was specified to be a File type, but in reality it was just a String (so I'm guessing the issue was similar in that it couldn't find the ""file""). Unfortunately, it validated just fine, but we weren't able to submit it. . I'd be happy to provide the WDL and JSON files (both the broken version and the fixed version) but they won't attach in a github comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581:129,error,error,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581,1,['error'],['error']
Availability,"Not sure what I should be doing. I have tried the following command:; ```; gcloud logging read 'timestamp>=""2020-09-01T00:00:00Z""' > logs; ```; And then:; ```; $ cat logs | grep 30148356615-compute@developer.gserviceaccount.com -A10 | grep -i permission | cut -d: -f2 | sort | uniq -c; 14 lifesciences.operations.cancel; 425 lifesciences.workflows.run; 12 storage.buckets.get; 30629 storage.objects.create; 30985 storage.objects.delete; 12819 storage.objects.get; 157 storage.objects.getIamPolicy; 6859 storage.objects.list; ```; It does seem to be the case that `storage.objects.delete` is requested many times, so that is definitely an issue when you only have roles `storage.objectCreator` and `storage.objectViewer` but not `storage.objectAdmin`. I did not observe any permission from role `iam.serviceAccountUser` but that role is indeed needed. And I observe some requests for permission `storage.buckets.get` that do end in ERROR, but it does not seem to affect the pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970:931,ERROR,ERROR,931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970,1,['ERROR'],['ERROR']
Availability,"Not sure what was going on there but that seems to have been a transient problem. I created a branch in our repo with your changes and it failed the same way for me once, but I restarted it and got past the error. You can see the progress of the builds here: https://travis-ci.com/broadinstitute/cromwell/builds/113681945",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497167230:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497167230,1,['error'],['error']
Availability,"Note that the travis failure is a good thing (in that the change is working, not that it's an awesome circumstance), as it was due to not all of the centaur tests passing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225329843:21,failure,failure,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225329843,1,['failure'],['failure']
Availability,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:549,Error,Error,549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440,1,['Error'],['Error']
Availability,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:168,failure,failures,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346,3,['failure'],['failures']
Availability,"Note to those trying to reproduce-- the wdl is reported to run fine on the combination of:; 1. Local; 2. Small inputs. It should reproducibly fail on the combination of:; 1. Current JES / Firecloud (cromwell 0.18); 2. Normal (aka large) input files. The JES logs showed that it started localizing, and then four minutes later began delocalizing files. The STAR stderr printed that the program was starting, but there was no other error other message in the stderr nor jes log. AFAIK, there did not seem to be any other current indication via the Firecloud interface as to why the job was exiting prematurely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497:430,error,error,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497,1,['error'],['error']
Availability,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,16,['error'],['error']
Availability,Note: We currently can't test CWLs on AWS with v37 Cromwell because of the input/output staging issue I mentioned over with the call caching problems. We just get the input/output error and the jobs won't move forward enough to even see what other issues might arise. . Related to:; https://github.com/broadinstitute/cromwell/issues/4563. That's a deal breaker for our institution. @wleepang These are all related.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587#issuecomment-467545872:180,error,error,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587#issuecomment-467545872,1,['error'],['error']
Availability,"Now I am having trouble running the stack again (I've edited it to change the default of ScratchMountPoint to /cromwell_root). I deleted the old stacks. Getting a failure when trying to create the ec2 instance, but there is no helpful error as to why. . Is it possible to change the value of AWS_CROMWELL_LOCAL_DISK? Where do I change that? In the config file somewhere? If I could change that from /cromwell_root to /scratch then things ought to work with my existing AMI....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705:163,failure,failure,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705,2,"['error', 'failure']","['error', 'failure']"
Availability,"OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392:128,resilien,resilient,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392,1,['resilien'],['resilient']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:93,avail,available,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172,5,['avail'],['available']
Availability,"OK. Let's hard code a `version ""28.1""` in the formula even though it won't match the URL, since that will prevent the checksum error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289:127,error,error,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289,1,['error'],['error']
Availability,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2528,echo,echo,2528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,14,['echo'],['echo']
Availability,"OOI, what's the use-case for this? I'm not saying this code change is bad (as a code change it looks totally fine) but the description is flagging up warning signs in my head. The WDL spec claims that workflows are robust to calls being specified in **any order** since the DAG is 100% implied by inter-call dependencies rather than list-order. Eg this should be fine:. ```; workflow x {; call b { input: i = a.i }; call a; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879:215,robust,robust,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879,1,['robust'],['robust']
Availability,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236,5,['failure'],"['failure', 'failures']"
Availability,"Oh go on, I'll ping @geoffjentry too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997:15,ping,ping,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997,1,['ping'],['ping']
Availability,"Oh nm, it looks like #751 is the Recover ticket. Is this the preemptibility ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096:33,Recover,Recover,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096,1,['Recover'],['Recover']
Availability,"Oh right, I forgot about this comment... sorry :sweat_smile: The issue turned out to be with the call-caching strategy we were using. Because there were a lot of files being created, cromwell needed to do a large amount of hashing, which used up all of the available CPUs eventually leading to the timeouts. We changed the call-caching strategy and are no now longer running into this error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713:257,avail,available,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713,2,"['avail', 'error']","['available', 'error']"
Availability,Oh yeah. IIRC the irritation I had here was that it'd not give me all the errors at once so i'd think i was done only to get hit by yet another block :). I mentioned the unused vars in slack. There are some *definite* unused ones in there. Most of them were due to our (over?)reliance on overloaded function signatures where we may or may not use all of the args. Some of them were due to our pattern (particularly in the backend) of supplying a default no-op implementation of things. . There were a handful though that were really bizarre. As in I couldn't figure out why the compiler thought it was unused.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598:74,error,errors,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598,1,['error'],['errors']
Availability,"Oh, apart from the old failure/timestamp that doesn't seem to be used anywhere any more",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011,1,['failure'],['failure']
Availability,"Oh, looks good then. I've restarted the Travis build but assuming the failures were just temporary, :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659:70,failure,failures,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659,1,['failure'],['failures']
Availability,"Ok - I have made it work via Cromwell using a specific version of deFuse / gmap / the reference dataset.; The reference dataset is built after downloading deFuse and gmap and I think it uses those tools to build the reference indexes. Here is what I've found:. ## Works locally and on Cromwell:; - deFuse 0.6.2; - gmap 2014-08-20; - Reference set ensembl v 75 (built using deFuse and gmap of the above versions). ## Works locally, fails on Cromwell:; - deFuse 0.8.1; - gmap 2018-07-04; - Reference set ensembl v 69 (built using deFuse and gmap of the above versions). ## Works locally, fails on Cromwell:; - deFuse 0.6.2; - gmap 2018-07-04; - Reference set ensembl v 69 (built using deFuse 0.8.1 and gmap 2018-07-04). ## Fails locally and on Cromwell:; - deFuse 0.6.2; - gmap 2018-07-04; - Reference set ensembl v 75 (built using deFuse 0.6.2 and gmap 2014-08-20). It seems like only when using an old version of deFuse, old gmap, and a newer reference set will deFuse run successfully via Cromwell. I am still in the dark about why some of these combinations succeed locally and don't work on Cromwell, but it looks like the heart of the problem is in newer versions of gmap.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-447169423:143,down,downloading,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-447169423,1,['down'],['downloading']
Availability,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:150,echo,echo,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191,1,['echo'],['echo']
Availability,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:87,error,errors,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140,5,"['ERROR', 'Failure', 'echo', 'error']","['ERROR', 'Failure', 'echo', 'errors']"
Availability,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,3,['error'],['error']
Availability,"Okay, I've make a very minimal test case that fails on AWS: https://github.com/TMiguelT/WdlFileNameTooLong. Hopefully it's faster than the centaur one so this ends up being useful. For me I get a failure in around 12 seconds, which is a good turnaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279#issuecomment-474714816:196,failure,failure,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279#issuecomment-474714816,1,['failure'],['failure']
Availability,"On my OS; ```; java -version; openjdk version ""11.0.8"" 2020-07-14; OpenJDK Runtime Environment (build 11.0.8+10-post-Debian-1deb10u1); OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Debian-1deb10u1, mixed mode, sharing); ```; Using conda-forge compiled java (default for conda package of cromwell); ```; java -version; openjdk version ""1.8.0_192""; OpenJDK Runtime Environment (Zulu 8.33.0.1-linux64) (build 1.8.0_192-b01); OpenJDK 64-Bit Server VM (Zulu 8.33.0.1-linux64) (build 25.192-b01, mixed mode); ```. Both give the same error. Maybe it is Oracle vs OpenJDK related errors?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711,2,['error'],"['error', 'errors']"
Availability,"On the servers where this will be used, . > Did you install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files?; > http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835:193,down,downloads,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835,2,['down'],"['download-', 'downloads']"
Availability,"Once I implemented a cache locally the workflow runs to Succeeded, but then the Centaur test fails because it's configured to expect workflowfailure. Judging by the fact that this test was part of [Tyburn](https://github.com/broadinstitute/tyburn/pull/27/files) which I believe lacked support for asserting failures, I don't think this is the correct expectation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892:307,failure,failures,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892,1,['failure'],['failures']
Availability,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:89,Error,Error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861,1,['Error'],['Error']
Availability,One quick thing that could be done to see if the NPE is related to other undesired behavior. In JesApiQueryManager.handleTerminated I put in some logic a while back to automatically retry any polling actor which failed under the belief that it was going to be some goofy google error. That'd also be picking up actors which died via NPE. Changing it we're either specially handling NPE or being specific about what *is* being handled (which should be part of #1914 anyways) would make this better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967,1,['error'],['error']
Availability,"One thing I'd like to see before trying stuff is to have a setup that can reliably reproduce the problem and allows us to at least sorta kinda measure how effective our stopgaps & solution attempts are. . For instance, is it possible that cranking the queue size causes something else to fall over? It'd be good to know that before diving in. (Probably not, but just trying to make the point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510:74,reliab,reliably,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510,1,['reliab'],['reliably']
Availability,One up. I have similar error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-517178996:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-517178996,1,['error'],['error']
Availability,Ooooh none of us could remember the exact reason but I think you’re right. And iirc there was some performance problem which was eventually tracked down to be something silly we hadn’t accounted for. This makes sense,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418:148,down,down,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418,1,['down'],['down']
Availability,Oops. Raised in error. This actually does work,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392,1,['error'],['error']
Availability,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:380,alive,alive,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,1,['alive'],['alive']
Availability,"Out of curiosity, did you observe a behavior that makes you think that tasks are not being run as early as they could be ?; Cromwell periodically traverses the ""unstarted"" nodes to determine if all their dependencies are satisfied, and if so starts the task, which should effectively run all tasks as soon as possible. *There is an exception to this for sub-workflows, tasks depending on a sub workflow output will only be run once the whole sub-workflow completes, even if this specific output is available before that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019:498,avail,available,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019,1,['avail'],['available']
Availability,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:187,toler,tolerant,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,3,"['error', 'toler']","['error', 'tolerant']"
Availability,"Parting thoughts:. * The detritus portion of this was not handled at all so conformance test 87 is being skipped for the time being. Conformance 87 runs a `find` in the execution directory which will not tolerate detritus and defeats our current detritus filtering hack.; * It could also turn out the prepopulated-`listing` fix for the IWDR is too narrow and real dynamicism may be required for other non-`listing` IWDR operations.; * Somewhat aligned with the above, the `listing` runs before inputs have been localized so it is pointed to a filesystem location relative to where the Cromwell server is currently running rather than the call's inputs directory. Input localization should possibly happen before we try to do anything related to IWDR listing determination, but that's not the order of operations now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3575#issuecomment-389168565:204,toler,tolerate,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575#issuecomment-389168565,1,['toler'],['tolerate']
Availability,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:702,echo,echo,702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238,2,['echo'],['echo']
Availability,"Per my investigation of #1740 I can confirm the validation is running synchronous to submission, which it should not be. But the API is returning an error for malformed input and not just timing out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341,1,['error'],['error']
Availability,Ping @grsterin for review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576,2,['Ping'],['Ping']
Availability,Ping me when it's ready for review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838,1,['Ping'],['Ping']
Availability,Ping! We still would like this. Is it on the roadmap?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477,1,['Ping'],['Ping']
Availability,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631,5,"['Ping', 'failure']","['Pinging', 'failures']"
Availability,Pinging @ansingh7115 @katevoss @abaumann for thoughts on a priority for this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921,1,['Ping'],['Pinging']
Availability,Pinging @chapmanb in case he has thoughts on what we could do here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460404199:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460404199,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne and @mcovarr for a hopefully quick review. Should fix `sbt test` when mysql is not setup.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/961#issuecomment-224062615:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/961#issuecomment-224062615,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne as I believe he has opinions on this :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366981017:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366981017,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne as he was the prometheus of `operations`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324401972:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324401972,1,['Ping'],['Pinging']
Availability,Pinging @danbills to ask whether the above sample output is sufficient for your use case or whether we need another ticket for a part II,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457355868:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457355868,1,['Ping'],['Pinging']
Availability,"Pinging @gbggrant on this one, when this is merged it's another tool in your all's arsenal for stability under load",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324,1,['Ping'],['Pinging']
Availability,Pinging @kcibul for priority,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052,1,['Ping'],['Pinging']
Availability,"Pinging @kshakir for an optional review since (1) you apparently ""recently edited these files"" and (2) faced the pain of this in UL so might have opinions on whether this counts as a fix",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325,1,['Ping'],['Pinging']
Availability,Pinging @kshakir for direction... can you think of any reason for us not to merge this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601,1,['Ping'],['Pinging']
Availability,Pinging @natechols and @kshakir in case they have thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663,1,['Ping'],['Pinging']
Availability,"Pinging @ruchim and @danbills, the current PO and TL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911,1,['Ping'],['Pinging']
Availability,Pinging for re-review since there were a few non-trivial fixups required,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025,1,['Ping'],['Pinging']
Availability,Pinging this ticket with another requester from the forums: http://gatkforums.broadinstitute.org/wdl/discussion/9936/is-there-a-way-to-tell-cromwell-to-ignore-the-runtime-section-of-the-tasks-in-a-wdl-script#latest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676,1,['Ping'],['Pinging']
Availability,"Please check this issue.; I think this issue is another problem . > Caused by: common.exception.AggregatedMessageException: Error(s):; > Could not evaluate expression: write_lines(array_of_files): Access Denied (Service: S3Client; Status; > Code: 403; Request ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:124,Error,Error,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,2,"['Error', 'error']","['Error', 'error']"
Availability,"Please file a new bug report in the format ""expected result"" / ""actual result"", taking into account that Cromwell has no control over whether Google servers return 504 errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632:168,error,errors,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632,1,['error'],['errors']
Availability,"Please ping @geoffjentry before picking up this ticket, he has some ideas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188:7,ping,ping,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188,1,['ping'],['ping']
Availability,Presumably to ensure we get the labels even when the workflow was invalid?. I think we can centaur that... I’m pretty sure we check metadata even on failure tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119021:149,failure,failure,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119021,1,['failure'],['failure']
Availability,"Pretty much yes, also like I said we use a publicly available jq image to parse the cwl.output.json on every task, which was meant to be a ""let's get CWL working on PAPIv2 "" but not a permanent solution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-415014146:52,avail,available,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-415014146,1,['avail'],['available']
Availability,Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1721,robust,robustPoll,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['robust'],['robustPoll']
Availability,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After they’ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:413,error,error,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,2,"['Error', 'error']","['Error', 'error']"
Availability,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:161,outage,outage,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215,1,['outage'],['outage']
Availability,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:31,avail,available,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949,2,"['Error', 'avail']","['Error', 'available']"
Availability,"Re @aednichols :. >It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄; >; > Behavior is undefined when groups are tied, whatever happens by default is fine. Are there any metrics we can add to look out in advance for disadvantaged users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722,1,['error'],['error']
Availability,"Re Kris's comment - sounds like a good idea. My only extra suggestion would be to make sure that the error reported to the user if the gcloud default auth isn't set up properly is as bulletproof as possible. Otherwise I can see some tedious ""why won't it work!"" head-scratching in the near future",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166588563:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166588563,1,['error'],['error']
Availability,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,redundant,redundant,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065,1,['redundant'],['redundant']
Availability,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:1025,down,down,1025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,1,['down'],['down']
Availability,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:389,failure,failure,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,"['error', 'failure']","['errors', 'failure']"
Availability,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:72,failure,failure,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126,3,"['error', 'failure']","['error', 'failure']"
Availability,"Recently for an operation task that updating labels for ~2500 workflows, we have to write a loop to end ~2500 PATCH /label requests to the Cromwell, which took more than 3 hrs. (In Cromwell IAM, this is even worse since a single token will expire in 60mins, so you have to also deal with the token refreshment) . We tried to use multi-threading to speed it up but ended up getting transient 500 errors when using a thread pool with a size of >=4 threads. . So having this batch feature will make a lot of things much easier!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631:395,error,errors,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631,1,['error'],['errors']
Availability,Red thumb required for two minor WOM changes (two files changed at the bottom of the PR - one gets a scaladoc and the other gets an error message improvement),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065,1,['error'],['error']
Availability,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Redundant,Redundant,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679,1,['Redundant'],['Redundant']
Availability,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685,4,['error'],"['error', 'errors']"
Availability,"Regarding Kris' comment, I'm game for removing features in general, including ""user"". Various optional tests will need some help though, as currently they use user credentials. They instead should switch to the service account. The application-default factory seemed to return a credential on our Travis, though I haven't yet investigated what that credential actually is. I'm also willing to see ""application-default"" be the GoogleCredentialFactory implicit default. Still, I understand if others vote for the config to be explicit, in which case I like the suggested name ""default"". I'll stand down on making changes for now while @mcovarr has the ticket. Will still look for docs on scopes though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338:596,down,down,596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338,1,['down'],['down']
Availability,"Regarding aborts, @Horneth made significant changes to aborts in Cromwell 30 (coming soon!) that should make it easier and more reliable to abort jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351:128,reliab,reliable,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351,1,['reliab'],['reliable']
Availability,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:269,echo,echo,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336,1,['echo'],['echo']
Availability,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943,1,['error'],['error']
Availability,"Reopening this issue. . For the following WDL:; ```wdl; task echo {. command {; echo ""Hello World!""; }. runtime {; docker: ""ubuntu:latest""; disks: ""local-disk 100 HDD, /test1 10 SSD""; }; }. workflow wf_echo {; call echo; }; ```. I get an error that the specification expects the form of `disks: ""local-disk, /test1""`. If we are going to change the syntax of the value of `disks` runtime attribute, then we should change the label as well. `host_mount_point` or `docker_volumes` would be more appropriate for the way Batch works. . But in the interest of test, I change the WDL to match expectations:; ```wdl; task echo {. command {; echo ""Hello World!""; }. runtime {; docker: ""ubuntu:latest""; disks: ""local-disk, /test1""; }; }. workflow wf_echo {; call echo; }; ```. And the following `volumes` & `mountPoints` were created in the AWS Batch JobDefinition:. ```json; ""volumes"": [; {; ""host"": {; ""sourcePath"": ""/cromwell_root/wf_echo.echo-None-1""; },; ""name"": ""local-disk""; },; {; ""host"": {; ""sourcePath"": ""/test1/wf_echo.echo-None-1""; },; ""name"": ""d-c919a18cd1e1254f560bb64acc581574""; }; ],; ""mountPoints"": [; {; ""containerPath"": ""/cromwell_root"",; ""sourceVolume"": ""local-disk""; },; {; ""containerPath"": ""/test1"",; ""sourceVolume"": ""d-c919a18cd1e1254f560bb64acc581574""; }; ]; ```. The `volumes[].host.sourcePath` should instead be `/container_host_root/wf_echo.echo-None-1/cromwell_root` and `/container_host_root/wf_echo.echo-None-1/test1`. The defined `mountPoints` are correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-403851615:61,echo,echo,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-403851615,11,"['echo', 'error']","['echo', 'echo-None-', 'error']"
Availability,"Reproduced the assembly failure in Travis on a different branch that was green last night. i.e., the problem isn't here, so I'm merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710,1,['failure'],['failure']
Availability,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,redundant,redundant,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604,1,['redundant'],['redundant']
Availability,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:63,down,down,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441,1,['down'],['down']
Availability,"Results:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14154,error,errors,14154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['errors']
Availability,"Right now we are trying to display these failures to users in firecloud because they are currently suppressed and you can only see them if you inspect the json. I'd prefer our UI to not show raw JSON, so we want to format the responses. For now, we could format the known cases and show raw JSON otherwise and hope there aren't other raw JSON formats other than this. This is definitely not a showstopper for us, but not just a nice to have, I'd say it's a P2 on a scale from P1 to P3 with P0 for showstoppers. . We could also flatten ourselves, but there's the other case where it says ""timestamp"" and the singular ""failure"", so it's not always consistent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176,2,['failure'],"['failure', 'failures']"
Availability,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:71,error,errors,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,4,"['down', 'error']","['download', 'errors']"
Availability,"Right, but I want to know what happened...not that a tool fails silently... On Tue, Jan 31, 2017 at 11:46 AM, Linlin Yan <notifications@github.com>; wrote:. > I see. My usual ""workaround"" for such fail (but continue) is like this:; >; > task foo {; > 	command {; > 		(echo foo; false) || (echo 1>&2 MSG; true); > 	}; > }; >; > workflow test {; > 	call foo; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0izLbBIY8rqgekOd7mNujSZ-DIRiks5rX2VXgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241:268,echo,echo,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241,2,['echo'],['echo']
Availability,"Right. When these issues come up they're part of a tension between people using docker and people using HPC on shared filesystems (who almost always are **not** using docker). . What we've done in the past has been to detect if the context is docker - if so, stick with something more permissive and if not lock it down. YMMV and all of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286:315,down,down,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286,1,['down'],['down']
Availability,"Running a slightly cleaned up version of your example:. ```; task my_task {; String a; String b = a + ""/"" + ""annotation.fa"". command {; echo ${b}; }; }. workflow my_workflow {; call my_task { input: a = ""my_path"" }; }; ```. I see this command go by in the logs:. ```; ... INFO - BackgroundConfigAsyncJobExecutionActor [...]: `echo my_path/annotation.fa`. ```. So this appears to be working as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555:136,echo,echo,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555,2,['echo'],['echo']
Availability,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:568,error,error,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247,6,"['error', 'failure']","['error', 'failures']"
Availability,"Running this on Cromwell 35 on PAPI v2 returns this error:; ```Task w.t:NA:1 failed. The job was stopped before the command finished. PAPI error code 5. Execution failed: selecting zone: no regions/zones match request```. AC: For both the PAPI v1/v2 backends, add more context to this error. Something along the lines of...; ```Unable to start job because the zones defined in the runtime parameter zones: ""$zones"" doesn't match zones/regions supported by GCE. Please resubmit the job with a list of supported zones/regions by consulting a list of options here: https://cloud.google.com/compute/docs/regions-zones/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804,3,['error'],['error']
Availability,"SBT fails with an error that looks very related; ```; should evaluate a sep expression containing a sub-call to prefix correctly *** FAILED *** (50 milliseconds). [info] EvaluatedValue(WomString(-i a -i b -i c),List()) was not equal to EvaluatedValue(WomString(a b c),List()) (ErrorOrAssertions.scala:11). [info] org.scalatest.exceptions.TestFailedException:; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981,2,"['Error', 'error']","['ErrorOrAssertions', 'error']"
Availability,"SBT failure is another `No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.` for those keeping track, the `SingleWorkflowRunnerActor` also failed previous to that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078,1,['failure'],['failure']
Availability,"STAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1671,ERROR,ERROR,1671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['ERROR'],['ERROR']
Availability,"Same error, maybe you will see something in this full output from start of server:. ```; [info] SHA-1: dceb5f4f7fdf8646b3aa9ed1257ceb7f1b35f3c7; [info] Packaging /Users/tdyar/workspace/cromwell/server/target/scala-2.12/cromwell-33-7a41a75-SNAP.jar ...; [info] Done packaging.; [info] Done packaging.; [info] Done packaging.; [success] Total time: 189 s, completed Jun 7, 2018 12:13:55 PM; US-M094110:cromwell tdyar$ nano ~/workspace/cromwell-31/my-cromwell.conf; US-M094110:cromwell tdyar$ java -Dconfig.file=/Users/tdyar/workspace/cromwell-31/my-cromwell_test_develop.conf -jar /Users/tdyar/workspace/cromwell/server/target/scala-2.12/cromwell-33-7a41a75-SNAP.jar server; Picked up _JAVA_OPTIONS: -Dswing.systemlaf=com.sun.javax.swing.plaf.metal.CrossPlatformLookAndFeel; 2018-06-07 12:16:03,575 INFO - Running with database db.url = jdbc:hsqldb:mem:3922af96-263f-4846-9018-fb0a4968d4ab;shutdown=false;hsqldb.tx=mvcc; 2018-06-07 12:16:09,027 INFO - Successfully acquired change log lock; 2018-06-07 12:16:10,243 INFO - Creating database history table with name: PUBLIC.DATABASECHANGELOG; 2018-06-07 12:16:10,246 INFO - Reading from PUBLIC.DATABASECHANGELOG; 2018-06-07 12:16:10,417 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2018-06-07 12:16:10,419 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2018-06-07 12:16:10,420 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2018-06-07 12:16:10,421 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2018-06-07 12:16:10,422 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 6ms; 2018-06-07 12:16:10,431 INFO - changelog.xml: changesets/db_sch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['error'],['error']
Availability,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,4,"['Error', 'down', 'error']","['Error', 'download', 'error']"
Availability,"Saw DSDEEPB-1549 was already filed. Minor time scaling issue for jenkins, then :+1: as far as I'm concerned. Btw, anyone have any idea what's up with travis-to-coveralls integration's SSL errors? We don't have coverage results anymore? :crying_cat_face:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086:188,error,errors,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086,1,['error'],['errors']
Availability,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496,1,['failure'],['failures']
Availability,"Scott's on vaca and hasn't chimed in on the ""I'm available"" thread - you might want to recalibrate your wheel :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/341#issuecomment-166621309:49,avail,available,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/341#issuecomment-166621309,1,['avail'],['available']
Availability,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025,4,['error'],['error']
Availability,"Seeing the same thing on grid engine with cromwell-37. grid engine job dispatches and completes just fine but cromwell throws the following:. ```; [2019-02-13 22:18:19,77] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: job id: 8550357; [2019-02-13 22:18:19,78] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Status change from - to Running; [2019-02-13 22:18:20,81] [warn] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:611,Error,Error,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Error'],['Error']
Availability,Seems like I am having a related issue: https://github.com/openwdl/wdl/issues/339. When using `write_json` with an `Array[Object]` it results in this error: . ```; Failed to process expression 'write_json(list)' (reason 1 of 1): ; Invalid parameter 'IdentifierLookup(list)'. Expected 'Object' but got 'Array[Object]'; ```. The spec says this would be allowed [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#arrayobject-serialization-using-write_json). Although from @aednichols comment seems like this is not currently working?. I was using WDL v1.0 and Cromwell v47.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4219#issuecomment-545564714:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4219#issuecomment-545564714,1,['error'],['error']
Availability,"Seems to be a transient (GCS/JES) issue as it was reproducible multiple times the day the error was first seen. Since then it has not been reproducible. Closing issue, will reopen if it pops up again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932#issuecomment-223980068:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932#issuecomment-223980068,1,['error'],['error']
Availability,"Server mode is the intended, first-class usage scenario for Cromwell. It's really [no harder to use](https://cromwell.readthedocs.io/en/stable/tutorials/ServerMode/) than run mode and enables way more features. Run mode is in maintenance and we do not anticipate making enhancements.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065:226,mainten,maintenance,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065,1,['mainten'],['maintenance']
Availability,Should I keep the error message to `JES` or should it say `Pipelies API`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751,1,['error'],['error']
Availability,"Side note for those **only** looking to put time limits on commands, and **not** looking for a workaround for cloud bugs-- here's a more-portable time limit option: `timeout`. `timeout` differs slightly in each distro. Example docs:; - https://busybox.net/downloads/BusyBox.html#timeout; - https://www.gnu.org/software/coreutils/manual/html_node/timeout-invocation.html; - https://manpages.debian.org/stretch/coreutils/timeout.1.en.html. The command `timeout` will most likely be unable to help for ""stuck"" cloud jobs. For example, if the command `echo hello world` actually exits but for some unknown reason the cloud VM sticks around forever, then `timeout 5s echo hello world` will likely have the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717:256,down,downloads,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717,3,"['down', 'echo']","['downloads', 'echo']"
Availability,Side note to this ticket: There's also the issue on how to handle catastrophic DB issues - e.g. DB goes down for a period of time. I've taken that up myself as a lower priority path which shall eventually turn into an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619:104,down,down,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619,1,['down'],['down']
Availability,"Side note:. The reason you seem to be getting a DNS error `Name or service not known` is that when we take an import and try it against all of the available resolvers, we're using a function called `firstSuccess` that does exactly what you'd expect. If all the resolvers fail, the error you get is from the last resolver that we tried, not the one it ""should have used"" (since we have no idea which one that would be ahead of time).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457245751:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457245751,3,"['avail', 'error']","['available', 'error']"
Availability,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:32,error,errors,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,1,['error'],['errors']
Availability,"Since I always build `womtool` myself instead of downloading, I did notice this problem but assumed there was some magic that made the version set correctly for release builds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044:49,down,downloading,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044,1,['down'],['downloading']
Availability,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:606,down,downstream,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,2,['down'],['downstream']
Availability,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:61,down,down,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054,2,['down'],['down']
Availability,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Since this post was about the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) being quite broken, here a few points:. 1) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; #Create a new service account called ""MyServiceAccount"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:638,error,errors,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:281,error,error,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623,1,['error'],['error']
Availability,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,redundant,redundant,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['redundant'],['redundant']
Availability,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:663,error,error,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692,1,['error'],['error']
Availability,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:76,ERROR,ERROR,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885,1,['ERROR'],['ERROR']
Availability,So I'm getting much different behavior than the ticket describes... basically this WDL file is hanging indefinitely. The first task finishes on both JES and local backends. The second behaves in very strange ways. It loops infinitely on JES and has a localization error on local. I suspect it might be because of the problem with the WDL file itself (see previous comment),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/891#issuecomment-224936166:264,error,error,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/891#issuecomment-224936166,1,['error'],['error']
Availability,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:548,error,error,548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895,2,['error'],['error']
Availability,"So apparently with 200 we should be returning (at least) an Allow header with a list of available HTTP verbs per endpoint. Looking around the internet (google, reddit, github, bbc), a 405 Method Not Allowed seems to be the standard ""I can't be bothered to implement content for this HTTP verb that's never used"" response. But, on the other hand, if this is just there to Make Swagger Work and it needs to be 200, then :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/191#issuecomment-142074975:88,avail,available,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/191#issuecomment-142074975,1,['avail'],['available']
Availability,"So here is a final update. I have tried running Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:620,error,error,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,3,['error'],['error']
Availability,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,2,"['error', 'failure']","['error', 'failures']"
Availability,"So the real reason of the failure I'm afraid is the JES one, which I don't think we can do much for except tell them about it ?; The Cromwell NPE is still a bug but I believe is an artifact of the job failing combined with the fact that the DB is swamped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203:26,failure,failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203,1,['failure'],['failure']
Availability,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067,1,['error'],['error']
Availability,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:67,down,download,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724,4,"['down', 'error']","['download', 'error']"
Availability,"So, setting `concurrent-job-limit` to 8 did resolve my issue (and I hit another, unrelated issue instead). However, I don't believe this is the ideal way to resolve this. I (and I assume most other users) want maximum concurrency with my jobs, we just want to avoid this error. If we caught this `Too Many Requests` error, and just waited for a few seconds before retrying these requests, it would surely resolve this issue in a cleaner way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323:271,error,error,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323,2,['error'],['error']
Availability,"Some genuine test failures and a lot of ToLs. Once you sort that all out, 👍 from me. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1836/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258,1,['failure'],['failures']
Availability,Some of the test failures seem very genuine... 😨,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342,1,['failure'],['failures']
Availability,"Some sloppy experimental procedure is to blame for incorrect conclusions in the previous (deleted) comment. It's enough for the input and task name to be the same:; ```; workflow x {; call cram; call y { input:; cram = cram.scram; }; }. task cram {; command {; echo "".""; }; output {; String scram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```; ```; ERROR: Bad target for member access 'cram.scram': 'cram' was a String (line 4, col 21):. cram = cram.scram; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826:261,echo,echo,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Sorry @aofarrel we need to look into this further, but on a brief first glance I'd bet this is an oversight in the miniwdl type checker, elaborated in the [linked issue](https://github.com/chanzuckerberg/miniwdl/issues/596). thanks @pshapiro4broad for the slack ping",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919:262,ping,ping,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919,1,['ping'],['ping']
Availability,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:771,failure,failures,771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495,2,['failure'],['failures']
Availability,"Sorry for the noise. I've succeeded in creating an AMI where the scratch partition is called /cromwell_root. I have submitted a job which failed with the error under discussion before, and I'm waiting to see what happens....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468834266:154,error,error,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468834266,1,['error'],['error']
Availability,"Sorry if this is a dumb question, but on a `SharedFilesystemBackend` that's using GCS inputs for a call, is it possible that `out` could be correctly interpreted as a ""GCS relative path""? i.e. wouldn't that have to be a local file? Per the docs and my understanding of this code, this only localizes down GCS inputs and doesn't delocalize outputs, and it didn't seem there's the concept of a GCS scratch bucket that could serve as the basis of a relative path. +1 to removing the possibility that JesBackend workflows could get their hands on a shared file system IO interface. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568:300,down,down,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568,1,['down'],['down']
Availability,"Sorry, I put the `select_first` in the wrong place. This works:; ```; String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """"; ```. My `test.wdl`:; ```; version 1.0. workflow W {; input { File? tsv_file_input }. String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """". call T { input: s = tsv_arg }; output { String out = T.out }; }. task T {; input { String s }; command { echo ~{s} }; output { String out = read_string(stdout()) }; }; ```; I checked and this works with both `miniwdl run` and `java -jar cromwell.jar run`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821:458,echo,echo,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821,1,['echo'],['echo']
Availability,"Sorry, I use to search in previous issues but I didn't this time - my fault. Issue #3161 is the one that describes what I would like to have in cromwell. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417#issuecomment-373403073:70,fault,fault,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417#issuecomment-373403073,1,['fault'],['fault']
Availability,"Sorry, but this seems very related and I can't see why this is the problem:. ```; task trim_PE {; input {; File f1; File f2; Int? cpu=1; # Int? disk_space_gb=ceil(2*size(f1, ""GB"")+2*size(f2, ""GB"")) + 20; Int? machine_mem_gb; Int? preemptible_attempts; String? adap; String b1 = basename(basename(basename(f1, "".fastq""), "".fq""), "".gz""); String b2 = basename(basename(basename(f2, "".fastq""), "".fq""), "".gz""); String p1 = ""W7_"" + b1 + "".fq.gz""; String p2 = ""W7_"" + b2 + "".fq.gz""; String u1 = ""W7_"" + b1 + ""_unpaired.fq.gz""; String u2 = ""W7_"" + b2 + ""_unpaired.fq.gz""; }; ```. This yields an error:; ![image](https://user-images.githubusercontent.com/16489606/92958456-931bba00-f46a-11ea-97cb-ccc709929ad8.png). Is this a similar problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382:587,error,error,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382,1,['error'],['error']
Availability,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:190,error,error,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938,1,['error'],['error']
Availability,"Specifically looking at the following: `gvcf_joint`, `prealign`, `rnaseq`, `somatic`, `svcall` from https://github.com/bcbio/test_bcbio_cwl. Note that there's a version of `somatic` with GS inputs available in the `gcp` subdir which might make testing smoother for that one. I've seen `prealign` work ok on PAPI2 but haven't had luck on anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-425209964:197,avail,available,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-425209964,1,['avail'],['available']
Availability,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:81,error,errors,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655,1,['error'],['errors']
Availability,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:97,echo,echo,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677,8,['echo'],['echo']
Availability,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:359,error,errors,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159,1,['error'],['errors']
Availability,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:758,echo,echo,758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304,8,"['echo', 'error']","['echo', 'error']"
Availability,Still getting this error today.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-856677347:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-856677347,1,['error'],['error']
Availability,"Strangely, this non-input workflow and non-AWS configuration, lead to the same error, so seems to be something about my build, not likely the AWS Batch specific part:. [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081156/my-cromwell.conf.txt); [myWorkflow.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081161/myWorkflow.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255,1,['error'],['error']
Availability,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,8,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:226,echo,echo,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Sure thing. Just ping me when you are back. Have a fun break. On Wed, Dec 21, 2016 at 3:28 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Let's touch base after the; > break (since I'm already on mine). I have a feeling this is one of those; > things where everyone present will say ""I agree that the current solution; > is right but I disagree with everyone else's vision"" :) So it'd probably; > take a while before getting it right.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268630429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk73KvoC7sKtWEqWT2w88t7Qh8h5Eks5rKYvRgaJpZM4LTPm1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416:17,ping,ping,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416,1,['ping'],['ping']
Availability,"Suspicion confirmed! There is a collector key for each scatter that is a part of the execution store in Cromwell--and it's being abandoned in the ""NotStarted"" state when there is a failure in the Scatter in ContinueWhilePossible mode. Because there is a ""NotStarted"" key in the store--the workflow remains in the ""Running"" state with a job that's not really an executable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092,1,['failure'],['failure']
Availability,"TOL and assuming the investigation requested above confirms it would be helpful:. The current blacklisting implementation asks ""did a read from this source bucket result in a 403?"" If the answer is no, Cromwell tries to copy from that source bucket at full blast. However if Cromwell's first attempts to read from that source bucket do in fact result in 403s, there can be a large number of failed copies before blacklisting kicks in. One possibility would be modifying the question to ""what is the status of this source bucket with respect to 403s?"" with valid responses of ""known good"", ""known bad"" and ""I don't know"". . For ""I don't know"" Cromwell could be more cautious in its handling of that source bucket and launch a single ""canary"" copy attempt. Based on the result of that canary attempt Cromwell could choose ""known good"" or ""known bad"" and blacklisting could proceed the same as it does today with fewer copy failures.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497:921,failure,failures,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497,1,['failure'],['failures']
Availability,"TOL: Is this intended to be a user-available script or just for jenkins to run? If the latter, is the cromwell repo the best place for it? And should we have ""don't use this yourself from the command line!!"" warnings on the script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790:35,avail,available,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790,1,['avail'],['available']
Availability,"Task.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I tried reading the code but I don't know scala so did not get very far:; https://github.com/broadinstitute/cromwell/blob/f1cce2cd2723b849c4d8f285510f30913ec188a0/filesystems/sra/src/main/scala/cromwell/filesystems/sra/SraPathBuilder.scala. The input I feed in the following:; ```; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ```. The whole json input looks like this:; ```; {; ""Mutect2.gatk_docker"": ""broadinstitute/gatk:4.1.4.1"",. ""Mutect2.intervals"": ""gs://gatk-best-practices/somatic-b37/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.baits.interval_list"",; ""Mutect2.scatter_count"": 50,; ""Mutect2.m2_extra_args"": ""--downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6"",; ""Mutect2.filter_funcotations"": ""True"",; ""Mutect2.funco_reference_version"": ""hg19"",; ""Mutect2.funco_data_sources_tar_gz"": ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"",; ""Mutect2.funco_transcript_selection_list"": ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"",. ""Mutect2.ref_fasta"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta"",; ""Mutect2.ref_dict"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.dict"",; ""Mutect2.ref_fai"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta.fai"",; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ""Mutect2.tumor_reads_index"": ""sra://SRR9247315/NWD751606NWD751606.b38.irc.v1.cram.crai"",. ""Mutect2.pon"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf"",; ""Mutect2.pon_idx"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:3418,down,downsampling-stride,3418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['down'],['downsampling-stride']
Availability,Test failure is unrelated to this PR. Will investigate that separately.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145959320:5,failure,failure,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145959320,1,['failure'],['failure']
Availability,"Test failures unrelated to this PR should be fixed, as of the merge I did just now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880:5,failure,failures,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880,1,['failure'],['failures']
Availability,"Tested with the app, confirmed that public files are able to be read and that we hit the expected TES error when trying to read other workspace blobs that are not private",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248:102,error,error,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248,1,['error'],['error']
Availability,"Thank you for that information, its helpful to gage the impact of this bug. I do think the error rate here is higher than we'd like, and so its worth addressing sooner.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424960611:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424960611,1,['error'],['error']
Availability,"Thank you for the quick reply, Adam!. I wish the error message was a little more helpful, but this was definitely the issue and I have it working now!! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904,1,['error'],['error']
Availability,Thank you for the thumbs! A friendly reminder that the related https://github.com/broadinstitute/firecloud-develop/pull/3194 needs similar thumbing to actually make this reference image available in Terra.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574,1,['avail'],['available']
Availability,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421,2,"['Error', 'error']","['Error', 'error']"
Availability,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:47,down,down,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553,1,['down'],['down']
Availability,Thanks @carbocation - based on what you're saying it sounds like those run creation requests are in fact succeeding but just taking longer than Cromwell's request timeout to respond. For whoever picks up this ticket: I believe that wiring through an option to increase [timeouts on the requests to Google](https://developers.google.com/api-client-library/java/google-api-java-client/errors) (and make the value configurable) is hopefully sufficient for fixing this error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019:383,error,errors,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019,2,['error'],"['error', 'errors']"
Availability,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:248,alive,alive,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,1,['alive'],['alive']
Availability,"Thanks @geoffjentry !. Yeah, I'd realised that I needed to pass an absolute path into the workflow I'm trying to run, which works for running on our local SLURM cluster, but definitely wouldn't be portable or the ""right"" way to create the workflow. It sounds like the Directory type would be the right way to do this in future, and as you say, passing a Directory down to a task to pick the needed files out. . In the meanwhile, do you know if Cromwell's CWL implementation supports the CWL Directory type? Switching to CWL might also make sense for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934:364,down,down,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934,1,['down'],['down']
Availability,"Thanks @illusional !. Here is the complete error and I can confirm there is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --en",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['error'],['error']
Availability,"Thanks @kshakir! . Mint team just noticed a similar issue for a few of our workflows, where the workflow status in Cromwell is ""running"" but the VM instance is no longer running. These specific workflows were ""stuck"" on the first task and did not start any subworkflows. When trying to abort the workflow, I get the following error: . ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 467b64cc-9aa4-4eaf-85ef-16ed4d540d4c because no workflow with that ID is in progress""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046:326,error,error,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046,2,['error'],['error']
Availability,"Thanks @mcovarr for your response. I realized that after my initial post and created a labels.json with the following contents:. ```; {; ""google_labels"": {; ""my-private-network"": ""xxx"",; ""my-private-subnetwork"": ""yyy""; }; }; ```. where xxx and yyy are my actual vpc network and subnet names in GCP. Then I added the ""-l labels.json"" option to the cromwell run command but that still gives me the same error. Am I missing something here? Apologies but this is what I am understanding from the posts/docs that needs to happen but won't work when I try it. Am I supposed to create some label in the actual GCP account as well?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870:401,error,error,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870,1,['error'],['error']
Availability,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:215,failure,failure,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000,3,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:346,error,error,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,2,"['Down', 'error']","['Downloads', 'error']"
Availability,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002,4,['error'],['error']
Availability,"Thanks @pshapiro4broad, sorry for detached example filename. I think I see more now what's happening now, but I'm less convinced it's the right behaviour. I believe the outputs section is using whatever is provided in the input, where I would expect it to be using the localised path. You're correct, if you use the full path in the inputs json (eg: `/path/to/inputs.json`), you get that full path in the outname. I'll illustrate it a little bit differently, note that I've annotated a container in the basenametest task of the following workflow:. **outputsnametest.wdl**; ```wdl ; version development . workflow basenameconnectiontest {; call createFile {}; call basenametest {; input:; inp=createFile.out; }. output {; String outname = basenametest.outname; String outbasename = basenametest.outbasename; String out = basenametest.out; }; }. task createFile {; input {}. command <<<; echo ""Hello, Broad!""; >>>. output {; File out = stdout(); }; }. task basenametest {; input {; File inp; }. command <<<; echo '~{inp}'; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }; ```. This time running with no inputs:. ```bash; java -jar cromwell-52 run outputsnametest.wdl; ```. ```json; {; ""outputs"": {; ""basenameconnectiontest.outbasename"": ""stdout"",; ""basenameconnectiontest.out"": ""/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-basenametest/inputs/-43085659/stdout\nstdout"",; ""basenameconnectiontest.outname"": ""/Users/franklinmichael/Desktop/tmp/wdltests/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-createFile/execution/stdout""; },; ""id"": ""51e0cb4d-f706-4540-9303-ed9c70a8764e""; }; ```. I would expect that the outname be the localised pathname (in the container), in this case that would be `/cromwell-executions/..../`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329:887,echo,echo,887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329,3,['echo'],['echo']
Availability,"Thanks @wleepang for the response. I was using the custom AMI as specified in the link. But I also had a LaunchTemplate that will mount EFS to batch computes. The problem was my userData in the LT had this line ""yum update"" that updated everything including the ECS-agent to the latest version and then it failed with the specified error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953:332,error,error,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953,1,['error'],['error']
Availability,Thanks Denis - this was my fault :confounded:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/414#issuecomment-178252223:27,fault,fault,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/414#issuecomment-178252223,1,['fault'],['fault']
Availability,Thanks again for looking into the null hash issue. I unfortunately don't have a small reproducible case but have a larger case that seems to always hit this problem if the traceback isn't enough information to debug. It's the `somatic-giab-mix` CWL validation set from here:. https://github.com/bcbio/bcbio_validation_workflows#somatic-genome-in-a-bottle-mixture. and the first errors start to occur ~2hr into the run. Sorry this isn't a minimal case but hope it's useful when you have an opportunity to look into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021:378,error,errors,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021,1,['error'],['errors']
Availability,"Thanks for getting back to me, Ruchi.; Truth be told, I only started seriously using Cromwell since 10 days ago, hence I’ve no idea about the rate before that. Sorry. . The error rate is rather low (whether acceptable, I’m not sure), considering I’m seeing this many failures out of ~ 4000 VM instances.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424946693:173,error,error,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424946693,2,"['error', 'failure']","['error', 'failures']"
Availability,Thanks for paying down the doc debt and creating the runtime attributes section! I would only request a couple of minor adjustments to the example I wrote then :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144363200:18,down,down,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144363200,1,['down'],['down']
Availability,"Thanks for reporting, @asmoe4. A few questions:; - Which Cromwell version are you using?; - Have you ever run this workflow successfully on a previous version of Cromwell?; - Does the workflow run on a backend other than AWS? You could try e.g. the [local backend](https://cromwell.readthedocs.io/en/stable/backends/Local/). ---. Developer notes:; - I've never seen the error message `error in opening zip file` before, a quick Google suggests it's coming straight out of `java.util.zip`; - The only recent zip change I can think of is one that Shouldn't Break Anything, https://github.com/broadinstitute/cromwell/pull/4399. If this is a regression (based on reported Cromwell version), that's where I'd start looking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608:370,error,error,370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608,2,['error'],['error']
Availability,"Thanks for reporting, @vruano - I've modified the title to reflect your discovery that this was caused by an empty scatter and that better logs from Cromwell would be nice when debugging. I think the yellow triangle in the UI would be a FireCloud change - perhaps you'd want to ask them to make a change too to make this even more visible? (the Cromwell team can only change what logs we produce and the error messages on failure, not how the UI interprets what we give them!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the clarification. I get same errors with `version development`.; ; > Since you specified version 1.0 it is not weird that it crashes on stuff that is not valid WDL 1.0. I find this a little surprising. Maybe it is a documentation issue. The 1.0 spec says object literal and then shows an example using map literal? . > Struct Assignment from Object Literal; > Structs can be assigned using an object literal. When Writing the object, all entries must conform or be coercible into the underlying type they are being assigned to; > ; > Person a = {""name"": ""John"",""age"": 30}. In any case I think I was confusing Object type with object literal. I took following to mean object notation was also being removed:. > Be careful when using Object. They are superceded by 'struct' in WDL 1.0 and are being removed outright in WDL 2.0. Can probably resolve this if this is intended usage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822,1,['error'],['errors']
Availability,Thanks for the comments. I pushed the README changes and Funnel is now being downloaded from the broad's fork. . It looks like both the Local and Tes Centaur build are failing now though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491:77,down,downloaded,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491,1,['down'],['downloaded']
Availability,Thanks for the heads up. We haven't gotten around to move our cluster to 51 yet. Pinging @DavyCats so he is also aware.; We will investigate the issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-638049145:81,Ping,Pinging,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-638049145,1,['Ping'],['Pinging']
Availability,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:101,failure,failure,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the ping, it had slipped my mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134:15,ping,ping,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134,1,['ping'],['ping']
Availability,Thanks for the quick replies. You're saying this happens for only a few shards in the same scatter ? If that's the case it would suggest this is some sort of transient failure of gsutil to authenticate properly but I'm not sure why that would result in this error message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112:168,failure,failure,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the report. The ""error"" is reproducible and should be cleaned up, along with the docs. Your original issue would be better discussed over in the [WDL forums](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team). If the hint below doesn't help, please make a forum post and drop a link here so that others may follow it back. I can post screen shots in the forum plus other users can follow what's going on. 1. To find the button you're probably looking for click on the first ""POST"" and then ""Try it out"". Again happy to post screenshots over in the forum.; 2. The ""error"" seen is a swagger-online error, not a swagger-ui error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403295623:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403295623,4,['error'],['error']
Availability,Thanks for the response @danbills - this is on a compute node which only has access to the outside network through an http(s)/ftp proxy so it can't do name resolution for outside addresses. Why does cromwell try to resolve that particular address? We don't have docker available and I tried with a config file that only defines a local and slurm backend and still get the same exceptions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738:269,avail,available,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738,1,['avail'],['available']
Availability,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:95,avail,available,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313,2,['avail'],['available']
Availability,"Thanks for this Ales,. I wonder if it's a slurm thing?; https://slurm.schedmd.com/resource_limits.html. Some key things to look at from here:; https://slurm.schedmd.com/slurm.conf.html. MaxJobCount - Number of jobs in the active database - this includes recently finished jobs, new jobs won't go into pending.; MaxSubmitJobs - Can be set for a per user too. Could Cromwell be continuously trying to submit jobs and waiting until a new position is available?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986:447,avail,available,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986,1,['avail'],['available']
Availability,"Thanks for your reply. I set the sql_mode ```SET GLOBAL sql_mode = 'ANSI_QUOTES';``` , than a new error occur. But when I change ``` driver = ""slick.driver.MySQLDriver$"" ``` to ```profile = ""slick.jdbc.MySQLProfile$""``` in cromwell.conf, all going well. ```; database {; # driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false""; user = ""user""; password = ""123456""; connectionTimeout = 5000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522,1,['error'],['error']
Availability,"Thanks much for testing this out. I'm happy to help with whatever I can for supporting this. I haven't seen this previously and am kind of surprised that it hits memory issues. This is a tiny test dataset so I'm not sure why it hits a 4Gb limit. It shouldn't use much memory at all.The error comes from within pyflow, which is an internal workflow system manta uses for running:. https://github.com/Illumina/pyflow/blob/aac143d6b95ddfdc1dad7b2a7226b03a41379b58/pyflow/src/pyflow.py#L3660. I wish it told us the memory it thought the system had and what it wants so we'd have more idea of what is happening. I don't think Cromwell is doing anything wrong here and asking for more memory would be the first thing I'd try as well. Let me know if this doesn't fix and we can try to explore more. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435950771:286,error,error,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435950771,1,['error'],['error']
Availability,"Thanks much for the helping with debugging on this. . Beyond the hash failure from Cromwell the other errors I get are all from the workflow itself due to not preserving the original file names. The numerical hashes for files get passed directly into the downstream tools, stripping off any extensions or other identifying information. This results in tool confusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:70,failure,failure,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,3,"['down', 'error', 'failure']","['downstream', 'errors', 'failure']"
Availability,"Thanks so much for the pointer to the hashing and details about adjusting that. You're exactly right that was what was happening. I saw the hash error and saw no new jobs launched, thought it had failed, so must have stopped the tasks before they finished the md5 summing and continued. I ran our analysis both with md5 checksumming and path based hashing, and it does save some time. The md5 based one took 6:43 and path based was 5:55. Doing based on paths works great for us so I'll swap to that as the default in bcbio pipelines. For database storage, are their known deficiencies with file based HSQL over MySQL? Do I have ways to mitigate those through different settings? We're planning to use MySQL in some circumstances but for providing something generally for users the file-based database will be the default and I'd like to make that as good and error free as possible. Is leaving this open for the `null` hash issue worthwhile? I also got some other hash errors on a different run that didn't affect completion but might be influencing re-use and storage that I could raise an issue on, but might be due to file based storage. If there are general things I can tweak and test I can do that first prior to opening additional issues. Thank you again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743:145,error,error,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743,3,['error'],"['error', 'errors']"
Availability,"Thanks so much, unfortunately there aren't any other errors. The runner kicks off 3 more jobs (for 3 variant callers on run on this failed cache input). Those all complete and then the main process just stops. There are no more submissions to the cluster or anything beyond the main job waiting. Here is the remainder of the log if it's helpful:; ```; [2018-05-02 15:22:58,85] [info] 9fa3ab92-97fd-4bed-a636-6eaf38941141-SubWorkflowActor-SubWorkflow-variantcall:1:1 [[38;5;2m9fa3ab92[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b0777d55-4f75-47aa-9655-3119936b10a5-SubWorkflowActor-SubWorkflow-variantcall:2:1 [[38;5;2mb0777d55[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b4328660-38fb-4bd7-8220-cd2f47bb26b2-SubWorkflowActor-SubWorkflow-variantcall:0:1 [[38;5;2mb4328660[0m]: Starting get_parallel_regions; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,98] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:23:00,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'get_parallel_regions' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=batch-split' 'sentinel_outputs=region_block' 'sentinel_inputs=batch_rec:record'[0m; [2018-05-02 15:23:00,79] [info] Di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:53,error,errors,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['error'],['errors']
Availability,Thanks very much for the error report and fix! 😄 :+1: . [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2591/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326653496:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326653496,1,['error'],['error']
Availability,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:864,Error,Errors,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641,1,['Error'],['Errors']
Availability,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:32,failure,failures,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132,2,['failure'],"['failure', 'failures']"
Availability,"Thanks, @wleepang. I had specified the correct `user_data` but had neglected to switch from the default `/scratch`. As of today, this worked for me after downloading the python AMI creation script you noted. ```; python create-genomics-ami.py \; --user-data cromwell-genomics-ami.cloud-init.yaml \; --key-pair-name KEYNAME \; --scratch-mount-point /cromwell_root \; --profile default \; --ami-description ""AMI for use with Cromwell""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434894610:154,down,downloading,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434894610,1,['down'],['downloading']
Availability,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:126,down,download,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699,1,['down'],['download']
Availability,"Thanks. That seems kind of googly, not sure if it maps to AWS concepts. Would I put a region in there (like us-west-2) or an availability zone, such as us-west-2a? I thought that AZ's were governed by the VPC used by the compute environment and thus could not be influenced by anything in aws.conf, a WDL, or workflow options....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493266406:125,avail,availability,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493266406,1,['avail'],['availability']
Availability,"Thanks. What's strange is that many other analyses _were_ being run successfully at the time. If the PAPI retry mechanism is somewhat deficient, maybe it makes sense for Cromwell to mask that by resubmitting the analysis? E.g. there could be a config setting for the number of times to retry analyses that fail for transient-looking reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779:182,mask,mask,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779,1,['mask'],['mask']
Availability,That certainly looks reasonable despite all the Travis redness. I restarted all the failures and assuming everything clears up :+1:. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2951/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333:84,failure,failures,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333,2,['failure'],['failures']
Availability,That error is a failure of jes to initialize a vm for you in the first place. There are no logs to write beyond what's in there. I just took a quick glance but it looks like you're specifying a file which doesn't exist,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,2,"['error', 'failure']","['error', 'failure']"
Availability,That is correct - this already uses the metadata endpoint to get its data. The fancification can indeed happen anywhere - you can download the HTML and put a metadata file next to it and it works just the same,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153127594:130,down,download,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153127594,1,['down'],['download']
Availability,"That is surprising, the order should be preserved. Is the order you're seeing consistently the same or does it randomly change if you run it several times ?; If you have a small set of inputs that reproduces the error that would be helpful too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368074313:212,error,error,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368074313,1,['error'],['error']
Availability,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:795,avail,available,795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855,1,['avail'],['available']
Availability,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:166,echo,echo,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"That's a good point, plugging an offset into [Khalid's playground](https://scastie.scala-lang.org/79qIiIkIRReJLtbyvWQ2cg) makes it explode; ```; val timeStr2 = ""2020-01-15T17:46:25.694148-02:00""; Try(LocalDateTime.parse(timeStr2)); ```; ```; Failure(java.time.format.DateTimeParseException:; Text '2020-01-15T17:46:25.694148-02:00' could not be parsed, unparsed text found at index 26):; scala.util.Try[java.time.LocalDateTime]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992:242,Failure,Failure,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992,1,['Failure'],['Failure']
Availability,That's probably going to bork a lot more stuff than just our tests. I can see why that change was put in but I don't think we can just blindly accept that behavior w/o running it past @kcibul - it's likely to cause a lot of commotion downstream.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201:234,down,downstream,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201,1,['down'],['downstream']
Availability,"That's true, from reading. >outside the command block, it has no affect. it does seem that it's an inadvertent no-op rather than an error. Good take!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945,1,['error'],['error']
Availability,"The AWS cromwell-aio template for Cromwell 43 (latest) returned the following error after a workflow is submitted. fyi. This template works fine with Cromwell 42. 2019-07-02 19:16:36,824 cromwell-system-akka.dispatchers.api-dispatcher-73 INFO - Unspecified type (Unspecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credent",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:78,error,error,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['error'],['error']
Availability,The AWS dependencies needs to be pruned. The entire known universe of AWS libraries [were pulled in](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/project/Dependencies.scala#L230) during the work-in-progress towards a new backend for cromwell. The image below was produced using [GrandPerspective](http://grandperspectiv.sourceforge.net/) on the expanded cromwell 33 jar. It shows the amount of AWS libraries that have been assembled transitively by that one `aws-sdk-java` [blanket](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.0.0-preview-9) dependency. ![cromwell_expanded](https://user-images.githubusercontent.com/791985/43986825-3ccde314-9ce5-11e8-8260-c9bbb66d3623.png). The dependencies should be slimmed down to only the required libs.; We don't need to have AWS [Route53](https://aws.amazon.com/route53/) etc. zipped in the jar to run workflows on the upcoming backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086:784,down,down,784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086,1,['down'],['down']
Availability,The CI unit tests are failing with:; > /home/travis/build/broadinstitute/cromwell/engine/src/test/scala/cromwell/webservice/SwaggerServiceSpec.scala:103:36: constructor Constructor in class Constructor is deprecated. I get the same build error locally. The deprecated class comes from SnakeYAML. Looking into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310:238,error,error,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310,1,['error'],['error']
Availability,"The Centaur failures here are real, ~reverting to Draft~ closing until I sort this out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997:12,failure,failures,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997,1,['failure'],['failures']
Availability,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:241,ping,ping,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['ping'],['ping']
Availability,"The DOS schema changed to a pattern not queryable by JSON Paths-- specifically returning a nested array-of-objects where the array is unordered but only a certain object-value with a prefix should be returned... Found that someone had made the more flexible JQ filter syntax available, so switched to that library. I've still seen no evidence that the DOS schema won't change again, but hopefully the JQ filters will allow flexibly plucking out the right field as the schema hardens. The other alternative was submitting another PR if/when the schema changes again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432:275,avail,available,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432,1,['avail'],['available']
Availability,The EJEA is sending RecoverJobCommand or ExecuteJobCommand depending on if the workflow is restarting or not. So I think this is already done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623:20,Recover,RecoverJobCommand,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623,1,['Recover'],['RecoverJobCommand']
Availability,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814,1,['error'],['error']
Availability,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262,2,['error'],['error']
Availability,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480,2,['error'],['error']
Availability,"The Travis complaint appears to be a genuine failure, investigating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308:45,failure,failure,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308,1,['failure'],['failure']
Availability,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:319,ERROR,ERROR,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722,1,['ERROR'],['ERROR']
Availability,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:220,error,error,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093,8,"['alive', 'error']","['alive', 'error']"
Availability,"The `cwl_dynamic_initial_workdir` fails with below stack trace:; ```; 2019-03-13 12:29:04,388 cromwell-system-akka.dispatchers.backend-dispatcher-88 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(c9194073)main:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:581); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:149,ERROR,ERROR,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,The `sbt` tests are reliably failing; any idea why?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410352775:20,reliab,reliably,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410352775,1,['reliab'],['reliably']
Availability,"The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163:110,error,errors,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163,2,['error'],['errors']
Availability,"The `thread.sleep` command would need to be added to whichever actor(s) is actually submitting messages to the API. This doesn't strike me as too onerous for the developers, but you're right, it's definitely part of the scala and not the config files. Some minimal exception catching is also called for. Rather than throttling concurrent _jobs_ it probably makes more sense to limit the number and frequency of concurrent _workflow submissions_:; ```# Cromwell ""system"" settings; system {; ; # Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000 # No practical limit on the number of total workflows. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 4 # Too conservative?. # Number of seconds between workflow launches; new-workflow-poll-rate = 5 # Too conservative?; }; ```; This should stagger submissions without limiting the total amount of work being done. The number of threads available to the backend-dispatcher also appears to settable. You could create an artificial bottleneck there to protect AWS's API.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279:1011,avail,available,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279,1,['avail'],['available']
Availability,"The build failures are in unrelated Swagger tests, rebasing on develop should get this green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941,1,['failure'],['failures']
Availability,The case I inspected has error in only one of the 17 shards (of the same scatter). For example:. ```bash; $ for i in $(ls shard-*/*.log); do echo $i; grep Requester $i; done; BaseRecalibrator-0.log; BaseRecalibrator-10.log; BaseRecalibrator-11.log; BaseRecalibrator-12.log; BaseRecalibrator-13.log; BaseRecalibrator-14.log; BaseRecalibrator-15.log; BaseRecalibrator-16.log; BaseRecalibrator-1.log; BaseRecalibrator-2.log; BaseRecalibrator-3.log; BaseRecalibrator-4.log; BaseRecalibrator-5.log; BaseRecalibrator-6.log; BaseRecalibrator-7.log; BaseRecalibrator-8.log; BaseRecalibrator-9.log; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622,2,"['echo', 'error']","['echo', 'error']"
Availability,"The cause and effect:. - An assumption in the token dispenser actor (that none of the internal queues were ever empty) turned out to not always be true; - As a result, not infrequently an attempted `dequeue` would cause the `JobExecutionTokenDispenserActor` to crash and be restarted - zeroing out all known token dispensations *and* all known actors waiting for tokens.; - The overall consequence of this was that jobs would submit their request for exeution tokens and be added to a queue. But that queue was lost when the `JobExecutionTokenDispenserActor` was restarted and therefore the jobs would sit forever waiting for a token which was never sent to them. The fix:; - First, add a sanity check before calling `dequeue`. If the queue is empty, don't do it. But hopefully will now be a redundant check thanks to:; - Second, one situation was identified which would lead to this state when token-requesting-actors were aborting before a token was dispensed. If that left the token queue for that hog group empty then the queue was not being correctly removed from the `JobExecutionTokenDispenserActor` - thus leaving an empty queue behind and triggering the ""dequeue on an empty queue"" bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810:792,redundant,redundant,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810,1,['redundant'],['redundant']
Availability,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,redundant,redundant,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836,1,['redundant'],['redundant']
Availability,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:976,down,down,976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,2,['down'],['down']
Availability,"The code looks nice, but I'm not sure what to make of that build failure. Can Travis possibly be too slow to get a connection from an in-memory database in less than a second, or is something else happening here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/231#issuecomment-146670780:65,failure,failure,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/231#issuecomment-146670780,1,['failure'],['failure']
Availability,"The comment in the keel PR asserts that the hashes returned in `Docker-Content-Digest` and within the body are the same, but in my limited testing on Docker Hub, Quay and GCR that did not seem to be the case. If the body value actually represents something other than the image digest it may cause downstream issues for Cromwell to treat as such. . Basically this issue needs more investigation. Since ECR support is apparently not as a high a priority as originally thought this issue has been deprioritized for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234:298,down,downstream,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234,1,['down'],['downstream']
Availability,"The constructor signature needs to be exactly that since it's built reflectively, and like Miguel said it's better if it shuts itself down with the ShutdownCommand. Plus it's literally 5 lines so.. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376551184:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376551184,1,['down'],['down']
Availability,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532,1,['error'],['error']
Availability,"The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550:249,down,down-featured,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550,1,['down'],['down-featured']
Availability,"The downloadable jar on Github was intentionally kept the same, and that's the URL used in the Homebrew formula. This whole incident has illustrated the need for better release & version management going forward but that doesn't exist right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639:4,down,downloadable,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639,1,['down'],['downloadable']
Availability,"The error is different nowadays and apparently downstream of the fail. The underlying issue appears to be with not hydrating a `WomMaybePopulatedFile` output from a dependent job on restart: . ```; java.lang.UnsupportedOperationException: value is not available: WomMaybePopulatedFile(None,None,None,None,None,List()); 	at wom.values.WomMaybePopulatedFile.$anonfun$value$2(WomFile.scala:244); 	at scala.Option.getOrElse(Option.scala:121); 	at wom.values.WomMaybePopulatedFile.value(WomFile.scala:244); 	at cwl.internal.EcmaScriptEncoder.encodeFile(EcmaScriptEncoder.scala:102); 	at cwl.internal.EcmaScriptEncoder.encodeFileOrDirectory(EcmaScriptEncoder.scala:90); 	at cwl.internal.EcmaScriptEncoder.encode(EcmaScriptEncoder.scala:39); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$3(EcmaScriptUtil.scala:105); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScrip",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,3,"['avail', 'down', 'error']","['available', 'downstream', 'error']"
Availability,"The error occurs again, I read this [thread](https://gatkforums.broadinstitute.org/wdl/discussion/9436/error-running-cromwell-27-snap-build-from-master), and configure the local mysql database rather in-memory database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258,2,['error'],"['error', 'error-running-cromwell-']"
Availability,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173,2,"['error', 'failure']","['error', 'failures']"
Availability,The failure looks like a random TravisCI glitch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972,1,['failure'],['failure']
Availability,"The first class of failure is similar to the second, we write outputs and execution events before attempting to write status. The code is written to `setOutputs` and `setExecutionEvents` and does not deal well with preexisting data. Similar fixes as per the second failure class should apply here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215134213:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215134213,2,['failure'],['failure']
Availability,"The following WDL reproduces the problem with the JES backend, though the magic number is consistently 1899 files. Local backend does not have this problem. . ```; task make_files {; command <<<; mkdir files; for i in {1..5000}; do echo ""foo"" > files/$i.txt; done; >>>; output {; Array[File] files = glob(""files/*.txt""); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow piles_of_files {; call make_files; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/699#issuecomment-210234833:232,echo,echo,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/699#issuecomment-210234833,1,['echo'],['echo']
Availability,The following WDL runs to a certain point and then fails (when running the STAR aligner) without writing errors to stdout/stderr (which it does when running out of resources when run locally). Example input JSON also attached. [samtofastq_star_rsem.txt](https://github.com/broadinstitute/cromwell/files/215739/samtofastq_star_rsem.txt); [inputs.txt](https://github.com/broadinstitute/cromwell/files/215783/inputs.txt),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209056982:105,error,errors,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209056982,1,['error'],['errors']
Availability,"The following tests failed in the `centaurPAPIv2` run in our CI (this doesn't run for external contributors for credentials reasons). This build does have issues with transient failures at times, but these failures all appear to be label-related:. * jes_labels; * google_labels_bad; * google_labels_sub; * google_labels_good",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497256233:177,failure,failures,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497256233,2,['failure'],['failures']
Availability,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:383,failure,failure,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438,1,['failure'],['failure']
Availability,The hash failures are expected with http inputs and should not be the cause of your workflow failure. Also we don't currently support `http` in engine filesystems. Do you see any other error messages than might provide some insight into what's happening?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:301,down,down,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685,4,['down'],['down']
Availability,"The most frequent use case I can imagine would be to iterate over all items in an array inside a `for` loop of some kind. However, I was under the impression that `for` loops weren't being implemented in WDL?. I agree with @vdauwera's suggestion on naming, provided that finding the aggregate file size of an array of files is something we want to do. If not, then extra naming distinctions would be confusing. (I can see a user trying to use `size()` rather than `array_size()`, particularly if they have previously written in a language that uses `size()` to check the length of an array. Personally, I often write things as I think they might be, and if there isn't an error thrown (i.e. `function size() does not exist`) and the behavior is not as I expect (returning a file size rather than an array length), I will spend a while trying to troubleshoot the error somewhere else, as I assume `size()` worked the way I wanted it to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593:672,error,error,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593,2,['error'],['error']
Availability,The most recent PR build succeeded but GitHub apparently did not take note of that during its earlier downtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611:102,downtime,downtime,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611,1,['downtime'],['downtime']
Availability,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666,3,"['avail', 'error']","['available', 'error']"
Availability,"The next error is:; ```; 2019-01-31 19:38:58,499 INFO - changelog.xml: changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi: ChangeSet changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$Enhanc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,10,"['ERROR', 'Error', 'down', 'error', 'failure']","['ERROR', 'Error', 'down', 'error', 'failure', 'failures']"
Availability,The nucleus team is available again ...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224591221:20,avail,available,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224591221,1,['avail'],['available']
Availability,The official word from Google is that the OS installed on Pipelines API v1 workers has aged enough that driver support is no longer available. The best I can do on behalf of Cromwell is recommend switching to v2; if you have any further questions feel free to reopen!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959:132,avail,available,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959,1,['avail'],['available']
Availability,"The one Travis failure in ""push"" occurs in the ""fail slow"" test :frowning:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-199443309:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-199443309,1,['failure'],['failure']
Availability,"The only JES failure is `sizeenginefunction` which seems to be suffering from some file path chimerism. In my selfish desire to move forward with Cromwell unification, I'd be more than OK with disabling that temporarily and merging what's here. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334931335:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334931335,1,['failure'],['failure']
Availability,"The only theoretical downside is that it could take longer before we realize it's a cache miss and fall back to running the job. In practice though, this query which is suppose to make us ""fail faster"" is under performing so badly that it's effectively slowing us down. So it's a net positive in call caching time and resources spent. The alternative would be to replace it with something more performant, which we can always do later if the need arises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723:21,down,downside,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723,2,['down'],"['down', 'downside']"
Availability,"The output was very verbose, so I didn't find the actual compilation error. I think I fixed it now. We'll see if the tests pass this time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493629183:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493629183,1,['error'],['error']
Availability,"The path is declared as a File. I even tried running the same thing by removing `?` to make non optional. But got the same error. The task definition is as follow:. ```; task FilterByOrientationBias {; File ref_fasta; File ref_index; File ref_dict; File ref_alt; File? intervals; File oncefiltered_mutect2_vcf; File oncefiltered_mutect2_vcf_index; File? pre_adapter_matrix. String? java_options; String? gatk_options; String? mutect2_extra_filtering_args. String out_base = basename(oncefiltered_mutect2_vcf ,"".mutect2.oncefiltered.vcf.gz""); String output_vcf = out_base + "".mutect2.twicefiltered.vcf.gz""; String output_vcf_index = out_base + "".mutect2.twicefiltered.vcf.gz.tbi"". Array[String] artifact_modes. command {; gatk FilterByOrientationBias \; ${""--java-options "" + java_options} \; -V ${oncefiltered_mutect2_vcf} \; -O ${output_vcf} \; ${""-P "" + pre_adapter_matrix} \; -R ${ref_fasta} \; ${""-L "" + intervals} \; -AM ${sep=' -AM ' artifact_modes} \; ${gatk_options}; }. runtime {; }. output {; File twice_filtered_vcf = ""${output_vcf}""; File twice_filtered_vcf_index = ""${output_vcf_index}""; File twice_filtered_pre_adapter_matrix = ""${pre_adapter_matrix}""; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436279203:123,error,error,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436279203,1,['error'],['error']
Availability,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:620,ERROR,ERROR,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['ERROR'],['ERROR']
Availability,"The problem turns out to be broader than globs as joint genotyping pointed out. The way we handle large arrays, particularly large file arrays chews up a massive amount of ram- in the case of a single large workflow it can be impossible to run. In a multi tenant case it can be death by a thousand cuts. . This will absolutely need to be improved before we get too far down the caas journey",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593:369,down,down,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593,1,['down'],['down']
Availability,"The pros / cons sort of mirror each other.; One pro of changing it is what @ffinfo suggested where you don't have to wait for the whole subworkflow to start downstream tasks.; However if your subworkflow is a coherent unit in the sense that it only really is successful if all of its calls complete successfully it might not be the desired behavior.; For example in the WDLs above, if task `Cat` is an expensive operation and the sleep task ends up failing, you could potentially have wasted time running `Cat` unnecessarily.; Of course this can be mitigated by having `Cat` depend on `Sleep`, but it's some sort of ""fake"" dependency. To be fair this behavior already exists with scatters so it might not be that much of a deal, but I remember it was brought up at the time. I think people could be surprised either way. Another not-quite-similar-but-related example is streaming of files from one task to the other, which [CWL lets you specify explicitly](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandInputParameter) (see `streamable` field).; You could imagine a scenario like this (if WDL had a similar streamable concept):. ```wdl. task A {; command {; ./my_script_generating_data.sh > streamable_out; echo ""hello"" > out; }; output {; File streamable_out = ""streamable_out""; File out = ""out""; }; parameter_meta {; streamable_out: {; ""streamable"": true; }; }; }. task B {; File in; command {; cat in | my_script_reading_data.sh; }; }. workflow w {; call A; call B { input: in = A.streamable_out }; call B as B2 { input: in = A.out }; }; ```. Where A and B would actually run simultaneously but B2 would have to wait for A to complete.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499:157,down,downstream,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499,2,"['down', 'echo']","['downstream', 'echo']"
Availability,"The regular status polling is totally separate from this option. . Regular polling backs off, so doesn't have a single ""interval"" value to configure or report (ie it starts off polling with short intervals but slows down as the job runs for longer and longer). That's true for all jobs across all backends. The `is-alive` check is a fixed (configurable) duration, and it's the timeout between `is-alive` returning false, and the job being considered failed, that are the same. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707:216,down,down,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707,3,"['alive', 'down']","['alive', 'down']"
Availability,"The removed `-Xms2g` was saying ""Never run sbt, ever, without less than 2g of memory"". Meanwhile, Intellij has its own ""[Maximum Heap Size](https://www.jetbrains.com/help/idea/sbt.html#82b10b37)"" configuration value for the amount of memory required to import an sbt project. The IDE doesn't use this for running tests, so it does not need to be as large as the sbt options one uses for `sbt test` from the command line. The net effect of having the Intellij maximum less than 2g and the sbt opts _minimum_ at 2g caused a cryptic error of: . ```; Error while importing sbt project:. Error occurred during initialization of VM; Initial heap size set to a larger value than the maximum heap size; ```. This PR still leaves .sbtopts maximum amount of memory for running `sbt test` at 4g. It just no longer states that the JVM should start at 2g of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767:530,error,error,530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767,3,"['Error', 'error']","['Error', 'error']"
Availability,"The results above were obtained with Cromwell 29. It seems that the issue still exists in Cromwell 35:; ### Length(); ```; [2018-10-08 13:39:46,36] [error] WorkflowManagerActor Workflow 88434c0b-2595-4aee-b044-932eb0ba59f4 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function length. length takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))); ```; ### Indexing; ```; [2018-10-08 13:40:22,66] [error] WorkflowManagerActor Workflow 506ae394-a8b2-428c-a80a-532e0a158438 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'; ```; ### Zip(); ```; [2018-10-08 13:38:36,15] [error] WorkflowManagerActor Workflow 6d784fbe-2db0-4215-a03d-e2c40c95218a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-427939436:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-427939436,3,['error'],['error']
Availability,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962,1,['error'],['errors']
Availability,"The test failure looks like it might be real: `should parse all manner of well-formed auths *** FAILED *** (1 second, 53 milliseconds)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213,1,['failure'],['failure']
Availability,The test_read_data.wdl has lines commented out that resulted in errors I'm reporting here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467,1,['error'],['errors']
Availability,The travis failure looks unrelated but sort of scary... perhaps one for the spreadsheet...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3389#issuecomment-371888809:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3389#issuecomment-371888809,1,['failure'],['failure']
Availability,"The urgency of this particular fix came as we starting adding more valid CWL to PAPI and the tests were creeping past 70 minutes. I'm open to parallelizing local too. The workflow is reusable and there's currently nothing technically stopping us from switching local to parallel also. Cromwell's ""randomization"" (aka internally using unordered sets/hashmaps) when launching scatter jobs makes debugging the entire suite of tests wicked painful. It's hard to tell when a failure occurs what test was running. While CWL is in a state of flux I kind of like slowly working my way through the serial logs of local-conformance when I break something. My 2¢/ToL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011:470,failure,failure,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011,1,['failure'],['failure']
Availability,"The workflow itself seems to succeed, but Centaur thinks there is a failure due to #962.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224113013:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224113013,1,['failure'],['failure']
Availability,"Then; ```; 2019-01-31 20:10:51,323 INFO - changelog.xml: changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne: ChangeSet changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:259,ERROR,ERROR,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,6,"['ERROR', 'Error', 'down', 'failure']","['ERROR', 'Error', 'down', 'failures']"
Availability,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:325,alive,alive,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952,1,['alive'],['alive']
Availability,"Theory: something in the new SBT is meaning that some background thread/process is no longer shutting down the same way as before. This means that:; * (a) we get a bunch of resource leaks when `-Dsbt.classloader.close=false` is not set in the SBT options; * (b) when `-Dsbt.classloader.close=false` *is* set in sbt options, the thread remains running in the background preventing the tests from exiting on completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425:102,down,down,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425,1,['down'],['down']
Availability,"There are again some issues with Travis build (strange errors) , I'll reopen PR to trigger it again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752:55,error,errors,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752,1,['error'],['errors']
Availability,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:677,echo,echo,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,4,['echo'],['echo']
Availability,"There are some real-looking test failures that might be due to the removal of the execution status setting logic from the backend info update method. Probably the way to fix these is to change the test expectations, but some investigation seems warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209,1,['failure'],['failures']
Availability,"There are two failures in WorkflowManagerActorSpec, but I believe they are both due to the asynchronicity of setting status that should be fixed (inelegantly) by #224.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/228#issuecomment-146242763:14,failure,failures,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/228#issuecomment-146242763,1,['failure'],['failures']
Availability,"There are use cases where we'd want to be able to shut down a Cromwell instance without stopping running jobs. Notably, we want to redeploy without interrupting any long-running tasks (Cromwell can then pick back up where it left off when it restarts). I think we should make this optional.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173967513:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173967513,1,['down'],['down']
Availability,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,recover,recovery,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['recover'],['recovery']
Availability,"There is a second issue that came up after Google fixed the first issue that is oddly ironic given your comment. If they hit API quotas, it appears that they were immediately failing the operations, leading to some odd error messages passed through to Cromwell (and then to me). In a sense, they were failing too quickly. Tough balancing act, it seems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439,1,['error'],['error']
Availability,There was a [bug](https://github.com/broadinstitute/cromwell/issues/3102) in the centaurCwlConformancePAPI test script that would have caused a false failure for you. The fixes for this bug are on develop so when you rebase this should no longer be an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3101#issuecomment-355339696:150,failure,failure,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3101#issuecomment-355339696,1,['failure'],['failure']
Availability,"These changes appear to break submissions in service account mode. Removing the `GenomicsScopes.all` from `GoogleScopes` fixes the problem but (presumably) will break `application-default`:. ```; 2015-12-21 14:05:11,203 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:340,error,errors,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,2,['error'],['errors']
Availability,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:582,avail,available,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996,1,['avail'],['available']
Availability,They upped our QPS on our projects so we can't actually reproduce 429 errors in our projects anymore with any tests that we currently have(we were able to do it before with our 50 workflow test). You would have to write some wdl that scatters wide and send in a number of them at the same time to cromwell in some non-upped QPS project.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305,1,['error'],['errors']
Availability,"This PR is a continuation of PR https://github.com/broadinstitute/cromwell/pull/4938. The idea is to propagate line numbers to the WOM structures, so you could report an error to the user with correct source locations. . In dxWDL, I need this also to recover the original ordering of the source code. The WOM structure is a partially sorted graph. For example, in a program like: . ```wdl; workflow foo {; call A; call B; }; ```. you don't know what came first, `A` or `B`, because they are unordered.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722,2,"['error', 'recover']","['error', 'recover']"
Availability,This [PR](https://github.com/broadinstitute/cromwell/pull/3813) added _some_ retries around (de)localization. Definitely might not be enough but it's worth looking if it helped with those failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-401892591:188,failure,failures,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-401892591,1,['failure'],['failures']
Availability,"This appears to be the way it already works. ``` scala; private def preempted(errorCode: Int, errorMessage: Option[String], jobDescriptor: BackendCallJobDescriptor, logger: WorkflowLogger): Boolean = {; def isPreemptionCode(code: Int) = code == 13 || code == 14. try {; errorCode == 10 && errorMessage.isDefined && isPreemptionCode(extractErrorCodeFromErrorMessage(errorMessage.get)) && jobDescriptor.preemptible; } catch {; case _: NumberFormatException | _: StringIndexOutOfBoundsException =>; logger.warn(s""Unable to parse JES error code from error message: ${errorMessage.get}, assuming this was not a preempted VM.""); false; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007:78,error,errorCode,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007,8,['error'],"['error', 'errorCode', 'errorMessage']"
Availability,This certainly hasn't improved test reliability and I have more pressing things to look at just now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581:36,reliab,reliability,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581,1,['reliab'],['reliability']
Availability,"This failure was spotted again, reopening",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4223#issuecomment-430263735:5,failure,failure,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4223#issuecomment-430263735,1,['failure'],['failure']
Availability,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088,1,['error'],['errors']
Availability,This is a [Not Acceptable](http://stackoverflow.com/questions/14251851/what-is-406-not-acceptable-response-in-http) error. The content types that Green said it would accept in the response were not compatible with the type of response Cromwell produced. It may be that Cromwell experienced a real error (which was hopefully logged) and returned a text/plain error response which I don't think you'd accept.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028,3,['error'],['error']
Availability,"This is a regression-- we should have a centaur test case that goes through this scenario, so this failure is surprising and worth investigating.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-476677441:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-476677441,1,['failure'],['failure']
Availability,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:1228,down,down,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002,1,['down'],['down']
Availability,"This is an exceptionally annoying error, any more thoughts on how to potentially fix this? Should we go through the pipelines team?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094,1,['error'],['error']
Availability,This is an issue with the error reporting. I have not checked (and I do not remember how to replicate).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047,1,['error'],['error']
Availability,"This is being discussed with faces. First with Hussein and his PO to see; if this is a priority for FireCloud. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Oct 25, 2016 at 11:21 AM, Chris Llanwarne notifications@github.com; wrote:. > Pinging @kcibul https://github.com/kcibul for priority; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1wBXEQL8kkTuRTeebKXBoJ8xfOpks5q3h56gaJpZM4KfN-O; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203:336,Ping,Pinging,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203,1,['Ping'],['Pinging']
Availability,"This is causing a problem for me trying to write a workflow. I want to use [string interpolation](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#expression-placeholders-and-string-interpolation), which is a WDL 1.1 feature. It is not specified as being available in WDL 1.0 outside of commands, but Cromwell happens to support it in all strings in 1.0 (and maybe also draft-2?). If I put a `version 1.1` statement in my workflow, Cromwell won't parse it, because it knows it doesn't support 1.1. But if I put a `version 1.0` statement, my workflow isn't actually compliant with the spec, because 1.0 doesn't say that this feature is available. So I have a workflow that both Cromwell and any 1.1-compliant runner can run, except I can't write a correct version statement for it, or Cromwell will reject it. One solution might be to tell Cromwell that it does support version 1.1, at least partially, instead of rejecting all 1.1 workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575:268,avail,available,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575,2,['avail'],['available']
Availability,This is currently plan A for addressing the 10 TB database limit. Plan B would definitely have non-trivial downtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472141525:107,downtime,downtime,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472141525,1,['downtime'],['downtime']
Availability,This is failing travis but my guess is that that's not my fault?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679:58,fault,fault,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679,1,['fault'],['fault']
Availability,"This is great, thanks @delocalizer !. The downside of you doing those followups is that now I don't have the open PR staring me in the face reminding me I have to do something ;) I'll try to put in some content re Firecloud in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290:42,down,downside,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290,1,['down'],['downside']
Availability,"This is great, thanks for making the documentation awesome!; Only thing I could improve is to explicitly specify which base the memory_*b conversion uses, (I think both are available? MiB and MB). And to specify the rounding behaviour of 'Int memory_gb', probably ceil.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468100117:173,avail,available,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468100117,1,['avail'],['available']
Availability,"This is likely a bogus failure, I went ahead and restarted the job",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470680968:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470680968,1,['failure'],['failure']
Availability,"This is old and can be closed. @Thib already explained how to do examine; log files. On Wed, Aug 31, 2016 at 2:20 PM, Thib notifications@github.com wrote:. > Failed to delocalize files Looks like JES couldn't delocalize a file that; > was expected as an output.; > The task probably failed to produce that output hence the failure.; > I thought the logs were copied regardless in that case but apparently not..; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk-NAFrpk76r_5joMUbWTtGqkJvXAks5qlcXqgaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172:323,failure,failure,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172,1,['failure'],['failure']
Availability,"This is pretty cool. The only thing I'd throw in is that for the run() function this to me is like the one I pointed to @mcovarr the other day. I think it's fine as-is but I'd ask for another pass through to see if there are places where breaking it down a bit makes sense. If not, no biggie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153541769:250,down,down,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153541769,1,['down'],['down']
Availability,"This is pretty old and may have already been fixed. On Mar 9, 2017 09:48, ""Ruchi"" <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm unable to reproduce the; > error you saw. Have you seen the same issue with more recent versions of; > Cromwell? What version were you using when you saw the initial error?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk782vulMdWbl5LYOzgueOZhAz_3bks5rkBEtgaJpZM4JmxQ5>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887:192,error,error,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887,2,['error'],['error']
Availability,"This is ready for review, remaining test failures are being handled in other PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190,1,['failure'],['failures']
Availability,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:508,echo,echo,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021,2,['echo'],['echo']
Availability,"This is the error that I'm getting:. ```; 2019/10/03 19:00:01 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= hisplan/samtools@sha256:12d34a022f43e87e6c51f4be29705ccb70d2562a2046f3746f04cca88674a2e9 /bin/bash /cromwell_root/script; [main] unrecognized command '/bin/bash'; ```. Even though the container does have `/bin/bash`, this error occurs if the container image is built with `ENTRYPOINT`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152,2,['error'],['error']
Availability,This is what a (simulated) failure looks like for the two areas edited by this PR:; https://travis-ci.org/broadinstitute/cromwell/builds/206678315,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662:27,failure,failure,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662,1,['failure'],['failure']
Availability,This looks like a useful fix. Is there a development build of the JAR file available anywhere?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4437#issuecomment-456294327:75,avail,available,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4437#issuecomment-456294327,1,['avail'],['available']
Availability,This looks like the preemption error codes are different in PAPI v2. I don't think we expected this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732#issuecomment-395066256:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732#issuecomment-395066256,1,['error'],['error']
Availability,"This might be conflating two issues, but in case it is related, another error we are consistently seeing that seems dependent on which docker container we use (which may be a red herring, but that's all I can narrow it down to), we'll run something and get this error: ; ```; ""callCaching"": {; ""hashFailures"": [; {; ""message"": ""[Attempted 1 time(s)] - NoSuchFileException: s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz""; }; ]; }; ],; ```. Meanwhile, the input location of the input file is this:; ```; ""inputs"": {; ""input_fastq"": {; ""format"": null,; ""location"": ""s3://some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""size"": null,; ""secondaryFiles"": [],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; },; ```; So it's being given a valid S3 URL but then when it's trying to get the hash, it's looking at an invalid S3 URL (the one with s3.amazonaws.com isn't valid, but wasn't supplied by us). Thoughts? Is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066:72,error,error,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066,3,"['down', 'error']","['down', 'error']"
Availability,"This might be related to #1782 somehow, I believe I've seen these in some of the errorstorms which wind up w/ the symptoms described in #1782",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315:81,error,errorstorms,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315,1,['error'],['errorstorms']
Availability,This one is different from previous JES issues that were error 500. So this issue might be a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456,1,['error'],['error']
Availability,"This second class of failure seems straightforward: there's code in `WorkflowActor` to `setRuntimeAttributes` that makes no allowance for the attributes already having been written, but this scenario can definitely occur. For a master-like branch, the most expedient solution is probably to turn these inserts into upserts. For develop it would be nice to write these attributes in an event-sourcish way that for writes doesn't clobber existing values, but which always takes the latest values for reads. Still investigating the first class of failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215125595:21,failure,failure,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215125595,2,['failure'],['failure']
Availability,"This seems like a pretty recoverable error. It fails the zamboni workflow but not the cromwell workflow. So it can easily be overcome with a reconsider in zamboni. Ideally the Zamboni workflow would catch and be robust to these sorts of things, or at least log the response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574:25,recover,recoverable,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574,3,"['error', 'recover', 'robust']","['error', 'recoverable', 'robust']"
Availability,This seems to be fixing the symptom rather than the cause?. Is it not the file evaluators themselves which should be evaluating things that look like `glob` into `WomGlobFile`s immediately rather than ever calling down into the `IoFunction`s?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427133149:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427133149,1,['down'],['down']
Availability,"This seems to be interestingly connected with the spec change https://github.com/openwdl/wdl/pull/315 (so cc @patmagee). As that PR is currently written, we would be fine to do the scheme like this, because the `memory` section says ""you can provide as much memory as you want, as long as it's over this amount"", but it feels like we're in danger of writing non-portable WDLs like this because the incentive is to write a small value first and rely on the doubling to catch you if necessary. FWIW I'd rather go down the route of:; - `memory` is treated as the ""guaranteed to work"" ceiling amount; - We could start by having a much lower `memory_to_try_first` attribute representing the first value to try; - If the task fails, we can then double it from the low baseline, until either the task succeeds or we reach the `memory` ceiling, and at that point we don't try any further. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634:511,down,down,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634,1,['down'],['down']
Availability,This seems to have been broken by a rebase. Closed for repairs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3609#issuecomment-388126038:55,repair,repairs,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3609#issuecomment-388126038,1,['repair'],['repairs']
Availability,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:366,failure,failure,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089,2,['failure'],['failure']
Availability,This strawman was brave... but it wasn't good enough. Since this no longer represents our current thinking I'm going to take this strawman down.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379:139,down,down,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379,1,['down'],['down']
Availability,"This task is covered by the DSDE-Docs Epic ""[Update Cromwell Documentation](https://github.com/broadinstitute/dsde-docs/issues/1514)"", which includes moving most of the content to the WDL website and [slimming down the README](https://github.com/broadinstitute/dsde-docs/issues/1515). Closing this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330:210,down,down,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330,1,['down'],['down']
Availability,"This tight-looping of the `could not download return code file, retrying:` is typical of a problem that's been fixed in C24. ; If you see the 'never finishing' problem again in C24+, it's probably due to a different cause so please repost with new logs and metadata files (sorry!). Closing this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148:37,down,download,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148,1,['down'],['download']
Availability,This was also reported [http://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure#latest](here) by @MatthewMah,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731:89,error,error-caused-by-a-job-submission-failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731,1,['error'],['error-caused-by-a-job-submission-failure']
Availability,"This was deliberately ""not allowed"", to try to force people to pass through the inputs and outputs to their workflows (ie so that the interface to a workflow was stable even if extra tasks were added or removed from its internal workings). In other words to encourage:. ```wdl; version 1.0. workflow Test2 {; input {; String? passthrough_text; }. call Echo {; input: text = passthrough_text; }. output {; }; }; ```. This would allow you to swap out the internal call to `Echo` for something else, or rename the call, or add another call after it, or replace the entire workflow itself with a single task, etc, etc... and nobody who's calling `Test2` needs to worry about your internal refactorings. They just `call Test2` and supply the input and are done. Now having said that, I've pretty much changed my mind about this being something to enforce rather than just something to encourage, and would ideally like to go down the route of saying ""best practices are to pass through inputs and outputs but it's not enforced because quite often it's super-annoying""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870:352,Echo,Echo,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870,3,"['Echo', 'down']","['Echo', 'down']"
Availability,This works for me. The test failures are unrelated to this change. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/226#issuecomment-146205665:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/226#issuecomment-146205665,1,['failure'],['failures']
Availability,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:340,robust,robust,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584,1,['robust'],['robust']
Availability,"Those requests are probably a red herring, but I suggest reaching out to us (DSP AppSec) on Slack for those ;). Re the dedicated SA, there're a couple issues with your config:. 1) We typically don't recommend downloading a SA key to a GCP VM, since all GCP VMs normally have a SA associated with them (when you start them). Cromwell will just pick them up automatically ""from the environment"". So please don't download a SA key to it and instead use this as the recommended option, per [Cromwell docs](https://cromwell.readthedocs.io/en/latest/backends/Google/):; ```hcl; {; name = ""application-default""; scheme = ""application_default""; },; ```. I can provide more details from the config I used previously, if this doesn't work. 2) Which SA is `MY-GOOGLE-PROJECT-############.json` for? From your earlier `gcloud projects add-iam-policy-binding` command, it seems like that was for `MY-NUMBER-compute@developer.gserviceaccount.com`, which is the so-called ""Default Compute Service Account"" in your project. Using it is not recommended, since it has pretty wide permissions from the get-go. So I'd recommend creating a separate SA and granting it those roles instead, and then assigning that SA to the Cromwell VM before you start it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315:209,down,downloading,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315,2,['down'],"['download', 'downloading']"
Availability,"Those two failures are probably unrelated test flakiness (the curl exit 6 is a ""Couldn't resolve host"" error). I've restarted the two builds, hopefully the errors will go away.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504405803:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504405803,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"Thoughts for a Monday Tech Talk™️:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497,2,"['down', 'recover']","['down', 'recovers']"
Availability,Thx for the pointer to the outdated docs. Removed that section since that error should no longer occur in IntelliJ.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-446070591:74,error,error,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-446070591,1,['error'],['error']
Availability,"To add some more context, not having this ability makes it difficult for us to use the On Hold status for queuing in Mint without a lot of overhead. In more detail:. We have a new service called Falcon that periodically queries our workflow collection in CaaS and starts the oldest On Hold workflow. Right now we have no choice but to query for *all* on hold workflows in each cycle, which for scale testing and production will be thousands of workflows -- even though we just want the oldest one or a few of the oldest ones. We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way. It would be much more efficient if we could ask Cromwell to reverse the order and get just the first page of say the 10 oldest On Hold workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522:670,reliab,reliably,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522,1,['reliab'],['reliably']
Availability,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:90,failure,failure,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308,2,['failure'],['failure']
Availability,"To correct the test case once this is fixed:. Add this task:; ```; task defined_in_task {; File? f; Boolean is_defined = defined(f); command {; ${true=""cat"" false=""echo no file"" is_defined} ${f}; }; runtime {; docker: ""ubuntu:latest""; }; output {; String out = read_string(stdout()); }; }; ```; Call it from inside the scatter:; ```; scatter (p in masked_indices) {; ...; call defined_in_task { input: f = mk_file.f }; }; ```; Add the output:; ```; Array[String] dit_out = defined_in_task.out; ```; Add expectations",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311:164,echo,echo,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311,1,['echo'],['echo']
Availability,"To echo @cjllanwarne - my interpretation of #972 isn't ""call level only"" but the whole enchilada",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226783657:3,echo,echo,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226783657,1,['echo'],['echo']
Availability,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:184,avail,available,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430,1,['avail'],['available']
Availability,"To put some concrete numbers on this, I'm using `ab` to submit workflows using 32 threads. . Using `develop` I get a sustained ~450 requests per second for as long as my patience lasts (I've not gone beyond 200k submissions). CPU utilization remains steady throughout. Using my changes for small bursts I get ~2500 rps, if I run 200k submits I get ~1400 rps in aggregate. It starts going down pretty rapidly after a few minutes and not long after that is down to roughly the rate of develop (if that) - by 400k submissions it gets too slow for me to wait around. You can see the impact in the CPU utilization of both the JVM and MySQL as well. Using my changes and removing the metadata write I get a sustained 4200 requests per second up until at least 800k workflows. CPU utilization remains steady throughout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525:388,down,down,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525,2,['down'],['down']
Availability,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:884,echo,echo,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599,1,['echo'],['echo']
Availability,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:264,Error,Error,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,1,['Error'],['Error']
Availability,"ToL: I'm fine if these actors hang as children off the initialization actor or some other _actor_, but not as singletons-in-the-system-jvm-referenced-from-the-non-actor-blaf. Relatedly, the name blaf is a slight misnomer, at it actually creates props, not actor refs, so maybe it should be the blpf? Pinging #1377 to record some of this as food for later thought.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349:300,Ping,Pinging,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349,1,['Ping'],['Pinging']
Availability,"ToL:. It'd probably be best to slim down and refactor the old engine `cromwell.CromwellTestkitSpec` to a `cromwell.core.CromwellTestKitSpec` and `cromwell.engine.WorkflowTestKitSpec`. Also, the actor system created in the current `CromwellTestkitSpec` uses a custom configuration. As it doesn't fall back to `ConfigFactory.load()`, it doesn't seem to be support modifying [`akka.test.timefactor`](https://github.com/akka/akka/blob/v2.3.12/akka-testkit/src/main/scala/akka/testkit/TestKit.scala#L712-L714) on the sbt command line.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926:36,down,down,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926,1,['down'],['down']
Availability,"Travis passed but Github is hanging getting the status back, possibly due to earlier github downtime (overheard in Slack)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4730#issuecomment-472146747:92,downtime,downtime,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4730#issuecomment-472146747,1,['downtime'],['downtime']
Availability,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['error'],['error']
Availability,"Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CommandLineParser$.main(CommandLineParser.scala:8); 	at cromwell.CommandLineParser.main(CommandLineParser.scala); Caused by: java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:2613,avail,available,2613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['avail'],['available']
Availability,Trying to fix a CI error by merging with the main development branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-497016193:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-497016193,1,['error'],['error']
Availability,"Two errors on the testing:; ```; - should successfully run curl *** FAILED *** (1 minute, 37 seconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: bb88b541-3f1a-490c-9121-7685b4ab54b3). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Job curl_wf.newsgrab:NA:1 exited with return code 6 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ""message"" : ""Workflow failed""; }; ]; ```. ```; info] PubSubMetadataServiceActorSpec:; [info] A PubSubMetadataActor with a subscription should ; [info] - should create the requested subscription *** FAILED *** (17 milliseconds); [info] java.lang.AssertionError: received 1 excess messages on InfoFilter(None,Left(Creating subscription baz),true); [info] at akka.testkit.EventFilter.intercept(TestEventListener.scala:116); [info] at cromwell.services.metadata.impl.pubsub.PubSubMetadataServiceActorSpec.$anonfun$new$9(PubSubMetadataServiceActorSpec.scala:40); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); [info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); ```; I don't see how these are caused by this PR. I would gladly fix them if I would know how.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025,2,"['error', 'failure']","['errors', 'failures']"
Availability,"Two independent ideas. a) do less work in each heartbeat transaction (say just one workflow I’d) and then commit so you’re holding fewer locks. B) rather that updating a single table with the latest heartbeat, how about an append only table with the heartbeats. Then there are no locks. When looking for workflows; To start just join to that table and get max(heartbeat time). > On Aug 20, 2018, at 4:25 PM, Adam Nichols <notifications@github.com> wrote:; > ; > Good idea, it's always possible it could still deadlock on an index; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414458430:47,heartbeat,heartbeat,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414458430,4,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"Typically, I do that through the Cloud Logging Console, instead of fetching the entire log (which could be huge, and expensive) ;); There, you can set up filters to narrow down on particular log entries. `iam.serviceAccountUser` is mostly about granting one `iam.serviceAccounts.actAs` permission on a service account. Not sure why it doesn't show up here, but this permission is required for the Cromwell server to be able to run a pipeline with a Compute SA. BTW `iam.serviceAccountUser` **should** be granted on a per-service-account level, not at the project level (not sure if you've set it up this way, just wanted to confirm). First make sure you don't have that permission granted at the project level, and then if you remove it from the service-account level, it should be able to be seen in the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109:172,down,down,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109,1,['down'],['down']
Availability,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:48,error,errors,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310,1,['error'],['errors']
Availability,"UUID(dfefc8c0)]: Parsing workflow as CWL v1.0; 2018-12-10 13:27:22,372 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/main-prealign.cwl; 2018-12-10 13:31:56,222 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/organize_noalign.cwl; 2018-12-10 13:32:14,196 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples_to_rec.cwl; 2018-12-10 13:32:32,071 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples.cwl; 2018-12-10 13:32:49,793 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl; 2018-12-10 13:33:07,284 cromwell-system-akka.dispatchers.engine-dispatcher-34 ERROR - WorkflowManagerActor Workflow dfefc8c0-c3a1-449c-a747-13147bf8b980 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 948, in round_trip_load; File ""/app/cromwell-37-416c665-SNAP.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:1183,ERROR,ERROR,1183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939,1,['ERROR'],['ERROR']
Availability,"Uh, if this is causing test failures, then we have something very wrong in our tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212404457:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212404457,1,['failure'],['failures']
Availability,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:180,down,down,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572,1,['down'],['down']
Availability,Unit test failure was `WorkflowOutputsSpec` which is a known flake (should fix). Restarted.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722:10,failure,failure,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722,1,['failure'],['failure']
Availability,Up to 15 total failures,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-228104876:15,failure,failures,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-228104876,1,['failure'],['failures']
Availability,"Updated to be way more specific in targeting Denis-reported errors. I've found this area can get difficult to reason about when we throw in ranges and very broad regexes, so I chose to be incredibly specific at the cost of more code & slightly less functionality (but Denis's functionality is there).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999:60,error,errors,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999,1,['error'],['errors']
Availability,Updating to say that I ran across the other day and it was only a vague memory of this issue which led me to not go down a giant rabbit hole of WTF,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3073#issuecomment-516557267:116,down,down,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3073#issuecomment-516557267,1,['down'],['down']
Availability,"Upon further reflection I think I might want to work on this a bit more before merging. This addresses the problems of the list limit being too low and not configurable, but it doesn't yet address the problem that when the list limit is hit the code silently truncates. I'm not sure if GCS provides an API to say how many object have a given prefix, but I'd like to see if something like that is available. If the number of objects that would be returned by list() exceeds our limit, I think the code should fail noisily and not silently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439:396,avail,available,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439,1,['avail'],['available']
Availability,"Ups, looks like I've made type error, closing this",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986,1,['error'],['error']
Availability,"Useful tidbit to note, [this is how you signal failure due to unimplemented functionality](https://github.com/common-workflow-language/common-workflow-language/pull/278/files#diff-ee814a9c027fc9750beb075c283a973cR49) - which I believe means that one can still be ""green"" on CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542:47,failure,failure,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542,1,['failure'],['failure']
Availability,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:159,error,errors,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939,1,['error'],['errors']
Availability,"Waaaaay back when @yfarjoun asked for a ""dry run"" feature which was similar. We got lost in the weeds as I was involved. I remember at the time noting that this wouldn't be particularly useful since you only check things which were fully resolvable at submission time but that was based on what I now believe to be a faulty assumption, that typically only the entrypoint calls would have resolvable inputs at submission time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582:317,fault,faulty,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582,1,['fault'],['faulty']
Availability,"Was there a solution to this? I am also encountering this using the broad institute mutect2 implementation here:; https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl. When implemented using slurm/singularity on my institute's HPC. Specifically my error is below and I think has to do like people above have said to do the 50 scatter I am currently using. `[INFO] [06/04/2024 06:56:15.291] [cromwell-system-akka.dispatchers.engine-dispatcher-32] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow ccfe50af-5661-4bcd-b351-9da287c5affb failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'M2.tumor_pileups': Future timed out after [60 seconds]; Bad output 'M2.normal_pileups': Future timed out after [60 seconds]; Bad output 'M2.tumor_sample': Failed to read_string(""tumor_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'M2.normal_sample': Failed to read_string(""normal_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:990); 	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932,1,['error'],['error']
Availability,We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667:305,avail,available,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667,1,['avail'],['available']
Availability,"We are trying to be 100% heads down on cwl work for probably another couple of weeks. If this is a raging fire we can divert attention from that but that not without cost towards that and the ripple effect on other goals. So is this “this sucks please fix soon” or “OMG we’re blocked, at the risk of cheesing off other users this needs to be fixed right this second”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084:31,down,down,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084,1,['down'],['down']
Availability,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:174,down,down,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637,3,"['down', 'error']","['down', 'error']"
Availability,We do a lot of files of filenames and I think we'd get the same error in the case of an empty line. Right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642,1,['error'],['error']
Availability,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:84,avail,available,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414,1,['avail'],['available']
Availability,"We feel this is another in a long series of weird concurrency issues which we see from time to time involving shared filesystems and fs synching. We've never been able to reliably reproduce these, but if someone can provide something here we can reopen and look at it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059:171,reliab,reliably,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059,1,['reliab'],['reliably']
Availability,"We get 'fail to delocalize' as the error message over a bunch of failure types. In particular, when the task failed with return code 0. This makes failures hard to debug in FireCloud, as users have to dig down into logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864:35,error,error,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864,4,"['down', 'error', 'failure']","['down', 'error', 'failure', 'failures']"
Availability,"We have a very similar use case. We'd like to be able to run a different annotator that has a massive pile of data sources ~20gb. We want an easy way to package different sets of test files and make them available for people to use with our docker image, without having to make a 20gb docker image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129:204,avail,available,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129,1,['avail'],['available']
Availability,"We have also seen the `address already in use` error. Are you saying that the error is false and we should ignore it?. If it is a real error, then it seems like we would want to continue seeing it, and have the workaround be turning off the SSH enablement option `enable_ssh_access` [0]. [0] https://cromwell.readthedocs.io/en/stable/wf_options/Google/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894:47,error,error,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894,3,['error'],['error']
Availability,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:660,error,error,660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,4,"['error', 'ping', 'reliab']","['error', 'pinged', 'reliably']"
Availability,"We lose this feature for local backend yes, but it's not the only one (we also lose support for the ability to use files in calls that were not explicitly set as inputs), which was the point of the ticket because that is what JES is doing.; In the end this was a downgrade of Local/SGE Backends to match JES. Now we can re-upgrade but it should be done on all backends at the same time IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098:263,down,downgrade,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098,1,['down'],['downgrade']
Availability,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357,2,"['error', 'failure']","['error', 'failures']"
Availability,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:136,avail,available,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646,1,['avail'],['available']
Availability,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:90,error,errors,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912,6,"['down', 'error']","['down', 'errors']"
Availability,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,redundant,redundantly,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802,1,['redundant'],['redundantly']
Availability,"We should leave this open. This is basically the same thing @danbills has been poking at for Firecloud but we weren't able to reproduce it. For their side of things we discovered that they weren't taking advantage of metadata batching, which they're going to change. It likely won't *solve* the issue but should make it robust enough that they don't see it anymore. However the underlying problem is still lurking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180:320,robust,robust,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180,1,['robust'],['robust']
Availability,We will likely need to upgrade liquibase at some point but for the moment PR #4619 does a temporary downgrade. Issue #4618 tracks that MariaDB needs to be CI tested to make sure this doesn't re-occur. Thanks for the report!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197:100,down,downgrade,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197,1,['down'],['downgrade']
Availability,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422,1,['error'],['error']
Availability,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:160,down,downstream,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132,1,['down'],['downstream']
Availability,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:223,down,down,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663,1,['down'],['down']
Availability,"We've now seen a new ""flavor"" of this failure mode, where this same ""Communications link failure"" error is cropping up but it's following some type of HttpRequest failure: ; 2016-05-16 23:42:39,080 cromwell-system-akka.actor.default-dispatcher-18 INFO - WorkflowActor [UUID(e7e4c6d2)]: Collection complete for Scattered Call CollectQualityYieldMetrics.; 2016-05-16 23:42:39,365 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(e7e4c6d2)]: persisting status of CollectQualityYieldMetrics to Done.; 2016-05-16 23:42:45,957 cromwell-system-akka.actor.default-dispatcher-21 INFO - JES Run [UUID(303ad2dd):ScatterIntervalList]: Status change from Running to Success; 2016-05-16 23:42:50,399 cromwell-system-akka.actor.default-dispatcher-18 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of ScatterIntervalList to Done.; 2016-05-16 23:42:51,977 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Starting.; 2016-05-16 23:42:55,593 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Done.; 2016-05-16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:38,failure,failure,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,4,"['error', 'failure']","['error', 'failure']"
Availability,"Weird, and definitely a bug! Thanks for reporting... It looks to me like maybe at graph construction time the temporary (anonymous) expression nodes (dashed lines, used to generate input values for calls) are accidentally being added into the pool of ""generally available values"" for expression lookup (but they shouldn't be - they should be one-time-only values for the call expression they're feeding)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-418386831:262,avail,available,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-418386831,1,['avail'],['available']
Availability,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:129,robust,robust,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['robust'],['robust']
Availability,Well that's what happens when we design something in a way where that's semi-intentional :) We should sit down and figure out how to work all of this in a way which doesn't tie up the whole system (i.e. the reason we went down this path in the first place),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330,2,['down'],['down']
Availability,"What I was going to suggest was looking at [WES](https://github.com/ga4gh/workflow-execution-schemas). . - Conceptually it's what we need to do anyways (""Hey, here's a workflow and it's of type X""); - We've signed on to support WES at a future date anyways. It's possible that it doesn't map particularly cleanly and/or doesn't make sense for some other reason but IMO it'd be good to at least poke at going down this path first. There's protobuf & swagger in that repo I linked. If there are complaints/feedback about the API and not the Cromwell implications of the API, let me know and we can surface them w/ the appropriate folks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042:408,down,down,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042,2,['down'],['down']
Availability,"What actually happens when `check-alive` is called?. Cromwell was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:34,alive,alive,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,2,['alive'],['alive']
Availability,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:173,down,down,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608,1,['down'],['down']
Availability,"What do you recommend instead on a personal laptop? Even 55 by the way is; not reliable and fails or stucks randomly. I feel like it is more of a; macOS issue, as I never had these on Ubuntu using same setup. 4 Şub 2023 Cmt 00:01 tarihinde Adam Nichols ***@***.***> şunu; yazdı:. > Desktop Docker is not the most reliable platform for real work in; > Cromwell, but it is interesting that a specific version broke it.; >; > Do you have time to do a git bisect between 55 and current?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHAHMHBVIQIXGMY5RKKIW7TWVVW37ANCNFSM6AAAAAAUQB7A2U>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366:79,reliab,reliable,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366,2,['reliab'],['reliable']
Availability,"What does the storage block in cromwell config look like? In particular, do you have a region set? If I recall correctly, I had to actually set the region (not default) or I had similar errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437349327:186,error,errors,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437349327,1,['error'],['errors']
Availability,What is being done is not exactly what this ticket says but it does test that at least one job is recovered properly during centaur tests.; I think this can be closed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508:98,recover,recovered,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111#issuecomment-329782508,1,['recover'],['recovered']
Availability,"When I run this I get a lot of unexpected messages. Presumably we just want to drop these acks:. ```; 2016-05-24 16:42:22,511 cromwell-system-akka.actor.default-dispatcher-30 INFO - Status change from Initializing to Running; 2016-05-24 16:42:22,515 cromwell-system-akka.actor.default-dispatcher-30 ERROR - Unexpected message MetadataPutAcknowledgement(PutMetadataAction(MetadataEvent(MetadataKey(da17555a-11bc-4e72-a338-a2f177718435,Some(MetadataJobKey(DeliciousFileSpam.rotateInner,None,1)),backendStatus),MetadataValue(Running),2016-05-24 16:42:22.511))).; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221395379:299,ERROR,ERROR,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221395379,1,['ERROR'],['ERROR']
Availability,When I tried to reproduce this locally Cromwell completed the workflow successfully despite the fact that `find` had been invoked with invalid syntax and no placeholder `.file`s had been created in my empty directories. It looks like our wrapper script is not checking for failures at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-460417893:273,failure,failures,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-460417893,1,['failure'],['failures']
Availability,"When an actor sends a message to another actor and that actor isn't expecting to handle that type of message, akka has its own way of logging the error. Some well intentioned Cromwellians often partake in a pattern where they explicitly catch these and log the error. It's unnecessary and adds to LOC. Really this is tech debt, i'll relabel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526:146,error,error,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328927526,2,['error'],['error']
Availability,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:47,failure,failure,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182,1,['failure'],['failure']
Availability,"When run the server modle; ```; root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:312,ERROR,ERROR,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads som",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:1532,down,download,1532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['down'],['download']
Availability,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:66,avail,available,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328,3,['avail'],['available']
Availability,"When using the `PAPIv2` backend, I have noticed that the same previous set of roles is not sufficient to be able to run the pipelines. Instead, after a long and tedious amount of work, I have figured that the following set of roles:; 1) [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2) [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3) [Firebase Develop](https://firebase.google.com/docs/projects/iam/roles-predefined-category#analytics_roles) Admin (`firebase.developAdmin`). are sufficient to run a pipelne on Google Cloud through a service account. I suppose that `lifesciences.workflowsRunner` is a replacement for `genomics.pipelinesRunner`, but I have no idea why `firebase.developAdmin` is required (or what else should be required in its place). To save my life, I could not find this information anywhere in the Cromwell documentation nor evince it from the Cromwell error messages themselves (nor understand what the `firebase.developAdmin` roles actually allows).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059:1044,error,error,1044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059,1,['error'],['error']
Availability,"When you make a call from an imported WDL, the call is automatically aliased to the simple task name, so in your case:. ```wdl; import ""a.wdl"" as a. workflow b {. # This task is 'a.t', but the callis given the alias 't'; call a.t {}. String x = t.s; }; ```. Similarly, this should be an error:; ```wdl; import ""a.wdl"" as a; import ""z.wdl"" as z. workflow b {. # Cannot do this because the calls would have the same alias:; call a.t {}; call z.t {}. # Which t?; String x = t.s; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834:287,error,error,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834,2,['error'],['error']
Availability,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:221,down,downloading,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419,1,['down'],['downloading']
Availability,"Where did you look? The file in the error message is the output of a task. On Tue, Aug 30, 2016 at 9:37 AM, Jeff Gentry notifications@github.com; wrote:. > That error is a failure of jes to initialize a vm for you in the first; > place. There are no logs to write beyond what's in there. I just took a; > quick glance but it looks like you're specifying a file which doesn't exist; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk0bYjGuwmLKdzJkzsIk8YUxr-Ee8ks5qlDIygaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243442366,3,"['error', 'failure']","['error', 'failure']"
Availability,"Which is amusing @Horneth because when we sat down to discuss it a few months ago I had switched to the side of chaos while others were advocating order ;) But really we've got the best of both worlds right now IMO, this is closeable",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324172692:46,down,down,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329#issuecomment-324172692,1,['down'],['down']
Availability,"While this ticket is being resolved for the womtool-on-the-releases-page, for docker users the latest WOMTool is available @ https://hub.docker.com/r/broadinstitute/womtool/tags/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3031#issuecomment-350516520:113,avail,available,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031#issuecomment-350516520,1,['avail'],['available']
Availability,Will need to investigate this failure-- java.lang.NoClassDefFoundError: com/vladsch/flexmark/util/ast/Node,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6532#issuecomment-976113622:30,failure,failure,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6532#issuecomment-976113622,1,['failure'],['failure']
Availability,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:195,error,error,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479,2,"['Error', 'error']","['Error', 'error']"
Availability,"With inputs; ```; {; ""wf.input_to_wf"": null; }; ```; and WDL; ```; version 1.0. workflow wf {; input {; String? input_to_wf = ""Neither should this""; }. call hello { input:; input_to_hello = input_to_wf; }; }. task hello {; input {; String? input_to_hello = ""This should not be here""; }. command {; echo ""Should be blank: ${input_to_hello}""; }; output {; String result = read_string(stdout()); }; }; ```; the result is; ```; ""Should be blank: This should not be here""; ```; ...so it would seem that Cromwell understands not to overwrite null with a default value _sometimes_, just not on calls.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3819#issuecomment-421435288:298,echo,echo,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3819#issuecomment-421435288,1,['echo'],['echo']
Availability,"With the current code I see in the logs:. ```; ""docker: Error response from daemon: error while creating mount source path '/mnt/11a4324d4472f639f3fc558b00afeacd': mkdir /mnt/11a4324d4472f639f3fc558b00afeacd: read-only file system.""; ```. and in the job metadata the following. Note the lack of `ro` specifiers in the docker command line invocation, and the presence of `ro` specifiers in the `volumes`:. ```; ""container"": {; ""commands"": [; ""-c"",; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ -v\\ /mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd\\ -v\\ /mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661\\ --entrypoint\\=/bin/bash\\ ubuntu@sha256:8a37d68f4f73ebf3d4efafbcf66379bf3728902a8038616808f04e34a9ab63ee\\ /mnt/disks/cromwell_root/script""; ],; ""entrypoint"": ""/bin/sh"",; ""imageUri"": ""gcr.io/google.com/cloudsdktool/cloud-sdk:461.0.0-alpine"",; ""volumes"": [; ""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:rw"",; ""/mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd:ro"",; ""/mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661:ro""; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484:56,Error,Error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484,2,"['Error', 'error']","['Error', 'error']"
Availability,With the rate throttled BOTH the jobDefinition errors AND the Too Many Requests errors have both been eliminated for a workflow that has 133 independent shards. However severe throttling of the job rate seems like something you need to be presented with up front as this only is acceptable for long running jobs with independent processes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-469868742:47,error,errors,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-469868742,2,['error'],['errors']
Availability,"Within the Broad, this file will cause a failure: /dsde/data/test_dl_oxoq/v2/nt/tumor_list.original",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-278390963:41,failure,failure,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-278390963,1,['failure'],['failure']
Availability,WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4377,Error,ErrorOr,4377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"Won't compile:. > [error] /Users/chrisl/IdeaProjects/cromwell/src/main/scala/cromwell/instrumentation/Instrumentation.scala:20: value getConfigOption is not a member of com.typesafe.config.Config; > [error] private val Config = ConfigFactory.load.getConfigOption(""instrumentation""); > [error] ^; > [error] one error found; > [error](compile:compileIncremental) Compilation failed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/344#issuecomment-166862524,6,['error'],['error']
Availability,"Working workflow:. ```wdl; workflow foo {; call bar; output {; Array[File] baz = bar.baz; }; }. task bar {; Array[Array[String]]? baz; command <<<; x=${ if defined(baz) then write_tsv(baz) else '' }; if [[ -z ""$x"" ]]; then; echo ""no file""; else; cp $x output.tsv; fi; >>>; output {; Array[File] baz = glob(""*.tsv""); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524324:224,echo,echo,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-493524324,1,['echo'],['echo']
Availability,"Yeah - It would be great to play within actors but I'm not sure how much of that's really possible within a shutdown hook. Here are replies to your bullets:; - The JVM shutdown API only takes a Thread, not a Future, so we don't really have a choice what to give it. I could use Scala's ShutdownHookThread if you like but I don't think there's anything like a Future available.; - This doesn't actually block the actor. All that happens in the actor is that the shutdown hook gets configured.; - I agree actors would be nice but delaying shutdown is inherently synchronous - since as soon as that thread returns then the JVM will disappear. If there's another way to get the current state other than my 'done' variable that might be nice though. Do you know of one?. All that's just AFAIK though. Am I missing anything?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173748643:366,avail,available,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173748643,1,['avail'],['available']
Availability,"Yeah I'm not too happy with adding code and not tests, but @scottfrazer apparently went down this road before and wasn't able to get it to work. I'm happy to invest the effort writing some tests if anyone knows how to get over the linking hurdles in Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142699848:88,down,down,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142699848,1,['down'],['down']
Availability,Yeah and we can fall back to try another cache hit if one is available. @geoffjentry feel free to leave the totem on my desk.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337:61,avail,available,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2347#issuecomment-307426337,1,['avail'],['available']
Availability,Yeah doesn't it come down to:. ```bash; # singularity; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; ```; vs; ```bash; # docker; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=docker cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/docker.json; ```; so you use the same *.wdl but just choose a different backend / and workflow opts?. ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732:21,down,down,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571732,1,['down'],['down']
Availability,"Yeah it was actually worse than I thought, it was not terribly broken but not robust to all edge cases either. I can explain to you if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/477#issuecomment-189433723:78,robust,robust,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/477#issuecomment-189433723,1,['robust'],['robust']
Availability,"Yeah that’s a sentry error. TBH I thought it was just on my home laptop, apparently not",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389026162:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389026162,1,['error'],['error']
Availability,"Yeah, a github-based scan of the code just now does make me think that this is a workflow-level slowdown, which again is total crap if you're the only workflow in the system. I don't know why only 3k wide would manifest here though, I've run things 200k wide and not seen a problem in this spot, and I don't think the processing here would be a function of the inputs/outputs/etc . What I was getting at with the single vs multi-workflow thing though is that many of the cases like this that I've seen the solution is to parallelize the operation, but in a system running thousands of workflows you don't just want to spam a ton of threads to do this as you'll choke out everyone else. In contrast if you're a single workflow on an n-core box you'd want to use as many cores as you can for this. In the general case I don't know how to resolve that other than to set up a special execution context for these sorts of things, default them to having a a bunch of CPU and then letting someone like firecloud configure that down. That said, in this particular area we've previously seen some highly inefficient copying/comparisons going on so that's also a possible culprit here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200:1020,down,down,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276963200,1,['down'],['down']
Availability,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:89,error,errors,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750,3,"['avail', 'error', 'failure']","['available', 'errors', 'failures']"
Availability,"Yeah, travis had to roll back a commit which was breaking different java versions (I found bug reports from people who were having similar issues with trying to use Java 14). A couple of your tests also bumped into a quay.io outage so I've restarted those and 🤞 we don't have any more transient failures on those!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115:225,outage,outage,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115,2,"['failure', 'outage']","['failures', 'outage']"
Availability,"Yep, you were right about this bug, it was because the IAM role that Cromwell uses did not have write access to the S3 bucket. The actual AWS Batch worker instances did have write access, so the workflow didn't fail until it hit `write_lines`, which is executed by the Cromwell server, not by the Batch machine. However, this is actually the fault of the CloudFormation scripts that generated this role. I used the scripts [hosted here](https://s3.amazonaws.com/aws-genomics-workflows/templates/cromwell/cromwell-server.template.yaml), from [this webpage](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/). Notice how the IAM role only has `s3:GetObject` permissions, not `PutObject` etc:. ```yaml; Ec2InstanceRole:; Type: AWS::IAM::Role; Properties:; Policies:; - PolicyName: !Sub CromwellServer-S3Bucket-Access-${AWS::Region}; PolicyDocument:; Version: 2012-10-17; Statement:; Effect: Allow; Resource: !Join ["""", [""arn:aws:s3:::"", !Ref S3BucketName, ""/*""]]; Action:; - ""s3:GetObject""; ```. @aednichols Do you happen to know who wrote these scripts or where the code is hosted? Because I would like to submit a change to them to fix this bug",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431235259:342,fault,fault,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431235259,1,['fault'],['fault']
Availability,"Yes that's correct, sorry I didn't see you are building from a docker uri. . I don't think there exists the exact functionality you want, but we are getting there. The current cache support is for docker layers, which will save you download time, but you still would rebuild from them each time. I think it would be worth the effort to ask for what you need. Take a look at the [latest release](https://github.com/sylabs/singularity/releases) that has some support for caching (for library images). Then I would open an issue and say something about it - you want the same caching but for docker pulled containers. This particular flow to pull (or build) and honor a cached image if it exists is very common and reasonable, and should be supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477:232,down,download,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477,1,['down'],['download']
Availability,"Yes the I/O actor really is a ""Filesystem I/O Actor"" and does not handle backend API calls. The PAPI backend has a custom actor for that (PipelinesApiManager I think) but it's not (yet) available to all backends unfortunately",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-437003042:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-437003042,1,['avail'],['available']
Availability,Yes this pretty much always happens with the recover code I merged to develop yesterday.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390:45,recover,recover,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235593390,1,['recover'],['recover']
Availability,"Yes, @jgentry is correct. Something that I can just cut and paste easily; to check intermediate results or debug failures. The metadata is a bit; much... On Mon, Oct 24, 2016 at 2:24 PM, Jeff Gentry notifications@github.com; wrote:. > @cjllanwarne https://github.com/cjllanwarne You can also specify a; > command line option to run which is a path to output the workflow; > metadata. I _have_ heard a request (there's probably an issue somewhere); > to allow it to be just the outputs instead of the full metadata firehose.; > ; > I suspect @LeeTL1220 https://github.com/LeeTL1220 might be looking for; > something more on the fly, for the eager person who wants to check stuff; > out prior to workflow completion; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-255823594,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk_9uFGV2wpj8ojO7kpkq2KBRqWMVks5q3PfLgaJpZM4Kezeu; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-256142741:113,failure,failures,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614#issuecomment-256142741,1,['failure'],['failures']
Availability,"Yes, I agree that both WDL and CWL have decided to override the entrypoint, which is perfectly reasonable. The problem is that this overriding doesn't work with Cromwell + PAPI, which is inconsistent with the rest of the backends, and thus causes confusing errors",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438111656:257,error,errors,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438111656,1,['error'],['errors']
Availability,"Yes, I also think dead letters message is no issue. . root@d0ef87b8b6b8:/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution# **cat stderr**; > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; > [M::bwa_idx_load_from_disk] read 0 ALT contigs; > [W::main_mem] when '-p' is in use, the second query file is ignored.; > . stdout is 0 byte. I'm running on local machine. ; Just I used 2 bam file only.; $ /BiO/Project/brandon-genome-analysis/data/NA12878_24RG_small.txt; /BiO/Project/brandon-genome-analysis/data/HJYFJ.4.NA12878.downsampled.query.sorted.unmapped.bam; /BiO/Project/brandon-genome-analysis/data/HJYFJ.5.NA12878.downsampled.query.sorted.unmapped.bam. I found some issue in stderr file. > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Could you suggest any comment for this ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315:427,ERROR,ERROR,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315,6,"['ERROR', 'down', 'error']","['ERROR', 'downsampled', 'errors']"
Availability,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:713,avail,available,713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,1,['avail'],['available']
Availability,"Yes, it would also be nice. Right now, most of the wasted time in my pipelines is due to the things like this: when something in cromwell crashes and I spend a lot of time figuring out what exactly.; P.S.; To be honest, I do not understand why you started to create a language with a lot of problems to solve ( like this one) instead of just providing better declarative scala DSL (I think it is even possible to make it look almost like current wdl syntax) where tooling and errors are solved by scala ecosystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365:476,error,errors,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365,1,['error'],['errors']
Availability,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:508,down,down,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843,2,['down'],['down']
Availability,"Yes, would be great to see a fix for this. Attached is a simple WDL repro using an expression over an http input that fails with the following. It appears the filename prefix is being incorrectly remapped using the current local directory instead of maintaining the http:// prefix. . java -jar cromwell.jar run http_inputs.wdl.txt --inputs http_inputs.json.txt. `[2019-08-14 22:23:39,50] [error] WorkflowManagerActor Workflow 09c208ef-58b5-4571-9587-e3e2e58a0831 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'disk_space' (reason 1 of 1): Evaluating ceil((size(jamie2, ""GB"") * 2.25)) failed: [Attempted 1 time(s)] - NoSuchFileException: /home/cromwellbuild/cromwell/centaur/src/main/resources/standardTestCases/http_inputs/http:/raw.githubusercontent.com/broadinstitute/cromwell/develop/docs/jamie_the_cromwell_pig.png`. [http_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/3503691/http_inputs.json.txt); [http_inputs.wdl.txt](https://github.com/broadinstitute/cromwell/files/3503693/http_inputs.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671:389,error,error,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671,2,['error'],['error']
Availability,"Yes. Not as bad, but still pretty hard to debug. A fair number of; transient errors when I use JES. On Tue, Apr 4, 2017 at 1:03 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> does this still happen?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291566093>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk-_InNBhbZ4MPXcOdseOkaYuW2N2ks5rsnf_gaJpZM4KiVST>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291572219:77,error,errors,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-291572219,1,['error'],['errors']
Availability,"Yes. This pulls in the cromwell ""server"" [application.conf](https://github.com/broadinstitute/cromwell/blob/1dc56871a958c3c2cfdf1a59c6ccc2bec3968300/server/src/main/resources/application.conf) with a few required overrides of akka's various reference.conf files, and reports an error if for some reason the file is not found. https://github.com/broadinstitute/cromwell/blob/1dc56871a958c3c2cfdf1a59c6ccc2bec3968300/server/src/main/resources/application.conf#L1-L23",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4204#issuecomment-426877561:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4204#issuecomment-426877561,1,['error'],['error']
Availability,"You could set `preemptible` very high to minimize the chance of preemption. I don't think there would be any issue setting it to 10 or even more. That said, it can be a bit of a false economy because failed attempts still cost real money. It may even be the case that falling back to non-preemptible saves money. Let's say preemptibles are $1 an hour and normal VMs are $3. If you run a 12 hour task that gets preempted 6 times at the 6 hour mark, that's 6 x 6 x $1 = $36 down the drain, a day and a half of wall clock time, and no results to show for it. Whereas a single non-preemptible run would be 12 x $3 = $36 and you'd have your results. Obviously this math will vary widely by use case and you will have to observe your preemption rates in practice to come up with the optimal balance. Thanks for an interesting discussion, I had never thought about the ""only preemptible"" use case before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650:472,down,down,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650,1,['down'],['down']
Availability,You might have to lower the `range(100)` down. That was me testing my concurrent job limits...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220:41,down,down,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461#issuecomment-248616220,1,['down'],['down']
Availability,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:867,error,errors,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['error'],['errors']
Availability,"[2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://g",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1428,ERROR,ERROR,1428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"[meta.txt](https://github.com/broadinstitute/cromwell/files/2147609/meta.txt); Checking the metadata, none of the 'inputs' are folders, or anything related to the error that I can tell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401233074:163,error,error,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401233074,1,['error'],['error']
Availability,"\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow failed""; }],; message: ""Workflow failed""; }; ],; ```. This step is executed in a scatter way, 17x per analysis (distinct genomic interval for each shard). Bellow follows the cromwell script of the shard that processed chromosome 12 and 13:. ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.a7701249"" && echo ""/cromwell_root/tmp.a7701249""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); oute4a6eeab=""${tmpDir}/out.$$"" erre4a6eeab=""${tmpDir}/err.$$""; mkfifo ""$oute4a6eeab"" ""$erre4a6eeab""; trap 'rm ""$oute4a6eeab"" ""$erre4a6eeab""' EXIT; tee '/cromwell_root/stdout' < ""$oute4a6eeab"" &; tee '/cromwell_root/stderr' < ""$erre4a6eeab"" >&2 &; (; cd /cromwell_root. /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R /cromwell_root/required-files/references/b37/human_g1k_v37_decoy.fasta \; -I /cromwell_root/temporary-files/XXXXXX-001/workspace/SingleSampleGenotyping/415cf327-c799-4d1d-a726-272028b4e8c5/call-ubam2bam/from_ubam.to_bam_workflow/36aabe2e-6ff6-456b-a7cf-d76cb7b93173/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:2516,echo,echo,2516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,1,['echo'],['echo']
Availability,"_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) > ""$outb631bc66"" 2> ""$errb631bc66""; echo $? > /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; sleep 5 && sync. ); mv /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-81",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:1980,echo,echo,1980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838,2,['echo'],['echo']
Availability,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1511,error,error,1511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['error'],['error']
Availability,`Failed to delocalize files` Looks like JES couldn't delocalize a file that was expected as an output.; The task probably failed to produce that output hence the failure.; I thought the logs were copied regardless in that case but apparently not..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114:162,failure,failure,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,1,['failure'],['failure']
Availability,"```; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ],; ```; could there be separate `errors` and `warnings` fields?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-443780262:6,error,errors,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-443780262,4,['error'],['errors']
Availability,"```; -terse Available for qsub only. -terse causes the qsub to display only the job-id of the job; being submitted rather than the regular ""Your job ..."" string.; In case of an error the error is reported on stderr as usual.; This can be helpful for scripts which need to parse qsub output; to get the job-id.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/181#issuecomment-140145560:12,Avail,Available,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/181#issuecomment-140145560,3,"['Avail', 'error']","['Available', 'error']"
Availability,"```; 2016-08-26 16:15:29,393 cromwell-system-akka.actor.default-dispatcher-3 ERROR - JobStoreReadFailure; java.lang.RuntimeException: JobStoreResultSimpletonEntry(metrics,""gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/5b7f21d8-0a96-4bbe-a96b-aa68c0454fd7/call-CollectQualityYieldMetrics/shard-8/HV2J2CCXX.8.Pond-555028.unmapped.quality_yield_metrics"",File,892420,Some(287367)): unrecognized WDL type: File; at cromwell.jobstore.SqlJobStore.cromwell$jobstore$SqlJobStore$$toSimpleton(SqlJobStore.scala:59) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5$$anonfun$3.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5$$anonfun$3.apply(SqlJobStore.scala:69) ~[classes/:na]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[scala-library-2.11.7.jar:1.0.0-M1]; at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:69) ~[classes/:na]; at cromwell.jobstore.SqlJobStore$$anonfun$readJobResult$1$$anonfun$apply$5.apply(SqlJobStore.scala:66) ~[classes/:na]; at scala.Option.map(Option.scala:146) ~[scala-library-2.11.7.jar:1.0.0-M1]; at crom",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472:77,ERROR,ERROR,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349#issuecomment-242840472,1,['ERROR'],['ERROR']
Availability,"```; 2016-09-16 14:45:19,558 cromwell-system-akka.dispatchers.backend-dispatcher-86 ERROR - Google credentials are invalid: Connection reset; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-d86697f6-ca39-417b-b575-fc955c808983/WorkflowExecutionActor-d86697f6-ca39-417b-b575-fc955c808983/d86697f6-ca39-417b-b575-fc955c808983-EngineJobExecutionActor-DeliciousFileSpam.FileSpam:364:1/d86697f6-ca39-417b-b575-fc955c808983-BackendJobExecutionActor-d86697f6:DeliciousFileSpam.FileSpam:364:1/JesAsyncBackendJobExecutionActor: exception during creation; at akka.actor.ActorInitializationException$.apply(Actor.scala:174); at akka.actor.ActorCell.create(ActorCell.scala:607); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: Connection reset; at cromwell.filesystems.gcs.GoogleAuthMode$class.validateCredentials(GoogleAuthMode.scala:79); at cromwell.filesystems.gcs.ApplicationDefaultMode.validateCredentials(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.credential(GoogleAuthMode.scala:63); at cromwell.filesystems.gcs.ApplicationDefaultMode.credential(GoogleAuthMode.scala:137); at cromwell.filesystems.gcs.GoogleAuthMode$class.buildStorage(GoogleAuthMode.scala:94); at cromwell.filesystems.gcs.ApplicationDefaultMode.buildStorage(GoogleAuthMode.scala:137); at cromwell.backen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719:84,ERROR,ERROR,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-247787719,1,['ERROR'],['ERROR']
Availability,```; Error 502 (Server Error)!!1; ```; I find this level of excitement about a server error deeply distrubing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492860544:5,Error,Error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492860544,3,"['Error', 'error']","['Error', 'error']"
Availability,"```; Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-57] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.StackOverflowError; 	at scala.collection.immutable.Set$EmptySet$.seq(Set.scala:68); 	at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141); 	at scala.collection.AbstractSet.$plus$plus(Set.scala:47); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:27); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	at slick.compiler.ExpandSums.slick$compiler$ExpandSums$$tr$1(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.compiler.ExpandSums$$anonfun$7.apply(ExpandSums.scala:32); 	at slick.util.ConstArray.endoMap(ConstArray.scala:122); 	at slick.ast.Node$class.mapChildren(Node.scala:51); 	at slick.ast.Apply.mapChildren(Node.scala:547); 	...; ```; It's easy to reproduce, just run +100 hello world workflows and then query for all those workflows ids using query POST API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315450361,3,"['down', 'error']","['down', 'error']"
Availability,"```; [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; main(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; project, account = bootstrapping.GetActiveProjectAndAccount(); File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; project_name = properties.VALUES.core.project.Get(validate=False); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; required); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; value = _GetPropertyWithoutDefault(prop, properties_file); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; value = callback(); File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 693, in _GetGCEProject; return c_gce.Metadata(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846:6,ERROR,ERROR,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298751846,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,```; [error] (wdlModelDraft2 / Test / compileIncremental) Compilation failed; ```; https://travis-ci.com/github/broadinstitute/cromwell/jobs/473221961,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163,1,['error'],['error']
Availability,```; [info] - should check that we classify error code 10 as a preemption *** FAILED *** (183 milliseconds); [info] Preempted was not equal to Preempted (GetRequestHandlerSpec.scala:207); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at cromwell.backend.google.pipelines.v2alpha1.api.request.GetRequestHandlerSpec.$anonfun$new$2(GetRequestHandlerSpec.scala:207); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265,1,['error'],['error']
Availability,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:14,error,erroring,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497,1,['error'],['erroring']
Availability,"`us-east1-a` is not a legitimate zone, but nevertheless Cromwell's handling of this user error should be better. https://cloud.google.com/compute/docs/regions-zones/regions-zones",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-275703147:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-275703147,1,['error'],['error']
Availability,"`womtool womgraph` is pretty interesting here. Seems like all calls beyond the first are taking their `out` from the LHS of the previous call assignment, instead of the global `out`. `input_overwriting.wdl`; ```; version 1.0. workflow test {; String out = ""hello""; call echo as a {; input:; out = out + ""1"" // prints ""hello1""; }; call echo as b {; input:; out = out + ""2"" // prints ""hello12""; }; call echo as c {; input:; out = out + ""3"" // prints ""hello123""; }; call echo as d {; input:; out = out + ""4"" // prints ""hello1234""; }; }; task echo {; input {; String out; }; command {; echo ~{out}; }; }; ```; ![input_overwriting](https://user-images.githubusercontent.com/1087943/44554203-0f18fd00-a6fe-11e8-86dd-8320a37c3a60.png). ---. `input_not_overwriting.wdl`; ```; version 1.0. workflow test {; String not_out = ""hello""; call echo as a {; input:; out = not_out + ""1""; }; call echo as b {; input:; out = not_out + ""2""; }; call echo as c {; input:; out = not_out + ""3""; }; call echo as d {; input:; out = not_out + ""4""; }; }; task echo {; input {; String out; }; command {; echo ~{out} ; }; }; ```; ![input_not_overwriting](https://user-images.githubusercontent.com/1087943/44554151-d4af6000-a6fd-11e8-888c-8ded8bd924c8.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-415587720:270,echo,echo,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-415587720,12,['echo'],['echo']
Availability,a:11); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$Ops.toWomCallable$(WomCallableMaker.scala:8); 	at wom.transforms.WomCallableMaker$ops$$anon$1.toWomCallable(WomCallableMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:129); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomCallNodeMaker$.toWomCallNode(WdlDraft2WomCallNodeMaker.scala:21); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$Ops.toWomCallNode$(WomCallNodeMaker.scala:9); 	at wom.transforms.WomCallNodeMaker$ops$$anon$1.toWomCallNode(WomCallNodeMaker.scala:9); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:68); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:3219,Error,ErrorOr,3219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,a:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:2737,Robust,RobustClientHelperSpec,2737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"a:20); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalat",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5100,Error,ErrorHandling,5100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['Error'],['ErrorHandling']
Availability,"ach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,885 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Failed.; 2016-04-26 18:26:09,888 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Call failed to initialize: Could not persist runtime attributes: Duplicate entry '163979-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; 2016-04-26 18:26:09,889 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeMetadata to Failed.; 2016-04-26 18:26:10,263 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Beginning transition from Running to Failed.; 2016-04-26 18:26:12,042 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: transitioning from Running to Failed. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:11014,ERROR,ERROR,11014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,2,['ERROR'],['ERROR']
Availability,"ackend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-07-27 16:48:51,320 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-07-27 16:48:51,631 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is now terminal. Shutting down.; 2016-07-27 16:48:51,637 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-07-27 16:48:51,663 cromwell-system-akka.dispatchers.engine-dispatcher-16 INFO - WorkflowExecutionActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Starting calls: hello.hello:NA:1; 2016-07-27 16:48:51,670 cromwell-system-akka.dispatchers.engine-dispatcher-16 INFO - WorkflowExecutionActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: WorkflowExecutionActor [UUID(c6eb4949)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-07-27 16:48:53,508 INFO - JesRun [UUID(c6eb4949)hello.hello:NA:1]: JES Run ID is operations/EMKB-PDiKhiz_dqSktq89pQBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-07-27 16:48:53,610 cromwell-system-akka.dispatchers.backend-dispatcher-17 INFO - $a [UUID(c6eb4949)hello.hello:NA:1]: ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:2618,down,down,2618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544,1,['down'],['down']
Availability,"ackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:934); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:926); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:75); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); ```. It's not the same issue as you other issue because 1) I'm not manipulating any file name in my task and 2) if you look at the `gatk` command, it correctly used the right localized file path. The error itself is something about the mount point:. ```; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; ```; I tried copying over the input files to a different path so that the file paths are shorter but I'm still h aving the exact same error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:5046,error,error,5046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,2,['error'],['error']
Availability,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9916,echo,echo,9916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['echo'],['echo']
Availability,"ads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:2064,echo,echo,2064,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['echo'],['echo']
Availability,"aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4822,echo,echoHelloWorld,4822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"ah I get it now, I see the build failure on your other PR :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5014#issuecomment-498276705:33,failure,failure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5014#issuecomment-498276705,1,['failure'],['failure']
Availability,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2873,error,error,2873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['error'],['error']
Availability,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1575,Fault,FaultHandling,1575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,6,['Fault'],['FaultHandling']
Availability,also note that this function retries all 404 errors. But if we know that is an expected outcome we handle it before we get to this function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1872#issuecomment-273598671:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1872#issuecomment-273598671,1,['error'],['errors']
Availability,any idea what's up with that test failure?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197557326:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/573#issuecomment-197557326,1,['failure'],['failure']
Availability,"assembly fails consistently in Travis with ""Connection reset"" errors during merge, but it seems to work fine locally. I'll give it a few more chances to straighten itself out in Travis before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202812799:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202812799,1,['error'],['errors']
Availability,at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at wom.callable.CommandTaskDefinition.$anonfun$instantiateCommand$3(CommandTaskDefinition.scala:109); 	at scala.collection.immutable.List.flatMap(List.scala:335); 	at wom.callable.CommandTaskDefinition.instantiateCommand(CommandTaskDefinition.scala:108); 	at wom.callable.CommandTaskDefinition.instantiateCommand$(CommandTaskDefinition.scala:97); 	at wom.callable.CallableTaskDefinition.instantiateCommand(CommandTaskDefinition.scala:136); 	at cromwell.backend.wdl.Command$.$anonfun$instantiate$1(Command.scala:32); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:20); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:31); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:349); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:4182,Error,ErrorOr,4182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,2,['Error'],['ErrorOr']
Availability,atSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:2769,Robust,RobustClientHelperSpec,2769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"ava -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1185,echo,echo,1185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['echo'],['echo']
Availability,"awesome! Yeah just ping on here when you are ready, and I'll be glad to help :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-301208260:19,ping,ping,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-301208260,1,['ping'],['ping']
Availability,"bSomaticSNVInDel.FilterByOrientationBias:1:1]: gatk FilterByOrientationBias \; \; -V /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.relativePathAndVolume(AwsBatchAsyncBackendJobExecutionActor.scala:236); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:1477,error,error,1477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,1,['error'],['error']
Availability,"broadinstitute/cromwell/blob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(Con",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:1214,Error,Error,1214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['Error'],['Error']
Availability,c(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.di,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:2640,robust,robustPoll,2640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['robust'],['robustPoll']
Availability,cala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.inte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:3627,Robust,RobustClientHelperSpec,3627,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"ce to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:1610,failure,failure,1610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['failure'],['failure']
Availability,"cho ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-340a5cf-SNAP.jar run scatterTest.wdl - - - -; ```. consistently (repeatably) dies with:; ```; [2016-12-21 13:01:37,48] [error] WorkflowManagerActor Workflow 28b55884-8fd1-43aa-92ea-eb4891c2c5ff failed (during ExecutingWorkflowState): Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:1695,echo,echo,1695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['echo'],['echo']
Availability,"ckend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Variable docker does not reference any declaration in the task (line 50, col 7):. ${docker} ${docker_script}; ^. Task defined here (line 20, col 6):. task submit_docker {; ^. at wdl.draft2.model.WdlNamespace$.$anonfun$apply$55(WdlNamespace.scala:444); at scala.collection.TraversableLike$WithFilter.$anonfun$map$2(TraversableLike.scala:827); at scala.collection.immutable.List.foreach(List.scala:392); at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:826); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$52(WdlNamespace.scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:6562,ERROR,ERROR,6562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['ERROR'],['ERROR']
Availability,"cloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. O",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1921,Error,Error,1921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,1,['Error'],['Error']
Availability,"concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures[%]%:failure' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2449); 	at com.mysql.jdbc.StatementImpl.executeInternal(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:3569,failure,failures,3569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,2,['failure'],"['failure', 'failures']"
Availability,"config.file is loaded, but . > workflow-options {; > workflow-failure-mode: ""ContinueWhilePossible""; > }. does not work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479:62,failure,failure-mode,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380#issuecomment-245645479,1,['failure'],['failure-mode']
Availability,ction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1845,Robust,RobustClientHelperSpec,1845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"ctor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-28 14:37:35,25] [error] The JES polling actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$a#551868791] unexpectedly terminated while conducting 5 polls. Making a new one...; [2016-10-28 14:37:35,25] [info] watching Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$c/$a/$b#797880880]; [2016-10-28 14:37:35,25] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:1:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:4582,error,error,4582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['error']
Availability,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2911,Error,Error,2911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,dAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:2052,robust,robustPoll,2052,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['robust'],['robustPoll']
Availability,"dBy: [{; causedBy: [],; message: ""Task MakeAnalysisReadyBam.BaseRecalibrator:9:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 19: unexpected exit status 1 was not ignored [Delocalization] Unexpected exit status 1 while running "" / bin / sh - c retry() { for i in `seq 3`; do gsutil - h\ ""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. ""; }],; message: ""Workflow faile",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:1062,echo,echo,1062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,1,['echo'],['echo']
Availability,draft-2 is in maintenance only mode and does not receive new features.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-1190685987:14,mainten,maintenance,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-1190685987,1,['mainten'],['maintenance']
Availability,"e 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99421,failure,failure,99421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2545,error,error,2545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,6,['error'],"['error', 'errors']"
Availability,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12312,echo,echo,12312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['echo'],['echo']
Availability,"e"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": 2,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 100 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""2 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""disk_size"": ""flowcell_small_disk"",; ""input_bam"": ""unmapped_bam"",; ""metrics_filename"": ""sub(sub(unmapped_bam, sub_strip_path, \""\""), sub_strip_unmapped, \""\"") + \"".unmapped.quality_yield_metrics\""""; },; ""failures"": [{; ""failure"": ""Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly."",; ""timestamp"": ""2016-04-24T20:04:45.145Z""; }],; ""jobId"": ""operations/EIXH28fEKhisk93Qxr_9-K4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-24T20:04:45.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.defau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:1180,down,down,1180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['down'],['down']
Availability,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1240,Error,Error,1240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,e.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:3600,Robust,RobustClientHelperSpec,3600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"e1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 3589864- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 3589865- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 3589866- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:23) ~[cromwell.jar:0.19]; 3589867- at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; 3589868- at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; 3589869- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 3589870- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 3589871- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 3589872- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 3589873:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 INFO - WorkflowActor [UUID(129f0510)]: persisting status of CollectQualityYieldMetrics:2 to Failed.; 3589874:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - WorkflowActor [UUID(129f0510)]: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:4887,ERROR,ERROR,4887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,3,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"eArray(File.scala:81); at better.files.File.contentAsString(File.scala:91); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply$mcI$sp(LocalBackend.scala:209); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply(LocalBackend.scala:208); at cromwell.engine.backend.local.LocalBackend$$anonfun$3.apply(LocalBackend.scala:208); at scala.util.Try$.apply(Try.scala:192); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:207); ... 10 common frames omitted; [2016-01-31 16:37:29,45] [info] WorkflowActor [2a89a995]: persisting status of hello to Failed.; [2016-01-31 16:37:29,45] [info] WorkflowActor [2a89a995]: Beginning transition from Running to Failed.; [2016-01-31 16:37:29,59] [info] WorkflowActor [2a89a995]: transitioning from Running to Failed.; [2016-01-31 16:37:29,60] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2016-01-31 16:37:29,61] [error] Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; java.lang.Throwable: Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0 transitioned to state Failed; at cromwell.engine.workflow.SingleWorkflowRunnerActor$RunnerData.addFailure(SingleWorkflowRunnerActor.scala:41); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:99); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$2.applyOrElse(SingleWorkflowRunnerActor.scala:77); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:52); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:598); at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:6858,error,error,6858,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['error'],['error']
Availability,"eConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCell.newActor(ActorCell.scala:624); 	at akka.actor.ActorCell.create(ActorCell.scala:650); 	... 9 common frames omitted; ```. I tried adding the [reference services block](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf#L480), but then I received yet another error: . ```; 2019-02-25 18:46:46,698 cromwell-system-akka.actor.default-dispatcher-3 ERROR - Class cromwell.services.womtool.impl.WomtoolServiceInCromwellActor for service Womtool cannot be found in the class path.; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:2517,ERROR,ERROR,2517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['ERROR'],['ERROR']
Availability,"e_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4751,error,error,4751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['error'],['error']
Availability,"ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:98980,Error,Error,98980,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['Error'],['Error']
Availability,"eam tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normaliz",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12341,down,downstream,12341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,2,['down'],['downstream']
Availability,"ecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1257,ERROR,ERROR,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['ERROR'],['ERROR']
Availability,"econds); - should successfully run standard_output_paths_colliding_prevented *** FAILED *** (3 minutes, 1 second); - should successfully run three_step_cwl *** FAILED *** (5 minutes, 29 seconds); - should NOT call cache the second run of readFromCacheFalse (3 minutes, 21 seconds); - should abort a workflow immediately after submission abort.instant_abort (5 seconds, 52 milliseconds); - should abort a workflow mid run abort.scheduled_abort (2 minutes, 20 seconds); - should abort a workflow mid run abort.sub_workflow_abort (3 minutes, 2 seconds); - should call cache the second run of cacheBetweenWF (2 minutes, 55 seconds); - should call cache the second run of call_cache_hit_prefixes_no_hint (1 minute, 40 seconds); - should call cache the second run of floating_tags (3 minutes, 25 seconds); - should fail during execution bad_docker_name (35 seconds, 238 milliseconds); - should fail during execution bad_workflow_failure_mode (5 seconds, 920 milliseconds); - should fail during execution chainfail (42 seconds, 454 milliseconds); - should fail during execution cont_while_possible (3 minutes, 57 seconds); - should fail during execution cont_while_possible_scatter (2 minutes, 27 seconds); - should fail during execution draft3_read_file_limits (3 minutes, 26 seconds); - should fail during execution empty_filename (16 seconds, 333 milliseconds); - should fail during execution failing_continue_on_return_code (55 seconds, 180 milliseconds); - should fail during execution failures.terminal_status (2 minutes, 55 seconds); - should fail during execution import_passwd (5 seconds, 348 milliseconds); - should fail during execution import_passwd_url (5 seconds, 610 milliseconds); - should fail during execution invalid_return_code (45 seconds, 607 milliseconds); - should fail during execution invalid_runtime_attributes (15 seconds, 637 milliseconds); - should fail during execution invalid_wdl (10 seconds, 626 milliseconds); - should fail during execution invalid_workflow_url (6 seconds,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:3718,failure,failures,3718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,1,['failure'],['failures']
Availability,"ecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:N",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:4694,ERROR,ERROR,4694,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['ERROR'],['ERROR']
Availability,ed unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1798,recover,recoverWith,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['recover'],['recoverWith']
Availability,"eduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1777,down,down,1777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['down'],['down']
Availability,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1849,Error,Error,1849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1503,error,error,1503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,2,['error'],['error']
Availability,"er-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionAbortingState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5265,echo,echoHelloWorld,5265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,6,['echo'],['echoHelloWorld']
Availability,"ervice account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1196,error,error,1196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,1,['error'],['error']
Availability,"ete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionToken",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96248,heartbeat,heartbeat,96248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"example of this - in one of my tests i was generating ~1000 metadata requests per second. every single one of those in turn generated a message back to my original actor which was ignored, which would slow down the original actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566:206,down,down,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-270128566,1,['down'],['down']
Availability,"existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12970,down,downstream,12970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,2,['down'],['downstream']
Availability,"extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1331,Fault,FaultHandling,1331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,"ey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; }; `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:1803,alive,alive,1803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680,1,['alive'],['alive']
Availability,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:11398,down,downstream,11398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151,4,['down'],['downstream']
Availability,"flow:; ```. workflow Mutect2 {; input {; ; File tumor_reads; Int small_task_cpu = 2; Int small_task_mem = 4; Int small_task_disk = 100; Int boot_disk_size = 12; Int learn_read_orientation_mem = 8000; Int filter_alignment_artifacts_mem = 9000. # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk. # These are multipliers to multipler inputs by to make sure we have enough disk to accommodate for possible output sizes; # Large is for Bams/WGS vcfs; # Small is for metrics/other vcfs; Float large_input_to_output_multiplier = 2.25; Float small_input_to_output_multiplier = 2.0; Float cram_to_bam_multiplier = 6.0; }. Int preemptible_or_default = 2; Int max_retries_or_default = 2. # Disk sizes used for dynamic sizing; Int ref_size = 10; Int tumor_only_reads_size = 10; Int tumor_reads_size = tumor_only_reads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_ro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:1087,down,downloads,1087,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['down'],['downloads']
Availability,forms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefiniti,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:5393,Error,ErrorOr,5393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; }; [2018-11-21 15:09:39,37] [info] WorkflowManagerActor WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47 is in a terminal state: WorkflowSucceededState; [2018-11-21 15:09:43,77] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test.hello.response"": ""s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello/helloWorld.txt""; },; ""id"": ""02306258-436a-4372-ab54-2dcd83c42b47""; }; [2018-11-21 15:09:44,59] [info] Workflow polling stopped; [2018-11-21 15:09:44,60] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] Aborting all running workflows.; [2018-11-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:4687,down,down,4687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['down'],['down']
Availability,"from @cjllanwarne:. > Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created.; Idea: occasionally run the check-alive command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436:179,alive,alive,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281#issuecomment-327807436,2,['alive'],['alive']
Availability,ft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinit,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:5443,Error,ErrorOr,5443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"h.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; last):; File ""/usr/local/share/google/google-cloud-sdk/bin/bo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400:1700,ERROR,ERROR,1700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298632400,1,['ERROR'],['ERROR']
Availability,"he Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File syste",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1639,echo,echo,1639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['echo'],['echo']
Availability,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:338,down,down,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,['down'],['down']
Availability,"his allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3522,error,error,3522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['error'],['error']
Availability,hmm so the whole subworkflow thing was a red 🐟 because the error message was so bad? 😬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347:59,error,error,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347,1,['error'],['error']
Availability,"hmm the same test failed on all 3 PAPI builds with a root cause of ""no space left on device"" 🤔 ; ```; failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798:128,Error,Error,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798,1,['Error'],['Error']
Availability,"host/port is nice when you think about autoscaling. having the config file need to be different on every machine will be a headache when you want an ELB to spin up more nodes based on load. in a non-container world, that's straightforward. In a container world, the hostname (as defined by /etc/hostname, which is what typical java apis use to determine hostname) is set to a unique container id. So even if you're running multiple containers on the same host on the same port (pre-NAT) you'll get unique IDs. ```; wm80b-899:~ $ echo $HOSTNAME; wm80b-899; wm80b-899:~ $ docker run -it ubuntu /bin/bash -i; root@62d62e5dc805:/# echo $HOSTNAME; 62d62e5dc805; root@62d62e5dc805:/# cat /etc/hostname; 62d62e5dc805; root@62d62e5dc805:/# ; ```. @jacmrob",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207:529,echo,echo,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371247207,2,['echo'],['echo']
Availability,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:65,error,error-message-the-job-was-aborted-from-outside-cromwell,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,10,"['error', 'failure']","['error', 'error-message-the-job-was-aborted-from-outside-cromwell', 'failure', 'failures']"
Availability,https://gatkforums.broadinstitute.org/firecloud/discussion/12490/getting-lots-of-papi-error-code-10-message-14-errors,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-424963752:86,error,error-code-,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-424963752,2,['error'],"['error-code-', 'errors']"
Availability,"ializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99548,failure,failure,99548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['failure'],['failure']
Availability,"in my view it is really quite important. so the sooner the better. there's a ton of stuff to work on, so its absence isn't a dealbreaker but; having it and making it easy for anvil users to take advantage of it, will; really; help the project. On Thu, Feb 28, 2019 at 4:25 PM Jeff Gentry <notifications@github.com>; wrote:. > @vjcitn <https://github.com/vjcitn> For the context of AnVIL, what's the; > preferred timeline for this sort of functionality?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468444538>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEaOwjOad-k6dtLtSVihKWltyAqGrF_Jks5vSEligaJpZM4a39vl>; > .; >. -- ; The information in this e-mail is intended only for the person to whom it ; is; addressed. If you believe this e-mail was sent to you in error and the ; e-mail; contains patient information, please contact the Partners Compliance ; HelpLine at; http://www.partners.org/complianceline ; <http://www.partners.org/complianceline> . If the e-mail was sent to you in ; error; but does not contain patient information, please contact the sender ; and properly; dispose of the e-mail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468446743:931,error,error,931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468446743,2,['error'],['error']
Availability,"ing from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3445,echo,echoHelloWorld,3445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"ion.AbstractIterator.foreach(Iterator.scala:1194) ~[cromwell.jar:0.19]; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[cromwell.jar:0.19]; at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:161) ~[cromwell.jar:0.19]; at slick.dbio.DBIOAction$$anon$1.run(DBIOAction.scala:158) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-04-26 18:26:09,846 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could not persist runtime attributes; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '163980-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:973) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3847) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3783) ~[cromwell.jar:0.19]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:6768,ERROR,ERROR,6768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['ERROR'],['ERROR']
Availability,"irectory, take the generated qsub command and try it on your cluster. Hopefully you get the same ""Illegal attribute"" error. Play around with the command until you get the correct syntax. From there we can get your Cromwell config setup such it transforms the `memory` attribute into a valid syntax. Some possible examples:. | Example qsub usage | Runtime Attribute | Description |; |------------------------|------------------------|-------------------------------------------------------------------------------------------------|; | `qsub -l mem=4.0GB …` | `Float memory_gb = 1` | decimal values allowed, units are two characters uppercase |; | `qsub -l mem=4g …` | `Int memory_gb = 1` | integer values only, no decimals, and units must be one character lowercase |; | `qsub -l mem=4000mb …` | `Int memory_mb = 1000` | integer values only, and it turns out gigabytes aren't even allowed as a unit, so use megabytes |. > I would like to use $PROJECT environment variable as the default value for raijin_project_id. Environment variables won't work within HOCON, but can be passed through down into the generated submit files. It will take a bit of escaping to get past WDL-draft2, as both POSIX and WDL-draft2 both use `${...}` for variable names. To escape past WDL-draft2, create two new runtime attributes and then use them in your submit. Example:; ```HOCON; runtime-attributes = """"""; String env_start=""${""; String env_end=""}""; # other variables here; """""". submit = """"""; qsub \; -P ${env_start}PROJECT:-raijin_project_id${env_end} \; ...; """"""; ```. > jobfs is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Unfortunately you cannot define a parameter as rich as `memory`. For custom attributes one only has the choice of `Float`, `Int`, or `String`. If you don't like `String`, you could use a `Float` and have the WDL use `runtime { jobfs_gb: 4.0 }`, or just `runtime { jobfs: 4.0 }` and tell everyone to always using gigabytes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439:1698,down,down,1698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-492892439,1,['down'],['down']
Availability,is this a zero downtime migration?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472122237:15,downtime,downtime,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472122237,1,['downtime'],['downtime']
Availability,"ist.scala:335); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.generateAwsBatchOutputs(AwsBatchAsyncBackendJobExecutionActor.scala:255); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:141); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:934); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:926); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:75); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); ```. It's not the same issue as you other issue because 1) I'm not manipulating any file name in my task and 2) if you look at the `gatk` command, it correctly used the right localized file path. The error itself is something about the mount point:. ```; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/v",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:4242,robust,robustExecuteOrRecover,4242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,1,['robust'],['robustExecuteOrRecover']
Availability,"it doesn't have to be in scatter. Basically, workflows or tasks finished successfully but there was something unexpected (no error msg in any kind of log files) while cromwell was copying outputs into bucket. At the end, cromwell actually did not copy outputs but created empty files in bucket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413221264:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413221264,1,['error'],['error']
Availability,"ith a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1831,ERROR,ERROR,1831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['ERROR'],['ERROR']
Availability,ker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$14(WdlDraft2WomGraphMaker.scala:98); 	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDra,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4327,Error,ErrorOr,4327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"kflow ID 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:34,999] [info] WorkflowActor [9cdf23a5]: ExecutionStoreCreated(Start) message received; [2016-01-25 18:25:35,11] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-9cdf23a5-1eaa-420a-8fae-ea3e4623d4db#-1942530845],Submitted); [2016-01-25 18:25:35,12] [info] WorkflowActor [9cdf23a5]: Beginning transition from Submitted to Running.; [2016-01-25 18:25:35,16] [info] WorkflowActor [9cdf23a5]: transitioning from Submitted to Running.; [2016-01-25 18:25:35,17] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-25 18:25:35,19] [info] WorkflowActor [9cdf23a5]: starting calls: w.hello; [2016-01-25 18:25:35,21] [info] WorkflowActor [9cdf23a5]: persisting status of hello to Starting.; [2016-01-25 18:25:35,174] [info] WorkflowActor [9cdf23a5]: inputs for call 'hello':; addressee -> WdlString(String); [2016-01-25 18:25:35,177] [info] WorkflowActor [9cdf23a5]: created call actor for hello.; [2016-01-25 18:25:35,186] [info] WorkflowActor [9cdf23a5]: persisting status of hello to Running.; [2016-01-25 18:25:35,231] [info] LocalBackend [9cdf23a5:hello]: echo ""Hello String!""; [2016-01-25 18:25:35,279] [info] LocalBackend [9cdf23a5:hello]: command: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/9cdf23a5-1eaa-420a-8fae-ea3e4623d4db/call-hello/script | /bin/bash <&0""; [2016-01-25 18:25:35,305] [info] LocalBackend [9cdf23a5:hello]: Return code: 0; [2016-01-25 18:25:35,377] [info] WorkflowActor [9cdf23a5]: persisting status of hello to Done.; [2016-01-25 18:25:35,413] [info] WorkflowActor [9cdf23a5]: Beginning transition from Running to Succeeded.; [2016-01-25 18:25:35,440] [info] WorkflowActor [9cdf23a5]: transitioning from Running to Succeeded.; {; ""w.hello.salutation"": ""Hello String!""; }; [2016-01-25 18:25:35,505] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; $; ```. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363:2824,echo,echo,2824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363,1,['echo'],['echo']
Availability,"kka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(Sta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3753,error,error,3753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['error'],['error']
Availability,"la):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirection",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1311,echo,echo,1311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['echo'],['echo']
Availability,"la:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures[%]%:failure' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(D",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2752,failure,failures,2752,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,5,['failure'],"['failure', 'failures']"
Availability,la:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:3234,Robust,RobustClientHelperSpec,3234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:324,down,down,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902,1,['down'],['down']
Availability,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1141,alive,alive,1141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['alive'],['alive']
Availability,log from my run includes potentially useful information on the step. `[ERROR] [05/17/2018 15:47:23.959] [cromwell-system-akka.dispatchers.engine-dispatcher-48] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-67f5112e-5c3d-4a03-9c78-97725bf0d9cf/WorkflowExecutionActor-67f5112e-5c3d-4a03-9c78-97725bf0d9cf/67f5112e-5c3d-4a03-9c78-97725bf0d9cf-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_67f5112e-5c3d-4a03-9c78-97725bf0d9cf:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-67f5112e-batch_for_variantcall:NA:1] Failed to hash null`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-389918760:71,ERROR,ERROR,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-389918760,1,['ERROR'],['ERROR']
Availability,looks like a missed `NonEmptyList` opportunity for `RunnerData#failures`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615#issuecomment-255774602:63,failure,failures,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615#issuecomment-255774602,1,['failure'],['failures']
Availability,"low and low, other than ""it's another thing to maintain, document, and such"". one thing i'll throw out there, one could do this if there maximum desired note size was smaller than the maximum label size. At the moment that number is 63 characters. . Note (pardon the pun) that just increasing that limit comes with other headaches but we can wait until we go down that path before discussing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326660895:359,down,down,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1679#issuecomment-326660895,1,['down'],['down']
Availability,luation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; wdl4s.exception.VariableLookupException: Couldn't resolve all inputs for dna_mapping_38.libraryMerge at index Some(0).:; Input evaluation for Call dna_mapping_38.libraryMerge failed.:; 	inputBams:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	outputBam:; 	Could not find the shard mapping to this scatter dna_mapping_38.$scatter_1; 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:49); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$resolveAndEvaluateInputs$1.applyOrElse(JobPreparationActor.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.resolveAndEvaluateInputs(JobPreparationActor.scala:48); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor$$anonfun$receive$1.applyOrElse(JobPreparationActor.scala:27); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.CallPreparationActor.aroundReceive(JobPreparationActor.scala:18); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:3357,Failure,Failure,3357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['Failure'],['Failure']
Availability,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2553,error,error,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['error'],['error']
Availability,"medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1440,error,error,1440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,2,['error'],['error']
Availability,"metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1072,failure,failures,1072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,2,['failure'],['failures']
Availability,"munshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1133,failure,failures,1133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,5,['failure'],"['failure', 'failures']"
Availability,"mwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:700) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; ... 54 common frames omitted; ```. and. ```; 2016-08-03 03:33:06,985 cromwell-system-akka.actor.default-dispatcher-3 WARN - Caught exception, retrying: Broken pipe; java.net.SocketException: Broken pipe; at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_72]; at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109) ~[na:1.8.0_72]; at java.net.SocketOutputStream.write(SocketOutputStream.java:153) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_72]; at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:876) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:847) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:6745,down,down,6745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['down'],['down']
Availability,"mwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to constructor java.nio.DirectByteBuffer(long,int); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:62); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(Abst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:3194,ERROR,ERROR,3194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['ERROR'],['ERROR']
Availability,"n to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1257,error,error,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['error'],['error']
Availability,n$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:4647,Error,ErrorHandling,4647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Error'],['ErrorHandling']
Availability,n$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:4384,Error,ErrorHandling,4384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,2,['Error'],['ErrorHandling']
Availability,"n(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:579); 	... 35 common frames omitted; ```. The above stack trace was consuming the actual error:; `NoSuchFileException:<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; `. Somewhere while resolving host to call root, instead of returning the path to inputs as `centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir`, it resolves the path by prefixing the call root context path, which is `<path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution`. As a result, it tries to look for inputs inside the `execution` folder rather than inside `centaur/.../testdir`. This test passes in Travis and it might have different folder structure, which could be the reason why it is failing locally and not on Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:4971,error,error,4971,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['error'],['error']
Availability,"nbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.ba",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1858,avail,available,1858,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['avail'],['available']
Availability,"nd.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:170); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:1744,error,error,1744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['error']
Availability,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2828,error,error,2828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,4,"['error', 'failure']","['error', 'failure']"
Availability,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4100,alive,alive,4100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['alive'],['alive']
Availability,nfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.scala:1129); at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16); at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213); at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210); at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208); at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795); at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793); at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:3295,Robust,RobustClientHelperSpec,3295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"ng(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1449,error,error,1449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,2,['error'],['error']
Availability,"nitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a99-b386-5931ae245324 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); at cromwell.engine.workflow.lifecycle.materialization.Materializ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:2498,error,error,2498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['error'],['error']
Availability,"nly once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or `None`, then it will read the file only if it is `None`. This is ugly, but it will work.; I hope I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:1116,error,error,1116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809,1,['error'],['error']
Availability,"ntaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2095,alive,alive,2095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['alive'],['alive']
Availability,"oad ""gatk4-data-processing"" pipeline on their sample data, on Google Cloud. Repository with their code: https://github.com/gatk-workflows/gatk4-data-processing. The only change I made to the .wdl was setting Pre-emption to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:992,ERROR,ERROR,992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['ERROR'],['ERROR']
Availability,obExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:644); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad ho,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3376,robust,robustExecuteOrRecover,3376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['robust'],['robustExecuteOrRecover']
Availability,"oin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:7183,error,errors,7183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['error'],['errors']
Availability,"ok, lets defer this one till after 0.20 (it's a robustness problem, not a fire)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229824713:48,robust,robustness,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229824713,1,['robust'],['robustness']
Availability,"okay awesome, thanks muchly! I have seen readthedocs have (rather silent) build errors, so it's very good to have a develop --> stable branch flow. It will be much appreciated as I read about cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402804431:80,error,errors,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402804431,1,['error'],['errors']
Availability,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:575,avail,available,575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110,2,['avail'],['available']
Availability,"ollowing workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same wa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1007,echo,echo,1007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,1,['echo'],['echo']
Availability,"om/engine/reference/commandline/run/#capture-container-id---cidfile) through `--dockercid expected/path/to/dockercid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1438,error,errors,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['error'],['errors']
Availability,on the failures - the extra commit at the tail end (cromwell api) i threw in quickly last night after i submitted the PR w/ the intention of finishing it this morning. apparently it didn't even compile. Busted!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347276597:7,failure,failures,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347276597,1,['failure'],['failures']
Availability,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2672,recover,recoverWith,2672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['recover'],['recoverWith']
Availability,on0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379); at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461); at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750); at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749); at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14); at org.scalatest.Suite.run(Suite.scala:1147); at org.scalatest.Suite.run$(Suite.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:1876,Robust,RobustClientHelperSpec,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,2,['Robust'],['RobustClientHelperSpec']
Availability,"only allowing `read_int()` in the output section is an awful implementation and completely screws up the call caching (i.e. time saving/money saving). This is equivalent to have a function in python print a string and then using `int()` to convert it. Why can't we just return an integer inside the task and assign the integer in the output. Let's think a little bit longer during the ""how might implementing something, break down the line in the big picture"". This is more of a senior leader oversight than a coding implementation issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-1128138480:426,down,down,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-1128138480,1,['down'],['down']
Availability,"ontext. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or `None`, then it will read the file only if it is `None`. This is ugly, but it will work.; I hope I gave you an understanding of a problem. Maybe you can give me some advice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:1314,error,error,1314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809,1,['error'],['error']
Availability,"orkflows_test_files/common_snps_sample-chr20.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""8f0ef5140437"",; ""CNVSomaticPairWorkflow.intervals"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/chr20.interval_list"",; ""CNVSomaticPairWorkflow.normal_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam.bai"",; ""CNVSomaticPairWorkflow.bin_length"": ""10000"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/wgs-no-gc.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam.bai"",; ""CNVSomaticPairWorkflow.gatk4_jar_override"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/build/libs/gatk.jar""; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590:1792,down,downsampled,1792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590,2,['down'],['downsampled']
Availability,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:339,error,error,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,2,"['Error', 'error']","['Error', 'error']"
Availability,ping! Just wanted to check here to see if you need anything from me?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414675317:0,ping,ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414675317,1,['ping'],['ping']
Availability,"pinging @aednichols for re-review now that this is ""Look at Me"" able",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5719#issuecomment-671453063:0,ping,pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5719#issuecomment-671453063,1,['ping'],['pinging']
Availability,"pl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:934); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:926); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:75); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); ```. It's not the same issue as you other issue because 1) I'm not manipulating any file name in my task and 2) if you look at the `gatk` command, it correctly used the right localized file path. The error itself is something about the mount point:. ```; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; ```; I tried copying over the input files to a different path so that the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225:4582,robust,robustExecuteOrRecover,4582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436327225,1,['robust'],['robustExecuteOrRecover']
Availability,"r.scala:90) ~[cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. AND 8 instances of these:. ```; 2016-05-03 17:58:04,687 cromwell-system-akka.actor.default-dispatcher-18 INFO - JES Run [UUID(d3ba97c6):ValidateReadGroupSamFile:13]: Status change from Running to Success; 2016-05-03 17:58:04,820 cromwell-system-akka.actor.default-dispatcher-8 WARN - Caught exception, retrying: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 504 Gateway Time-out; {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:6838,error,errors,6838,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,1,['error'],['errors']
Availability,rSeqOptimized.scala:122); 	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); 	at scala.collection.immutable.List.foldLeft(List.scala:86); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:98); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$9(WdlDraft2WomScatterNodeMaker.scala:55); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.$anonfun$toWomScatterNode$7(WdlDraft2WomScatterNodeMaker.scala:52); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:51); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomScatterNodeMaker$.toWomScatterNode(WdlDraft2WomScatterNodeMaker.scala:15); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$Ops.toWomScatterNode$(WomScatterNodeMaker.scala:10); 	at wom.transforms.WomScatterNodeMaker$ops$$anon$1.toWomScatterNode(WomScatterNodeMaker.scala:10); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.buildNode$1(WdlDraft2WomGraphMaker.scala:90); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.$anonfun$toWomGraph$3(WdlDraft2WomGraphMaker.scala:38); 	at common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.foldFunction$1(WdlDraft2WomGraphMaker.scala:37,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:4552,Error,ErrorOr,4552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['Error'],['ErrorOr']
Availability,"ravis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1152,alive,alive,1152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['alive'],['alive']
Availability,"re slimming down conf, one doesn't need a large conf file. While many folks seem to go the route of copying in the entire reference.conf and making mods, I only ever include the exact bits that I'm tweaking and my conf files are pretty tight. That doesn't address the other issue. If only we had a tech writer joining our ranks soon .... ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081:12,down,down,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255498081,1,['down'],['down']
Availability,really weird. It looks like cromwell is able to run my docker container based on output and log files. It seems like I can ignore the error. seem like a bug to me. I spend a couple of hours trying convince my self it was working,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962:134,error,error,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962,1,['error'],['error']
Availability,"rence_bundle\"":\""bf51d668-3e14-4843-9bc7-5d676fdf0e01\"",\""AdapterSs2RsemSingleSample.rrna_interval\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/hg19.rRNA.interval_list\"",\""AdapterSs2RsemSingleSample.rsem_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/rsem_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.runtime_environment\"":\""dev\"",\""AdapterSs2RsemSingleSample.ref_flat\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/refFlat.txt\"",\""AdapterSs2RsemSingleSample.format_map\"":\""gs://broad-dsde-mint-dev-teststorage/format_map_example.json\"",\""AdapterSs2RsemSingleSample.bundle_uuid\"":\""c59a5720-ca82-429d-9d5b-6116987e221d\"",\""AdapterSs2RsemSingleSample.ref_fasta\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Hg19.fa\"",\""AdapterSs2RsemSingleSample.run_type\"":\""run\"",\""AdapterSs2RsemSingleSample.bundle_version\"":\""2017-09-20T211432.976293Z\"",\""AdapterSs2RsemSingleSample.gtf\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/Gencode_v19/Gencode_v19.GTF\"",\""AdapterSs2RsemSingleSample.retry_seconds\"":1E+1,\""AdapterSs2RsemSingleSample.method\"":\""Ss2RsemSingleSample\"",\""AdapterSs2RsemSingleSample.dss_url\"":\""https://dss.staging.data.humancellatlas.org/v1\"",\""AdapterSs2RsemSingleSample.submit_url\"":\""http://api.ingest.staging.data.humancellatlas.org/\"",\""AdapterSs2RsemSingleSample.star_genome\"":\""gs://broad-dsde-mint-dev-teststorage/reference/Hg19_kco/star_hg19_gencode_v19.tar.gz\"",\""AdapterSs2RsemSingleSample.schema_version\"":\""v3\""}"",; ""labels"": ""{}""; },; ""calls"": {},; ""outputs"": {},; ""id"": ""f1ccad5e-73d4-4905-b62f-81ab96dded19"",; ""inputs"": {},; ""submission"": ""2017-12-14T17:16:01.748Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ""end"": ""2017-12-14T17:16:20.791Z"",; ""start"": ""2017-12-14T17:16:20.747Z""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:5686,failure,failures,5686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,1,['failure'],['failures']
Availability,rent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:983); at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:977); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:2312,robust,robustPoll,2312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['robust'],['robustPoll']
Availability,"restarted your build which failed due to CROM-6791, the leading cause of my build failures...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6467#issuecomment-897794038:82,failure,failures,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6467#issuecomment-897794038,1,['failure'],['failures']
Availability,"rics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 3589863- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2587,down,down,2587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['down'],['down']
Availability,"right now that's not the default in FC, nor do we expose it in the UI - people have used it and it does help for some circumstances where you need it, but it seems like overkill when all you want is reliable statuses. it also won't help with the aborting issue which is what the gatk post was",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869:199,reliab,reliable,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334489869,1,['reliab'],['reliable']
Availability,"rkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from ExecutingWorkflowState to WorkflowAbortingState; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3586,echo,echoHelloWorld,3586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"rom other scripts:; `file_copy_test.wdl`; ```; workflow WGS_BAM_to_GVCF {; 	String input_file = ""s3://bucket/file"". 	# Merge per-interval GVCFs; 	call MergeGVCFs {; 		input:; 			input_file = input_file; 	 }. 	# Outputs that will be retained when execution is complete; 	output {; 		File output_vcf = MergeGVCFs.output_vcf; 	}; }. #### TASKS ####. # Merge GVCFs generated per-interval for the same sample; task MergeGVCFs {; 	File input_file; String output_file_name = ""output.txt"". 	Int machine_mem_gb = 2; 	Int command_mem_gb = machine_mem_gb - 1. command {; echo ${input_file} > ${output_file_name}; }. 	runtime {; 		docker: ""ubuntu""; memory: ""${machine_mem_gb}G""; cpu: 1; 	}. 	output {; 		File output_vcf = ""${output_file_name}""; 	}; }; ```. `cromwell_options.json`; ```; {; ""final_workflow_outputs_dir"":""s3://3-bucket"",; ""use_relative_output_paths"":true,; ""final_workflow_log_dir"":""s3://s3-bucket/wf_logs""; }; ```. error:; ```; [2019-06-15 19:50:15,63] [error] WorkflowManagerActor Workflow c9dd69e1-121e-45bc-911f-92d6bb6a2074 failed (during FinalizingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/WGS_BAM_to_GVCF/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; Caused by: java.lang.IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:216); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:420); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:160); 	at cromwell.engine.io.nio.NioFlow.$an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435:1146,error,error,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435,2,['error'],['error']
Availability,"romwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); ```; I was trying to lift off how things were done with the Google/gcp resolution so added it in there to fix this issue. Is there a different ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1775,Failure,Failures,1775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['Failure'],['Failures']
Availability,"ropose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8924,down,down,8924,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['down'],['down']
Availability,"rrors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:2020,alive,alive,2020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['alive'],['alive']
Availability,"running into the same issue as well. `java -Dconfig.file=cromwell-aws.conf -jar cromwell-42.jar run file_copy_test.wdl --options cromwell-options.json`. Tried with V40,41,42 and same issues.. Quick test script hacked together from other scripts:; `file_copy_test.wdl`; ```; workflow WGS_BAM_to_GVCF {; 	String input_file = ""s3://bucket/file"". 	# Merge per-interval GVCFs; 	call MergeGVCFs {; 		input:; 			input_file = input_file; 	 }. 	# Outputs that will be retained when execution is complete; 	output {; 		File output_vcf = MergeGVCFs.output_vcf; 	}; }. #### TASKS ####. # Merge GVCFs generated per-interval for the same sample; task MergeGVCFs {; 	File input_file; String output_file_name = ""output.txt"". 	Int machine_mem_gb = 2; 	Int command_mem_gb = machine_mem_gb - 1. command {; echo ${input_file} > ${output_file_name}; }. 	runtime {; 		docker: ""ubuntu""; memory: ""${machine_mem_gb}G""; cpu: 1; 	}. 	output {; 		File output_vcf = ""${output_file_name}""; 	}; }; ```. `cromwell_options.json`; ```; {; ""final_workflow_outputs_dir"":""s3://3-bucket"",; ""use_relative_output_paths"":true,; ""final_workflow_log_dir"":""s3://s3-bucket/wf_logs""; }; ```. error:; ```; [2019-06-15 19:50:15,63] [error] WorkflowManagerActor Workflow c9dd69e1-121e-45bc-911f-92d6bb6a2074 failed (during FinalizingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/WGS_BAM_to_GVCF/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; Caused by: java.lang.IllegalArgumentException: copying directories is not yet supported: s3://s3bucket/c9dd69e1-121e-45bc-911f-92d6bb6a2074/call-MergeGVCFs/output.txt; 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:216); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:420); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMeth",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435:787,echo,echo,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-502394435,1,['echo'],['echo']
Availability,"s evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; as well as; ```; List(""/mnt/read/only/container:/mnt/read/only/container:""); ```; versus; ```; List(""/mnt/read/only/container:/mnt/read/only/container""); ```. Plausibly responsible party t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1443,Error,Error,1443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,1,['Error'],['Error']
Availability,"s going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:1030,error,errors,1030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,3,"['Error', 'error']","['Error', 'errors']"
Availability,"s.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately, I keep getting the same error multiple times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:3160,error,error,3160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['error'],['error']
Availability,"s.engine-dispatcher-20 INFO - WorkflowExecutionActor [UUID(aed1aad8)]: Abort received. Aborting 1 EJEAs; 2016-09-09 15:51:00,402 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionInProgressState to WorkflowExecutionAbortingState.; 2016-09-09 15:51:00,416 cromwell-system-akka.dispatchers.backend-dispatcher-29 ERROR - Unexpected message KvKeyLookupFailed(KvGet(ScopedKey(aed1aad8-588d-4f84-aa09-da0f663d68c0,KvJobKey(printHelloAndGoodbye.echoHelloWorld,None,1),__jes_operation_id))).; 2016-09-09 15:51:01,316 INFO - JesRun [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JES Run ID is operations/EI6qg4TxKhid_JjDtaqaiegBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl; 2016-09-09 15:51:01,532 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from - to Initializing; 2016-09-09 15:51:39,435 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Initializing to Running; 2016-09-09 15:53:29,935 cromwell-system-akka.dispatchers.backend-dispatcher-29 INFO - $a [UUID(aed1aad8)printHelloAndGoodbye.echoHelloWorld:NA:1]: JesAsyncBackendJobExecutionActor [UUID(aed1aad8):printHelloAndGoodbye.echoHelloWorld:NA:1] Status change from Running to Success; 2016-09-09 15:53:31,525 cromwell-system-akka.dispatchers.engine-dispatcher-24 WARN - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] received an unhandled message: SucceededResponse(printHelloAndGoodbye.echoHelloWorld:-1:1,Some(0),Map()) in state: WorkflowExecutionA",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:5173,echo,echoHelloWorld,5173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"s: case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1, case_gatk_acnv_workflow.PlotACNVResults:3:1; [2016-10-28 14:38:18,14] [info] JesRun [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JES Run ID is operations/ENPH6N2AKxi-zoCK0M65gEAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:18,31] [info] JesRun [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.recei",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:13808,failure,failures,13808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['failure'],['failures']
Availability,"sa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3251,echo,echo,3251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['echo'],['echo']
Availability,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-10-23 17:49:24,53] [info] WorkflowManagerActor WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c is in a terminal state: WorkflowFailedState; [2018-10-23 17:49:27,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-10-23 17:49:32,16] [info] Workflow polling stopped; [2018-10-23 17:49:32,17] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Aborting all running workflows.; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7423,down,down,7423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,4,['down'],['down']
Availability,"seActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:25",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1577,Error,Error,1577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['Error'],['Error']
Availability,seems to have compile errors...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-580869825:22,error,errors,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-580869825,1,['error'],['errors']
Availability,"service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1662,error,errors,1662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,st.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Failed; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6716,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:4401,Error,ErrorHandling,4401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,2,['Error'],['ErrorHandling']
Availability,"st.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5127,Error,ErrorHandling,5127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['Error'],['ErrorHandling']
Availability,"stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:812,Failure,Failures,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852,1,['Failure'],['Failures']
Availability,"stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/129f0510-5d6b-4c4c-b266-116a9a52f325/call-CollectQualityYieldMetrics/shard-2/CollectQualityYieldMetrics-2.log""; },; ""start"": ""2016-04-24T15:50:19.000Z""; }. ```. Log stack trace: . ```; 3589853:2016-04-24 20:04:45,142 cromwell-system-akka.actor.default-dispatcher-16 INFO - JES Run [UUID(129f0510):CollectQualityYieldMetrics:2]: Status change from Running to Failed; 3589854:2016-04-24 20:04:45,145 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(129f0510):CollectQualityYieldMetrics:2]: Failing call: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589855:java.lang.Throwable: Task 129f0510-5d6b-4c4c-b266-116a9a52f325:CollectQualityYieldMetrics failed: error code 10. Message: 13: VM ggp-12606127296447203756 shut down unexpectedly.; 3589856- at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$handleFailure(JesBackend.scala:774) ~[cromwell.jar:0.19]; 3589857- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:685) ~[cromwell.jar:0.19]; 3589858- at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19]; 3589859- at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; 3589860- at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; 3589861- at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; 3589862- at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862:2400,down,down,2400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215222862,1,['down'],['down']
Availability,syncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:548); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:538); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 500 Internal Server Error; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:3156,Error,Error,3156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,2,['Error'],['Error']
Availability,"t of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1462,error,error,1462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,2,"['Error', 'error']","['Error', 'error']"
Availability,"t$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:10889,failure,failures,10889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['failure'],['failures']
Availability,"tHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from InitializingWorkflowState to ExecutingWorkflowState; 2016-09-09 15:50:58,130 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Starting calls: printHelloAndGoodbye.echoHelloWorld:NA:1; 2016-09-09 15:50:58,138 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - EJEA_aed1aad8:printHelloAndGoodbye.echoHelloWorld:-1:1: Effective call caching mode: CallCachingOff; 2016-09-09 15:50:58,139 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowExecutionActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: WorkflowExecutionActor [UUID(aed1aad8)] transitioning from WorkflowExecutionPendingState to WorkflowExecutionInProgressState.; 2016-09-09 15:51:00,401 cromwell-system-akka.dispatchers.engine-disp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:3012,down,down,3012,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['down'],['down']
Availability,"ta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks:; - name: local-disk; sizeGb: 10; sourceImage: ''; type: pd-ssd; labels:; cromwell-sub-workflow-name: wf-svcall-cwl; cromwell-work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1989,ERROR,ERROR,1989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['ERROR'],['ERROR']
Availability,"tch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:697); 	at scala.util.Try$.apply(Try.scala:209); 	... 25 more. [2019-02-13 22:18:20,91] [error] WorkflowManagerActor Workflow bc35173d-fde7-4727-8ae1-d4d3f132296c failed (during ExecutingWorkflowState): java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:3914,Error,Error,3914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Error'],['Error']
Availability,"tched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-07-27 16:48:51,320 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-07-27 16:48:51,631 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowInitializationActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: State is now terminal. Shutting down.; 2016-07-27 16:48:51,637 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from InitializingWorkflowState to ExecutingWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:1914,down,down,1914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544,1,['down'],['down']
Availability,"te(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-2a89a995-aa89-4172-a5e1-1054cbccd9e0#2034772397],Submitted); [2016-01-31 16:37:28,800] [info] WorkflowActor [2a89a995]: transitioning from Submitted to Running.; [2016-01-31 16:37:28,801] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-31 16:37:28,804] [info] WorkflowActor [2a89a995]: starting calls: w.hello; [2016-01-31 16:37:28,805] [info] WorkflowActor [2a89a995]: persisting status of hello to Starting.; [2016-01-31 16:37:28,959] [info] WorkflowActor [2a89a995]: inputs for call 'hello':; addressee -> WdlString(String); [2016-01-31 16:37:28,962] [info] WorkflowActor [2a89a995]: created call actor for hello.; [2016-01-31 16:37:28,970] [info] WorkflowActor [2a89a995]: persisting status of hello to Running.; [2016-01-31 16:37:28,996] [info] LocalBackend [2a89a995:hello]: echo ""Hello String!"" && kill -SIGINT $BASHPID; [2016-01-31 16:37:29,19] [info] LocalBackend [2a89a995:hello]: command: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0""; [2016-01-31 16:37:29,43] [error] CallActor [2a89a995:hello]: Failing call: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. java.lang.Throwable: Call w.hello, Workflow 2a89a995-aa89-4172-a5e1-1054cbccd9e0: return code was (none). Full command was: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/script | /bin/bash <&0"". Contents of cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054cbccd9e0/call-hello/stderr were empty. at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:246); at cromwell.engine.back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:3239,echo,echo,3239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,2,"['echo', 'error']","['echo', 'error']"
Availability,"terations; Int? num_smoothing_iterations_per_fit; String? output_dir; File? gatk4_jar_override. # Runtime parameters; String gatk_docker; Int? mem_gb; Int? disk_space_gb; Boolean use_ssd = false; Int? cpu; Int? preemptible_attempts. Int machine_mem_mb = select_first([mem_gb, 13]) * 1000; # ModelSegments seems to need at least 3GB of overhead to run; Int command_mem_mb = machine_mem_mb - 3000. # If optional output_dir not specified, use ""out""; String output_dir_ = select_first([output_dir, ""out/""]) . command <<<; set -e; mkdir ${output_dir_}; export GATK_LOCAL_JAR=${default=""/root/gatk.jar"" gatk4_jar_override}. gatk --java-options ""-Xmx${command_mem_mb}m"" ModelSegments \; --denoised-copy-ratios ${denoised_copy_ratios} \; --allelic-counts ${allelic_counts} \; ${""--normal-allelic-counts "" + normal_allelic_counts} \; --minimum-total-allele-count ${default=""30"" min_total_allele_count} \; --genotyping-homozygous-log-ratio-threshold ${default=""-10.0"" genotyping_homozygous_log_ratio_threshold} \; --genotyping-base-error-rate ${default=""0.05"" genotyping_base_error_rate} \; --maximum-number-of-segments-per-chromosome ${default=""1000"" max_num_segments_per_chromosome} \; --kernel-variance-copy-ratio ${default=""0.0"" kernel_variance_copy_ratio} \; --kernel-variance-allele-fraction ${default=""0.025"" kernel_variance_allele_fraction} \; --kernel-scaling-allele-fraction ${default=""1.0"" kernel_scaling_allele_fraction} \; --kernel-approximation-dimension ${default=""100"" kernel_approximation_dimension} \; --window-size ${sep="" --window-size "" window_sizes} \; --number-of-changepoints-penalty-factor ${default=""1.0"" num_changepoints_penalty_factor} \; --minor-allele-fraction-prior-alpha ${default=""25.0"" minor_allele_fraction_prior_alpha} \; --number-of-samples-copy-ratio ${default=100 num_samples_copy_ratio} \; --number-of-burn-in-samples-copy-ratio ${default=50 num_burn_in_copy_ratio} \; --number-of-samples-allele-fraction ${default=100 num_samples_allele_fraction} \; --number-of-burn-in-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669:24126,error,error-rate,24126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871669,1,['error'],['error-rate']
Availability,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:130,avail,available,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509,2,['avail'],['available']
Availability,"the old pebkac error, missed '@' in the curl command line :blush:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1799#issuecomment-268196742:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1799#issuecomment-268196742,1,['error'],['error']
Availability,"there are still config errors in this space, might want to hold off starting reviews until those are sorted out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552:23,error,errors,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729#issuecomment-394766552,1,['error'],['errors']
Availability,"tils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Delegating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:2676,failure,failures,2676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,2,['failure'],['failures']
Availability,"tions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1109,down,downstream,1109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,1,['down'],['downstream']
Availability,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1335,recover,recovery,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,2,['recover'],['recovery']
Availability,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:62,failure,failure,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344,6,['failure'],['failure']
Availability,"today, and I definitely have read / write access from my EC2 instance. I also can't see any Cromwell-execution folders in the bucket, but I do see the cromwell-workflow-logs on my EC2 instance. I created the AMI with the cromwell type, and I've checked that my IAM profile has access to the execution and storage bucket, and confirmed this in the CLI. . ```; Caused by: java.io.IOException: Could not read from s3://<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt: s3://s3.amazonaws.com/<bucket-name>/cromwell-execution/gatkRecalNormal/df58d76a-c3fe-4fb7-94c6-f4bd9ad1d5de/call-gatkBaseRecalibrator/gatkBaseRecalibrator-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkj",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651:1302,Failure,Failure,1302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-437251651,1,['Failure'],['Failure']
Availability,"tor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3f23e1f07040; [2018-11-21 15:09:14,62] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from - to Running; [2018-11-21 15:09:37,18] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: Status change from Running to Succeeded; [2018-11-21 15:09:39,33] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Workflow test complete. Final Outputs:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2953,echo,echo,2953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['echo'],['echo']
Availability,"torage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serviceAccounts.list; roles/iam.serviceAccountUser resourcemanager.projects.get; roles/iam.serviceAccountUser resourcemanager.projects.list; roles/lifesciences.workflowsRunner lifesciences.operations.cancel; roles/lifesciences.workflowsRunner lifesciences.operations.get; roles/lifesciences.workflowsRunner lifesciences.operations.list; roles/lifesciences.workflowsRunner lifesciences.workflows.run; roles/storage.objectAdmin resourcemanager.projects.get; roles/storage.objectAdmin resourcemanager.projects.list; roles/storage.objectAdmin storage.objects.create; roles/storage.objectAdmin storage.objects.delete; roles/storage.objectAdmin storage.objects.get; roles/storage.objectAdmin storage.objects.getIamPolicy; roles/storage.objectAdmin storage.objects.list; roles/storage.objectAdmin storage.objects.setIamPolicy; roles/storage.objectAdmin storage.objects.update; roles/storage.objectCreator resourcemanager.projects.get; roles/storage.objectCreator resourcemanager.projects.list; roles/storage.objectCreator storage.objects.create; roles/storage.objectViewer resourcemanager.projects.get; roles/storage.objectViewer resourcemanager.projects.list; roles/storage.objectViewer storage.objects.get; roles/storage.objectViewer storage.objects.list; ```; Somehow the [tutorial](https://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/) suggests to add roles `storage.objectCreator` and `storage.objectViewer` but these do not include one of the four permissions `storage.objects.delete`, `storage.objects.getIamPolicy`, `storage.objects.setIamPolicy`, or `storage.objects.update` that are further added when adding also role `storage.objectAdmin` and at least one of these must be further needed by Cromwell. Either than by trial and error, I still do not understand how users are supposed to understand this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:4417,error,error,4417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['error'],['error']
Availability,"tring job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:2447,Error,Error,2447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['Error'],['Error']
Availability,"ttempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2713,echo,echo,2713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['echo'],['echo']
Availability,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2512,down,downloads,2512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['down'],['downloads']
Availability,"uen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as CWL v1.0; [2018-11-09 10:25:19,60] [info] Pre-Processing /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl; [2018-11-09 10:25:30,04] [error] WorkflowManagerActor Workflow ec689f2a-c5a8-4c3a-9356-531b3cf0f2da failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_file_ec689f2a-c5a8-4c3a-9356-531b3cf0f2da.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/test/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.Va",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:2377,error,error,2377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['error'],['error']
Availability,"ularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6690,down,down,6690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['down'],['down']
Availability,ully run google_labels_good !!! IGNORED !!!; - should successfully run google_labels_sub !!! IGNORED !!!; - should successfully run gpu_cuda_image !!! IGNORED !!!; - should successfully run gpu_on_papi_valid !!! IGNORED !!!; - should successfully run http_inputs !!! IGNORED !!!; - should successfully run http_inputs_cwl !!! IGNORED !!!; - should successfully run input_expressions !!! IGNORED !!!; - should successfully run input_from_bucket_with_requester_pays !!! IGNORED !!!; - should successfully run inter_scatter_dependencies !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes !!! IGNORED !!!; - should successfully run invalidate_bad_caches_jes_no_copy !!! IGNORED !!!; - should successfully run jes_labels !!! IGNORED !!!; - should successfully run length_slurm_no_docker !!! IGNORED !!!; - should successfully run local_gcs !!! IGNORED !!!; - should successfully run monitoring_log !!! IGNORED !!!; - should successfully run monitoring_log_papiv1 !!! IGNORED !!!; - should successfully run papi_cpu_platform !!! IGNORED !!!; - should successfully run papi_v2_log !!! IGNORED !!!; - should successfully run papiv1_streams !!! IGNORED !!!; - should successfully run prepare_scatter_gather_papi !!! IGNORED !!!; - should successfully run refresh_token !!! IGNORED !!!; - should successfully run refresh_token_sub_workflow !!! IGNORED !!!; - should successfully run requester_pays_engine_functions !!! IGNORED !!!; - should successfully run requester_pays_localization !!! IGNORED !!!; - should successfully run super_massive_array_output !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv1 !!! IGNORED !!!; - should successfully run workbench_health_monitor_check_papiv2 !!! IGNORED !!!; - should successfully run workflow_type_and_version_cwl !!! IGNORED !!!; - should survive a Cromwell restart and recover jobs restart_jes_with_recover !!! IGNORED !!!; - should survive a Cromwell restart when a workflow was failing and recover jobs failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132:21770,recover,recover,21770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512361132,3,"['failure', 'recover']","['failures', 'recover']"
Availability,ute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(Run.scala:143) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.checkStatus(Run.scala:156) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1$$anonfun$42.apply(JesBackend.scala:933) ~[cromwell.jar:0.19]; at scala.util.Try$.apply(Try.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:933) [cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$poll$1.apply(JesBackend.scala:927) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [cromwell.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```. We suspect that of all the retries from the errors above plus the new succeeded tasks that now needed post processing created a backlog that may have cause Cromwell to become sluggish and eventually run out of memory. It may have also been the opposite direction where Cromwell becoming sluggish caused these errors to start popping up. @dshiga any additions to explain what we saw when you launched the 20k callset with 3000 intervals?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:11238,error,errors,11238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,2,['error'],['errors']
Availability,"uteTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); ```. Does anyone have any advice for repairing the db, or working out more what's happened?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:3119,repair,repairing,3119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['repair'],['repairing']
Availability,"ution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is becau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1477,echo,echo,1477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['echo'],['echo']
Availability,"vedTerminated(ActorCell.scala:374); #011at akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46); #011at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.aroundReceive(JesApiQueryManager.scala:26); #011at akka.actor.Actor$class.aroundReceive(Actor.scala:496); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$receive$1.applyOrElse(JesApiQueryManager.scala:51); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager.cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated(JesApiQueryManager.scala:101); #011at scala.collection.immutable.List.foreach(List.scala:381); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:101); #011at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$$anonfun$cromwell$backend$impl$jes$statuspolling$JesApiQueryManager$$handleTerminated$1.apply(JesApiQueryManager.scala:103); cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$JesApiException: Unable to complete JES Api Request; 2017-08-10 08:29:38,408 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow e3f4d391-a0bc-480f-9203-d0ef4ee5b876 failed (during ExecutingWorkflowState): Unable to complete JES Api Request; 2017-08-10 08:29:37,713 cromwell-system-akka.dispatchers.io-dispatcher-38 INFO - $e [UUID(fb9254a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81.log to gs://fc-35446f22-ea37-483a-bd6c-5e9fc56851ff/9ef84046-7047-423b-88e8-bb138181f5a8/workflow.logs/workflow.fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81.log; 2017-08-10 08:29:37,713 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowManagerActor WorkflowActor-fb9254a8-00ec-48bc-b8c8-2f0e5e74fa81 is in a terminal state: WorkflowFailedState```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390:2025,ERROR,ERROR,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-321847390,1,['ERROR'],['ERROR']
Availability,"w workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignments: printHelloAndGoodbye.echoHelloWorld -> Jes; 2016-09-09 15:50:56,282 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-09-09 15:50:56,286 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; 2016-09-09 15:50:56,326 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is transitioning from InitializationPendingState to InitializationInProgressState.; 2016-09-09 15:50:58,078 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowInitializationActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: State is now terminal. Shutting down.; 2016-09-09 15:50:58,084 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:2026,echo,echoHelloWorld,2026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['echo'],['echoHelloWorld']
Availability,"w workflows fetched; 2018-06-07 12:16:52,349 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Starting workflow UUID(dd0b1399-ebb6-4d9b-89ea-7da193994220); 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor Successfully started WorkflowActor-dd0b1399-ebb6-4d9b-89ea-7da193994220; 2018-06-07 12:16:52,353 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 12:16:52,362 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-07 12:16:52,443 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(dd0b1399)]: Parsing workflow as WDL draft-2; 2018-06-07 12:16:52,498 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - WorkflowManagerActor Workflow dd0b1399-ebb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$ada",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:98715,ERROR,ERROR,98715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['ERROR'],['ERROR']
Availability,wdl from the first error; [wdl.txt](https://github.com/broadinstitute/cromwell/files/238897/wdl.txt). wdl from the second error; [wdl2.txt](https://github.com/broadinstitute/cromwell/files/238908/wdl2.txt),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215127512:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215127512,2,['error'],['error']
Availability,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:39,failure,failures,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423,1,['failure'],['failures']
Availability,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:406,error,errors,406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,12,"['avail', 'error', 'failure']","['available', 'error', 'errors', 'failures']"
Availability,"what does the Y axis represent? Maximum requests/s? What is the takeaway? . It _looks_ like performance degraded slightly, no? So the question is whether to move ahead despite this? If that is the case I vote yes, let's move ahead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832:104,degraded,degraded,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832,1,['degraded'],['degraded']
Availability,what was the scenario that caused the failure (so we can reproduce)? bad credential for a remote db? botched config?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079:38,failure,failure,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128#issuecomment-230869079,1,['failure'],['failure']
Availability,"when trying to use `--imports tasks.zip` with Cromwell v31. I assume it is related to this issue. I can't seem to make it work with imports. I assume the parameter `--workflow-root` may be an alternative to using `imports`? However, instead of using zip imports, I tried using `--workflow-root` and still did not work. Thanks in advance for any help. Details of my scenario:. In my workflow I have:. ```; import ""tasks/task-1.wdl"" as task1; import ""tasks/task-2.wdl"" as task2; ```. my zip file looks like this:. ```; c4b301bb01ef:Desktop gonzalezma$ unzip -l tasks.zip ; Archive: tasks.zip; Length Date Time Name; -------- ---- ---- ----; 128 03-17-18 11:57 tasks/task-1.wdl; 119 03-17-18 11:57 tasks/task-2.wdl; -------- -------; 247 2 files; ```. Cromwell fails and says it can't find the task wdl files. How do I make this work? . The detailed error is as follows:. ```; [2018-03-17 14:23:42,03] [error] WorkflowManagerActor Workflow 6d61108d-3a3c-4850-8bd9-2862660f953c failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:168); 	at scala.runt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284:921,error,error,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284,1,['error'],['error']
Availability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5748,echo,echo,5748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['echo'],['echo']
Availability,"writing all of that crap we write to logs is a nontrivial performance impact, but we like logs so we've got to do it. we have a pretty vanilla logging setup, we could be *way* smarter about things in terms of impact to performance. this would help there. . risk of dropping is small and tunable, where as one tunes the risk down so goes the performance gain (and vice versa). one of those things where if you can live with the slight possibility that any particular specific log message never makes it you're AOK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294:324,down,down,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329791294,1,['down'],['down']
Availability,"xecutionStoreCreated(Restart) message received; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Beginning transition from Submitted to Running.; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: transitioning from Submitted to Running.; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: starting calls: GenomeStripBamWorkflow.ComputeMetadata, GenomeStripBamWorkflow.ComputeStatistics; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeStatistics to Starting.; 2016-04-26 18:26:09,657 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: persisting status of ComputeMetadata to Starting. 2016-04-26 18:26:09,845 cromwell-system-akka.actor.default-dispatcher-11 ERROR - WorkflowActor [UUID(ea0272fc)]: Could not persist runtime attributes; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '163979-preemptible' for key 'UK_RUNTIME_ATTRIBUTE'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:973) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3847) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3783) ~[cromwell.jar:0.19]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:2522,ERROR,ERROR,2522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['ERROR'],['ERROR']
Availability,"xecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```; However, this error occurs only about 80 percent of the time when I'm trying to run a job. 2. Cromwell doesn't localize sra files to the right level; You can see below ; `2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786`; but the workflow calls the file in my script; `mv: cannot stat ‘/cromwell_root/sra-SRR2806786/SRR2806786/SRR2806786’: No such file or directory`. Full log:; ```; 2020/08/25 17:32:38 Starting container setup.; 2020/08/25 17:32:43 Done container setup.; 2020/08/25 17:32:44 Starting localization.; 2020/08/25 17:32:51 Localization script execution started...; 2020/08/25 17:32:51 Localizing input gs://chip_dbgap/Mutect2/b749ec2f-c549-4d26-8fec-2fe271a37b75/call-renameBamIndex/script -> /cromwell_root/script; 2020/08/25 17:32:52 Localization script execution complete.; 2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786; 2020/08/25 17:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:3923,error,error,3923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['error'],['error']
Availability,"ye sorry about that, did a rollback and saved the other changes to this PR: https://github.com/broadinstitute/cromwell/pull/4205",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126:27,rollback,rollback,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126,1,['rollback'],['rollback']
Availability,"yeah that's a lot, we just found out about this issue a week ago while testing large scale for joint genotyping but looks like ""large scale"" is quickly becoming ""normal scale"".; Definitely keep filing those failure modes as they come up !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272:207,failure,failure,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060272,1,['failure'],['failure']
Availability,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,recover,recovering,1456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['recover'],['recovering']
Availability,"zon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8992,ERROR,ERROR,8992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['ERROR'],['ERROR']
Availability,"{cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of running containers without *any* privileges at all. @geoffjentry all this discussion is obviously going way beyond this original PR. Once we've settled on our recommendations, how do you think we should share this information with the Cromwell community? Is an example config in the Cromwell repo the best way (like this PR), or would it serve better to have a new page in the Cromwell documentation? I'm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:1388,down,download,1388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['down'],['download']
Availability,"~The values in `AdditionalRetryableHttpCodes` are evaluated against `gcs.getCode` not `gcs.getMessage`, so the fact the error copy is different shouldn't matter (so I _think_ it should work for you)~. nvm, obviously did not fully understand your message before replying 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544:120,error,error,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544,1,['error'],['error']
Availability,~closing & will reopen later - going to do the same for the PAPI codes~. Should have actually done it before making such a bold claim. Doesn't seem to be available in the jar,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319217009:154,avail,available,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2508#issuecomment-319217009,1,['avail'],['available']
Availability,"~~Am I correct in saying that this PR doesn't capture error messages coming from JES (polling, job creation, job failed...) ?~~; Nop, the error is bubbled up from JES Backend in `Call***Failure`s my bad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/475#issuecomment-190207686:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/475#issuecomment-190207686,3,"['Failure', 'error']","['Failure', 'error']"
Availability,👍 . I was trying to do this with @sooheelee using the data available from the google genomics wdl runner and it was impossible to figure out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202328:59,avail,available,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479#issuecomment-249202328,1,['avail'],['available']
Availability,👍 after comments are addressed and the _non-centaur_ builds are repaired. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682#issuecomment-262129570:64,repair,repaired,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682#issuecomment-262129570,1,['repair'],['repaired']
Availability,"👍 but it'd be nice to get travis to go green before merging, even if the current failure is unrelated to this change. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2121/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291:81,failure,failure,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291,1,['failure'],['failure']
Availability,"👍 from me. If you want, I can re-add failure retry support into my PR and then one of us can revisit it on the EJEA side. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1130/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230923488:37,failure,failure,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1130#issuecomment-230923488,1,['failure'],['failure']
Availability,👍 modulo the ultimate resolution of the error thing. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2832/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342872053:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342872053,1,['error'],['error']
Availability,👍 once centaur isn't downloading from an external repo. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1979/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280730117:21,down,downloading,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280730117,1,['down'],['downloading']
Availability,👍 👍 . This makes debugging failures so difficult. Even with includeKey and excludeKey.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321421217:27,failure,failures,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321421217,1,['failure'],['failures']
Deployability,	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingAct,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:4907,update,updateStatuses,4907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateStatuses']
Deployability,	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1363,pipeline,pipeline,1363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability," && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2227,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will pro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2625,configurat,configuration,2625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['configurat'],['configuration']
Deployability," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1540,pipeline,pipeline,1540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,2,['pipeline'],['pipeline']
Deployability," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,configurat,configuration,6188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,4,"['configurat', 'integrat']","['configuration', 'integrate']"
Deployability," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1079,pipeline,pipeline,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843,2,['pipeline'],['pipeline']
Deployability, Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.Batchin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1295,pipeline,pipelines,1295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability," \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4760,configurat,configuration,4760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configuration']
Deployability, com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1616,Pipeline,PipelinesApiRunCreationClient,1616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability," environment and was able to copy files into the cromwell executions bucket. Though something weird seems to be going on with the authentication because the instance appears to have write permissions for all the s3 buckets in the region, which appears to be due to the AmazonEC2RoleforSSM policy attached to the instance IAM:. ```; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssm:DescribeAssociation"",; ""ssm:GetDeployablePatchSnapshotForInstance"",; ""ssm:GetDocument"",; ""ssm:GetManifest"",; ""ssm:GetParameters"",; ""ssm:ListAssociations"",; ""ssm:ListInstanceAssociations"",; ""ssm:PutInventory"",; ""ssm:PutComplianceItems"",; ""ssm:PutConfigurePackageResult"",; ""ssm:UpdateAssociationStatus"",; ""ssm:UpdateInstanceAssociationStatus"",; ""ssm:UpdateInstanceInformation""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ssmmessages:CreateControlChannel"",; ""ssmmessages:CreateDataChannel"",; ""ssmmessages:OpenControlChannel"",; ""ssmmessages:OpenDataChannel""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2messages:AcknowledgeMessage"",; ""ec2messages:DeleteMessage"",; ""ec2messages:FailMessage"",; ""ec2messages:GetEndpoint"",; ""ec2messages:GetMessages"",; ""ec2messages:SendReply""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""cloudwatch:PutMetricData""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ec2:DescribeInstanceStatus""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""ds:CreateComputer"",; ""ds:DescribeDirectories""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""logs:CreateLogGroup"",; ""logs:CreateLogStream"",; ""logs:DescribeLogGroups"",; ""logs:DescribeLogStreams"",; ""logs:PutLogEvents""; ],; ""Resource"": ""*""; },; {; ""Effect"": ""Allow"",; ""Action"": [; ""s3:GetBucketLocation"",; ""s3:PutObject"",; ""s3:GetObject"",; ""s3:GetEncryptionConfiguration"",; ""s3:AbortMultipartUpload"",; ""s3:ListMultipartUploadParts"",; ""s3:ListBucket"",; ""s3:ListBucketMultipartUploads""; ],; ""Resource"": ""*""; }; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292:730,Update,UpdateAssociationStatus,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435109292,3,['Update'],"['UpdateAssociationStatus', 'UpdateInstanceAssociationStatus', 'UpdateInstanceInformation']"
Deployability," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,configurat,configuration,4564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['configurat'],['configuration']
Deployability," general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8579,pipeline,pipeline,8579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['pipeline'],['pipeline']
Deployability," keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3496,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2862,configurat,configuration,2862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['configurat'],['configuration']
Deployability," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10914,pipeline,pipeline,10914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['pipeline'],['pipeline']
Deployability," output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3823,Pipeline,Pipelines,3823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['Pipeline'],['Pipelines']
Deployability," pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1196,release,releases,1196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['release'],['releases']
Deployability," the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/jes/src/test/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActorSpec.scala: // TODO: PBE: This spec may run faster by going back to mocks? Also, building the actor ref is copy/pasted a lot; ./supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala: // TODO: PBE: The REST endpoint toggles this value... how/where? Meanwhile, we read it decide to use the cache...; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: This test needs work. If the abort fires to quickly, it causes a race condition in waitAndPostProcess.; ./supportedBackends/sfs/src/test/scala/cromwell/backend/sfs/SharedFileSystemJobExecutionActorSpec.scala: // TODO: PBE: abort doesn't actually seem to abort. It runs the full 10 seconsds, then returns the response.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:2112,toggle,toggles,2112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,2,['toggle'],['toggles']
Deployability," was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockCont",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2128,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:551,release,release,551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016,4,['release'],['release']
Deployability,"""Absence of evidence is not evidence of absence"", but still... ""proof"" these changes didn't make any thing worse in Jenkins, so far.; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/778/. Also [this log](https://travis-ci.org/broadinstitute/cromwell/jobs/445976070) shows a 10s timeout for the additionally patched `SimpleWorkflowActorSpec`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067:334,patch,patched,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4313#issuecomment-432909067,1,['patch'],['patched']
Deployability,"""submit-docker"" (sorry for reversal) is one of the configuration option in Cromwell config file. See eg. here how additional volumes are mounted (last section): https://davetang.org/muse/2019/12/24/execute-gatk-workflows-locally. In the same way, you can run docker command that passes `--privileged=true` option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239:51,configurat,configuration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239,1,['configurat'],['configuration']
Deployability,"# Re: Sublime support. ## Added support for if/then/else; Runtime was already present?. ## Easier Sublime Installation; In order to streamline Sublime installation, I [created a package](https://github.com/broadinstitute/wdl-sublime-syntax-highlighter) for their package manager Package Control. ## Requires moving those syntax highlighter files out of wdl repository; Sadly the package hosting mechanism requires the syntax files at the root of a repo and I didn't think the wdl repo would like that. As soon as [this PR](https://github.com/wbond/package_control_channel/pull/6579) is merged it should be available and much easier to install from Sublime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495:106,Install,Installation,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-329036495,3,"['Install', 'install']","['Installation', 'install', 'installation']"
Deployability,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1600,update,update,1600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119,2,['update'],['update']
Deployability,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527,2,['update'],['update']
Deployability,#466 Fixed one of the tests. Updated this PR to just tag the docker test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/467#issuecomment-187769722:29,Update,Updated,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/467#issuecomment-187769722,1,['Update'],['Updated']
Deployability,%)` | :arrow_down: |; | [...ala/wdl/draft2/model/WdlSyntaxErrorFormatter.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbFN5bnRheEVycm9yRm9ybWF0dGVyLnNjYWxh) | `70.19% <0%> (-0.67%)` | :arrow_down: |; | [.../scala/cromwell/database/slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2296,pipeline,pipelines,2296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,2,['pipeline'],['pipelines']
Deployability,"'${bundle_uuid}'\n version = '${bundle_version}'\n dss_url = '${dss_url}'\n retry_seconds = ${retry_seconds}\n timeout_seconds = ${timeout_seconds}\n print('Getting bundle manifest for id {0}, version {1}'.format(uuid, version))\n manifest_files = utils.get_manifest_files(uuid, version, dss_url, timeout_seconds, retry_seconds)\n\n print('Downloading assay.json')\n assay_json_uuid = manifest_files['name_to_meta']['assay.json']['uuid']\n assay_json = utils.get_file_by_uuid(assay_json_uuid, dss_url)\n\n sample_id = assay_json['sample_id']\n fastq_1_name = assay_json['seq']['lanes'][0]['r1']\n fastq_2_name = assay_json['seq']['lanes'][0]['r2']\n fastq_1_url = manifest_files['name_to_meta'][fastq_1_name]['url']\n fastq_2_url = manifest_files['name_to_meta'][fastq_2_name]['url']\n\n print('Creating input map')\n with open('inputs.tsv', 'w') as f:\n f.write('fastq_1\\tfastq_2\\tsample_id\\n')\n f.write('{0}\\t{1}\\t{2}\\n'.format(fastq_1_url, fastq_2_url, sample_id))\n print('Wrote input map')\n CODE\n >>>\n runtime {\n docker: \""humancellatlas/pipeline-tools:0.1.4\""\n }\n output {\n Object inputs = read_object(\""inputs.tsv\"")\n }\n}\n\nworkflow AdapterSs2RsemSingleSample {\n String bundle_uuid\n String bundle_version\n\n File gtf\n File ref_fasta\n File rrna_interval\n File ref_flat\n String star_genome\n String rsem_genome\n String reference_bundle\n\n # Submission\n File format_map\n String dss_url\n String submit_url\n String method\n String schema_version\n String run_type\n Int retry_seconds\n Int timeout_seconds\n\n # Set runtime environment such as \""dev\"" or \""staging\"" or \""prod\"" so submit task could choose proper docker image to use\n String runtime_environment\n\n call GetInputs as prep {\n input:\n bundle_uuid = bundle_uuid,\n bundle_version = bundle_version,\n dss_url = dss_url,\n retry_seconds = retry_seconds,\n timeout_seconds = timeout_seconds\n }\n\n call ss2.Ss2RsemSingleSample as analysis {\n input:\n fastq_read1 = prep.inputs.fastq_1,\n fastq_read2 = p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:1438,pipeline,pipeline-tools,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,1,['pipeline'],['pipeline-tools']
Deployability,"(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1580,update,update,1580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['update'],['update']
Deployability,(Updated) Brain dump on how this PR would be used:; https://drive.google.com/open?id=1MBsgFHhMtSZPK1Egfr4NCkH3C4zN5NDW,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3886#issuecomment-404051207:1,Update,Updated,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3886#issuecomment-404051207,1,['Update'],['Updated']
Deployability,"(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this m",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1254,configurat,configuration,1254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['configurat'],['configuration']
Deployability,"(user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4956,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3227,update,update,3227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['update'],['update']
Deployability,* `brew install git-secrets`; * for each repo; `git-secrets --install`; `git-secrets --add 'private_key'`; `git-secrets --add 'private_key_id'`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745:8,install,install,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-317758745,2,['install'],['install']
Deployability,* one must install `cwltool` to test the code and/or use CWL functionality; * show people how to use CWL parsing ; * give some pointers as to how to use coproducts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-335918217:11,install,install,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-335918217,1,['install'],['install']
Deployability,"**Status update**. tl;dr If one had to try to run the single sample pipeline on AWS, use a single i3.16xlarge spot instance, SUSE ECS optimized Linux, user data something like [this](https://github.com/broadinstitute/aws-backend-private/blob/master/scripts/suse-monster.sh), and otherwise follow the [lengthy script](https://docs.google.com/document/d/1qwY0QBo04WIAvsACbvtIshfxbs1OJRvNp93oclxCRk8/edit). But there’s still very little chance (like < 2%) that the workflow will succeed with full size BAMs. **SEGVs**. Amazon Linux ECS optimized (currently 64-bit amzn-ami-2016.09.g-amazon-ecs-optimized - ami-275ffe31) produces SEGVs with a consistency that makes it rare for workflows to survive to the MarkDuplicates step. No known workaround. Action: Don’t use Amazon Linux. **EFS**. EFS is currently several times slower for most tasks and orders of magnitude slower for MarkDuplicates. No known workaround. Action: Don’t use EFS, use a single-machine cluster. **NVMe drive disconnection**. SUSE Linux has a bug where the NVMe drives on i3 instance types (my preferred instance type with lots of fast instance storage) are randomly disconnected at boot time. Action: Live with it, harden the user data script to roll with whatever drives are available. Post in the forums. Nick reported this is a known problem and will be fixed upstream in SUSE eventually, but Amazon doesn’t control this AMI. **Random Docker CannotStartContainerErrors**. SUSE Linux consistently gets further in the workflow than Amazon Linux, but also consistently exhibits CannotStartContainerErrors deeper into the workflow. Action: Post in the forums. Although SUSE is more stable than Amazon Linux, there’s still not enough stability to run the workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531:9,update,update,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1542#issuecomment-292297531,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = … ++ assemblySettings ++ …`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:1073,release,releases-only,1073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,4,"['continuous', 'release']","['continuously', 'releases', 'releases-and-snapshots', 'releases-only']"
Deployability,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:2,Update,Update,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427,1,['Update'],['Update']
Deployability,"+1 to this feature. Is there a feature request submitted to the Cloud Health team to have the Pipelines API v2 distinguish regular preemption vs. preemption at 24 hours?. We needed to workaround this recently for an RNA alignment workflow (using STAR). What we did was to use the [timeout](https://linux.die.net/man/1/timeout) command:. ```; timeout 23.5h STAR <args>; ```. - We made it 23.5 hours to account for localization and delocalization time.; - By using the timeout, we got a hard-failure and avoided the ""run 24 hours and get preempted and retried"" cycle.; - We then manually re-ran these failures, setting the preemptible retries to 0 (and removing the timeout). . Managing the workflows in Terra made this all relatively painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563:94,Pipeline,Pipelines,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-499563563,1,['Pipeline'],['Pipelines']
Deployability,", METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3105,Update,UpdateVisitor,3105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['Update'],['UpdateVisitor']
Deployability,",983 INFO - changelog.xml: changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet: ChangeSet changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: Columns RESTARTED(BOOLEAN) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: Columns WORKFLOW_ROOT(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Column WORKFLOW_STORE_ENTRY.RESTARTED dropped; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizonta",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:89164,update,update-restartable,89164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,2,['update'],"['update-restartable', 'updated']"
Deployability,"- PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1731,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:44,configurat,configuration,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262,4,"['configurat', 'patch', 'update']","['configuration', 'patch', 'updates']"
Deployability,"-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4117,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4251,configurat,configuration,4251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configuration']
Deployability,"-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4429,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"-c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2046,pipeline,pipelines,2046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - ‘cromwell-36.jar’ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database db.url = jdbc:hsqldb:mem:254e87aa-251d-4bd6-bc6f-663624317535;shutdown=false;hsqldb.tx=mvcc; <snip>; [2018-11-09 10:25:19,54] [info] MaterializeWorkflowDescriptorActor [ec689f2a]: Parsing workflow as CWL v1.0; [2018-11-09 10:25:19,60] [info] Pre-Processing /tmp/cwl_temp_dir_2148913290991206234/cwl_temp_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:1301,release,releases,1301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['release'],['releases']
Deployability,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1746,release,releases,1746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,3,"['release', 'rollback']","['releases', 'rollback']"
Deployability,". task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.con",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3236,configurat,configuration,3236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['configurat'],['configuration']
Deployability,.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5400,update,updateStatuses,5400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateStatuses']
Deployability,.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockConte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5814,update,updateStatuses,5814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateStatuses']
Deployability,.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1921,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2472,pipeline,pipeline,2472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,".backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": 18.0,; ""drs_usa_jdr.hash1"": ""faf12e94c25bef7df62e4a5eb62573f5"",; ""drs_usa_jdr.cloud1"": ""gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc""; - should successfully run",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8596,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,".engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts wer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1277,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1837,pipeline,pipelines,1837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3349,pipeline,pipeline,3349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.R,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1894,pipeline,pipeline,1894,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(B,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2889,pipeline,pipeline,2889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1664,pipeline,pipelines,1664,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review f,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2974,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool.Main.main(Main.scala); ```. It would also be nice if the documentation included the fact that [`cwltool`](https://github.com/common-workflow-language/cwltool) needs to be installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:2173,install,installed,2173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['install'],['installed']
Deployability,"/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1521,install,installing,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,2,['install'],['installing']
Deployability,"0.4 is the latest release... according to github... (Yes, I realize that there have been tags since, but in the past, I had; been told to avoid these). On Tue, Jan 10, 2017 at 4:24 PM, Thib <notifications@github.com> wrote:. > It's expected that wdltool 0.4 will not validate this as the String; > main_output = hello_and_goodbye.hello_output syntax in workflow outputs; > was introduced specifically for sub workflows which wdltool 0.4 pre-dates.; > Try to update to the latest version of wdltool and it should validate.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2mhGzhFvb8rAvqTnkXnmX_L-KYAks5rQ_bwgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642:18,release,release,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271704642,2,"['release', 'update']","['release', 'update']"
Deployability,0003 7020 -2983; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...ne/src/main/scala/cromwell/engine/io/IoActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS9pby9Jb0FjdG9yLnNjYWxh) | `69.84% <100%> (-2.29%)` | :arrow_down: |; | [...ala/wdl/draft2/model/WdlSyntaxErrorFormatter.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbFN5bnRheEVycm9yRm9ybWF0dGVyLnNjYWxh) | `70.19% <0%> (-0.67%)` | :arrow_down: |; | [.../scala/cromwell/database/slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ=,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:1908,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1815,update,updateSchema,1815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['update'],['updateSchema']
Deployability,"07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: Columns RESTARTED(BOOLEAN) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: Columns WORKFLOW_ROOT(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Column WORKFLOW_STORE_ENTRY.RESTARTED dropped; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns CROMWELL_ID(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,993 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:89414,update,update-restartable,89414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['update'],['update-restartable']
Deployability,"0:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1642,update,updateSchema,1642,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['updateSchema']
Deployability,"20 is a very old patch version for Java 8. Oracle is up to 181 now, you might try upgrading the non-OpenJDK version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420675536:17,patch,patch,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420675536,1,['patch'],['patch']
Deployability,"286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionAc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1522,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.For,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1685,pipeline,pipelines,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,"3 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7907,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,7907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,30 has been released.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966#issuecomment-349111326:12,release,released,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966#issuecomment-349111326,2,['release'],['released']
Deployability,"4/19 PM Hi Evan, a patch went out to fix this at 10 AM this morning. Can you confirm that you no longer see this?. 4/19 AM Hi Evan - were you signed into the forum when you got this error? Can you send me the url of the page you were on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047:19,patch,patch,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382796047,1,['patch'],['patch']
Deployability,"415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatte",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5871,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,49 hotfix here: #5466,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606117531:3,hotfix,hotfix,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606117531,1,['hotfix'],['hotfix']
Deployability,"79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$po",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4632,configurat,configuration,4632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['configurat'],['configuration']
Deployability,"8972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3011,configurat,configuration,3011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configuration']
Deployability,9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; name: mutectMergedRawVCF-0; - localCopy:; disk: local-disk; path: firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; name: VEP_File-0; - localCopy:; disk: local-disk; path: exec.sh; name: exec; name: CallingGroup_Workflow; outputParameters:; - localCopy:; disk: local-disk; path: VEP_Task-rc.txt; name: VEP_Task-rc.txt; - localCopy:; disk: local-disk; path: dstat.log.txt; name: dstat.log.txt; - localCopy:; disk: local-disk; path: df.log.txt; name: df.log.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt; name: variant_effect_output.txt; - localCopy:; disk: local-disk; path: variant_effect_output.txt_summary.html; name: variant_effect_output.txt_summary.html; projectId: broad-firecloud-benchmark; resources:; bootDiskSizeGb: 10; disks:; - mountPoint: /cromwell_root; name: local-disk; sizeGb: 31; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; zones:; - us-central1-b; - us-central1-c; - us-central1-f; pipelineArgs:; clientId: ''; inputs:; VEP_File-0: gs://firecloud-tcga-open-access/tutorial/reference/my_dot_vep.zip; __extra_config_gcs_path: gs://cromwell-auth-broad-firecloud-benchmark/04b3f189-18f3-47b3-972c-0e59d2a56174_auth.json; exec: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/exec.sh; mutectMergedRawVCF-0: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-GatherAndOncotate_Task/MuTect1.call_stats.vcf; labels: {}; logging:; gcsPath: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a888840045d/CallingGroup_Workflow/04b3f189-18f3-47b3-972c-0e59d2a56174/call-VEP_Task/VEP_Task.log; outputs:; VEP_Task-rc.txt: gs://fc-58202a28-b82d-49da-a226-b9e14cf3c995/3a64e707-2544-409b-8a56-9a8888400,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145:3688,pipeline,pipelineArgs,3688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1860#issuecomment-273271145,1,['pipeline'],['pipelineArgs']
Deployability,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1438,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,": Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1846,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,": changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_JOB_STORE_ENTRY_WEU created; 2018-06-07 12:16:10,858 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WN created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_STORE_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,860 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: ChangeSet changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir ran successfully in 54ms; 2018-06-07 12:16:10,880 INFO - Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; 2018-06-07 12:16:10,896 INFO - [RenameWorkflowOptionsInMetadata] 100%; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: RenameWorkflowOptionsInMetadata complete.; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: ChangeSet changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet ran successfully in 17ms; 2018-06-07 12:16:10,898 INFO - chang",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:68783,update,updated,68783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['update'],['updated']
Deployability,":+1: . Yeah this is going to change a lot in the shadow world, but these changes do eliminate a lot of db stuff from the backend, integrate retries on the upserts, and test the heck out of restarts. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650:130,integrat,integrate,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650,1,['integrat'],['integrate']
Deployability,:+1: I like dis patch. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2499/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318449775:16,patch,patch,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318449775,1,['patch'],['patch']
Deployability,:+1: I particularly like @cjllanwarne's ADT suggestions but I'd prefer to do that in a separate PR and not gate the release on that. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1801/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-270474842:116,release,release,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-270474842,1,['release'],['release']
Deployability,:+1: glad you caught this before release! 😌 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1472/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1472#issuecomment-248970549:33,release,release,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1472#issuecomment-248970549,1,['release'],['release']
Deployability,":11,093 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Data deleted from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,093 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96077,release,released,96077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['release'],['released']
Deployability,":57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1845,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,":NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3148,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,":causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1382,Update,UpdateVisitor,1382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Update'],['UpdateVisitor']
Deployability,:frowning: Another argument for why we should integration test. Do we need to do a release?. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940,2,"['integrat', 'release']","['integration', 'release']"
Deployability,"; WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1632,update,update,1632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['update'],['update']
Deployability,"; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1681,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1888,configurat,configuration,1888,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configuration']
Deployability,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:494,update,updates,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,2,['update'],['updates']
Deployability,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:133,toggle,toggled,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038,1,['toggle'],['toggled']
Deployability,"> > Should this be documented in the Cromwell docs?; > ; > That's a good question, I'm not sure. It feels a little provisional right now - would we be free to change or remove it later once it was out there?. I'd suggest you discuss the Cromwell documentation strategy with the folks who have been here longer. I have updated it for bug fixes/changes, but those were more straightforward (with no future work planned).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6542#issuecomment-947647448:318,update,updated,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6542#issuecomment-947647448,1,['update'],['updated']
Deployability,"> @TimurIs - out of curiosity, where did you find that configuration option? I don't see it documented in the [example conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf). Examined the source code. Wanted to add a manual delay in the code, but, just by luck, found the reference to this config option",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443401652:55,configurat,configuration,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443401652,1,['configurat'],['configuration']
Deployability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:127,update,update,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['update'],['update']
Deployability,"> @grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s).; > ; > There probably _are_ clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward. Sounds good. But in this case, I'd suggest to update ticket description in order to eliminate discrepancy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140:567,update,update,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140,2,['update'],['update']
Deployability,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:37,integrat,integration,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198,1,['integrat'],['integration']
Deployability,> Actually one minor request: could you please rebase on `develop` and add an entry to the version 70 release notes for this added functionality with a credit to yourself. slightly_smiling_face. I will do it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936544171:102,release,release,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936544171,1,['release'],['release']
Deployability,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:74,integrat,integrating,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460,1,['integrat'],['integrating']
Deployability,"> Also have this error, using Cromwell 52, installed using this manual :; > ; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > ; > logs say : fetch_and_run.is is a directory. Extra info : cloning job & resubmitting through aws console runs fine. so it seems to be a temporary issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613:43,install,installed,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747603613,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,configurat,configuration,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167,4,"['configurat', 'update']","['configuration', 'update']"
Deployability,"> Are there a lot of users on SGE?. I would also ask the methods team, say ldgauthier or LeeTL1220. > It's probably more a ""we don't know how"" than ""if we did, it'd be a lot of work"". Yep, we are firmly in the camp of ""we don't know how"", with a heap of ""we never rtfm'ed'. There are a number of examples out there, and folks probably willing to help us, we just haven't prioritized this ticket. I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:895,install,installer,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,2,['install'],"['installed', 'installer']"
Deployability,"> Are there any config properties which you know of that might help with this?. Not that I can think of unfortunately :/ One quick thing we could maybe do before the fixing it ""the right way"" would be to enable retries at the GCS library level. We've disabled it because we have our own retry mechanism which is more reliable and asynchronous (but WDL functions couldn't use it so far, which is why they're not retried). We're about to release Cromwell 30 imminently so I don't think this can make it before then but we could consider hotfixing it if this is really becoming urgent. Edit: actually looking at it more closely, even though we don't supply an ""retry settings"" to the GCS library they have default ones allowing for 6 attempts. However like I said we've found their retries to not always be reliable so it might be that for some particular errors it's not retried at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774:436,release,release,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576#issuecomment-349031774,2,"['hotfix', 'release']","['hotfixing', 'release']"
Deployability,"> Did we update the akka http library in the last few days?. FYI, the import resolver, that during centaur tries to download the non-existent file from GitHub, [doesn't use akka-http](https://github.com/broadinstitute/cromwell/blob/49/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L191) like most of cromwell/cromiam/centaur/etc. It uses yet another jvm http client called [sttp](https://github.com/softwaremill/sttp#readme). Still, I think what you're running into is that GitHub changed their 404 response. See `curl -i https://raw.githubusercontent.com/broadinstitute/cromwell/develop/my_workflow`. I don't know how stable the change is either... they may change the body of the 404 response again [without notice](https://xkcd.com/1172/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242:9,update,update,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242,1,['update'],['update']
Deployability,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:504,install,installation,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984,1,['install'],['installation']
Deployability,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:207,update,update,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043,1,['update'],['update']
Deployability,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:264,configurat,configuration,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165,4,['configurat'],['configuration']
Deployability,"> FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied.; > ; > Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275:499,update,updated,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512635275,1,['update'],['updated']
Deployability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:56,release,release,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,1,['release'],['release']
Deployability,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. I built cromwell 88. Then I ran two large preemptible runs, one with cromwell 87 and the other with 88. The e2 with standard machines ran faster, and was pre-empted less, than the n1 with custom machines.; 11% faster, and 424 preemptions vs 276. Many of the 424 were preemptions that happened really early in the run process. So it may be that I just had bad luck with someone kicking off a large non-spot run while my 88 run was going but teh 87 was finished. But I've been advised that custom machine types are much more likely to be preempted than standard ones. Would it be possible for you to default to n1 or n2 standard machines, rather than custom ones?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654:56,release,release,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654,1,['release'],['release']
Deployability,"> For instance, now our build servers must have git secrets installed where it is irrelevant. @danbills That's not true - unless they want to commit code. This only asks them to configure a set of git hooks which they'll never end up using (or to add a compile time option which makes the compilation ignore the check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597:60,install,installed,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597,1,['install'],['installed']
Deployability,"> Have y'all tried womtool womgraph?. As I said above:. > The womgraph command still works, but the output from that command is so verbose it's unusable for viewing our workflows. `womgraph` may be helpful for simple workflows or debugging womtool but it's not a substitute for `graph` when viewing more complex workflows. . For example, here's the output for the GATK best practices exome pipeline. [exome.pdf](https://github.com/broadinstitute/cromwell/files/3933299/exome.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348:390,pipeline,pipeline,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348,2,['pipeline'],['pipeline']
Deployability,"> Hey @asmoe4; > ; > It would help to confirm that your Cromwell config contains a stanza that looks like:; > ; > ```; > engine {; > filesystems {; > s3 {; > auth = ""default""; > }; > }; > }; > ```. @ruchim -- Yes, I have the above configuration defined in my config file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916868:231,configurat,configuration,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916868,1,['configurat'],['configuration']
Deployability,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:775,pipeline,pipeline,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,4,['pipeline'],['pipeline']
Deployability,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1105,configurat,configuration,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081,8,['configurat'],['configuration']
Deployability,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:174,configurat,configuration-reference,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295,1,['configurat'],['configuration-reference']
Deployability,"> I see a warning when this runs: `Node.js 12 actions are deprecated...`. Looking into this. . It stems from the fact that we're using the olafurpg (sic) github action to install scala on the VM for us. That action/repo looks a bit stale, so I'm looking into better/newer ways to get java/sbt installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346:171,install,install,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346,2,['install'],"['install', 'installed']"
Deployability,> I updated this doc. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877275322:4,update,updated,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877275322,1,['update'],['updated']
Deployability,> I wasn't sure how the sbt magic you put in for the API docs worked. Pass the word on. Currently available both in the updated [wiki](https://github.com/broadinstitute/cromwell/wiki/DevZone#generating-a-markdown-document-of-the-swagger-yaml) and the more comprehensive mega doc-on-docs in our [team drive](https://drive.google.com/drive/u/1/folders/0By3wA7o30lk3VmF3aldBNkEzaDg). EDIT: Maybe we can leave a link at the top of the cromwell.yaml?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690:120,update,updated,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2936#issuecomment-347185690,1,['update'],['updated']
Deployability,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1536,pipeline,pipelines,1536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936,2,['pipeline'],['pipelines']
Deployability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:687,update,updated,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,2,['update'],['updated']
Deployability,"> If that isn't good enough (e.g. you need to do something to a before you scatter over the bs), you can put the inner scatter into a sub-workflow and Cromwell will be able to run it just fine. This is what I have implemented in order to get around this but it adds significant complexity to code. I agree that support for a nested scatter option is an excellent idea and would save pipeline writers a lot of time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-327008191:383,pipeline,pipeline,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-327008191,1,['pipeline'],['pipeline']
Deployability,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:120,pipeline,pipelines,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901,1,['pipeline'],['pipelines']
Deployability,"> Is it a huge overhead/burden to also turn on the draft-3 versions?. Whenever the originals get updated, these should (in theory) be kept in sync. The point of the CRON tests is to run as close as possible what the real world workflows are running. As many of the originals run in FC, I believe they should be draft-2 for now. If one wanted to additionally clone draft-3/1.0 versions I think that would be fine. ToL: A better version of the CRON tests would just point-to/reference the originals from the source with smaller inputs, instead of having clones in this git repo. EDIT: More specifically re: burden-- this PR is just trying to get the tests green and then move on. I personally don't know enough about ""what's an input, what's an input-with-defaults, what's a non-input-but-calculated-from-an-input"" to go through the hundreds of lines for a ""quick"" convert.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458:97,update,updated,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458,1,['update'],['updated']
Deployability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:18,Pipeline,Pipelines,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,4,['Pipeline'],['Pipelines']
Deployability,"> Is this run on all jobs? If so Is it potentially something a user would want to turn off?. Yes, it runs on all jobs by default. I'm not sure if users would explicitly want to turn it off, since it doesn't interfere with anything as far as I can tell, and the pricing issue is virtually non-existent. > Also did you produce that graph manually? Is there a way to generate it easily for a workflow?. Yep, the graph can be easily produced through Stackdriver monitoring console with a few clicks, or a link to it can be constructed programmatically and exposed to the user. The graph is interactive, so there's no need to ""pre-render"" it - it is constructed dynamically by the monitoring console, based on user inputs and/or the link. > Can you include your monitor python/image code in this PR? Would be easier to maintain that way. Sure! Is there a folder path you'd prefer to keep it at? Perhaps I could put it under `supportedBackends/google/pipelines/v2alpha1/src/monitoring`?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862:945,pipeline,pipelines,945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862,1,['pipeline'],['pipelines']
Deployability,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing 📖 (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:89,update,update,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755,2,['update'],"['update', 'updated']"
Deployability,> Just curious if putting a sleep [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L341) or [here](https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L198) are viable solutions until something more generic in the core of Cromwell can be implemented. Any update and interim solution available? I am curious if there is an alternative like python aiohttp concurrent requests number limit rather than sleep.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911:527,update,update,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-440734911,1,['update'],['update']
Deployability,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:307,install,installed,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508,1,['install'],['installed']
Deployability,"> Ok @Horneth , thank you for the explanation. With call-caching enabled is not so problematic starting the workflow again. Dear @lmtani , would you please give example how to use call-caching in GATK pipeline?. Here is my code:; gcloud \; alpha genomics pipelines run \; --pipeline-file /open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \; --inputs-from-file WDL=DNAseqPairedEndSingleSample_Fastq.wdl,\; WORKFLOW_INPUTS=sample1_gpd.inputs.json,\; WORKFLOW_OPTIONS=dna_variant_calling.options.json \; --env-vars WORKSPACE=gs://timdata/exomevcf/workspace,\; OUTPUTS=gs://timdata/exomevcf/result/sample1 \; --logging gs://timdata/exomevcf/logging/sample1.log --memory 5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450226579:201,pipeline,pipeline,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450226579,3,['pipeline'],"['pipeline', 'pipeline-file', 'pipelines']"
Deployability,"> Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. To clarify, is Cromwell retrying preemptibles the specified number of times and then running one more time on non-preemptible?. As of today that is the [expected behavior](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#preemptible) because it is assumed that a user isn't going to completely give up on their analysis just because it got interrupted repeatedly:. > Take an Int as a value that indicates the maximum number of times Cromwell should request a preemptible machine for this task before defaulting back to a non-preemptible one. A change to categorically disable this behavior would break existing users and can't merge, but what might work is a boolean runtime attribute that skips the regular VM. That said, the team must think carefully about increasing the configuration surface area of the product and I can't promise that such a PR would be accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179:895,configurat,configuration,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179,1,['configurat'],['configuration']
Deployability,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:71,configurat,configuration,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,3,"['configurat', 'release', 'upgrade']","['configuration', 'release', 'upgraded']"
Deployability,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:122,release,release,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204,2,['release'],['release']
Deployability,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:357,release,release,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655,1,['release'],['release']
Deployability,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140,2,['update'],['update']
Deployability,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:125,update,updated,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333,5,"['install', 'update']","['installing', 'update', 'updated']"
Deployability,"> That kind of confusion is exactly why WDL 1.0 (you're currently writing in draft-2) is adding input sections. Ie:. Wow! That is some great functionality.I love how WDL is continuously identifying real problems, and finding solutions for them in a readable, logical and easily to understand syntax. :+1:. Will this inputs section solve this bug? Presumably it will make it at least easier to solve it for the Cromwell developers?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386522096:173,continuous,continuously,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386522096,1,['continuous'],['continuously']
Deployability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:1114,rollback,rollbackException,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,1,['rollback'],['rollbackException']
Deployability,"> There's quite a bit of debate internally about this PR. Some team members remain deeply uncomfortable with how locking is handled, but it would take us a lot of time to research and recommend a better solution. If I may reiterate: by default this does not break anything for anyone. The locking only happens when `cached-copy` is set in the config consciously by the user. I maintain [my own patched jar for cromwell](https://github.com/rhpvorderman/cromwell/releases/tag/41-LUMC-patches), because this is taking very long already. We are running this in production and not experiencing problems. (There are only problems when the maximum number of hardlinks is reached, then cromwell defaults to copying again. It does not break anything, but it will use a lot of space on the filesystem and it will slowdown pipeline runs. I am working on a fix for that as well.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142:394,patch,patched,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504046142,4,"['patch', 'pipeline', 'release']","['patched', 'patches', 'pipeline', 'releases']"
Deployability,> This LGTM :) I'd ask that you add an entry to CHANGELOG.MD so that users know about this when we release the next version. Thank you. I added my bit to the changelog.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4405#issuecomment-440727214:99,release,release,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4405#issuecomment-440727214,1,['release'],['release']
Deployability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:461,upgrade,upgrades,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,2,['upgrade'],"['upgrade', 'upgrades']"
Deployability,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,configurat,configuration,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258,4,['configurat'],['configuration']
Deployability,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:24,Update,Updates,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,3,"['Update', 'hotfix']","['Updates', 'hotfix']"
Deployability,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:388,release,release,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['release'],['release']
Deployability,"> Why not use something like miniwdl?; > ; > run mode was originally created for **cromwell** development purposes, although for most of time there wasn't really an alternative for workflow development. Hi Geoff thanks for your suggestion. I have checked miniwdl but it has a docker dependency which does not fit my need for a sudo-less installation. The devs will certainly benefit from a quick REPL for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808:337,install,installation,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808,1,['install'],['installation']
Deployability,> codecov/patch — 100% of diff hit (target 50%). ... nice!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3379#issuecomment-371580539:10,patch,patch,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3379#issuecomment-371580539,1,['patch'],['patch']
Deployability,"> got an example of a fork in the liquibase scripts?. > One workaround that I'm already using in a couple of places is having a separate changeSet specific to postgres. See this link: http://www.liquibase.org/2009/03/what-effects-changeset-checksums.html. Those ""couple of places"" are likely the ""forks"" @geoffjentry was referring to. Additional changesets are fine, but ""adjusting the database migrations"" will add additional setup and test criteria regarding the MD5s. Here are two examples:. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/sync_not_null_constraints.xml#L20-L36. https://github.com/broadinstitute/cromwell/blob/70cff69799f149191bdaec5d8878dd2a3d5202b7/database/migration/src/main/resources/changesets/lengthen_wdl_value.xml#L5-L15. At minimum for the former changelog I suspect that fixes for Postgres (and [MariaDB](https://github.com/broadinstitute/cromwell/issues/4618) 🤔) will probably change the MD5s. As the link at the top says, there are workarounds to update/ignore the MD5s. But those workarounds will need to be implemented and CI tested-- along w/ [Postgres support](https://docs.travis-ci.com/user/database-setup/#postgresql).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616:1070,update,update,1070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475014616,1,['update'],['update']
Deployability,"> it is trying to resubmit jobs to the local engine. Do you mean jobs that were running when you stopped Cromwell were ""restarted"" on the local backend ?; Or new downstream jobs for the same workflow were then submitted to the local backend ?; Or both ?. I imagine you did not change your configuration in between the stop/start ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539:289,configurat,configuration,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4215#issuecomment-444536539,1,['configurat'],['configuration']
Deployability,"> it's mostly a mystery what the summarizer is up to. 🤔 If we didn't want to log updates, maybe yet-another graphite metric to see summarization throughput?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132:81,update,updates,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4757#issuecomment-475105132,1,['update'],['updates']
Deployability,> pinning to a known good version is a good idea regardless. Agreed and I'd even say that embedding the cwltool executable in Cromwell is what we should do. Or whatever makes sense such that when I download the latest Cromwell release and try to run a CWL I don't get an error because I don't have cwltool or I have the wrong version.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263:227,release,release,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992#issuecomment-411918263,1,['release'],['release']
Deployability,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,configurat,configuration,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550,2,['configurat'],['configuration']
Deployability,"> which will never leave the Running state? If so this was recently resolved in develop. Yes, it is great that it is fixed now. BTW, when do you plan to make a next release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2934#issuecomment-346841079:165,release,release,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2934#issuecomment-346841079,1,['release'],['release']
Deployability,"> …it is not strictly the case that every time \[expressionLib\] appears, it is ""next to"" a Parameter Context. Meant the other way around, Parameter Context is always ""next to"" a expressionLib. If it continues to be that way as CWL settles, one can always be update the Parameter Context via a future PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3140#issuecomment-358177285:259,update,update,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3140#issuecomment-358177285,1,['update'],['update']
Deployability,">. As you're aware there's a discussion on OpenWDL regarding the Directory type. closing in favor of that. The key question is when are you going to add Directory type to cromwell? I am quite pragmatic regarding cromwell, many tools in my pipeline require folders instead of files and I have to make it work somehow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383#issuecomment-414086585:239,pipeline,pipeline,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383#issuecomment-414086585,1,['pipeline'],['pipeline']
Deployability,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:231,update,updated,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123,1,['update'],['updated']
Deployability,">If you're ok with waiting until the next release (likely 31, potentially 30.3), it'll also be fixed for you. Everything depends on how soon you are agoing to publish 30.3 If it is a week, I am ok to wait, if it will take longer - I will build from source",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273:42,release,release,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273,1,['release'],['release']
Deployability,">Sorry for the confusion on this, hopefully it doesn't happen again. In the future if the jar needs to be modified after initial release, please bump the version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316312024:129,release,release,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316312024,1,['release'],['release']
Deployability,">This change appears to validate the format of the disk requirements but then do nothing with the actual values? Is that correct?. AWS has auto-sizing and auto-expanding disks, so the concept of specifying a disk size is not applicable in this universe. This PR lets Cromwell ignore everything after `local-disk` instead of issuing an error. >Can we update the test cases which now work? I suspect custom_mount_point at least could be re-enabled?. `custom_mount_point` is not on the excluded list in `testCentaurAws.sh`. Are you requesting new coverage by adding `awsbatch` to the backends for `custom_mount_point.test`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441:350,update,update,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485902441,1,['update'],['update']
Deployability,"? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1474,Install,Installing,1474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['Install'],['Installing']
Deployability,?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1420,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"@ALTree if you're referring specifically to PBS, that can now be supported with a little kludging purely through configuration. The only feature of PBS cluster that needs special treatment is the handling of stderr and stdout, which by default on PBS are copied to the execution directory only _after_ the job completes. [The config file in this gist](https://gist.github.com/delocalizer/fa29139675fb4118e908a4c80249dffb) works for me. Note that it requires that PBS_JOBDIR (the user's home directory by default, but can be a custom value) is shared across compute nodes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500:113,configurat,configuration,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254165500,1,['configurat'],['configuration']
Deployability,@DavyCats the fix will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-420797210:38,release,release,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-420797210,1,['release'],['release']
Deployability,@EvanTheB I've updated my PR with a fix and a new test for this problem in task outputs. Thanks again for the bug report!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403059207:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-403059207,1,['update'],['updated']
Deployability,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:41,configurat,configuration,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['configurat'],['configuration']
Deployability,"@EvanTheB `reference.conf` is the configuration file used by Cromwell.; > Also is there any way to actually enumerate all the available settings?. I am not sure how we can do that. But you are right, it might be a lot of effort to restructure/modify those files to be able to enumerate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687:34,configurat,configuration,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4776#issuecomment-478589687,1,['configurat'],['configuration']
Deployability,"@Ghost-in-a-Jar You're good to go, `can-i-deploy` is working now after I manually record latest `drshub-alpha` deployment in Pact Broker. Take a look at [https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets](https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets) with tag `0.47.0` here [https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27](https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949:42,deploy,deploy,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949,4,"['deploy', 'release']","['deploy', 'deployment', 'releases']"
Deployability,"@Horneth . > Is there an equivalent for JES runtime attributes validation that could need an update as well ?. Not that I can think of. JES's runtime attributes are [hardcoded](https://github.com/broadinstitute/cromwell/blob/23/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L19-L28) into the scala code. Meanwhile, these are the declarations of runtime attributes for the Config based backend. Via the config, one can specify the list of valid runtime attributes for _your_ backend, PBS, LSF, SGE, etc. See #1737 for an example of where this was broken.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049:93,update,update,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264924049,1,['update'],['update']
Deployability,@Horneth @cpavanrun ; Should be good to merge now. Only still added something to the change log and did fix a link in the release 36 change log ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432506335:122,release,release,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-432506335,1,['release'],['release']
Deployability,@Horneth @geoffjentry updated per suggestions,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/239#issuecomment-148785142:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/239#issuecomment-148785142,1,['update'],['updated']
Deployability,@Horneth Could you update w/ a description of what's being fixed/improved/etc here and/or link to an issue? I can see the code but need help wrapping my head around what it's all doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222,1,['update'],['update']
Deployability,@Horneth I came here to update the latest failure and noticed your comment. Considering that @kshakir noted Liquibase weirdness in #4320 perhaps these are related in some way?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823:24,update,update,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-435606823,1,['update'],['update']
Deployability,@Horneth Make sure the readme gets updated as appropriate,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-178212782:35,update,updated,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-178212782,1,['update'],['updated']
Deployability,"@Horneth Out of curiosity, do you have any real world numbers on the impact memory-wise? Is this only really going to be a big impact for huge WFs like in the JG case or should this also help out w/ installations like FC?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2542#issuecomment-322078309:199,install,installations,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2542#issuecomment-322078309,1,['install'],['installations']
Deployability,"@Horneth True, I can set that up quick to see how it goes. . If it does go well, this might end up being a good simple use case to start rolling w/ streams: https://groups.google.com/forum/#!topic/akka-user/v01YeU6Zb-o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690:137,rolling,rolling,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690,2,['rolling'],['rolling']
Deployability,@Horneth Updated to write start date.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/212#issuecomment-145131882:9,Update,Updated,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/212#issuecomment-145131882,1,['Update'],['Updated']
Deployability,@Horneth are there any more issues for this Epic? ; @gemlam3 do we have a release improvement selected for the next release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505:74,release,release,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-331550505,2,['release'],['release']
Deployability,@Horneth could you add a cache configuration option that will switch on caring about the filenames when caching?; Non-chaching of filenames can get many users in a really big trouble and sometimes screw whole research or medical diagnosis. If I did not discover that the files were not written because of caching my colleagues would have treated cow data as if it was human.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209:31,configurat,configuration,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351129209,1,['configurat'],['configuration']
Deployability,"@Horneth if I remember correctly it can be healthchecks. However, the best way to try is just to init swarm on your laptop and run your yml as a docker stack. Often they just ignore non-supported properties without crashes.; I personally use cromwell in a swarm settings ( https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml ), however, not froma real nead but more of convenience, as there is dockerswarmpit with a nice UI to manage services that does not work with compose",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4044#issuecomment-417479168:341,pipeline,pipelines,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4044#issuecomment-417479168,1,['pipeline'],['pipelines']
Deployability,@Horneth is this issue to auto-release wdltool when we release Cromwell?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298:31,release,release,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-332633298,2,['release'],['release']
Deployability,"@Horneth right, and we don't need to do that any more because we rebuild the entire execution store every time we restart. I've updated this to now have stable subworkflow names for those ""nested scatter"" subworkflows. Do we have an existing ""check subworkflows restart correctly"" test I can duplicate for draft 3?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372432202:128,update,updated,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372432202,1,['update'],['updated']
Deployability,@Horneth what else needs to happen for the release to be a push button process?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333240686:43,release,release,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333240686,1,['release'],['release']
Deployability,@Horneth why is it useful to have one github account for the release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333240777:61,release,release,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333240777,1,['release'],['release']
Deployability,"@IsanEmory thanks for the update, that's good to know. ; @ruchim can you tell me a bit more about what's going on to cause the message to show up?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786,1,['update'],['update']
Deployability,@KevinDuringWork: would you be willing to work with the team here to look at how we introduce this into the main branch and allow easier updates for you and others?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887634687:137,update,updates,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887634687,1,['update'],['updates']
Deployability,@LeeTL1220 ; The configuration path seems to be different from the one you have (don't know if / when / why it changed).; Here's the current location for default runtime attributes: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L531. I did check and this is honored,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048:17,configurat,configuration,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321086048,1,['configurat'],['configuration']
Deployability,"@LeeTL1220 Any more updates on this ticket?. In general we were wondering if a `gcloud logout` and then `gcloud login` helped. If this is no longer an issue, mind closing this one, and open another in the future with current wdl / details / version-info?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487:20,update,updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487,1,['update'],['updates']
Deployability,"@LeeTL1220 In the file you sent me the lines look like this `/local/cga-fh/cga/Lee_Normal_Analysis/Pair/CESC-HSCX1005-TP-NB--/jobs/capture/mut/oncotate/job.83173721/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated`. In the ""fixed"" one you sent me the lines look like this `/dsde/data/test_dl_oxoq/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated`. Further, even running it through `dos2unix` doesn't change this fact. Was `full_m1_oncotated_list_pc.txt` the actual file which caused the problem? If so could either this have been a GIGO situation or something else in the WDL run putting the wrong paths in your file? I find it hard to believe that those paths are what you meant. Whatever is going on here I don't believe it has to do with DOS-style newline chars as that doesn't seem to matter, even when I forcibly insert them. I'm closing this issue as one way or the other it appears to be a misnomer. However let's continue to followup either here or in person and potentially open a new issue w/ updated info.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415:1001,update,updated,1001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415,1,['update'],['updated']
Deployability,"@LeeTL1220 could you confirm that you are still unable to kill Cromwell with Ctl-C? If so, then I'll update this issue to add support for it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-325444341:101,update,update,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-325444341,1,['update'],['update']
Deployability,"@LeeTL1220 there is doc in the README and CHANGELOG (in this PR) on how to disable it.; For SGE, since it doesn't honor the docker runtime attribute I don't think there can be false positive because of that. ; On a backend that does honor the docker attribute, if a tag is used, then yes it can yield false positives if the tag is updated, since Cromwell won't lookup the hash.; There can be false negatives though on SGE, if you change the value of the docker attribute in the WDL, it won't call cache, although it could because SGE will ignore the docker value anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784:331,update,updated,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2139#issuecomment-292010784,1,['update'],['updated']
Deployability,"@LeeTL1220 thx for the info, I will update the reference.conf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321550028:36,update,update,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321550028,1,['update'],['update']
Deployability,@TMiguelT - I just release them today after a lot of much needed updates:; https://github.com/aws-samples/aws-genomics-workflows/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-469487155:19,release,release,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-469487155,2,"['release', 'update']","['release', 'updates']"
Deployability,@TMiguelT - I maintain these CloudFormation scripts and have published an update that should address your issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431601551:74,update,update,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-431601551,1,['update'],['update']
Deployability,@TMiguelT - This should be resolved in the latest update to the CloudFormation templates at:; https://docs.opendata.aws/genomics-workflows. The use of a Custom AMI is now deprecated in favor of EC2 Launch Templates.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-470335668:50,update,update,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-470335668,1,['update'],['update']
Deployability,@TMiguelT - Yes! I'm working on getting them approved for release. It would be great to have community support for improving them.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-432840559:58,release,release,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-432840559,1,['release'],['release']
Deployability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:648,configurat,configuration,648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,1,['configurat'],['configuration']
Deployability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:72,configurat,configuration,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['configurat'],['configuration']
Deployability,"@TMiguelT I'm largely channeling the thoughts [in this paper](https://f1000research.com/articles/7-742/v1). The main issue I have w/ `ENTRYPOINT` is that it makes things less explicit in terms of what's happening when one reads the workflow descriptor. . The other main argument which tends to come up is that this effectively locks the workflow to using the docker container, vs systems where one could ensure the appropriate software is installed. This one is definitely a real world use case but I feel less strongly about it as the use of the container is part of the selling point.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-438696562:439,install,installed,439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-438696562,1,['install'],['installed']
Deployability,@TMiguelT also the GATK docker was recently updated to be much smaller. . @vdauwera are there updated versions of the GATK workflows to use the smaller images?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434526633:44,update,updated,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434526633,2,['update'],['updated']
Deployability,"@TimurIs - out of curiosity, where did you find that configuration option? I don't see it documented in the [example conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443400527:53,configurat,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443400527,1,['configurat'],['configuration']
Deployability,"@abaumann is the issue that Cromwell doesn't update the state of the ""Running"" task, which has failed (or stopped), or that it keeps running that task, which should have failed (or stopped)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-290740467:45,update,update,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-290740467,1,['update'],['update']
Deployability,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:301,patch,patch,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,6,"['patch', 'update']","['patch', 'patches', 'updated']"
Deployability,"@adamstruck Thanks for the offer! I was using v0.2 release, and I'm guessing it's something cromwell is doing and not you guys :). I will look again later this week and check back in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332612226:51,release,release,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332612226,1,['release'],['release']
Deployability,"@aednichols . Thanks for your response. We're in a University and the sys admins are worried that with users submitting thousands of jobs, depending on what cromwell actually sends MySQL there may be quite a bit of overhead. Do you have a link to what Cromwell stores in the MySQL database? That may assuage some of their concerns. Using SQLite would just be easier, users can create a local instance and be on with it. @rhpvorderman . That sounds like a workable option. That sounds exactly like our situation, it would be great if you could keep us updated! It would definitely be very useful for us!. Thanks,; Bobbie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678:551,update,updated,551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678,1,['update'],['updated']
Deployability,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:82,integrat,integrate,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921,3,"['deploy', 'integrat', 'release']","['deployed', 'integrate', 'release']"
Deployability,@aednichols @cjllanwarne Any updates?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-823447669:29,update,updates,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-823447669,1,['update'],['updates']
Deployability,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:1148,pipeline,pipelines,1148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,1,['pipeline'],['pipelines']
Deployability,@aednichols I use development version (until recently cromwell:develop was more stable than latest release) and my cromwell container is based on cromwell:develop,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464256509:99,release,release,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555#issuecomment-464256509,1,['release'],['release']
Deployability,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:359,configurat,configuration,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017,1,['configurat'],['configuration']
Deployability,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:489,release,release,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735,4,['release'],['release']
Deployability,"@aednichols a couple of interesting points in that comment: 😉 . 1) IMHO there should be a hotfix for this considering the impact and narrowness of the code changes. I can do that once this is merged.; 2) I added an entry to the CHANGELOG.md, requesting re-review because words are hard.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511111210:90,hotfix,hotfix,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511111210,1,['hotfix'],['hotfix']
Deployability,"@aednichols and @mcovarr Thank you very much for the reviews. @aednichols Thank you very much for keeping me informed about the status of this PR, it is much appreciated. The code has been updated to reflect the comments. I also merged the latest branch of develop in and updated the changelog.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504301482:189,update,updated,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504301482,2,['update'],['updated']
Deployability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:91,install,installed,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,2,['install'],['installed']
Deployability,@aednichols thanks for the update. We are currently in the process of updating the WDL spec at https://github.com/openwdl/wdl/pull/243. This will hopefully make clear what sort of regular expression should be used. After that it will be a lot easier to decide whether the regular expression evaluation is broken or not.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749,2,['update'],['update']
Deployability,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:68,patch,patch,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481,3,"['patch', 'release']","['patch', 'release']"
Deployability,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:744,release,release,744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857,2,"['patch', 'release']","['patched', 'release']"
Deployability,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:202,integrat,integration,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868,1,['integrat'],['integration']
Deployability,"@alexiswl I agree that all the CWL tickets should have been closed ""as not planned"" given the public statements that CWL in Cromwell is deprecated (and likely to be removed). https://terra.bio/terras-roadmap-to-supporting-more-workflow-languages/. https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#last-release-with-cwl-support. https://github.com/broadinstitute/cromwell/releases/tag/79. ![Screenshot_20221027-113759](https://user-images.githubusercontent.com/1330696/198250098-fe5c53e4-115f-43e4-b334-2898a6e7c687.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120:322,release,release-with-cwl-support,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120,2,['release'],"['release-with-cwl-support', 'releases']"
Deployability,"@anton-khodak I'm glad to hear that you find the docs user-friendly, can you elaborate what you find particularly helpful? I want to make sure I preserve it as I review and update the docs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323:173,update,update,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323,2,['update'],['update']
Deployability,"@antonkulaga @cjllanwarne ; I have tested call-caching with hardlinks and cached-copy strategy. For hashing-strategy I used path+modtime. These were the results for the call-caching:. **It works!**. So this part of the docs should be updated indeed. I have no idea why it works though, so I am a bit hesitant to add it to the docs. @cjllanwarne Do you know if anything changed in the code base that made the call-caching work for hard links?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831:234,update,updated,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077#issuecomment-513136831,1,['update'],['updated']
Deployability,"@antonkulaga I will update Readme to point right configuration, however on `what should be given when running java -jar Cromwell.jar` it doesn't need any additional runtime attributes besides uncommenting Spark backend configuration from `reference.conf` and on `what should be put to wdl` is referred here : [WDL](https://github.com/broadinstitute/cromwell#sample-wdl)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283214134,3,"['configurat', 'update']","['configuration', 'update']"
Deployability,"@antonkulaga Internally we're not allowed to use docker on our unix machines for exactly this reason, so handling this problem was never prioritized by our product ownership. We had to deal with it for our work on the upcoming AWS backend, as @mcovarr pointed out. The 25 release should be out in a matter of days",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869:272,release,release,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283050869,1,['release'],['release']
Deployability,"@antonkulaga It appears that changing the `name` only changes the filename of the output produced by your task, but not the actual processing. So I bet if you look at the files produced by the diamond blast task for all 3 workflows you'll see that even though their names differ they have the same content.; Cromwell by default only cares about the content of a file with respect to call caching and its name is ignored. In this case it likely md5ed the files and found they had the same hash so the copy task was cached.; I'll wait for you to confirm that the output files have indeed the same hash before closing this:. ```; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/ab101af5-26ba-45c8-b592-fb37e06a523d/call-diamond_blast/execution/graywhale_in_human_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/3d75657d-7dc9-4b6f-bbc8-ae579a3fa773/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; md5 /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/276d6f9e-15b1-4dc3-a8a7-889414406511/call-diamond_blast/execution/graywhale_in_cow_blastp.m8; ```. should all produce the same hash",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450:632,pipeline,pipelines,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044#issuecomment-351125450,3,['pipeline'],['pipelines']
Deployability,@antonkulaga It'll probably be > 1 week. We can't predict the frequency of the dot releases as they're almost always in response to some fire that erupts in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955:83,release,releases,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445955,1,['release'],['releases']
Deployability,"@antonkulaga What @cjllanwarne means is that if you build off of the `develop` branch it should work for you. If you're ok with waiting until the next release (likely 31, potentially 30.3), it'll also be fixed for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367443014:151,release,release,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367443014,1,['release'],['release']
Deployability,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:157,pipeline,pipeline,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943,4,['pipeline'],['pipeline']
Deployability,"@breilly2 oops you're right, sorry 😬 I'll recycle your other non-Cromwell PRs for the rest of the hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219:98,hotfix,hotfix,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219,1,['hotfix'],['hotfix']
Deployability,"@byoo fixed, will be in soon-to-be released 31.1 and next major version 32",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3421#issuecomment-373764453:35,release,released,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421#issuecomment-373764453,1,['release'],['released']
Deployability,@cahrens as this is already merged I will add this as a TODO to the release ticket BT-509,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472:68,release,release,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472,1,['release'],['release']
Deployability,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:419,configurat,configuration,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237,2,['configurat'],['configuration']
Deployability,"@chapmanb Also can you explain what the different systems are? It looks like everyone is using the same configuration here, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025:104,configurat,configuration,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-387853025,1,['configurat'],['configuration']
Deployability,"@chapmanb Somatic completed successfully by bumping the memory (I doubled it to 8GB) :); I have another question about the rnaseq pipeline if you don't mind.; I'm hitting this error on the `pipeline_summary` task:. ```; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/cyvcf2/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .cyvcf2 import (VCF, Variant, Writer, r_ as r_unphased, par_relatedness,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (hashtable as _hashtable,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import algos, lib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import hashing, tslib; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; from pandas._libs import (lib, index as libindex, tslib as libts,; /usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88; import pandas._libs.tslibs.offsets as liboffsets; /usr/loca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:130,pipeline,pipeline,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,@chapmanb things lined up nicely to make this a pretty easy fix; it will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415427329:88,release,release,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023#issuecomment-415427329,1,['release'],['release']
Deployability,"@cjllanwarne , @curoli ; I am writing a UI to deal with cromwell. There I just make Ajax calls to cromwell from ScalaJS without bothering about redirecting everything to the server. I had to configure nginx to provide allow-origin, however,it will be way better if there will be allow-origin option in cromwell config, so people will be able to use my UI without messing with nginx configurations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228:382,configurat,configurations,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-344307228,1,['configurat'],['configurations']
Deployability,"@cjllanwarne -- great suggestion, but I want to make the least amount of possible changes pre-release. I can ticket it; @mcovarr other than re-starting the JES build, is there anything else require to get a thumb?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1464#issuecomment-248677461,1,['release'],['release']
Deployability,"@cjllanwarne @aednichols I have updated this to reflect Adam's changes, plus naming convention changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922,1,['update'],['updated']
Deployability,"@cjllanwarne @kshakir Thinking more about this, I'm wondering if the ""Backend Return Code"" is something we want to consider abstract/general enough that every backend should have the option to return one (in which case an `Option[BackendRc]` should be wire from the backend back to the CA, like for the script rc), or if it's something really only specific to JES in which case we should treat it the same way we do the JES status and Run ID (which are updated directly from JES Backend, which is not great but will probably be fixed with intel changes when Backends become actors)..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184747668:453,update,updated,453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184747668,1,['update'],['updated']
Deployability,@cjllanwarne @mcovarr code has been updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170566228:36,update,updated,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170566228,1,['update'],['updated']
Deployability,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:188,update,updated,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961,1,['update'],['updated']
Deployability,"@cjllanwarne Although perhaps the solution there is to have a release assembly which bundles everything together and a lighter weight one as well, which we can use for the situation you describe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208424103:62,release,release,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/681#issuecomment-208424103,1,['release'],['release']
Deployability,@cjllanwarne Good point on the rc file location. The command script should be updated to redirect to an rc file qualified by the path of the call directory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143254145:78,update,updated,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143254145,1,['update'],['updated']
Deployability,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:66,hotfix,hotfix,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,2,"['hotfix', 'integrat']","['hotfix', 'integration']"
Deployability,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:122,install,installed,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015,2,"['install', 'update']","['installed', 'update']"
Deployability,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406,1,['update'],['updated']
Deployability,"@cjllanwarne I know you just made an update to the IntelliJ plugin, did you fix this too?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393:37,update,update,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393,1,['update'],['update']
Deployability,"@cjllanwarne I run it on cromwell 30.2 release, the latest release at the moment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367442607:39,release,release,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367442607,2,['release'],['release']
Deployability,@cjllanwarne I'll update that too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/594#issuecomment-199399532:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/594#issuecomment-199399532,1,['update'],['update']
Deployability,@cjllanwarne Just need to update the piece around private dockerhub support --otherwise approved 👍,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4633#issuecomment-486211090:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4633#issuecomment-486211090,1,['update'],['update']
Deployability,"@cjllanwarne My plan was following the next release I'd make the change in develop, so 2 releases from now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174552313:44,release,release,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/402#issuecomment-174552313,2,['release'],"['release', 'releases']"
Deployability,"@cjllanwarne Thanks for fixing this so fast. For validation I just skip version 32 ;) When is 33 planned?. We want to upgrade to wdl 1.0 anyway but first need to complete our testing framework around wdl, see also https://github.com/biopet/biowdl-test-utils (library) and https://github.com/biowdl/QC (real pipeline with testing)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541:118,upgrade,upgrade,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-398279541,2,"['pipeline', 'upgrade']","['pipeline', 'upgrade']"
Deployability,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:93,pipeline,pipelines,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996,3,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1228,release,release,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176,2,['release'],['release']
Deployability,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,integrat,integration,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:900,Pipeline,Pipeline,900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,2,['Pipeline'],['Pipeline']
Deployability,"@cjllanwarne Yes, my money is that you have pegged this exactly. I would love this update. I prefer the ``Int? mem=4`` for specifying default values. Otherwise, we have to pepper our command and runtime blocks with ``${default=4 mem}`` or, even worse, something with ``select_first``. We often have inputs that are derived (e.g. ``e``) and we do not want these exposed in ``wdltool inputs ...``. I do not have a good idea for how to handle ``f``. I'm assuming you do not have access to the raw expression when rendering ``wdltool inputs ...``, so can you just say that it has a complex default expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565:83,update,update,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2532#issuecomment-321288565,1,['update'],['update']
Deployability,"@cjllanwarne any update on this issue? Is it still to-do?; Also, what is it for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2316#issuecomment-332235465,1,['update'],['update']
Deployability,@cjllanwarne any update on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316#issuecomment-313485510,1,['update'],['update']
Deployability,"@cjllanwarne any update on whether this is still an issue? I know aborts is a tangled mess, is this still part of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-329662040,1,['update'],['update']
Deployability,"@cjllanwarne having the docker root be a runtime option as then it's hardcoded to the WDL. . At least for this use case it's not and should not be a WDL concept - the reason it comes up is they are using Singularity for their docker containers, and that's a Cromwell-wide configuration thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761:272,configurat,configuration,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420768761,1,['configurat'],['configuration']
Deployability,"@cjllanwarne new lines adjusted, so are my intellij settings thanks to Jose. I'm thinking the integrationTestCases should run weekly instead of nightly even. @jsotobroad as the original creator of the integrationTestCases -- does that sound okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523:94,integrat,integrationTestCases,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523,2,['integrat'],['integrationTestCases']
Deployability,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:183,configurat,configuration,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801,2,['configurat'],['configuration']
Deployability,@cjllanwarne thanks for merging! Can the changelog bits be retroactively added to the releases page so it is out there for everyone to see? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-488564412:86,release,releases,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-488564412,1,['release'],['releases']
Deployability,"@cjllanwarne this wasn't fixed in your PR, right? What would be involved in fixing this for the next release? I don't want to hold it up but improving our magic release would be nice :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414:101,release,release,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-300302414,2,['release'],['release']
Deployability,@cjllanwarne updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170668783:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170668783,1,['update'],['updated']
Deployability,"@cjllanwarne updated the container image. I'm not inclined to add a glob test as I don't think it's really adding any value. As I said previously, if you'd like to add it knock yourself out",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484174726,1,['update'],['updated']
Deployability,"@cjllanwarne we could, but it's really just for our internal deployments with devops so imo it's not necessary and would just add to the visual noise of our docs. It'd only take a couple of minutes to create said visual noise if you really think it'd be helpful",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2541#issuecomment-321910634:61,deploy,deployments,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2541#issuecomment-321910634,1,['deploy'],['deployments']
Deployability,"@cjllanwarne, here is the PR. This is only for workflow definitions, and only for line numbers. I found that it is, as you were saying, hard to extract reliable information from Hermes for column numbers. I *would* like to get the entire extent in the source file covered by an AST. It was slow slog to updates the tests to correctly check line numbers. Let's start with this change, and see how it goes. . Thank you,; Ohad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117:303,update,updates,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-489182117,1,['update'],['updates']
Deployability,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:65,upgrade,upgrade,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111,4,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"@coreone Merge at will. If the database isn't updated, the new build will crash with an error containing the SQL that needs to be run. The paths to the scripts have also changed, so even if an old jenkins job tries to run the scripts manually, I suspect it would fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974:46,update,updated,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-171434974,1,['update'],['updated']
Deployability,"@cpavanrun If I understood you correctly I think this change will give you what you want, albeit you'll need to opt in to that behavior via configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522:140,configurat,configuration,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424755522,1,['configurat'],['configuration']
Deployability,@cy-bao is this still the case on the latest version of Cromwell in FireCloud? . All future updates to this issue will be made in JIRA: ; https://broadworkbench.atlassian.net/browse/BA-2791; Sorry for the inconvenience.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-506470150:92,update,updates,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-506470150,1,['update'],['updates']
Deployability,@danbills I'd be happy to help debug any funnel issues you run into. Were you testing the v0.2 release or the latest on our master branch?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270:95,release,release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2396#issuecomment-332604270,1,['release'],['release']
Deployability,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:753,install,installed,753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,2,"['deploy', 'install']","['deployed', 'installed']"
Deployability,@danbills any update on this ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2719#issuecomment-335898405:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2719#issuecomment-335898405,1,['update'],['update']
Deployability,"@danbills does the following sound accurate to you?. As a **user running workflows on Pipelines API**, I want **Cromwell to notify me when it hits the limit for retries**, so that **I know why my workflow failed**.; * Effort: Small; * Risk: Small; * Business value: Small to Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095:86,Pipeline,Pipelines,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2426#issuecomment-332648095,1,['Pipeline'],['Pipelines']
Deployability,"@davidangb I think that's fair. How about us saying that we'll continue to run the rawls/agora upgrades, but only if changes are made to the WDL draft 2 libraries. Otherwise we'll leave them at their current version?. In effect that probably means we won't be changing anything in the draft-2 libraries for the next few weeks... but it's good to have a fall back in case an urgent fix is needed (we didn't have anything planned in any case, which is why we didn't mind this freeze in the first place).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490096475:95,upgrade,upgrades,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490096475,1,['upgrade'],['upgrades']
Deployability,@davidbernick @hjfbynara would you please confirm update script has been run so that I can rule out pingdom/firewall issues?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659:50,update,update,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-452850659,1,['update'],['update']
Deployability,"@delagoya I can't seem to find the thread, but the pathing reversal from @kshakir's notes above was purposeful. By using /test1/... rather than .../test1, we get the advantage of a more useful host path when traversing manually for debugging purposes for example, or by being able to segment what we know are large tasks to different filesystems. I'm comfortable changing the disks configuration name but this should probably be tracked in a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348:382,configurat,configuration,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-405687348,1,['configurat'],['configuration']
Deployability,@delagoya The plan is to bake this into an AMI similar to the way ecs agent is installed. It will be a container with a always-on restart policy. https://docs.docker.com/config/containers/start-containers-automatically/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494:79,install,installed,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-410858494,1,['install'],['installed']
Deployability,"@delagoya your dependencies update might conflict with a round I just did in our `develop` branch. Namely we are using cats 1.0.1, and I'm not 100% sure 1.1 will work. So if you pull/rebase you will get most of what you posted minus the sttp update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268,2,['update'],['update']
Deployability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,integrat,integration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,1,['integrat'],['integration']
Deployability,@delocalizer Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the `better files` library we use and I filed a ticket on their github https://github.com/pathikrit/better-files/issues/115.; In the meantime I added a workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706,1,['update'],['update']
Deployability,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,integrat,integration,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482,1,['integrat'],['integration']
Deployability,"@delocalizer ah, thanks. I didn't realize the new backend system was this flexible (the `README` is a little opaque). Thank you for sharing your PBS configuration file! I confirm it works for me, too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921:149,configurat,configuration,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254206921,1,['configurat'],['configuration']
Deployability,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:98,release,releases,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046,1,['release'],['releases']
Deployability,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:105,update,update,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911,2,['update'],['update']
Deployability,@dmohs I can't find it at the moment but this is effectively a dupe of another ticket. It's been a known problem for years now that the logs Cromwell emits (console or otherwise) are trying to cover multiple user personas at once (and IMO not doing a great job for any of them) and that a more holistic solution needs to be put into place. . When I find the ticket I redirect all of this stuff towards I'll update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448:407,update,update,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062#issuecomment-417414448,1,['update'],['update']
Deployability,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:957,pipeline,pipelines,957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972,5,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"@droazen not that i'm aware of. If you're referring to what I think you're referring to, @leetl1220 is experiencing these errors as part of the Pipelines API process which isn't code we control.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531:144,Pipeline,Pipelines,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318397531,1,['Pipeline'],['Pipelines']
Deployability,"@dspeck1 I updated this branch to trigger another CI run, let's see what happens",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404:11,update,updated,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404,1,['update'],['updated']
Deployability,"@dtenenba , @vortexing - The [docs](https://docs.opendata.aws/genomics-workflows) for creating the genomics workflow environment (i.e. AWS Batch and related resources) have been updated. Use of custom AMIs has been deprecated in favor of using EC2 Launch Templates. There's also additional parameter validation under the hood around setting up an environment for Cromwell to avoid these configuration errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885:178,update,updated,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-470339885,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:143,install,installed,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,2,['install'],['installed']
Deployability,"@dvoet the reason for not having been done yet was that the primary motivating use case (or problem turner upper as it were) was GOTC who were able to work around it by not globbing in the first place. I believe this will be part of the upcoming 23 release. re S3, good news there for you, we have to do our own localization so don't need to rely on others to tell us what's going on ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209:249,release,release,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395#issuecomment-256901209,1,['release'],['release']
Deployability,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:128,deploy,deployed,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880,1,['deploy'],['deployed']
Deployability,"@elerch Careful with that `always-on` restart policy from docker. In my experience, it did not re-read `env-files` (in my case those env vars are sitting on the host's `/etc/defaults/ecs`). I expected SIGHUP-like behavior when changing ecs-agent attributes like `ECS_CLUSTER`, i.e:. https://github.com/umccr/umccrise/blob/master/deploy/roles/brainstorm.umccrise-docker/files/bootstrap_instance.sh#L39. Instead, I had to resort to a systemd service that re-runs the ecs-agent docker container on boot:. https://github.com/umccr/umccrise/blob/master/deploy/roles/brainstorm.ecs-agent/tasks/main.yml#L75",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-413092230:329,deploy,deploy,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-413092230,2,['deploy'],['deploy']
Deployability,"@ernoc Can you provide some more info on your setup? What backend you're using, what DB configuration, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617:88,configurat,configuration,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423360617,1,['configurat'],['configuration']
Deployability,"@ernoc So this sounds like there's a disconnect between the status changing in memory and getting updated in the db (as the REST endpoints report status from the db). . 1. Do you see anything in your logs that indicate db errors?; 2. What does your db config look like? ; 3. When you report the REST endpoint shows the workflow as 'Running', what about the `executionStatus` key in the metadata? Are some jobs marked as 'Running' as well?; 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400:98,update,updated,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445245400,1,['update'],['updated']
Deployability,@ernoc We hope to have this feature released in the next few months that involves being able to cleanup workflow outputs!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227:36,release,released,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-417331227,1,['release'],['released']
Deployability,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:53,release,release,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['release'],['release']
Deployability,@ffinfo the fix will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-420759722:36,release,release,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051#issuecomment-420759722,1,['release'],['release']
Deployability,"@francares @geoffjentry I'm going to close this version of the PR. The comments are all heading to the other PR and Master will be updated in about a week anyway when we publish 0.17. . At that point, the Master PR will essentially be equivalent to this one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/399#issuecomment-174653940:131,update,updated,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/399#issuecomment-174653940,1,['update'],['updated']
Deployability,@francares Can you update the story on waffle (#884) and move it to in-review?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1100#issuecomment-229752230,1,['update'],['update']
Deployability,@francares I believe the only attribute strictly required is docker as all of the other ones should have defaults set by the google genomics pipeline API (e.g. disk defaults to 500GB). Anything provided should be valid however.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212482498:141,pipeline,pipeline,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212482498,1,['pipeline'],['pipeline']
Deployability,"@francares sorry for the slow responses this week. We've got people out this week so we're falling a bit behind on code reviews (also this week was a release week, which has taken up a good amount of my time). I will definitely review again by EOD tomorrow, but I'm not avoiding this! I need to review @kshakir's PR first because I'm way overdue on that one, but this one is next.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625:150,release,release,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191947625,1,['release'],['release']
Deployability,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:26,patch,patched,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826,2,"['patch', 'update']","['patched', 'updated']"
Deployability,@gauravs90 @francares Please note I've rebased and therefore had to update the ValidateActor to no longer require a backend on construction. I've also modified ValidateActorSpec to feed in the mock backend to the static CromwellBackend pool during testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/586#issuecomment-199336078,1,['update'],['update']
Deployability,@gauravs90 I'm happy to handle the tidy-up of this PR. Could you though review it (including my integration of your changes) and thumbs-up if you're happy? Cheers!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226,1,['integrat'],['integration']
Deployability,"@gauravs90 a couple of global comments:; - The symbol store and execution store in the old WorkflowActor were not necessarily database-backed. They just stored which calls were completed and which were in flight.; - The graph is a nice idea but currently isn't as eager as it could be. Consider:. ```; A -> B -> C; X -> Y -> Z; ```; - I believe this will run this in pairs, `(A,X)`, `(B,Y)`, `(C,Z)`. But what if A and B are really quick but X and Y are really slow - we're slowed down from executing C because the unrelated tasks X and Y haven't finished yet.; - I think I would prefer the existing method of determining (after every job completes) the set of jobs which have now become runnable. There's already an implicit DAG there. ; - A major reason is, it has already been shown to work with the scatter/gather and other features which update the graph at run-time and I can't see how that would work with this static graph approach?. _NB Sorry all for the repeated almost-identical edits to the above comment. Markdown is hard... :(_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919:843,update,update,843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215527919,1,['update'],['update']
Deployability,"@gemmalam Just wanted to check up on this since it's been a month - will it be reviewed as part of the current sprint, and hopefully included in release 43?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-499651256:145,release,release,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-499651256,1,['release'],['release']
Deployability,"@geoffjentry & @Horneth for review please, including the lenthall patch needed to get this cromwell build repaired",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040:66,patch,patch,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213251040,1,['patch'],['patch']
Deployability,"@geoffjentry - we are facing something similar. Our SGE was recently updated and job submissions randomly fail due `unable to contact qmaster`. Our HPC team is looking into it. In the meantime, I am looking for a way to configure Cromwell to retry failed job submissions using the SGE backend. I have tried adding `maxRetries` to the runtime attributes to retry failed job submissions, but seems like this does not retry job submission errors. Only retries task errors. Is that correct? Any advice would be appreciated. Is this a feature that is currently supported? Thanks in advance. I also have seen various different configs on the WDL/Cromwell forum, but not sure if any are still supported. For example:. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/10475/cromewell-28-root-configuration-not-working); ```; system {; max-retries = 10; }; backend {; max-job-retries = 4; }; ```. [forum post](https://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure); ```; system {; max-retries = 50; job-rate-control {; jobs = 5; per = 1 second; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897:69,update,updated,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-510029897,2,"['configurat', 'update']","['configuration-not-working', 'updated']"
Deployability,"@geoffjentry -- as of Cromwell 35, `backend` key in the workflow options is honored above the default backend. In case of the workflow option asking for a backend that doesn't exit, Cromwell explicitly fails with:. `Backend for call <call-name> ('<backend-name') not registered in configuration file...`. I believe this issue has been resolved with these changes. Feel free to re-open if something was missed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274:281,configurat,configuration,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-424937274,1,['configurat'],['configuration']
Deployability,"@geoffjentry -- to put this in your head. Let's not jump to it being an engine feature. But I think the point here is a good one. . I think this, and other use cases, might be best met by a set of ""built-in"" tasks that cromwell could come with. For example, through the use of imports and having cromwell released with a equivalent of ""genomics-stdlib"" we could provide genomics specific manipulations as tasks. This solves the problem of the user having to write them themselves. Then through the use of smart multi-backend support we could also have some of these stdlib tasks run on the same machine as cromwell. This requires a bunch of advances to the engine, but I think it's where we can provide a lot of value to the users. The first step in this could be having Kate & Crew (along with our help) publish that ""gatk-stdlib.wdl"" that performs these functions and people could import. Then if that is successful we could see how we would best provide that sort of support in a batteries included fashion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918:305,release,released,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-255404918,1,['release'],['released']
Deployability,"@geoffjentry ; I trying to write some kind of integration test for my fix of this task and me with @TimurKustov came to idea of executing two workflows sequential in order to get outputs, results and call logs copied after execution of the first workflow and assure that they are exist and correctly placed by running second workflow, which would check these files locations and existence.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075,1,['integrat'],['integration']
Deployability,@geoffjentry @Horneth this could use a re-review since I updated it a bunch,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/992#issuecomment-225997411:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/992#issuecomment-225997411,1,['update'],['updated']
Deployability,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:173,rolling,rolling,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179,2,"['patch', 'rolling']","['patch', 'rolling']"
Deployability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:161,pipeline,pipelines,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,6,"['Continuous', 'Pipeline', 'integrat', 'pipeline']","['Continuous', 'Pipelines', 'integration', 'pipelined', 'pipelines']"
Deployability,"@geoffjentry Ah, thanks for the clarification -- by ""Pipelines API process"", do you mean JES? Are you in touch with the people who maintain that (or are we able to submit PRs to that project)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842:53,Pipeline,Pipelines,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495#issuecomment-318399842,1,['Pipeline'],['Pipelines']
Deployability,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:74,configurat,configuration,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849,5,"['configurat', 'update']","['configuration', 'update']"
Deployability,"@geoffjentry Current state of Reliance Point backend, that is being implemented here, requires during initialization to know the order of calls execution. This is because the component behind this backend needs to know basic execution order since it still does not understand WDL therefore it can't read a complete DAG. ; I thought this was a bug due to the loose of oder when a fold or groupBy is applied in map of call -> backend.; We have a possible release date (given by the team who is developing it) of the RP component for beginning next year which will support WDL spec but for now we need to support this.; Please let me know if we can keep this change in the engine since it does not hurt. Later when RP WDL version is in place we revert this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476:453,release,release,453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235952476,1,['release'],['release']
Deployability,@geoffjentry I am currently debugging a workflow on SLURM and can provide a beta backend configuration. Should I submit this as a merge request to `core/src/main/resources/reference.conf`?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740:89,configurat,configuration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-291606740,1,['configurat'],['configuration']
Deployability,"@geoffjentry I believe recently released a change with ""null"", is that related here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992:32,release,released,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-330607992,1,['release'],['released']
Deployability,"@geoffjentry I removed retry in two spots which hopefully could be added back per this scheme as part of #808. Note that neither of those spots corresponds to GCS auth upload, which should be happening in the initialization actor and is the subject of #806. I actually thought this ticket was meant for the hotfix branch to deal with problems the Greenies had seen, but there doesn't seem to be any more detail or labeling to confirm or refute that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863:307,hotfix,hotfix,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863,1,['hotfix'],['hotfix']
Deployability,@geoffjentry I totally agree with @rtitle that this is something that should be added to next FC release as the performance improvement is significant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184:97,release,release,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317803184,1,['release'],['release']
Deployability,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:254,update,update,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['update'],['update']
Deployability,@geoffjentry Is the use case that a Cromwell dev updated liquidbase and a user needs to migrate? How much effort would this take?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907:49,update,updated,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429#issuecomment-333185907,1,['update'],['updated']
Deployability,"@geoffjentry Not really solved. The pipeline could be terminated by the same error, i just extract the samples that are not processed and run it again. It would be better with local MySQL database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294:36,pipeline,pipeline,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-462649294,1,['pipeline'],['pipeline']
Deployability,"@geoffjentry Thanks for the quick response and the suggestion. I wrote a quick wdl script, that runs on cromwell + local backend, but when deployed on the AWS batch backend it fails in the same way..; The script `exampleWorkflows/bbmap.wdl` is: . ```; task bbmaptask {; File f1; File f2; command {; reformat.sh maxcalledquality=40 in=${f1} in2=${f2} out=${f1}.ref.fq.gz out2=${f2}.ref.fq.gz; }; output {; Array[File] response = glob(""*R?.ref.fq.gz""); }; runtime {; docker: ""*********.dkr.ecr.us-east-1.amazonaws.com/ngs/bbmap:v37.64""; }. }. workflow bbmapwf{; 	call bbmaptask; }; ```. The input file `exampleInput/fastq.s3.wdl.json`; ```; {; ""bbmapwf.bbmaptask.f1"": ""s3://bucket/fastq.20180820-150001/DA0000317_WSU-DLCL_R_02_01_02_S36_R1_001.fastq.gz"",; ""bbmapwf.bbmaptask.f2"": ""s3://bucket/fastq.20180820-150001/DA0000317_WSU-DLCL_R_02_01_02_S36_R2_001.fastq.gz""; }. ```. I run cromwell as:. ```; java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run -i exampleInput/fastq.s3.wdl.json exampleWorkflows/bbmap.wdl. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-453807479:139,deploy,deployed,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542#issuecomment-453807479,1,['deploy'],['deployed']
Deployability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:186,configurat,configuration,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,2,['configurat'],['configuration']
Deployability,"@geoffjentry The main advantage of running these test cases daily vs weekly seems to be that it’s easier to narrow down which change couldve caused this test suite to fail. However, it seems unlikely to me that these tests could find breaking changes everyday that the centaur standard test cases wouldn’t already uncover. I see these tests as a release requirement for Cromwell more than anything else. However, it’s totally upto the team on whatever makes them most comfortable, I don’t have a strong opinion on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368:346,release,release,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352073368,1,['release'],['release']
Deployability,"@geoffjentry This is my github ID. :). @katevoss We've been using parameters to tasks to override runtime attributes. However, I am not sure how this affects call caching, it is very clunky, and the standard names my group uses may different from another. This lattermost is a headache for pipeline engineers. Anyway, it would be nice to have a mechanism for overriding runtime attributes, mostly for users, not developers. This would cover times where WDL writers have hardcoded runtime attributes that do not fit a user's need. For me, this is no longer high priority... . Though is another issue needed for these parameters that I use adversely affecting call caching (e.g. my `preemptible_attempts` parameter is only used in the runtime block and should not cause a cache miss if it is changed)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645:290,pipeline,pipeline,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-325718645,1,['pipeline'],['pipeline']
Deployability,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,integrat,integration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433,1,['integrat'],['integration']
Deployability,"@geoffjentry We are not using Docker, but you are right that it might be better to make it a configuration flag like @danbills initially suggested, rather than automatic, in case it would break someone's workflow... like maybe if they were using Docker and non-Docker in the same workflow?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374724001,1,['configurat'],['configuration']
Deployability,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:741,deploy,deployment,741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['deploy'],['deployment']
Deployability,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:873,pipeline,pipeline,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901,2,['pipeline'],['pipeline']
Deployability,"@geoffjentry can you update the ticket with more info? Where is this parameter used, and what is the effect of this being ignored?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709:21,update,update,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1097#issuecomment-229629709,1,['update'],['update']
Deployability,"@geoffjentry do you mean from a default configuration perspective? We could have a pluggable ""metadata service"" in Cromwell (which I think we already have) with two implementations (direct DB write vs JMS emitter) could go into mainstream cromwell. Of course the listener for that would be a separate service (and easy to scale). Maybe that's what you mean though?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697:40,configurat,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320245697,1,['configurat'],['configuration']
Deployability,"@geoffjentry in such case I have not idea how I can do anything other than super-simple linear pipelines with wdl: tsv-s with headers cannot be read, read_json does not work, loops do not work, I cannot make even simpliest preprocessing of input arrays or maps with wdl!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915:95,pipeline,pipelines,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915,2,['pipeline'],['pipelines']
Deployability,@geoffjentry is there any update on udocker support or is that already works with some tricks ?. I got same question for singularity as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-385569620,1,['update'],['update']
Deployability,@geoffjentry it was partial thought :) My mind can be scattered. Ive update the comment,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653:69,update,update,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-333223653,1,['update'],['update']
Deployability,"@geoffjentry sounds like this will be necessary in a CaaS world, do you agree?. As a **FC/GOTC developer**, I want **Cromwell to test with Cloud SQL after every release**, so that I can **avoid critical (? @helgridly a bug in Cloud SQL would be critical, right?) regressions and issues in production**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302:161,release,release,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328536302,1,['release'],['release']
Deployability,"@geoffjentry thanks for the update, I'll keep an eye on the pull request.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511141850,1,['update'],['update']
Deployability,"@geoffjentry yes I'm fine with merging this after release (or the release branch is cut), barring any substantive problems turned up by eagle-eyed reviewers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203034975:50,release,release,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203034975,2,['release'],['release']
Deployability,"@geoffjentry yes. My current deployment is v42. If you have access to the GATK forums, i put more details in my post there: https://gatkforums.broadinstitute.org/wdl/discussion/24268/aws-batch-randomly-fails-when-running-multiple-workflows/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738:29,deploy,deployment,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514937738,1,['deploy'],['deployment']
Deployability,"@geoffjentry, is the MetadataSummarizer config toggled with the `metadata-summary-refresh-interval` in the reference configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631:47,toggle,toggled,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-347328631,2,"['configurat', 'toggle']","['configuration', 'toggled']"
Deployability,"@geoffjentry, thanks for clearing that up. I guess I know more if the documentation gets updated. @ffinfo, thanks for picking this up!. We do have a very well trained HPC admin team that will bash (hehe) anyone abusing the SGE queque master too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349,2,['update'],['updated']
Deployability,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:417,integrat,integration,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,1,['integrat'],['integration']
Deployability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:125,update,update,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['update'],['update']
Deployability,@grsterin I have updated the key to replace 'Connected Components' with 'In-memory Storage'.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453:17,update,updated,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453,1,['update'],['updated']
Deployability,@grsterin LGTM. Reminder to also make a hotfix edition of this change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140,1,['hotfix'],['hotfix']
Deployability,"@grsterin added support for the updated backend and tested with the same workflow/inputs as above, but using the new backend and all seems to be well. Thanks for taking a look at this!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445,1,['update'],['updated']
Deployability,@grsterin remember to update the PR base before merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960,1,['update'],['update']
Deployability,@guma44 we have moved to Jira and this ticket is in review at the moment on our new board. Please check out this ticket https://broadworkbench.atlassian.net/browse/BA-5933 or this Pull Request for updates https://github.com/broadinstitute/cromwell/pull/5180,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929:197,update,updates,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-535943929,1,['update'],['updates']
Deployability,"@helgridly @davidangb for context, our understanding is that the absolute worst case here is, ""Cromwell might make a change, meaning that Rawls/Agora won't validate something that Cromwell could run - if only it were to be submitted"". Or alternatively, ""Cromwell will regress and no longer succeed a workflow that Rawls thinks is fine"". Given the slow rate of change in the WDL draft-2 libraries recently, and the (very significant) pain in updating Rawls and Agora every release, that feels like a reasonable position for a few months while we await the switchover to womtool-as-a-service. Are we missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477:472,release,release,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-489665477,1,['release'],['release']
Deployability,"@hmkim ; I continue the break point to run it again, it works now.; What part of process takes long idle time in your instance? what makes the long idle time?; In fact, the pipeline always consists of multiple processes and works on hundreds of samples. ; In case of time, what should i config to avoid this errors not run it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197:173,pipeline,pipeline,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-439905197,1,['pipeline'],['pipeline']
Deployability,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:267,rolling,rolling,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,2,['rolling'],['rolling']
Deployability,@illusional I have also provided jars here: https://github.com/rhpvorderman/cromwell/releases/tag/50-dev-lumctest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995:85,release,releases,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995,1,['release'],['releases']
Deployability,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:141,configurat,configuration,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399,6,['configurat'],['configuration']
Deployability,@illusional I'm sure it's true that squashfs is a build requirement. The admins installed it on our cluster worker nodes for that reason. So yes we should probably mention that in the Singularity installation section. Is that what you're asking?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468090428:80,install,installed,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468090428,2,['install'],"['installation', 'installed']"
Deployability,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:251,configurat,configuration,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330,1,['configurat'],['configuration']
Deployability,"@illusional, I've added Singularity installation docs, and udocker cache docs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286:36,install,installation,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464537286,1,['install'],['installation']
Deployability,@ilovezfs Ok. Will update the PR in a few.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067822,1,['update'],['update']
Deployability,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512,1,['integrat'],['integration']
Deployability,"@jacarey as per what @mcovarr said, can you update the PR to be against develop?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/179#issuecomment-139891013:44,update,update,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/179#issuecomment-139891013,1,['update'],['update']
Deployability,@jainh Can you update / help clarify the readme docs for spark?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283210838,1,['update'],['update']
Deployability,"@jainh Yes, there are multiple `--conf` attributes.; If there is no space in the value of `--conf` attribute, single quote is not needed; otherwise, I think it's needed. However the [gatk-launch](https://github.com/broadinstitute/gatk/blob/70edbb6e4caa2b7cf1b8678450443c0c590a2b76/gatk-launch) in GATK beta 4 does not produce the single quote for such case; but if I run the following without single quote, it leads to error:; >Error: Unrecognized option: -Dsamjdk.use_async_io_read_samtools=false. command:; ```; /opt/spark-latest/bin/spark-submit --master spark://localhost:6066 --deploy-mode cluster \; --driver-cores 4 --driver-memory 8g --executor-memory 4g --total-executor-cores 10 \; --conf 'spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false' \; --conf 'spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true' \; ...; ```; Here is a related [post](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) on stackoverflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862:583,deploy,deploy-mode,583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-330666862,1,['deploy'],['deploy-mode']
Deployability,"@jbakerpmc GCP Batch reference disks are broken in Cromwell 87, you'll need to run from the `develop` branch at least until Cromwell 88 is released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954:139,release,released,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954,1,['release'],['released']
Deployability,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:69,configurat,configuration,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005,1,['configurat'],['configuration']
Deployability,@jdidion I talked to a site admin and this setting should be updated. Please let me know if you run into this error again and I will add you manually and continue to communicate with our site admin about this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542:61,update,updated,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-499936542,1,['update'],['updated']
Deployability,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:21,update,updates,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995,1,['update'],['updates']
Deployability,"@jgainerdewar those Java files are auto-generated, so we would need to update the parser-generator in order to maintainably address the warnings. https://github.com/broadinstitute/cromwell/blob/622c8e6b79b4ce123912ace0ed37b28f1c461324/wdl/transforms/draft3/src/main/java/wdl/draft3/parser/WdlParser.java#L2-L14",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277:71,update,update,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277,1,['update'],['update']
Deployability,@jishuxu @rexwangcc -- have you updated your Cromwell version recently?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413966538:32,update,updated,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-413966538,1,['update'],['updated']
Deployability,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,integrat,integration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['integrat'],['integration']
Deployability,@katevoss ; I would like to add our use-case for the ability to access an ID of a workflow/subworkflow. Our users want the output of the Cromwell to be copied to their local locations and they complain that they cannot read the directory structure - I agree with them as they are data scientists and they want something useful after the pipeline finishes. As we provide the service we are responsible to provide them with something. An idea was to use [croo](https://github.com/ENCODE-DCC/croo) to achieve this. It is really useful solution but it requires manual intervention and knowledge of the pipeline IDs etc. Thus I though I could split the workflow into root and two sub-workflows: `do-the-job` and `copy-files`. However to achieve this the `copy-files` would need to have an access to the `do-the-job` sub-workflow ID or at least the root workflow ID to query for the metadata. I agree it is not deterministic and it should not be. Such a task cannot be cached too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825:337,pipeline,pipeline,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-537417825,2,['pipeline'],['pipeline']
Deployability,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:496,release,release,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,integrat,integrate,618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,1,['integrat'],['integrate']
Deployability,"@katevoss I've re-tested with release 29 and this works as expected now — and may have been working for some time, I haven't revisited this for a while. Thanks to all for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025:30,release,release,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702#issuecomment-331019025,1,['release'],['release']
Deployability,"@katevoss This one is important. Although I am open to alternatives, I believe that we need a way to set any parameter from the json file -- no matter how deeply buried the parameter is within subworkflows, etc. Having to explicitly expose parameters has become too big a hardship on developers and has now led to a bugfix GATK4 release with another one forthcoming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390:329,release,release,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-363466390,1,['release'],['release']
Deployability,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:87,pipeline,pipeline-tools,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['pipeline'],['pipeline-tools']
Deployability,"@kbergin `womtool validate` now (technically, next time Cromwell is released) has an optional flag to supply an `inputs.json` to validate against - is that what you meant?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040:68,release,released,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503#issuecomment-386009040,1,['release'],['released']
Deployability,"@kcibul ; I am missing few things in the implementation like the way to construct Spark command inside Spark backend for this PR, Let me add that to make it Spark'ified. It should be same like Htcondor but entertain Runtime attributes like executor-memory, cores and configuration based master to execute Spark job both in Dockerized or non-Dockerized mode. ; So let me close this pull request (**I will not merge it** ) and create a new one after changes. My goal is to create Spark backend to run Spark jobs in standalone cluster mode for this PR, then later to add other Resource manager related support. ; Does it make sense ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019:267,configurat,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231182019,1,['configurat'],['configuration']
Deployability,"@kcibul A few questions about this:; - Should it support in-place DB migration, or DB A to DB B, or both ?; - Is it a separate program that runs independently ? If yes does it still live in the Cromwell repo ? Or a new flag in cromwell (like server, run, migration), ? Is there any ""automagic"" detection that my DB needs an update when I run cromwell 0.20 on a pre-0.20 DB ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983:324,update,update,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226247983,1,['update'],['update']
Deployability,@kcibul I strike to close this - no one has answered your last question for multiple months now and upgrades to 0.2x are underway,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-253877756:100,upgrade,upgrades,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-253877756,1,['upgrade'],['upgrades']
Deployability,@kcibul This is important. I was testing to see if the new release fixed the hanging issue previously reported #1649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265341832,1,['release'],['release']
Deployability,@kcibul next time I won't update tests or docs to keep the file count down ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267825669,1,['update'],['update']
Deployability,"@kcibul np, we just need to remember to cherry pick it into master and update staging when merged",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134213848:71,update,update,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134213848,1,['update'],['update']
Deployability,@knoblett is this still blocking Cromwell's call caching because calls aren't updated as succeeded? I haven't heard this come up in a while.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802:78,update,updated,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-327935802,1,['update'],['updated']
Deployability,"@kshakir @mcovarr re the Either, after dusting off our set theory books and exploring the options it looks like the two of us chiming in from the peanut gallery have agreed that the right thing here would be to set up a little sealed trait/case class ADT to capture the disjunction here. . \/ has a built in right bias. Either has an implicit right bias in the way it's used. The UnionTypes @mcovarr mentioned are, well, interesting. Then consider that this signature is being used in two places and it seemed like rolling our own sum type would be the way to go here. Agree? Disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142310052:515,rolling,rolling,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142310052,1,['rolling'],['rolling']
Deployability,"@kshakir Beyond unit tests, how was this tested? It'd be good to try to throw this at some live mysql installations of various configurations to make sure it doesn't blow up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500:102,install,installations,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324406500,2,"['configurat', 'install']","['configurations', 'installations']"
Deployability,"@kshakir I updated the swagger description of the input files in the submit endpoint to say ""JSON or YAML"". It doesn't get automatically updated in the docs but @geoffjentry said we'll figure that out when @katevoss is back.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-343156709:11,update,updated,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-343156709,2,['update'],['updated']
Deployability,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:481,update,updated,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627,4,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,@kshakir Updated the release doc with instruction for generating a commit list and adding the output to the release ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813:9,Update,Updated,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813,3,"['Update', 'release']","['Updated', 'release']"
Deployability,"@kshakir good point thanks, I'll update local too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389289995:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389289995,1,['update'],['update']
Deployability,@kshakir is this the library that we downgraded the last time we tried to upgrade all of our libraries?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251:74,upgrade,upgrade,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4701#issuecomment-469719251,1,['upgrade'],['upgrade']
Deployability,@kshakir updated the config value,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497:9,update,updated,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-274852497,1,['update'],['updated']
Deployability,"@ktibbett I finally have time to follow up on this, can you explain more about why you want the WDL pipeline to copy workflow outputs into two locations?; How often do you face this problem? Is it a weekly/daily/monthly thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623:100,pipeline,pipeline,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642#issuecomment-325751623,1,['pipeline'],['pipeline']
Deployability,@ktibbett can you comment if this is something you need patched into hotfix? Or something you can tolerate until you're on 0.20+,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661:56,patch,patched,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-230537661,2,"['hotfix', 'patch']","['hotfix', 'patched']"
Deployability,"@ktibbett is this still a feature that would help the production pipeline? ; @geoffjentry aside from the risk of duplicate naming for output and logs, are there any other risks involved? What would be the effort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959:65,pipeline,pipeline,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-325671959,1,['pipeline'],['pipeline']
Deployability,"@ldgauthier has the actual use case. We've already upgraded the methods cromwell. Is there a way to scatter over an iterator, so the whole list does not have to be read into RAM?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338348644:51,upgrade,upgraded,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338348644,1,['upgrade'],['upgraded']
Deployability,@leipzig can you please target the `develop` branch rather than `master`? We only merge to `master` on releases. 🙏,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874:103,release,releases,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874,1,['release'],['releases']
Deployability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['integrat'],['integration']
Deployability,@likeanowl any update on the test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533,1,['update'],['update']
Deployability,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:162,update,update,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775,1,['update'],['update']
Deployability,@mcovarr -- I think this is part of what you've done addressing #1065. Can you update here?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1015#issuecomment-229632978:79,update,update,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1015#issuecomment-229632978,1,['update'],['update']
Deployability,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:206,release,release,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522,1,['release'],['release']
Deployability,@mcovarr @Horneth - the code is slightly different than the hotfix to account for slightly different realities (plus a moving-stuff-around refactor) but more or less the same.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400:60,hotfix,hotfix,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/864#issuecomment-220720400,1,['hotfix'],['hotfix']
Deployability,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:72,release,release,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,@mcovarr @cjllanwarne updated again,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170628682:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170628682,1,['update'],['updated']
Deployability,@mcovarr @kcibul I updated the examples above to reflect the SQL queries generated post PR feedback. Main changes:; - No `EXISTS` for the types of query that Job Manager 0.5.9 produces (labelsAnd and labelsOr).; - Because we now use `JOIN`s for both `LabelAND` and `LabelOR` queries.; - Using `1` instead of `*` for those sub-selects. Would appreciate any other thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241,2,['update'],['updated']
Deployability,@mcovarr @wdesouza I will try to update this pr asap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536,1,['update'],['update']
Deployability,@mcovarr I forgot this before… you should update the changelog to include that statsdproxy was removed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813:42,update,update,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813,1,['update'],['update']
Deployability,@mcovarr I investigated as part of the ticket. I wasn't able to recreate the problem but I couldn't see why the option to update the abort function should be disallowed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222:122,update,update,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164849222,1,['update'],['update']
Deployability,"@mcovarr I should have made that on the hotfix one, it's more in how that's communicated (and if they even care)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812:40,hotfix,hotfix,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292596812,1,['hotfix'],['hotfix']
Deployability,"@mcovarr I think the intention is to start moving people ASAP to WDL 1.0 once it's released next month. We'll need to support workflows which predate that, but it doesn't mean that we need to give those users new functionality and thus reasons to not upgrade",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372489504:83,release,released,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-372489504,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"@mcovarr I was running the GotC pipeline x 20. It was scattered quite wide at the time, I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-226490409:32,pipeline,pipeline,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1007#issuecomment-226490409,1,['pipeline'],['pipeline']
Deployability,@mcovarr I've left the update enabled but added a new warning. So hopefully it's at least as vocal and warn-y as before but won't outright ignore the updated abort function.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510:23,update,update,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/326#issuecomment-164886510,2,['update'],"['update', 'updated']"
Deployability,@mcovarr I've updated to report earlier,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170090256:14,update,updated,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/362#issuecomment-170090256,1,['update'],['updated']
Deployability,"@mcovarr Indeed, these tests were checking that updateBackendExecutionStatus was updating the execution table which is no longer its responsibility. Looking over the tests, this seems to be checked for the setStatus call elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965:48,update,updateBackendExecutionStatus,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145657965,1,['update'],['updateBackendExecutionStatus']
Deployability,"@mcovarr It's more concise but it makes several implicit assumptions to fit exactly our 2 current JES use cases (GotC and FireCloud).; IMO it should be flexible enough so that GotC and Firecloud are just combinations of configuration entries among all the possible ones. > // The JES backend is assumed to use the GCS filesystem with user authentication, dropping back to Cromwell; > // authentication if user authentication is not defined. In GotC there is no user authentication, so; > // Cromwell authentication it is. This is tailor made to accommodate GotC and FireCloud with as few configuration changes as possible, but I think we should move towards something more generic, even if the confs look a bit more different between the two.; For example what if you want to use refresh token auth mode for creating the pipeline as well ?; Or refresh token only for pipeline and service account for gcs ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031:220,configurat,configuration,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203453031,4,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"@mcovarr Need to update the readme and changelog, thanks for the reminder :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1652#issuecomment-259157871,1,['update'],['update']
Deployability,"@mcovarr README updated, wording okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/504#issuecomment-191889151:16,update,updated,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/504#issuecomment-191889151,1,['update'],['updated']
Deployability,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:83,update,updated,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,1,['update'],['updated']
Deployability,@mcovarr any chance of getting it re-released as 28.1 or 29? Unfortunately users will just get a checksum mismatch error if the jar is already in their cache since cromwell is `bottle :unneeded`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288:37,release,released,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316010288,1,['release'],['released']
Deployability,"@mcovarr as first reviewer. Travis already seems configured to ignore integration tests, so no other changes appeared necessary for this temporary fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511:70,integrat,integration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511,1,['integrat'],['integration']
Deployability,"@mcovarr correct, the more general problem exists throughout cromwell though and I didn't want to get into a rabbit hole with this but after digging a bit more it's actually not that bad. I'll update the PR with some better crash handling instead",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405:193,update,update,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229134405,1,['update'],['update']
Deployability,"@mcovarr fixed this in hotfix, so please check that out when fixing this in develop",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-217263652:23,hotfix,hotfix,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-217263652,1,['hotfix'],['hotfix']
Deployability,@mcovarr https://github.com/broadinstitute/wdl4s/pull/6. Also I updated the code in this PR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171758540:64,update,updated,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171758540,1,['update'],['updated']
Deployability,@mcovarr is there a reason why your test WDL is installing `jq` in the command rather than picking a docker image which already has it installed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349:48,install,installing,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-461103349,2,['install'],"['installed', 'installing']"
Deployability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:48,Pipeline,PipelinesApiRequestWorker,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,4,"['Pipeline', 'pipeline']","['PipelinesApiRequestWorker', 'pipelines']"
Deployability,"@mcovarr, @cjllanwarne agree on your comments, I implemented a PoC on graph topology creation using topological sort algorithm based on DFS calculation at the beginning of this year. The idea was to propose the integration of that concept to Cromwell after PBE.; Coming back to this issue, If we need to recreate a DAG in the backend side it means there is something wrong with that backend (we know it). I explained why we try to do so in the Waffle.io ticket but I think we can explain better in the next meeting we may have. Anyways @jainh will not proceed with this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218:211,integrat,integration,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218,1,['integrat'],['integration']
Deployability,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:708,pipeline,pipeline,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601,1,['pipeline'],['pipeline']
Deployability,@meganshand I've confirmed this bug is resolved in release 0.21! I'm closing this issue for that reason.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-252242676:51,release,release,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/781#issuecomment-252242676,1,['release'],['release']
Deployability,"@meganshand if this is still happening on a modern cromwell, please open a new ticket w/ updated info",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-414067237,1,['update'],['updated']
Deployability,@meganshand thanks for the update! I'll lower this in priority.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-289777291,1,['update'],['update']
Deployability,@mwalker174 thanks for circling back to this and giving us this update!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-488065955:64,update,update,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4873#issuecomment-488065955,1,['update'],['update']
Deployability,@natechols we are currently releasing every 3 weeks and next week is when we are aiming to release 41. We'll make sure to circle back to this PR and work with you on what to test when we have more time. The goal would be to get this released in 42 if all goes well!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799:91,release,release,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488378799,2,['release'],"['release', 'released']"
Deployability,"@noblem The primary difference is that the path forward to have the spec change is to present a PR and not to have an issue open. My point in my last reply is that someone might see it at some point and make a PR for you but there's no guarantee of that happening in a timely fashion whereas opening a PR against the spec yourself does guarantee that happening :). FWIW I realize that the switcharoo in process can be frustrating. As an example of how we feel the pain as well, there are cases where we (Cromwell team) forgot to update the spec document when we made changes which now means that those changes aren't actually part of the spec. So for instance [this PR](https://github.com/openwdl/wdl/pull/148) is just trying to specify behavior Cromwell already does but is likely to wind up different than what we did & thus making Cromwell non-conforming. That's what we get for being lazy 6 months ago :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670:529,update,update,529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342223670,1,['update'],['update']
Deployability,"@nrockweiler hello, please refer to https://github.com/broadinstitute/cromwell/issues/2916 for requester pays updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3449#issuecomment-387113756:110,update,updates,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3449#issuecomment-387113756,1,['update'],['updates']
Deployability,@nrockweiler requester pays buckets can be used in Cromwell when the Cromwell 32 is released. It is scheduled for this month,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394:84,release,released,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394,1,['release'],['released']
Deployability,"@orodeh I don't think we'll do a hotfix for this back into 36, but the better news is that 37 isn't too far away, so it shouldn't be too long before this gets released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4559#issuecomment-455640456:33,hotfix,hotfix,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4559#issuecomment-455640456,2,"['hotfix', 'release']","['hotfix', 'released']"
Deployability,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:146,configurat,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794,1,['configurat'],['configuration']
Deployability,"@orodeh thanks for reporting, the fix will be included in the next release of Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-422889488:67,release,release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-422889488,1,['release'],['release']
Deployability,@osha52101 - please run this with the latest release (87). I believe this issue was resolved.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858:45,release,release,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858,1,['release'],['release']
Deployability,"@patmagee not to my knowledge but I can start the ball rolling. ; @geoffjentry & @mcovarr, any opinions on supporting enums?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471:55,rolling,rolling,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325807471,1,['rolling'],['rolling']
Deployability,@pshapiro4broad ; Is there any update on this or planned inclusion? I am also running into the same issue. It would be nice if Cromwell supported the latest specification(s) of WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753,1,['update'],['update']
Deployability,"@raylim At the moment, no. Also note that if you're using the google backend that the pipelines API currently has the same limitation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-327320882:86,pipeline,pipelines,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-327320882,1,['pipeline'],['pipelines']
Deployability,"@rhpvorderman I usually solve it by:; ```; ln -s ~{bam} ~{basename(bam)}; ```; However, even though I have workarounds I think it is a big flow of Cromwell that it allows the tools to mutate the Input folder instead of writing to executions. I believe Inputs should be immutable by design. Many new Cromwell users can waste hours (like I did) before untile they realize that some tools produce output but write in Inputs instead of execution. P.S. Also, I am a big fun of BioWDL and I get inspiration from it in my pipelines. However, as the workflows of our lab are different I do not git-clone them but prefer to implement my own versions. I am also open to contribute the tasks for the tools that are not in BioWDL but which I wrote in my pipelines and which may be interesting for many users",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864:515,pipeline,pipelines,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864,2,['pipeline'],['pipelines']
Deployability,"@rhpvorderman to give you an update - I agree that there's something wrong here, but I can't identify which component requires updating - the WDL spec, the WDL grammar in spec repo, or Cromwell. I will consult my teammate @cjllanwarne when he comes back from vacation in about 1.5 weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415466354:29,update,update,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415466354,1,['update'],['update']
Deployability,"@rsasch @cjllanwarne I have updated the diagram and added a key. Regarding (Chris's) Comment 3, we can surely go 5000ft higher and create a 10000ft diagram. I will take a look. Somehow 10000ft higher is harder to visualize! :) ; Update: Ticket for The 10000ft view of actor system has been created. JIRA ticket [WA-67](https://broadworkbench.atlassian.net/browse/WA-67)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283:28,update,updated,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283,2,"['Update', 'update']","['Update', 'updated']"
Deployability,"@rsasch I believe you're next on the release rotation. This might be a nice thing to do prior to the next release. Once an account has been generated, its likely that the release checklist needs a first step of ""generate a token for this user"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935:37,release,release,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-424932935,3,['release'],['release']
Deployability,"@rtitle ; The real meat of this ticket though is less the ""omg the DB barfed"" but rather ""omg we don't do anything smart when omg the DB barfed"" :). That said, the bulk of our data storage *is* separated out, just not practically in our default ""Jane User"" configuration. Currently the ""Jane User"" configuration is the only one which exists. So e.g. for CaaS that's not going to be the case, and we'll probably need to support horizontal scaling scenarios that don't take adantage of GCP tooling for external customers as well. . The lion's share of our DB activity consists of writes coming from the write side of our cqrs to the read side's event store and reads on that event store coming from the API. It's logically all separated out but in stock Cromwell they're sharing the same DB/connection/etc. So e.g. one possibility (which has come up before for other reasons) would be to make it easier for a user to bifurcate those to using separate DBs (or at least separate connections), although you'd still possibly have this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999:257,configurat,configuration,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-316435999,2,['configurat'],['configuration']
Deployability,@ruchim -- can you update this ticket with the current status (e.g. how far we scaled initially)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-248658974:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794#issuecomment-248658974,1,['update'],['update']
Deployability,"@ruchim . I think it's fine to update the README even though it's not usable - the develop branch is known to be unstable at the moment, and isn't the default branch on github. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232,2,['update'],['update']
Deployability,@ruchim @danbills did this get updated in the intellij repository?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288:31,update,updated,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595288,1,['update'],['updated']
Deployability,"@ruchim @kshakir Yes, @Horneth could you please add to hotfix branch? They are getting 32 ready for FC, would like to get this in there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3715#issuecomment-393910783:55,hotfix,hotfix,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715#issuecomment-393910783,1,['hotfix'],['hotfix']
Deployability,"@ruchim Any updates on this issue? We are running this issue with CaaS/Cromwell again, unfortunately :( The current workaround is, for a query of 1000 workflows, we send 1000 individual requests to the Cromwell, which could also put pressure to Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-453156671:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115#issuecomment-453156671,1,['update'],['updates']
Deployability,@ruchim I believe this is now supported as of Cromwell 27 (released yesterday).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962:59,release,released,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2106#issuecomment-305232962,1,['release'],['released']
Deployability,"@ruchim I should clarify. There are a lot of people at this workshop (if not all of them) that cannot use the cloud, due to regulations, clinical requirements, etc. Or they need to be able to run WDL on their local on-prem compute cluster for testing on small cohorts, etc. This is a common configuration that prohibits docker. We really do not want these users to be forced to change the WDL that we (DSP methods) write and test. In order to stay backend-agnostic, can we implement a null option for docker as described in this issue (and #1804 )?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605:291,configurat,configuration,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423147605,1,['configurat'],['configuration']
Deployability,"@ruchim I think @geoffjentry is spot on - adding a scope by itself won't change existing behavior. It's only when the user sets `monitoring_image` to `quay.io/broadinstitute/cromwell-monitor-bigquery`, _then_ it will fail if the SA for the task doesn't have `bigquery.tables.updateData` permission on the monitoring dataset. So existing users on Terra won't be affected, unless we start routinely adding that option to all workflows and don't adjust the IAM permissions on pets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348:275,update,updateData,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348,1,['update'],['updateData']
Deployability,"@ruchim I updated the description and title, this is not nearly as bad as the previous title made it sound. It's a very weird case found in the centaur test failures that I'm looking at.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202#issuecomment-427022611,1,['update'],['updated']
