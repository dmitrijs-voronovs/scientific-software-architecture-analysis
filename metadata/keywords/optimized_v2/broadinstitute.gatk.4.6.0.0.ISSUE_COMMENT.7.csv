quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance," 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6619,concurren,concurrent,6619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance," 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3688baab{/,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fe2dd02{/api,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@726a8729{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a2724d3{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-07 11:33:27 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:33:27 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:46828/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1546878807984; 2019-01-07 11:33:28 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-07 11:33:29 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-07 11:33:30 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-07 11:33:30 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-07 11:33:30 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-07 11:33:30 INFO Client:54 - Setting up container launch context for our AM; 2019-01-07 11:33:30 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-07 11:33:30 INFO Client:54 - Preparing resources for our AM container; 2019-01-07 11:33:30 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1883879239_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-07 11:33:30 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11334 for farrell on ha-hdfs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:10619,load,loaded,10619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['load'],['loaded']
Performance," 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16b64a03{/,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1584c019{/api,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5817f1ca{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b395581{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-09 13:35:12 INFO SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:12 INFO SparkContext:54 - Added JAR file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar at spark://scc-hadoop.bu.edu:42689/jars/gatk-package-4.0.12.0-spark.jar with timestamp 1547058912934; 2019-01-09 13:35:13 INFO GoogleHadoopFileSystemBase:607 - GHFS version: 1.6.3-hadoop2; 2019-01-09 13:35:13 WARN DomainSocketFactory:117 - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-09 13:35:14 INFO Client:54 - Requesting a new application from cluster with 21 NodeManagers; 2019-01-09 13:35:14 INFO Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-09 13:35:14 INFO Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-09 13:35:14 INFO Client:54 - Setting up container launch context for our AM; 2019-01-09 13:35:14 INFO Client:54 - Setting up the launch environment for our AM container; 2019-01-09 13:35:14 INFO Client:54 - Preparing resources for our AM container; 2019-01-09 13:35:14 INFO HadoopFSDelegationTokenProvider:54 - getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-682487019_1, ugi=farrell@AD.BU.EDU (auth:KERBEROS)]]; 2019-01-09 13:35:14 INFO DFSClient:1023 - Created HDFS_DELEGATION_TOKEN token 11353 for farrell on ha-hdfs:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:10359,load,loaded,10359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['load'],['loaded']
Performance," 26|2, 2], DP=94, ECNT=1, GERMQ=93, MBQ=[31, 20], MFRL=[288, 110], MMQ=[60, 60], MPOS=56, NALOD=1.37, NLOD=6.17, POPAF=4.6, ROQ=93, TLOD=10.97} GT=GT:AD:AF:DP:F1R2:F2R1:SB 0/1:46,4:0.07:50:14,3:10,0:28,18,2,2 0/0:23,0:0.041:23:8,0:5,0:15,8,0,0 filters=; 11:43:25.661 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000441716.2 for problem variant: chr6:167976552-167976594(ACAGTGGGGGTCATTCCCCCTGCAGTGTGTTGGGAGGAGGAGG* -> A); 11:44:04.904 INFO ProgressMeter - chr8:677091 4.5 3000 666.0; 11:45:35.226 INFO ProgressMeter - chr11:62279639 6.0 4000 665.6; 11:46:54.284 INFO ProgressMeter - chr15:19905537 7.3 5000 682.4; 11:48:12.767 WARN FuncotatorUtils - createAminoAcidSequence given a coding sequence of length not divisible by 3. Dropping bases from the end: 2 (size=293, ref allele: G); 11:48:16.949 ERROR GencodeFuncotationFactory - Problem creating a GencodeFuncotation on transcript ENST00000379751.5 for variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T): Cannot yet handle indels starting outside an exon and ending within an exon.; 11:48:16.949 WARN GencodeFuncotationFactory - Creating default GencodeFuncotation on transcript ENST00000379751.5 for problem variant: chr20:3786474-3786537(TGGGGCCCATCCCGGCGCGCCCCCCGCCCCGGGGCCCGGCGCCGCCGCCGCCGCCCCGGGGCGG* -> T); 11:48:31.506 INFO ProgressMeter - chr21:18282114 8.9 6000 670.6; 11:49:08.210 INFO ProgressMeter - chr21:18282114 9.6 6888 720.6; 11:49:08.210 INFO ProgressMeter - Traversal complete. Processed 6888 total variants in 9.6 minutes.; 11:49:08.210 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2; 11:49:08.211 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/4781; 11:49:08.230 INFO Funcotator - Shutting down engine; [July 7, 2021 11:49:08 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 9.72 minutes.; Runtime.totalMemory()=4879548416; Tool returned:; true",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422:2306,cache,cache,2306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-887961422,2,['cache'],['cache']
Performance," > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6689,concurren,concurrent,6689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance," HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3311,perform,performance,3311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance," INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2568,Load,Loading,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance," ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.051 INFO GenomicsDBImport - Done importing batch 1/1; 05:39:42.060 INFO ProgressMeter - 3:1 2531.4 1 0.0; 05:39:42.061 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2237,optimiz,optimizations,2237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['optimiz'],['optimizations']
Performance, [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMCompute,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2568,cache,cachemanager,2568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance," accounting for 1) sample-specific depth (which determines the means of the negative-binomial distributions), 2) multiplicative contig-specific bias (which is mild, at least for WGS), and 3) additive sample-contig-specific mosaicism or bias (note that the above genotype priors imply that mosaicism/bias on top of a baseline of CN = 2 is the only deviation allowed for the autosomes, which is somewhat restrictive but greatly aids convergence). I put together a pure PyMC3 prototype that seems to work relatively well. Here are the per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:2495,optimiz,optimizer,2495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,2,['optimiz'],['optimizer']
Performance," acls to: hdfs; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing view acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: Changing modify acls groups to: ; 17/10/13 18:11:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); groups with view permissions: Set(); users with modify permissions: Set(hdfs); groups with modify permissions: Set(); 17/10/13 18:11:37 INFO yarn.Client: Submitting application application_1507856833944_0003 to ResourceManager; 17/10/13 18:11:37 INFO impl.YarnClientImpl: Submitted application application_1507856833944_0003; 17/10/13 18:11:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1507856833944_0003 and attemptId None; 17/10/13 18:11:38 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:38 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:39 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.C",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:11718,queue,queue,11718,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,2,['queue'],['queue']
Performance," advance. ```. Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants --version; Using GATK jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar CNNScoreVariants -R /home/fmbuga/tools/hg38/hg38.fa -V /home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf -O ./08_vcf_1dCNN/SRR16299720_dedup_AORRG_recal_raw_1dCNN_scored.vcf; 05:39:39.149 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:39:39.304 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.305 INFO CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.2.6.1; 05:39:39.305 INFO CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:39:39.305 INFO CNNScoreVariants - Executing as fmbuga@node05.cluster on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 05:39:39.305 INFO CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_332-b09; 05:39:39.305 INFO CNNScoreVariants - Start Date/Time: October 9, 2022 5:39:39 AM PDT; 05:39:39.305 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants - ------------------------------------------------------------; 05:39:39.306 INFO CNNScoreVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:1114,Load,Loading,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance, at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.inter,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9165,Cache,CacheStep,9165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:759,optimiz,optimized,759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2359,optimiz,optimization,2359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379,1,['optimiz'],['optimization']
Performance," in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before inference optimizations, assuming we address #5716 to minimize disk costs. @asmirnov239 can you review? And maybe you can address dCR output in PostprocessGermlineCNVCalls and expose the number of samples in a separate PR? We can make some further changes to the dCR format there if we need.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:1475,scalab,scalability,1475,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,6,"['optimiz', 'perform', 'scalab']","['optimization', 'optimizations', 'optimized', 'performance', 'scalability']"
Performance," increased dictionary size, because they are more repetitive than the DNA data. And shorter BAMs would be different because they are less repetitive (usually less coverage), so their compression relies more on CPU-expensive crunching of the ""2bit nature"" of the DNA.; So they might logically suffer more from a lower compression level.; It might be instructive to compare compression sizes of raw sequence data of two BAM files with output of faToTwoBit / 2.bit files.; 2bit files are compressed by a factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge differenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:2700,optimiz,optimized,2700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['optimiz'],['optimized']
Performance, java.io.InputStreamReader.read(InputStreamReader.java:184); 	at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4625,concurren,concurrent,4625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance," normals with 5M bins each, CombineReadCounts took ~1 min, CreatePanelOfNormals (with no QC) took ~4.5 minutes (although ~1 minute of this is writing target weights, which I haven't added to the new version yet) and generated a 2.7GB PoN, and NormalizeSomaticReadCounts took ~8 minutes (~7.5 minutes of which was spent composing/writing results, thanks to overhead from ReadCountCollection). In comparison, the new CreateReadCountPanelOfNormals took ~1 minute (which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actuall",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1105,perform,performed,1105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performed']
Performance," phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5381890 1.0 22460 22033.3; 14:51:31.097 INFO ProgressMeter - chr17:6474462 1.2 27070 22821.4; 14:51:41.535 INFO ProgressMeter - chr17:7455949 1.4 31150 22902.4; 14:51:51.542 INFO ProgressMeter - chr17:8073825 1.5 33820 22149.2; 14:52:01.549 INFO ProgressMeter - chr17:9138632 1.7 38220 22566.0; 14:52:11.962 INFO ProgressMeter - chr17:10514361 1.9 43840 23478.4; 14:52:21.975 INFO ProgressMeter - chr17:11679575 2.0 48560 23872.6; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:7233,multi-thread,multi-threaded,7233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['multi-thread'],['multi-threaded']
Performance," physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5872,load,load,5872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance," remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoise",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:2269,perform,performed,2269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['performed']
Performance," reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1019,concurren,concurrent,1019,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance," sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2049,Load,Loaded,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loaded']
Performance, shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5992,concurren,concurrent,5992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1977,optimiz,optimization,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289,1,['optimiz'],['optimization']
Performance," should proceed here, if at all? @ldgauthier reminded me that this story was unfinished and is getting a little stale. @fleharty take note if we want to report progress on this front to our MalariaGEN collaborators. On my end, there are a couple of things to do:; - [x] rebase and resolve conflicts; - [x] change TSV input as discussed above; - [x] add doc strings for new arguments; - [x] add integration tests to make absolutely sure exposure was done correctly, perhaps? I'm open to discussion about how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/present",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:934,optimiz,optimizations,934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance," still generating the error which it does not (even on my side). However, my few tests made today resulted in interesting observations. ## Traces. Below the command and trace produced from my real case. I annonymized it but the number of characters in path were kept. ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; D",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1199,Load,Loading,1199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance," the last developer who left this in a reasonable state a beverage of their choice.; # (This may be yourself, and you'll appreciate that beverage while you tinker with dependencies!); #; # When changing dependencies or versions in this file, check to see if the ""supportedPythonPackages"" DataProvider; # used by the testGATKPythonEnvironmentPackagePresent test in PythonEnvironmentIntegrationTest needs to be updated; # to reflect the changes.; #; name: gatk; channels:; # if channels other than conda-forge are added and the channel order is changed (note that conda channel_priority is currently set to flexible),; # verify that key dependencies are installed from the correct channel and compiled against MKL; - conda-forge; - defaults; dependencies:. # core python dependencies; - conda-forge::python=3.6.10 # do not update; - pip=20.0.2 # specifying channel may cause a warning to be emitted by conda; - conda-forge::mkl=2019.5 # MKL typically provides dramatic performance increases for theano, tensorflow, and other key dependencies; - conda-forge::mkl-service=2.3.0; - conda-forge::numpy=1.17.5 # do not update, this will break scipy=0.19.1; # verify that numpy is compiled against MKL (e.g., by checking *_mkl_info using numpy.show_config()); # and that it is used in tensorflow, theano, and other key dependencies; - conda-forge::theano=1.0.4 # it is unlikely that new versions of theano will be released; # verify that this is using numpy compiled against MKL (e.g., by the presence of -lmkl_rt in theano.config.blas.ldflags); - defaults::tensorflow=1.15.0 # update only if absolutely necessary, as this may cause conflicts with other core dependencies; # verify that this is using numpy compiled against MKL (e.g., by checking tensorflow.pywrap_tensorflow.IsMklEnabled()); - conda-forge::scipy=1.0.0 # do not update, this will break a scipy.misc.logsumexp import (deprecated in scipy=1.0.0) in pymc3=3.1; - conda-forge::pymc3=3.1 # do not update, this will break gcnvkernel; - conda-forge:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868:1632,perform,performance,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6656#issuecomment-643526868,2,['perform'],['performance']
Performance," the log file: ```*** Error in `javaâ€™: munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***```. The respective backtraces: . ```; *** Error in `java': double free or corruption (out): 0x00007f6364699340 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f636ba307e5]; /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f636ba3937a]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f636ba3d53c]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f63123c8fa8]; /cromwell_root/tmp.7626fbcf/libgkl_smithwaterman1454827346682980108.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f63123c8bf8]; [0x7f6355bff192]; ```. ```; *** Error in `java': munmap_chunk(): invalid pointer: 0x00007f685d06c840 ***; ======= Backtrace: =========; /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f68634c37e5]; /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7f68634d0698]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f6830cf2fa8]; /cromwell_root/tmp.4eeeda3c/libgkl_smithwaterman7538158038428947321.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f6830cf2bf8]; [0x7f684dc31f92]; ```. In each of these occurrences, the filtered vcf file was produced, but the vcf.idx file was missing. Although the java errors occur, the last line of the log denotes the step as a success: (This might be true, but only when the option --create-output-variant-index is set to false.; `SetOperationStatus(copied 0 file(s) to <destinations_folder> succeeded""`. I also performed a test based on machine type. (outside of the full workflow, starting the steps on my own on a separate instance & replicating the steps of the workflow); - Using an instance with 2 vCPU's, 7.5 GB of ram, just ran out of memory.; - Using an instance with 8 vCPU's, 30 GB of ram finished successfully, producing both the filtered vcf & vcf.idx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262:2481,perform,performed,2481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-652696262,1,['perform'],['performed']
Performance," this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give better performance on large matrices. See https://arxiv.org/pdf/1007.5510.pdf and https://research.fb.com/fast-randomized-svd/. For now, I'll require that the coverage matrix can fit in RAM, but more sophisticated versions of the algorithm could be implemented in the future.; - [ ] Update methods doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performance']
Performance," to messages in stdout. Includes # total records, number of records that were trimmed, # variant records skipped due to ref allele being too long and finally the max-indel-length value that needs to be set to include these in the leftalignandtrim. This is an improvement to previous stdout messaging. Upping max-indel-length; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --max-indel-length 250 -O zeta_snippet_leftalign_250_96branch.vcf.gz; 14:03:44.243 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 2:03:44 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 14:03:44.358 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 14:03:44.358 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 14:03:44.359 INFO LeftAlignAndTrimVariants - For support and documentation go ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:6768,Load,Loading,6768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance, | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `63.636% <84.615%> (+15.249%)` | `9 <3> (+4)` | :arrow_up: |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:3220,cache,cachemanager,3220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -V gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf -I gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/209d1183-ed9a-4755-a4b3-d595797640ea/PreProcessingForVariantDiscovery_GATK4/9f7c0ab6-b61b-4797-92f1-7929bbf677d8/call-GatherBamFiles/22.hg38.bam --bwa-mem-index-image /cromwe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:5205,multi-thread,multi-threaded,5205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['multi-thread'],['multi-threaded']
Performance,"!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1341,multi-thread,multi-threaded,1341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['multi-thread'],['multi-threaded']
Performance,"![weighted](https://user-images.githubusercontent.com/11076296/97032266-8ab9a300-152f-11eb-8d73-148ff99963be.png). Here is the result of optimizing for sensitivity in the high-confidence, low-compexity region of chr22 in CHM, allowing haplotype-to-reference and read-to-haplotype (match, mismatch, gap open) to range over ([1, 20], [-20, -1], [-20, -1]) and fixing gap extend penalties to -1. The optimal (match, mismatch, gap open) parameters found in this run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:137,optimiz,optimizing,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimizing', 'performance']"
Performance,"# Steps to reproduce; Try to do a PostprocessGermlineCNVCalls with not all the autosomal chromosomes. #### Command. ```/home/tintest/miniconda2/bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar PostprocessGermlineCNVCalls -L Refseq_GrCh38_1-3-9-17-Y-M.bed -R hg38_1-3-9-17-Y-M.fasta --calls-shard-path GermlineCNVCaller/GermlineCNVCaller-calls/ --contig-ploidy-calls DetermineGermlineContigPloidy/DetermineGermlineContigPloidy-calls/ --model-shard-path GermlineCNVCaller/GermlineCNVCaller-model --sample-index 274 --autosomal-ref-copy-number 2 --allosomal-contig Y --output-genotyped-intervals intervals/SAMPLE_274_PostprocessGermlineCNVCalls_interval.vcf --output-genotyped-segments segments/SAMPLE_274_PostprocessGermlineCNVCalls_segments.vcf```. #### Output; ```14:21:35.446 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:21:35.534 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.536 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.0.5.1; 14:21:35.537 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:21:35.538 INFO PostprocessGermlineCNVCalls - Executing as tintest@dahu63 on Linux v4.9.0-6-amd64 amd64; 14:21:35.539 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 14:21:35.541 INFO PostprocessGermlineCNVCalls - Start Date/Time: August 1, 2018 2:21:35 PM CEST; 14:21:35.542 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 14:21:35.543 INFO PostprocessGermlineCNVCalls - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231:1968,Load,Loading,1968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-409558231,1,['Load'],['Loading']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=h1) Report; > Merging [#2811](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/56e6baa79b4e56ebee5fb8d2b2288373a4269fa8?src=pr&el=desc) will **increase** coverage by `0.022%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2811 +/- ##; =============================================; + Coverage 79.978% 80% +0.022% ; - Complexity 16726 16795 +69 ; =============================================; Files 1139 1139 ; Lines 60894 61155 +261 ; Branches 9436 9497 +61 ; =============================================; + Hits 48702 48924 +222 ; - Misses 8396 8422 +26 ; - Partials 3796 3809 +13; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <Ã¸> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...ender/tools/spark/pipelines/BQSRPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQlFTUlBpcGVsaW5lU3BhcmsuamF2YQ==) | `100% <0%> (Ã¸)` | `15% <0%> (+7%)` | :arrow_up: |; | [...nstitute/hellbender/utils/help/GATKHelpDoclet.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtIZWxwRG9jbGV0LmphdmE=) | `100% <0%> (Ã¸)` | `9% <0%> (+3%)` | :arrow_up: |; | [...stitute/hellbender/cmdline/CommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/2811?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGU,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892:929,Perform,PerformAlleleFractionSegmentation,929,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2811#issuecomment-306008892,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=h1) Report; > Merging [#3036](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.331%`.; > The diff coverage is `88.554%`. ```diff; @@ Coverage Diff @@; ## master #3036 +/- ##; ===============================================; + Coverage 79.973% 80.304% +0.331% ; - Complexity 16727 17771 +1044 ; ===============================================; Files 1139 1152 +13 ; Lines 60902 65165 +4263 ; Branches 9437 10284 +847 ; ===============================================; + Hits 48705 52330 +3625 ; - Misses 8401 8900 +499 ; - Partials 3796 3935 +139; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ome/segmentation/PerformCopyRatioSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUNvcHlSYXRpb1NlZ21lbnRhdGlvbi5qYXZh) | `86.667% <Ã¸> (+6.667%)` | `4 <0> (+2)` | :arrow_up: |; | [...institute/hellbender/tools/exome/ACNVModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9BQ05WTW9kZWxsZXIuamF2YQ==) | `97.143% <Ã¸> (-0.079%)` | `17 <0> (Ã¸)` | |; | [...ellbender/tools/exome/copyratio/CopyRatioData.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9jb3B5cmF0aW8vQ29weVJhdGlvRGF0YS5qYXZh) | `95.349% <0%> (-2.27%)` | `14 <1> (+1)` | |; | [...nder/tools/exome/segmentation/AFCRHiddenState.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:960,Perform,PerformCopyRatioSegmentation,960,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformCopyRatioSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=h1) Report; > Merging [#3157](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/eab8761cbdfdaf24a5bf7551172b9f262d26d8cf?src=pr&el=desc) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3157 +/- ##; ===========================================; Coverage 80.132% 80.132% ; Complexity 16993 16993 ; ===========================================; Files 1145 1145 ; Lines 61641 61641 ; Branches 9606 9606 ; ===========================================; Hits 49394 49394 ; Misses 8419 8419 ; Partials 3828 3828; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3157?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <Ã¸> (Ã¸)` | `6 <0> (Ã¸)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829:887,Perform,PerformSegmentation,887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3157#issuecomment-311469829,1,['Perform'],['PerformSegmentation']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=h1) Report; > Merging [#3183](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/64eba53c96ea739638d34222f0f2c61c39153a64?src=pr&el=desc) will **increase** coverage by `0.05%`.; > The diff coverage is `89.931%`. ```diff; @@ Coverage Diff @@; ## master #3183 +/- ##; ==============================================; + Coverage 80.415% 80.465% +0.05% ; - Complexity 17294 17368 +74 ; ==============================================; Files 1165 1165 ; Lines 62573 62785 +212 ; Branches 9763 9789 +26 ; ==============================================; + Hits 50318 50520 +202 ; - Misses 8350 8353 +3 ; - Partials 3905 3912 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <Ã¸> (Ã¸)` | `63 <0> (Ã¸)` | :arrow_down: |; | [...nder/cmdline/ExomeStandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0V4b21lU3RhbmRhcmRBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `0% <Ã¸> (Ã¸)` | `0 <0> (Ã¸)` | :arrow_down: |; | [...der/tools/coveragemodel/nd4jutils/Nd4jIOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL25kNGp1dGlscy9OZDRqSU9VdGlscy5qYXZh) | `81.731% <Ã¸> (Ã¸)` | `19 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/TargetAnnotationCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3183?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643:932,cache,cachemanager,932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3183#issuecomment-314620643,1,['cache'],['cachemanager']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=h1) Report; > Merging [#3515](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/a218b6bbf6b3f243bce34a30f2458308319aadf3?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `84.946%`. ```diff; @@ Coverage Diff @@; ## master #3515 +/- ##; ===============================================; + Coverage 79.905% 79.917% +0.012% ; - Complexity 17918 17945 +27 ; ===============================================; Files 1199 1200 +1 ; Lines 65102 65195 +93 ; Branches 10142 10160 +18 ; ===============================================; + Hits 52020 52102 +82 ; - Misses 9042 9049 +7 ; - Partials 4040 4044 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <84.946%> (Ã¸)` | `27 <27> (?)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `78.571% <0%> (+0.649%)` | `39% <0%> (Ã¸)` | :arrow_down: |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/3515?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `73.973% <0%> (+2.74%)` | `11% <0%> (Ã¸)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094:944,optimiz,optimization,944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3515#issuecomment-325021094,1,['optimiz'],['optimization']
Performance,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=h1) Report; > Merging [#3590](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/58108d0f3f1a760884201a62469105bc55c09a29?src=pr&el=desc) will **increase** coverage by `0.358%`.; > The diff coverage is `93.939%`. ```diff; @@ Coverage Diff @@; ## master #3590 +/- ##; ===============================================; + Coverage 79.736% 80.094% +0.358% ; - Complexity 18148 18799 +651 ; ===============================================; Files 1217 1226 +9 ; Lines 66602 69015 +2413 ; Branches 10429 11073 +644 ; ===============================================; + Hits 53106 55277 +2171 ; - Misses 9289 9415 +126 ; - Partials 4207 4323 +116; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `84.946% <Ã¸> (Ã¸)` | `27 <0> (Ã¸)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `93.939% <93.939%> (Ã¸)` | `44 <44> (?)` | |; | [.../tools/spark/sv/evidence/QNamesForKmersFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9RTmFtZXNGb3JLbWVyc0ZpbmRlci5qYXZh) | `83.333% <0%> (-16.667%)` | `7% <0%> (Ã¸)` | |; | [...nder/tools/spark/pathseq/PSPathogenTaxonScore.java](https://codecov.io/gh/broadinstitute/gatk/pull/3590?src=pr&el=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077:954,optimiz,optimization,954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3590#issuecomment-330916077,1,['optimiz'],['optimization']
Performance,"$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7330,concurren,concurrent,7330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174:3959,concurren,concurrent,3959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979#issuecomment-498620174,2,['concurren'],['concurrent']
Performance,% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1878,scalab,scalable,1878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"(1) I am studying GMS mappability scores. To the best of my knowledge, it is the only such analysis that considers both paired-end reads and base calling error rate characteristic of Illumina machines. We could feed the GMS score as a feature file to the coverage collector tool for filtering. (2) I am also working on the ""optimal strategy"" for different SV types. (3) @samuelklee, do we get the same wavy pattern in other samples in the same region? in other words, it is sample-specific or region-specific?. (4) While fragment-based GC correction is difficult (and probably unnecessary) to perform without keeping a full index of aligned reads (like GS), it might be worthwhile to at least collect per-sample per-interval fragment-based average GC content (perhaps along with other summaries such as average fragment length, MQ, etc). It is easy to show that that the difference between full fragment-based GC correction and correction only using the observed average fragment GC content for the pile-up is of the order of the curvature of the GC curve, which is presumably small. We could collect these statistics either on-the-go during coverage collection, or from the sparse counts table as you suggested before (most sensible approach, once we figure out a way to represent sparse tensors).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152:593,perform,perform,593,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372413152,1,['perform'],['perform']
Performance,"(90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Conne",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1014,load,loadHeaderFromVCFUri,1014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['load'],['loadHeaderFromVCFUri']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5748,concurren,concurrent,5748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,(DefaultTaskExecutionGraph.java:355); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4622,concurren,concurrent,4622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.<init>(RawLocalFileSystem.j,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4867,concurren,concurrent,4867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,") get rid of the warnings about missing .so files. As an aside, I'm curious whether PowerPC architecture has an instruction; set similar to AVX. This is something I might actually be able to; contribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1427,Load,Loading,1427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"* The performance should be fine - TileDB/GenomicsDB stores each field in a separate file (columnar storage) and so adding MIN_DP file to the list of queried fields (~5-10 INFO fields) should be fine.; * One possible source of performance improvement - I was querying the PL field in the sites only query (not producing it in the output VariantContext objects). I think it can be dropped from the query. I was assuming that the PL field would be needed to correctly handle spanning deletions (spanning deletion corresponds to deletion allele with min PL). However, for spanning deletions, all INFO fields are dropped. Hence, any INFO fields that depend on the allele order (allele specific annotations) would be dropped for the spanning deletion. Hence, the exact deletion allele corresponding to the spanning deletion is irrelevant making it possible to drop the PL field from the query as well. Is that correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568:6,perform,performance,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377305568,2,['perform'],['performance']
Performance,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:602,perform,performance,602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,3,"['load', 'perform']","['loading', 'performance']"
Performance,"************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - -------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4325,Load,Loading,4325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['Load'],['Loading']
Performance,"**The following work has been done:**; - We performed a round of evaluations against XHMM and cn.MOPS on a cohort of 160 samples from SFARI project (which is described in our ASHG poster). For ground truth we used a callset generated from Talkowski lab SV pipeline on matched whole genome samples. Unfortunately, SFARI cohort is not public and cannot be used for public facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:44,perform,performed,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,5,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance', 'performed']"
Performance,+32 ; Lines 146768 147415 +647 ; Branches 16223 16225 +2 ; ================================================; - Hits 127666 55100 -72566 ; - Misses 13189 87388 +74199 ; + Partials 5913 4927 -986; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5732?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...s/copynumber/models/AlleleFractionInitializer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvbkluaXRpYWxpemVyLmphdmE=) | `89.063% <Ã¸> (Ã¸)` | `17 <0> (Ã¸)` | :arrow_down: |; | [...r/tools/copynumber/models/AlleleFractionState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9BbGxlbGVGcmFjdGlvblN0YXRlLmphdmE=) | `100% <Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...umber/utils/optimization/PersistenceOptimizer.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplci5qYXZh) | `77.419% <Ã¸> (-10.753%)` | `24 <0> (-4)` | |; | [...bender/tools/copynumber/models/CopyRatioState.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL21vZGVscy9Db3B5UmF0aW9TdGF0ZS5qYXZh) | `100% <Ã¸> (Ã¸)` | `5 <0> (Ã¸)` | :arrow_down: |; | [...copynumber/utils/segmentation/KernelSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5732/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL3NlZ21lbnRhdGlvbi9LZXJuZWxTZWdtZW50ZXIuamF2YQ==) | `95.671% <Ã¸> (-2.164%)` | `45 <0> (-3)` | |; | [...s/copynumber/models/AlleleFractionLikelihoods.java](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496:1595,optimiz,optimization,1595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5732#issuecomment-470293496,1,['optimiz'],['optimization']
Performance,+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (Ã¸)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (Ã¸)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (Ã¸)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (Ã¸)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> (Ã¸)` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4733,scalab,scalable,4733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,", et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. However, it is clear that GATK's present reading of the full tbi is not scalable given the memory requirements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:2214,scalab,scalable,2214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,2,['scalab'],['scalable']
Performance,"- Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 1.8392900000000002E-4; > 38 15:07:54.412 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.03020143; > 39 15:07:54.412 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.05 sec; > 40 15:07:54.413 INFO Mutect2 - Shutting down engine; > 41 [June 19, 2023 at 3:07:54 PM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; > 42 Runtime.totalMemory()=285212672; > 43 java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 1; > 4",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3628,multi-thread,multi-threaded,3628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['multi-thread'],['multi-threaded']
Performance,"- Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; 13:38:54.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:38:55.869 INFO CountReads - ------------------------------------------------------------; 13:38:55.870 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:38:55.870 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:38:55.871 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:38:55.871 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:38:55.871 INFO CountReads - Start Date/Time: January 9, 2019 1:38:54 PM EST; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.871 INFO CountReads - ------------------------------------------------------------; 13:38:55.872 INFO CountReads - HTSJDK Version: 2.18.1; 13:38:55.873 INFO CountReads - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:44003,Load,Loading,44003,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"-------------------------------------------; 16:26:35.422 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:26:35.423 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:26:35.423 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:26:35.426 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:26:35.427 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:26:35.427 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:26:35.427 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:26:35.427 INFO GenotypeGVCFs - Requester pays: disabled; 16:26:35.427 INFO GenotypeGVCFs - Initializing engine; 16:26:37.201 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:26:39.459 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:3044,load,load,3044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:54.146 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:54.146 INFO GenotypeGVCFs - Deflater: IntelDeflater; 16:27:54.146 INFO GenotypeGVCFs - Inflater: IntelInflater; 16:27:54.146 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 16:27:54.146 INFO GenotypeGVCFs - Requester pays: disabled; 16:27:54.146 INFO GenotypeGVCFs - Initializing engine; 16:27:55.873 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 16:27:58.483 INFO GenotypeGVCFs - Shutting down engine; [January 6, 2021 4:27:58 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2231894016; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:6399,load,load,6399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['load'],['load']
Performance,"-------------------------------------------; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 21:16:35.498 INFO GenotypeGVCFs - Picard Version: 2.22.8; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:16:35.498 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:16:35.498 INFO GenotypeGVCFs - Deflater: IntelDeflater; 21:16:35.499 INFO GenotypeGVCFs - Inflater: IntelInflater; 21:16:35.499 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 21:16:35.499 INFO GenotypeGVCFs - Requester pays: disabled; 21:16:35.499 INFO GenotypeGVCFs - Initializing engine; 21:16:36.737 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed.; 21:16:38.472 INFO GenotypeGVCFs - Shutting down engine; [January 17, 2021 9:16:38 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2551709696; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Couldn't create GenomicsDBFeatureReader; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:410); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:4512,load,load,4512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['load'],['load']
Performance,---------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1460850 1.3 5240 4105.1; 09:50:34.165 INFO ProgressMeter - 1:1960541 1.5 7060 4860.1; 09:50:46.537 INFO ProgressMeter - 1:2489135 1.7 8930 5383.3; 09:50:56.541 INFO ProgressMeter - 1:3195743 1.8 11330 6206,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1985,multi-thread,multi-threaded,1985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['multi-thread'],['multi-threaded']
Performance,"-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time: July 25, 2020 1:39:03 AM GMT; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.362 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.363 INFO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2358,Load,Loading,2358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFTestSample/Benchmark/362a3e75-6a39-4bde-bb79-e6562dc66dd9/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:18366,cache,cacheCopy,18366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not compl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1679,optimiz,optimized,1679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimized']
Performance,"-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(Na",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1865,load,load,1865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"-output-prefix {params.prefix} -imr OVERLAPPING_ONLY -O {output.ploidy_calls}`. When solved, {params.files} creates something like ""-I sample1.hdf5 -I sample2.hdf5"" etc... Involved **software versions**:; **gcnvkernel** = 0.7; **gatk** = 4.2.0.0; **Python** = 3.6.10. **Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.68",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1303,Load,Loading,1303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['Load'],['Loading']
Performance,. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2260,concurren,concurrent,2260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlQ2FjaGVOb2RlLmphdmE=) | `89.189% <80%> (+32.779%)` | `18 <17> (+2)` | :arrow_up: |; | [...ols/coveragemodel/CoverageModelEMComputeBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxFTUNvbXB1dGVCbG9jay5qYXZh) | `77.617% <82.558%> (-1.61%)` | `49 <2> (-1)` | |; | [...dinstitute/hellbender/utils/MathObjectAsserts.java](https://c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2910,cache,cachemanager,2910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,2,"['Cache', 'cache']","['CacheNode', 'cachemanager']"
Performance,.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Cosmic.db -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic/hg38/Cosmic.db; > 12:28:19.401 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_tissue.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_tissue/hg38/cosmic_tissue.tsv; > 12:28:19.487 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_a_bed : 100000; > 12:28:19.495 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.500 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.config; > 12:28:19.505 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:13006,cache,cache,13006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,".1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1086,Load,Loading,1086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,".679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910646 and possibly subsequent; at least 10 samples must have called genotypes 13:40:49.413 INFO ProgressMeter - chr19:55910600 0.9 186370 213817.0; 13:40:55.466 INFO HaplotypeCaller - 0 read(s) filtered by: MappingQualityReadFilter; 0 r",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13703,multi-thread,multi-threaded,13703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['multi-thread'],['multi-threaded']
Performance,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2173,concurren,concurrent,2173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575,2,['concurren'],['concurrent']
Performance,".csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFTestSample/Benchmark/87985440-93fa-4a33-ac09-e4cbead32bfb/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:14468,cache,cacheCopy,14468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,".driver.maxResultSize=0,spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2301,load,load,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['load']
Performance,".gz --tmp-dir=/tmp --sample-ploidy 24 -L chrom2; ```. It failed at the same region it was failing before, with this error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1697,load,loadNextFeature,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextFeature']
Performance,.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9183,Cache,CacheStep,9183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2473,Cache,CacheSupplementedGoogleCloudStorage,2473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,".reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7699,concurren,concurrent,7699,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.hapl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7519,concurren,concurrent,7519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); 	at shaded.clou,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6580,concurren,concurrent,6580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:192); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newFileSystem(CloudStorageFileSystemProvider.java:83); 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:336); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:140); 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:121); 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:170); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:130); 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:67); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:6410,concurren,concurrent,6410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,2,['concurren'],['concurrent']
Performance,/3146?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ellbender/tools/exome/CalculateTargetCoverage.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVUYXJnZXRDb3ZlcmFnZS5qYXZh) | `92.265% <Ã¸> (Ã¸)` | `32 <0> (Ã¸)` | :arrow_down: |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `77.143% <0%> (-2.024%)` | `10% <0%> (+4%)` | |; | [...bender/tools/exome/germlinehmm/xhmm/XHMMModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Nb2RlbC5qYXZh) | `100% <0%> (Ã¸)` | `14% <0%> (+7%)` | :arrow_up: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> (Ã¸)` | `6% <0%> (+3%)` | :arrow_up: |; | [...ender/utils/hmm/segmentation/HMMPostProcessor.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vc2VnbWVudGF0aW9uL0hNTVBvc3RQcm9jZXNzb3IuamF2YQ==) | `85.439% <0%> (+1.215%)` | `92% <0%> (+15%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (Ã¸)` | :arrow_down: |; | [.../coverage/pca/HDF5PCACoveragePoNCreationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3146?src=pr&el=tree#diff-c3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216:1843,Perform,PerformSegmentation,1843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3146#issuecomment-311542216,1,['Perform'],['PerformSegmentation']
Performance,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,perform,performance,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['perform'],['performance']
Performance,"/arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.l",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1995,load,load,1995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,/codecov.io/gh/broadinstitute/gatk/pull/4902?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `88.318% <Ã¸> (Ã¸)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `74.603% <57.143%> (+24.603%)` | :arrow_up: |; | [...ender/engine/cache/SideReadInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (Ã¸)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3J,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:2336,cache,cache,2336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.002 INFO DataSourceUtils - Setting lookahead cache for data source: Oreganno : 100000; > 12:28:18.009 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.020 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.config; > 12:28:18.120 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > 12:28:18.121 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/oreganno.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/oreganno/hg38/oreganno.tsv; > WARNING 2020-07-21 12:28:18 AsciiLineReader Creating an indexable source for an Asci,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:9578,cache,cache,9578,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/hg38/gencode_xhgnc_v90_38.hg38.tsv; > 12:28:17.925 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/achilles_lineage_results.import.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:17.932 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:17.939 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:17.939 INFO Funcotator - Finalizing data sources (this step can be long if data sources are cloud-based)...; > 12:28:17.940 INFO DataSourceUtils - Setting lookahead cache for data source: chr1_b_bed : 100000; > 12:28:17.951 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.967 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.config; > 12:28:17.995 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > 12:28:17.997 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_b_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_b_bed/hg38/chr1_b_bed.tsv; > WARNING 2020-07-21 12:28:17 AsciiLineReader Creating an index,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:8388,cache,cache,8388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Åuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6490,Load,Loading,6490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3603,concurren,concurrent,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:8716,Concurren,ConcurrentModificationException,8716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,0.3 35000 101621.1; 14:56:26.027 INFO ProgressMeter - 1:5856032 0.5 55000 105867.6; ...; 19:37:05.295 INFO ProgressMeter - GL000209.1:48811 281.2 30739000 109323.8; 19:37:15.543 INFO ProgressMeter - GL000224.1:65537 281.3 30758000 109324.9; 19:37:25.847 INFO ProgressMeter - GL000248.1:21736 281.5 30768000 109293.8; 19:37:25.906 INFO FilterMutectCalls - Finished pass 0 through the variants; 19:50:04.590 INFO FilterMutectCalls - Shutting down engine; [9 January 2020 7:50:04 PM] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 294.19 minutes.; Runtime.totalMemory()=14966849536; java.lang.IllegalArgumentException: Values in probability array sum to a negative number NaN; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:731); 	at org.broadinstitute.hellbender.utils.MathUtils.normalizeSumToOne(MathUtils.java:731); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:336); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:306); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:158); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:159); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:4476,perform,performEMIteration,4476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,2,['perform'],['performEMIteration']
Performance,"0/13 18:11:40 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:41 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 17/10/13 18:11:41 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> mg, PROXY_URI_BASES -> http://mg:8088/proxy/application_1507856833944_0003), /proxy/application_1507856833944_0003; 17/10/13 18:11:41 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 17/10/13 18:11:41 INFO yarn.Client: Application report for application_1507856833944_0003 (state: ACCEPTED); 17/10/13 18:11:42 INFO yarn.Client: Application report for application_1507856833944_0003 (state: RUNNING); 17/10/13 18:11:42 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: 10.131.101.145; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507889497661; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507856833944_0003/; 	 user: hdfs; 17/10/13 18:11:42 INFO cluster.YarnClientSchedulerBackend: Application application_1507856833944_0003 has started running.; 17/10/13 18:11:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44818.; 17/10/13 18:11:42 INFO netty.NettyBlockTransferService: Server created on 10.131.101.159:44818; 17/10/13 18:11:42 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 17/10/13 18:11:42 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:44818 with 366.3 MB RAM, BlockManagerId(driver, 10.131.101.159, 44818, None); 17/10/13 18:11:42 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:13009,queue,queue,13009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,2,['queue'],['queue']
Performance,"00; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - -------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2412,load,load,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1522,perform,performs,1522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['perform'],['performs']
Performance,"015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apache/logging/log4j/core/config/plugins/PluginVisitorStrategy.class)]]; [done in 5759 ms]; 1 error; :gatkTabComplete FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkTabComplete'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/opt/Software/gatk/build/tmp/gatkTabComplete/jadoc.options'. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 7.431 secs",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2897,load,loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,4,"['cache', 'load']","['caches', 'loading']"
Performance,"05f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFTestSample/Benchmark/7f7c4522-e293-4a03-ada8-9541a585250b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,07/25 01:37:55 Done container setup.; 2020/07/25 01:37:56 Starting localization.; 2020/07/25 01:38:02 Localization script execution started...; 2020/07/25 01:38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlig,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:1941,Load,Loading,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,"08 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15383,perform,performed,15383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['perform'],['performed']
Performance,0b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `0% <0%> (-89.583%)` | `0% <0%> (-12%)` | |; | [...adinstitute/hellbender/engine/ReadContextData.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZENvbnRleHREYXRhLmphdmE=) | `0% <0%> (-70.37%)` | `0% <0%> (-6%)` | |; | [...n/java/org/broadinstitute/hellbender/utils/KV.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9LVi5qYXZh) | `0% <0%> (-57.143%)` | `0% <0%> (-5%)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...ls/funcotator/metadata/VcfFuncotationMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1ZjZkZ1bmNvdGF0aW9uTWV0YWRhdGEuamF2YQ==) | `71.429% <0%> (-28.571%)` | `8% <0%> (+3%)` | |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `69.231% <0%> (-26.007%)` | `6% <0%> (+2%)` | |; | ... and [600 more](https://codecov.io/gh/broadinstitute/gatk/pull/5251/diff?src=pr&,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671:3052,Concurren,ConcurrentAFCalculatorProvider,3052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5251#issuecomment-426437671,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,"1	NM:i:7	MQ:i:60	AS:i:105	XS:i:20; EOF. bind 'set disable-completion off'. samtools view reads.sam -b > reads.bam; samtools index reads.bam. gatk HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF ; bcftools view output.g.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail ; gatk GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf ; bcftools view output.vcf -c1 | bcftools annotate -x INFO,FORMAT/SB,FORMAT/PL | tail. ```. output:; ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar Running: java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar HaplotypeCaller -R chr19.fa -I reads.bam -O output.g.vcf -ERC GVCF Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true 13:39:56.569 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 13:39:56.647 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.660 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0 13:39:56.666 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 13:39:56.666 INFO HaplotypeCaller - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64 13:39:56.666 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724 13:39:56.667 INFO HaplotypeCaller - Start Date/Time: October 26, 2023 at 1:39:56 PM CEST 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.667 INFO HaplotypeCaller - ------------------------------------------------------------ 13:39:56.669 INFO HaplotypeCaller - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:11059,Load,Loading,11059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,"1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40778 40780 35116 1412 0.5373 0.9665 0.6907; None 41994 41994 43760 196 0.4897 0.9954 0.6564; ::::::::::::::; NA12878/O1D1/STANDARD_NGS/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 84.000 40743 40745 35255 1447 0.5361 0.9657 0.6895; None 41955 41954 43903 235 0.4886 0.9944 0.6553; ::::::::::::::; NA12878/O1D2/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:1711,perform,performance,1711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,1,['perform'],['performance']
Performance,"1-07 11:33:52 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153), /proxy/application_1542127286896_0153; 2019-01-07 11:33:52 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-07 11:33:52 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:53 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-07 11:33:53 INFO Client:54 - Application report for application_1542127286896_0153 (state: RUNNING); 2019-01-07 11:33:53 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.193; ApplicationMaster RPC port: 0; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:53 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0153 has started running.; 2019-01-07 11:33:53 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45270.; 2019-01-07 11:33:53 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:45270; 2019-01-07 11:33:53 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:45270 with 408.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 45270, None); 2019-01-07 11:33:53 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:15896,queue,queue,15896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['queue'],['queue']
Performance,"1-09 13:35:32 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> scc-hsn1.scc.bu.edu, PROXY_URI_BASES -> https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166), /proxy/application_1542127286896_0166; 2019-01-09 13:35:33 INFO JettyUtils:54 - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 2019-01-09 13:35:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 2019-01-09 13:35:33 INFO Client:54 - Application report for application_1542127286896_0166 (state: RUNNING); 2019-01-09 13:35:33 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: 192.168.18.195; ApplicationMaster RPC port: 0; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:33 INFO YarnClientSchedulerBackend:54 - Application application_1542127286896_0166 has started running.; 2019-01-09 13:35:33 INFO Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43627.; 2019-01-09 13:35:33 INFO NettyBlockTransferService:54 - Server created on scc-hadoop.bu.edu:43627; 2019-01-09 13:35:33 INFO BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMasterEndpoint:54 - Registering block manager scc-hadoop.bu.edu:43627 with 372.6 MB RAM, BlockManagerId(driver, scc-hadoop.bu.edu, 43627, None); 2019-01-09 13:35:33 INFO BlockManagerMaster:54 - Register",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:15195,queue,queue,15195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['queue'],['queue']
Performance,"1. Why can't standard tools operate on the distributed workspaces? You could run `GenotypeGVCFs` on these workspaces in a distributed fashion and then concatenate the results together if you want. I think you were initially considering this route - and still think it would be more performant. 2. Again, you can process/query a whatever set of intervals you want -- in a distributed fashion, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782:282,perform,performant,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640791782,1,['perform'],['performant']
Performance,"113 / 643 of them were CNV test files that I deleted in #3907, which is a shade bit more than 1%... only a small fraction of these are loaded dynamically or are index/dict files, which tests will catch (and they have already, now that I check---6 / 113). I will concede that it is likely that most of the remaining files are index files, etc., but I do see a few more bams, etc. that could stand deletion. I'm curious as to what the best way to do this sort of thing actually is!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077:135,load,loaded,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348598077,1,['load'],['loaded']
Performance,"119579965 4.4 5479000 1242549.2; 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6178,concurren,concurrent,6178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,"11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /ref/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter2.test.vcf.gz --use-jdk-inflater true --use-jdk-deflater true; 19:41:38.956 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:41:39.332 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation; ```; Hope this helps and we're looking forward the GKL fix. Cheers,; Richard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:5815,Load,Loading,5815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFTestSample/Benchmark/2071078a-158e-4c3e-9b2f-907bd501821b/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.7573"",; ""EXOME1 controlindelPrecision"": ""0.6882"",; ""EXOME1 controlsnpF1Score"": ""0.9896"",; ""EXOME1 controlsnpPrecision"": ""0.9852"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1Sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.S",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:2068,load,load,2068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"1625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFTestSample/Benchmark/faae76f3-8378-4271-9822-5d2587113415/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpCl,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:6077,concurren,concurrent,6077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"20-g00a40ea-SNAPSHOT-spark.jar; Running:; /mnt/raid5/frankliu/code/SPARK/spark-2.0.2//bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1121,load,load,1121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"2; > 15:39:25.463 INFO ProgressMeter - 10:119579965 4.4 5479000 1242549.2; > 15:39:35.700 INFO ProgressMeter - 11:118752077 4.6 5530000 1207397.2; > 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; > 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6543,concurren,concurrent,6543,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"2_BM.microbe_aligned.paired.bam:33554432+33554432; 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5); java.util.NoSuchElementException: next on empty iterator; 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); 	at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); 	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31); 	at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)`. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319#issuecomment-660292360,2,['concurren'],['concurrent']
Performance,38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:12798,Load,Loading,12798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,38:02 Localizing input gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle -> /cromwell_root/gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle; 2020/07/25 01:38:40 Localizing input gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-FilterAlignmentArtifacts/attempt-3/script -> /cromwell_root/script; 2020/07/25 01:38:45 Localization script execution complete.; 2020/07/25 01:38:58 Done localization.; 2020/07/25 01:38:59 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gatk/gatk@sha256:8051adab0ff725e7e9c2af5997680346f3c3799b2df3785dd51d4abdd3da747b /bin/bash /cromwell_root/script; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.6c58e0ba; 01:39:02.909 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 01:39:02.925 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 01:39:02.927 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 01:39:03.142 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:39:03.361 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 01:39:03.361 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:39:03.361 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:39:03.362 INFO FilterAlignmentArtifacts - Executing as root@3f245e278eba on Linux v4.19.112+ amd64; 01:39:03.362 INFO FilterAlignmentArtifacts - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 01:39:03.362 INFO FilterAlignmentArtifacts - Start Date/Time:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:2095,Load,Loading,2095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,3YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <Ã¸> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <Ã¸> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <Ã¸> (Ã¸)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <Ã¸> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <Ã¸> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=gith,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:2711,scalab,scalable,2711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like itâ€™s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so itâ€™s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2454,optimiz,optimizing,2454,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,4,"['cache', 'optimiz']","['cached', 'optimizing']"
Performance,"4180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.N",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1761,load,loadLibrary,1761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['loadLibrary']
Performance,45816 +473 ; Branches 16090 16107 +17 ; ===============================================; + Hits 126517 126891 +374 ; - Misses 12966 13048 +82 ; - Partials 5860 5877 +17; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5601?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.857% <100%> (Ã¸)` | `17 <0> (Ã¸)` | :arrow_down: |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5601/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | [...broadinstitute/hellbender/engine/FeatureInput.java](https://codecov.io/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343:1611,Concurren,ConcurrentAFCalculatorProvider,1611,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601#issuecomment-456985343,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,5); at htsjdk.samtools.MemoryMappedFileBuffer.readBytes(MemoryMappedFileBuffer.java:34); at htsjdk.samtools.AbstractBAMFileIndex.readBytes(AbstractBAMFileIndex.java:439); at htsjdk.samtools.AbstractBAMFileIndex.verifyIndexMagicNumber(AbstractBAMFileIndex.java:376); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:70); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:1367,load,loadNextIterator,1367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709,1,['load'],['loadNextIterator']
Performance,"58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6318,concurren,concurrent,6318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2557,concurren,concurrent,2557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFTestSample/Benchmark/6b79227b-3ca8-4f5b-96b6-60d57760cc5b/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:22032,cache,cacheCopy,22032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFTestSample/Benchmark/e62b142c-c39c-4c1f-9a08-c41a96647879/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:21372,cache,cacheCopy,21372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/ga",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1970,Load,Loading,1970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,7.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.01,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2278,Load,Loading,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,8); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4680,concurren,concurrent,4680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"8.2; 23:10:12.683 INFO CountReadsSpark - Picard Version: 2.18.25; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 23:10:12.683 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:10:12.684 INFO CountReadsSpark - Deflater: IntelDeflater; 23:10:12.684 INFO CountReadsSpark - Inflater: IntelInflater; 23:10:12.684 INFO CountReadsSpark - GCS max retries/reopens: 20; 23:10:12.684 INFO CountReadsSpark - Requester pays: disabled; 23:10:12.684 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:10:12.685 INFO CountReadsSpark - Initializing engine; 23:10:12.685 INFO CountReadsSpark - Done initializing engine; 19/02/05 23:10:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 19/02/05 23:10:15 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 19/02/05 23:10:18 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(6,WrappedArray()); 19/02/05 23:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(13,WrappedArray()); 23:11:51.429 INFO CountReadsSpark - Shutting down engine; [February 5, 2019 11:11:51 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 1.67 minutes.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:5473,load,load,5473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,2,['load'],"['load', 'loaded']"
Performance,"9.0/gatk-package-4.1.9.0-local.jar VariantRecalibrator -V temp/vatiant_germline/sites.only.vcf.gz -O temp/vatiant_germline/recaliberation.indel.vcf --tranches-file temp/vatiant_germline/tranches.indel.txt --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.5 -tranche 99.0 -tranche 97.0 -tranche 96.0 -tranche 95.0 -tranche 94.0 -tranche 93.5 -tranche 93.0 -tranche 92.0 -tranche 91.0 -tranche 90.0 -an DP -an FS -an MQRankSum -an QD -an ReadPosRankSum -an SOR -mode INDEL --max-gaussians 4 -resource:mills,known=false,training=true,truth=true,prior=12 ~/db/mutect2_support/b37/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=2 ~/db/mutect2_support/b37/hg19_v0_dbsnp_138.b37.vcf.gz --use-allele-specific-annotations -resource:axiomPoly,known=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - -----------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532:1622,Load,Loading,1622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6701#issuecomment-726406532,1,['Load'],['Loading']
Performance,98); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Existing mirrorFile and resourceId don't match isDirectory status! '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' (dir: 'false') vs 'gs://hellbender/test/output/gatk4-spark/recalibrated.bam/' (dir: 'true'); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.getCacheEntryInternal(FileSystemBackedDirectoryListCache.java:198); 	at com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache.putResourceId(FileSystemBackedDirectoryListCache.java:363); 	at com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage.createEmptyObjects(CacheSupplementedGoogleCloudStorage.java:150); 	at com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.mkdirs(GoogleCloudStorageFileSystem.java:578); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1372); 	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881); 	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:313); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1150); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1078); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191:2418,Cache,CacheSupplementedGoogleCloudStorage,2418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271419191,1,['Cache'],['CacheSupplementedGoogleCloudStorage']
Performance,"99027448764 2.1547132944470522E-5; CSCC_0007-M1 0.00135679065051943 2.0692791516445317E-5; CSCC_0008-M1 0.004394182081805844 3.748225078434626E-5; CSCC_0009-M1 0.0019614948575730397 2.4555494660711082E-5; CSCC_0010-M1 0.004122282756273677 3.6210336627748355E-5; CSCC_0010-P1 0.001888852796306713 3.040210329291157E-5; CSCC_0011-M1 0.004616869166852859 3.9987828482322895E-5; CSCC_0012-M1 0.0013866025034395032 2.1835070542119526E-5; CSCC_0012-P1 0.9856060967650699 0.0023006992152694522; CSCC_0013-M1 0.014792148767770472 9.498068474793835E-5; CSCC_0014-M1 0.0028227703351458118 4.122117743000552E-5; CSCC_0015-M1 0.01467099675552882 9.531218938517413E-5; CSCC_0016-P1 0.0014411085088999514 2.716291906758587E-5; CSCC_0017-P1 0.0015213899870480127 2.712650453920576E-5; CSCC_0018-P1 0.001694677662867099 2.913615483470931E-5; CSCC_0019-P1 0.0016654868517623346 2.8602851235266697E-5; CSCC_0020-P1 0.0015496166402163914 2.7824601469663656E-5; ```. The procedure done is. > Base quality score recalibration was performed with GATK 4.1.2.0 (Van der Auwera et al. 2013). GATK SplitIntervals was used to define 32 evenly-sized genomic intervals over which GATK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:1563,perform,performed,1563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,9scy9leG9tZS9Bbm5vdGF0ZVRhcmdldHMuamF2YQ==) | `78.049% <Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...llbender/tools/exome/plotting/PlotACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9wbG90dGluZy9QbG90QUNOVlJlc3VsdHMuamF2YQ==) | `84.615% <Ã¸> (Ã¸)` | `22 <0> (Ã¸)` | :arrow_down: |; | [...adinstitute/hellbender/tools/exome/PadTargets.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QYWRUYXJnZXRzLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...hellbender/tools/genome/SparkGenomeReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWUvU3BhcmtHZW5vbWVSZWFkQ291bnRzLmphdmE=) | `91.089% <Ã¸> (Ã¸)` | `18 <0> (Ã¸)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <Ã¸> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...llbender/tools/walkers/validation/Concordance.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vQ29uY29yZGFuY2UuamF2YQ==) | `88.542% <Ã¸> (Ã¸)` | `28 <0> (Ã¸)` | :arrow_down: |; | [...ute/hellbender/tools/exome/ConvertACNVResults.java](https://codecov.io/gh/broadinstitute/gatk/pull/3135?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Db252ZXJ0QUNOVlJlc3VsdHMuamF2YQ==) | `87.805% <Ã¸> (Ã¸)` | `4 <0> (Ã¸)` | :arrow_down: |; | [...idation/AnnotateVcfWithExpectedAlleleFraction.java](https://codecov.io/gh/broadinstitute/gatk/pull/313,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624:2345,Perform,PerformAlleleFractionSegmentation,2345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3135#issuecomment-309876624,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2329,concurren,concurrent,2329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,": false; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:19:40.101 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:19:40.101 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:19:40.101 INFO GenomicsDBImport - Inflater: IntelInflater; 11:19:40.101 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:19:40.102 INFO GenomicsDBImport - Requester pays: disabled; 11:19:40.102 INFO GenomicsDBImport - Initializing engine; 11:19:40.385 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/data/project/reseq/KPSNY042021067K/result/03.bwa_dup_gvcf/geno/chr33.bed; 11:19:40.390 INFO IntervalArgumentCollection - Processing 10664 bp from intervals; 11:19:40.391 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 11:19:40.429 INFO GenomicsDBImport - Done initializing engine; 11:19:40.624 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.0-e701905; 11:19:40.625 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/data/chr33.db/vidmap.json; 11:19:40.625 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/data/chr33.db/callset.json; 11:19:40.625 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/data/chr33.db/vcfheader.vcf; 11:19:40.625 INFO GenomicsDBImport - Importing to workspace - /mnt/data/chr33.db; 11:19:40.625 INFO ProgressMeter - Starting traversal; 11:19:40.625 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:19:49.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:12.073 INFO GenomicsDBImport - Importing batch 1 with 1115 samples; 11:20:32.582 INFO GenomicsDBImport - Importing batch 1 with 1115",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:3383,perform,performance,3383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['perform'],['performance']
Performance,":///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect2 - Shutting down engine; [July 2, 2020 11:47:52 AM CEST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2511863808; java.lang.IllegalArgumentException: Read bases and read quality arrays aren't the same size: Bases: 38 vs Base Q's: 38 vs Insert Q's: 146 vs Delete Q's: 146.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:734); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:4348,multi-thread,multi-threaded,4348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['multi-thread'],['multi-threaded']
Performance,":118752077 4.6 5530000 1207397.2; 15:39:46.028 INFO ProgressMeter - 12:58875536 4.8 5592000 1176709.9; 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; [March 2, 2023 3:40:16 PM EST] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 5.27 minutes.; Runtime.totalMemory()=3432513536; java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to java.lang.Comparable; 	at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); 	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.util.TimSort.sort(TimSort.java:234); 	at java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); 	at java.util.Arrays.parallelSort(Arrays.java:1180); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.lang.Thread.run(Thread.java:750); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:6247,concurren,concurrent,6247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['concurren'],['concurrent']
Performance,":22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2851,Load,Loading,2851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,":49.226 INFO GenomicsDBImport - Done importing batch 2/5; 23 Feb 2022 18:26:19,107 DEBUG: 	18:26:19.105 INFO GenomicsDBImport - Done importing batch 3/5; 23 Feb 2022 19:20:18,500 DEBUG: 	19:20:18.478 INFO GenomicsDBImport - Done importing batch 4/5; 24 Feb 2022 16:51:19,017 DEBUG: 	[TileDB::utils] Error: (gzip_handle_error) Cannot decompress with GZIP: inflateInit error: Z_MEM_ERROR; 24 Feb 2022 16:51:19,048 DEBUG: 	[TileDB::Codec] Error: Could not decompress with GZIP.; 24 Feb 2022 16:51:19,056 DEBUG: 	[TileDB::ReadState] Error: Cannot decompress tile for /home/exacloud/gscratch/prime-seq/workDir/0950f56b-7565-103a-a738-f8f3fc8675d2/Job2.work/WGS_1852_consolidated.gdb/2$1$196197964/__e7217c9e-767d-4295-b75a-9162c22c6996139785909643008_1613563029631/END.tdb.; 24 Feb 2022 16:51:51,388 DEBUG: 	16:51:51.388 erro NativeGenomicsDB - pid=225263 tid=225739 VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,405 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:51:51,412 DEBUG: 	terminate called after throwing an instance of 'VariantStorageManagerException'; 24 Feb 2022 16:51:51,419 DEBUG: 	 what(): VariantStorageManagerException exception : Error while consolidating TileDB array 2$1$196197964; 24 Feb 2022 16:51:51,427 DEBUG: 	TileDB error message : ; 24 Feb 2022 16:52:27,478 WARN : 	process exited with non-zero value: 134; ```. Does that give anything to suggest troubleshooting steps?. The full command is:; ```; /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; 	Xmx497g -Xms497g -Xss2m \; 	-jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar \; 	GenomicsDBImport \; 	-V 25780.g.vcf.gz \; 	-V <total of 92 gVCFs> \; 	--genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; 	--batch-size 10 \; 	--reader-threads 12 \; 	--consolidate \; 	--genomicsdb-shared-posixfs-optimizations \; 	--bypass-feature-reader \; 	-R 128_Mmul_10.fasta; ```. this is GATK v4.2.5.0. Thanks i advance for any ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022:2931,optimiz,optimizations,2931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1050434022,1,['optimiz'],['optimizations']
Performance,":50:16.818 INFO HaplotypeCaller - Deflater: IntelDeflater; 14:50:16.818 INFO HaplotypeCaller - Inflater: IntelInflater; 14:50:16.818 INFO HaplotypeCaller - GCS max retries/reopens: 20; 14:50:16.818 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:24",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6625,Load,Loading,6625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,; 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). but if I change memory as below: it works. ; qlogin -l s_vmem=20G -l mem_req=20G,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3601,concurren,concurrent,3601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,5,['concurren'],['concurrent']
Performance,; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4364,concurren,concurrent,4364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,; Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2151,concurren,concurrent,2151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,; callable 1.0; ```; results in FilterMutectCalls exception; ```; java.lang.IllegalArgumentException: logValues must be non-infinite and non-NAN; at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeFromLogToLinearSpace(NaturalLogUtils.java:27); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.posteriorProbabilityOfError(Mutect2FilteringEngine.java:93); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:140); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSomaticVariant(SomaticClusteringModel.java:146); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.performEMIteration(SomaticClusteringModel.java:345); at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.learnAndClearAccumulatedData(SomaticClusteringModel.java:330); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.learnParameters(Mutect2FilteringEngine.java:153); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.afterNthPass(FilterMutectCalls.java:165); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:44); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047:1769,perform,performEMIteration,1769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7276#issuecomment-1293969047,2,['perform'],['performEMIteration']
Performance,; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Åuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6708,load,load,6708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,===========; Files 1139 1146 +7 ; Lines 60902 62029 +1127 ; Branches 9437 9684 +247 ; ===============================================; + Hits 48705 49752 +1047 ; - Misses 8401 8435 +34 ; - Partials 3796 3842 +46; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ellbender/tools/exome/FilterByOrientationBias.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9GaWx0ZXJCeU9yaWVudGF0aW9uQmlhcy5qYXZh) | `83.019% <Ã¸> (Ã¸)` | `14 <0> (Ã¸)` | :arrow_down: |; | [...ls/walkers/mutect/CreateSomaticPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9DcmVhdGVTb21hdGljUGFuZWxPZk5vcm1hbHMuamF2YQ==) | `100% <0%> (Ã¸)` | `10% <0%> (+3%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> (Ã¸)` | `4% <0%> (+2%)` | :arrow_up: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> (Ã¸)` | `32% <0%> (+16%)` | :arrow_up: |; | [...s/spark/pathseq/PSBuildReferenceTaxonomyUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3031?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BTQnVpbGRSZWZlcmVuY2VUYXhvbm9teVV0aWxzLmphdmE=) | `88.961% <0%> (Ã¸)` | `39% <0%> (?)` | |; | [.../hellbender/tools/spark/utils/LongBloomFilter.java](https://codecov.io/gh/broadinstitute/gat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974:1560,Perform,PerformAlleleFractionSegmentation,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3031#issuecomment-306370974,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,====================; Files 1138 1142 +4 ; Lines 62637 62823 +186 ; Branches 9521 9548 +27 ; ===============================================; + Hits 49731 49871 +140 ; - Misses 9110 9142 +32 ; - Partials 3796 3810 +14; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ber/coverage/readcount/ReadCountFileHeaderKey.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2NvdmVyYWdlL3JlYWRjb3VudC9SZWFkQ291bnRGaWxlSGVhZGVyS2V5LmphdmE=) | `0% <0%> (Ã¸)` | `0 <0> (?)` | |; | [...der/utils/test/IntegerReadCountFileComparator.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVnZXJSZWFkQ291bnRGaWxlQ29tcGFyYXRvci5qYXZh) | `70.732% <70.732%> (Ã¸)` | `6 <6> (?)` | |; | [...pynumber/utils/CachedBinarySearchIntervalList.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0NhY2hlZEJpbmFyeVNlYXJjaEludGVydmFsTGlzdC5qYXZh) | `74.603% <74.603%> (Ã¸)` | `18 <18> (?)` | |; | [...bender/tools/copynumber/CollectFragmentCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RGcmFnbWVudENvdW50cy5qYXZh) | `88.406% <88.406%> (Ã¸)` | `11 <11> (?)` | |; | [...er/tools/spark/sv/discovery/AlignmentInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQWxpZ25tZW50SW50ZXJ2YWwuamF2YQ==) | `90.517% <0%> (-0.431%)` | `63% <0%> (-1%)` | |; | [...ute/hellbender/utils/read/ArtificialReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3690?sr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649:1557,Cache,CachedBinarySearchIntervalList,1557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3690#issuecomment-335961649,1,['Cache'],['CachedBinarySearchIntervalList']
Performance,========================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `Ã¸` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `Ã¸` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1950,scalab,scalable,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (Ã¸)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (Ã¸)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (Ã¸)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (Ã¸)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3501,cache,cache,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"> 15:39:56.079 INFO ProgressMeter - 13:37022394 4.9 5628000 1143956.7; > 15:40:06.117 INFO ProgressMeter - 16:14727212 5.1 5728000 1125992.7; > 15:40:16.383 INFO SplitNCigarReads - Shutting down engine; > [March 2, 2023 3:40:16 PM EST]; > org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done.; > Elapsed time: 5.27 minutes.; > Runtime.totalMemory()=3432513536; > java.lang.ClassCastException: htsjdk.samtools.BAMRecord cannot be cast to; > java.lang.Comparable; > at java.util.Arrays$NaturalOrder.compare(Arrays.java:102); > at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); > at java.util.TimSort.sort(TimSort.java:234); > at; > java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); > at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731); > at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); > at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401); > at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734); > at java.util.Arrays.parallelSort(Arrays.java:1180); > at; > htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); > at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); > at; > htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); > at; > htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); > at; > htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); > at java.lang.Thread.run(Thread.java:750); > Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record; > to closed writer.; > at; > htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); > at; > htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); > at; > org.broadinstitute.hellbender.utils.read.SAMFi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:6761,concurren,concurrent,6761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['concurren'],['concurrent']
Performance,"> > Hi, Meng,; > > I also meet this issue, have you solve it?; > > Thank you; > ; > Hello,; > Actually, I didn't solve it completely.; > When I change to another server, it can run 2.14GB's data well.; > I think it's because the data is too large, and the server can't perform it normally.; > If it's not necessary, you can choose other tools.; > ; > Best wish; > Meng. Hi @MengZhang2019 Meng, Thank you; I also didn't solve this problem.; Do you have any tools recommend? I have tried other methods, but the running time was too long, they do not suit large sample data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636:269,perform,perform,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-625218636,1,['perform'],['perform']
Performance,"> > In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?; > ; > You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity. Okay. Rip it out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509:221,optimiz,optimization,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475756509,1,['optimiz'],['optimization']
Performance,"> > Not to mention, in theory one could have some job trying to read the original workspace, which might get hosed if some other job is trying to edit that workspace in place.; > ; > This is not possible as new GenomicsDB workspace fragments are created for the incremental updates. During the actual finalization of the fragments, the array in the workspace is locked using Posix file locks for concurrency. As @nalinigans says, individual arrays/intervals in a GenomicsDB workspace will be consistent during incremental import. One caveat though, since each array is independently updated, different arrays/intervals will finish adding samples at different times while incremental import is in progress. So, querying a workspace that is being incrementally imported into isn't recommended.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801:396,concurren,concurrency,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617405801,1,['concurren'],['concurrency']
Performance,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ22æ—¥ ä¸‹åˆ05æ—¶49åˆ†50ç§’; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:574,optimiz,optimizations,574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"> At the very least we should add a unit test that generates the evidenceIndexBySampleIndex cache, then calls marginalize() (both types) and asserts that we have emptied the cache. I would do the same for appendEvidence() and addMissingAlleles(). It's simpler than this because allele operations such as `marginalize()` and `addMissingAlleles` don't modify the evidence list. While they require care with the likelihoods arrays they don't require anything at all from the evidence-to-index caches. As I mentioned above, I left the cache updating in `appendEvidence` as it was because it was so simple. I will try to write the test for removing evidence tomorrow. Tempting to try tonight, but I'm trying to accept the reality that working until 2 am is a bad idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869:92,cache,cache,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6593#issuecomment-633180869,8,['cache'],"['cache', 'caches']"
Performance,"> Do you have numbers for the performance here? How big is this code in the profiler before or after? I'm curious. The unit tests are about 5% faster. In practice, this won't affect HaplotyeCaller because the overwhelming majority of the CPU cost of those tests comes from the ploidy = 20, allele count = 6 cases. For anything else the genotyping likelihoods calculation is not only not a bottleneck, it's completely negligible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-580337113,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"> Hi, I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception: 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/ 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------ 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes 01:13:16.078 INFO HaplotypeCaller - Initializing engine 01:13:17.087 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-1605272955,1,['Load'],['Loading']
Performance,"> Hi, Meng,; > I also meet this issue, have you solve it?; > Thank you. Hello,; Actually, I didn't solve it completely.; When I change to another server, it can run 2.14GB's data well.; I think it's because the data is too large, and the server can't perform it normally. ; If it's not necessary, you can choose other tools. Best wish; Meng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600:251,perform,perform,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6293#issuecomment-623968600,1,['perform'],['perform']
Performance,> I also encounter this error when most samples have been imported. I ran importing in batches '--batch-size 50 --consolidate '. The error occured at the last batch. Can I reuse some of the imported data files or have to rerun the whole importing again?. ...; 13:13:26.069 INFO GenomicsDBImport - Done importing batch 21/22; 13:13:26.069 INFO GenomicsDBImport - Starting batch input file preload; 13:13:27.440 INFO GenomicsDBImport - Finished batch preload; 13:13:27.440 INFO GenomicsDBImport - Importing batch 22 with 22 samples; [TileDB::Buffer] Error: Cannot read from buffer; End of buffer reached.; [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed.; terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while consolidating TileDB array chrY$1$57227415; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading tile offsets failed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886:640,load,load,640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6519#issuecomment-641091886,2,['load'],['load']
Performance,"> I think we've seen similar issues before. Libgomp needs to be installed and findable. I think it typically is installed when you install gcc. See #6012 for more discussion. Thank you, I checked my system and found gcc module not loaded. I reloaded my gcc by typing; `module load gcc/5.4.0`; and it works now. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969:231,load,loaded,231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8194#issuecomment-1425155969,2,['load'],"['load', 'loaded']"
Performance,"> If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?. Did you use the `--consolidate` option with GenomicsDBImport? This option consolidates fragments to help with query performance later. Usually not needed for very small batch sizes.; Also, available from 4.1.7.0 is a `--genomicsdb-shared-posixfs-optimizations` option. Can you try GenotypeGVCFs with this knob turned on if your workspace is on NFS/Lustre and let us know?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356:135,perform,performed,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669344356,3,"['optimiz', 'perform']","['optimizations', 'performance', 'performed']"
Performance,"> It would appear this does what it says on the tin, though I'm curious what the scenario was where the absence of the timestamp caused a call caching issue. I reuse a dataset for development (rsa_gvs_quickstart_dev). One past run was did not use compressed references, so that is always used when call caching is turned on, even though the dataset has reingested compressed references since then. This is the exact scenario that `GetBQTableLastModifiedDatetime` was created for â€” database-based tasks that we want to be able to call cache accurately.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421:534,cache,cache,534,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8667#issuecomment-1917045421,1,['cache'],['cache']
Performance,"> Just curious, why no last modified checks? Was it to keep the code simpler?. Mostly because I couldn't readily think of a scenario where I would actually want this to call cache, but I could easily imagine call caching leading to undesired clobbering of previously generated results. We can certainly revisit this decision if it turns out we're using the script in ways where we really would want call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990:174,cache,cache,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8002#issuecomment-1227739990,2,['cache'],['cache']
Performance,"> One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?; > ; > 09:03:37.460 FATAL GenomicsDBLibLoader -; > java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; > at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtilsJni.(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; > at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; > at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; > at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]. Yes, Windows is not supported by GenomicsDB. This is mentioned obliquely in the requirements for gatk too -; ```; Operating system. The GATK runs natively on most if not all flavors of UNIX, which includes MacOSX, Linux and BSD. It is possible to get it ; running on some recent versions of Windows, but we don't provide any support nor instructions for that. If you need to run on; a Windows machine, consider using Docker.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359:470,load,loadLibraryFromJar,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754106359,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"> Running with default arguments locally the runtime (for a WGS full chr15) drops from ~8.9 minutes to ~4.7 minutes after this patch. If I had to peg something else to optimize it would be replacing CSVWriter which seems to be somewhat slow but I can be contented that this tool is reasonably fast when nothing pathological is being triggered. Hello, you mentioned that running DepthOfCoverage for a WGS full chr15 only takes ~4.7 minutes. Would you mind letting me know what is the coverage for the BAM you use? It took days for me to run DepthOfCoverage on a 80X WGS. And, will there be a Spark implementation for DepthOfCoverage in the near future? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687:168,optimiz,optimize,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6740#issuecomment-723391687,1,['optimiz'],['optimize']
Performance,"> Therefore we don't actually have to take the log of the probability and normalize it, we can just take the probability straight from HypergeometricDistribution. The problem is mostly inside `HypergeometricDistribution`. A better implementation of this class should cache the last value, such that computing `hypergeo(i)` and then `hypergeo(i+1)` consecutively does not unnecessarily trigger full computation, which is quite expensive. Anyway, the current implementation is fine given that exact test is unlikely to be a bottleneck in its current use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573:267,cache,cache,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-266857573,2,"['bottleneck', 'cache']","['bottleneck', 'cache']"
Performance,"> What happen to the bundling performance improvement changes by the way?. The large 2D file array can be handled by the latest Cromwell versions, so we do not need to bundle. It is much more elegant and readable this way and should actually improve performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-672068385,2,['perform'],['performance']
Performance,">In the latest filtering paradigm, how would somebody who only wanted variants with really high quality bases change the default parameters?. You could decrease `f-score-beta` (default 1.0) to bias the threshold optimization in favor of precision versus sensitivity.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493:212,optimiz,optimization,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827#issuecomment-475728493,1,['optimiz'],['optimization']
Performance,"@Aqoolare Hello. There are a a few things going on here. The `unrecognized runtime attribute keys` warning is coming from cromwell. It's telling you that the cromwell **local** backend doesn't understand those keys, which is true. That means it's just ignoring them. I think the actual problem is different though. You're running the spark tool in spark local mode, which in this case isn't configured to use the correct amount of cores or memory. I think the intent of this wdl script was that it would be run in a container on a cluster and the container would restrict the cores and memory options. In any case, it's not configured correctly for what you need. I would skip running cromwell and just invoke gatk directly since this wdl only executes a single job. Since this is going to run spark in local mode you need to specify the number of cores using the `--spark-master` argument, and set the memory using the `--java-options ""-Xmx""` arguments. For example:. ```; gatk ReadsPipelineSpark ; --java-options ""-Xmx16G"" ; --spark-master 'local[8]'; --I yourbam.; ... etc; ```. The above command is specifying to use 8 (that's what the local[**8**] means) cores for spark and give it 16G of memory. Your job was accidentally using 200 cores so it doesn't surprise me that it would run into memory issues. Using spark with more than 16ish cores in a single process is going to bog down a lot. I think 8 is a good starting place to try. If you want to go wider you should really look into running a proper cluster (or using dataproc), but there's pretty heavy diminishing returns. Try 8 or 16 and tune the memory from there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771:1599,tune,tune,1599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1108913771,1,['tune'],['tune']
Performance,"@Aqoolare You might, but there are scaling issues when running within a single java process which is what you're doing. There are issues with lock contention and garbage collection which cause more cores to not be utilized very well. (Lots of cores waiting while garbage collection stops the world, that sort of thing.). . You could definitely test it. We found that 8-16 cores was optimal for our use cases for running in spark local mode, but spark performance is extremely finicky and it's very possible your system might do better than what I'm used to. If you wantt to go very parallel it works better to run an actual spark cluster. You should be able to utilize cores more efficiently that way, but it's more complicated to set up and operatte and there are still bottlenecks that keep it from being infinitely scaleable. (IO bandwidth and network traffic being important ones). . There are lots of articles on the internet about how to set up a local yarn cluster that can help walk you through it if you want to try. . I'm going to close this ticket since it seems like the problem is solved. Feel free to reopen or open a new one if you have other issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992:451,perform,performance,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7796#issuecomment-1111473992,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"@Bowen1992 **I got the same error, do you have a solution now?**. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx76800m -jar /home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R data/ref/CL200105941_L02.fa -V gendb://results/genotype/genodb/group2 -O results/genotype/vcfs/group2.vcf.gz --tmp-dir ./tmp; 18:24:10.205 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/zwc1988/miniconda3/envs/gatk/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:24:10.451 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.452 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 18:24:10.452 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:24:10.452 INFO GenotypeGVCFs - Executing as zwc1988@fat01 on Linux v3.10.0-957.el7.x86_64 amd64; 18:24:10.453 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 18:24:10.453 INFO GenotypeGVCFs - Start Date/Time: June 19, 2022 6:24:10 PM CST; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.453 INFO GenotypeGVCFs - ------------------------------------------------------------; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 18:24:10.454 INFO GenotypeGVCFs - Picard Version: 2.27.1; 18:24:10.454 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 18:24:10.454 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:24:10.455 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:24:10.455 INFO Genotype",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894:523,Load,Loading,523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1159695894,1,['Load'],['Loading']
Performance,"@Cashalow Can you include the full command line? 50 human genomes (ploidy 2) would need less than 7GB memory in my experience, even for highly multi-allelic indel sites. I would expect ploidy 1 calls (as most users run microbial genomes) would require even less, but we have some diploid-specific optimizations. Are you using the `new-qual` argument? That QUAL calculation algorithm is less computationally intensive and I believe it is applicable to all ploidies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977:297,optimiz,optimizations,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370462977,1,['optimiz'],['optimizations']
Performance,"@DarioS I wonder if you're looking at the JVM garbage collector threads -- by default, Java uses a multi-threaded garbage collector. You can control the number of threads it uses via the `-XX:ParallelGCThreads=N` argument, where N is the number of garbage collector threads. To pass this option into GATK, use the `--java-options` argument. Eg., `./gatk --java-options '-XX:ParallelGCThreads=4'`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075:99,multi-thread,multi-threaded,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7156#issuecomment-804282075,1,['multi-thread'],['multi-threaded']
Performance,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"@EdwardDixon I did not know that! In that case master does already require AVX. If it only impacts this tool and we provide sufficient warning and instructions, I think the single intel-optimized conda environment will be so much easier to test and maintain. Users who don't have AVX can simply install an older tensorflow in their environment, but GATK doesn't need to worry about it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837:186,optimiz,optimized,186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429142837,2,['optimiz'],['optimized']
Performance,"@EdwardDixon Thanks for this! I think the AVX check in CNNScoreVariants is good. As @droazen points out, we still want the split environments, though now with the check in place I think we can make intel optimized the default. Other thoughts @droazen, @cmnbroad ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123:204,optimiz,optimized,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428272123,1,['optimiz'],['optimized']
Performance,"@EdwardDixon Well, you'd be surprised at some of the hardware we have to deal with. Even some machines here at the Broad don't have AVX. In general, our policy with hardware-dependent optimizations in GATK has been to insist on having a transparent fallback mechanism when the required hardware isn't present -- I'd really prefer not to start making exceptions to that rule. Could the Intel-optimized Tensorflow be patched to fall back to vanilla tensorflow when AVX is not present? Is that an option? Or could it at least be patched to not actually crash in that case?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151:184,optimiz,optimizations,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5142#issuecomment-417073151,2,['optimiz'],"['optimizations', 'optimized']"
Performance,"@EvanTheB crai index is supported. This ticket is quite old, and quite a few changes have been made to CRAM in the interim (especially to index queries, some of which affected both .crai and .bai). The TL;DR version is that while there are still open tickets in htsjdk relating to CRAM performance, and some to index queries, I don't see any that would have an obvious connection to the HC discrepancies reported in the forum post.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781:286,perform,performance,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850#issuecomment-405930781,1,['perform'],['performance']
Performance,@Fruit-loops this happens for many-allelic sites. SelectVariants historically assumed every input was a germline site with PL values (genotype likelihoods) for every possible combination of alleles and ploidy and generated an expensive cache of these indices. For somatic sites this concept is meaningless and for many-allelic sites it's combinatorially expensive. The PR mentioned above skips the expensive calculation for somatic sites that lack PLs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6254#issuecomment-583801876,1,['cache'],['cache']
Performance,"@Horneth: what @lbergelson said. Java will use all of the memory you give it, regardless of how much the program asks for. The only difference is how long it takes until the memory's used up and the garbage collector needs to kick in. Thus lowering the memory requirements of a Java program tends to increase its performance, up to the point where we cut memory that it needed. . I would suggest either measuring the memory immediately after a garbage collection or, more directly, measure the performance of the program as you vary buffer sizes. Make sure to control for cache effects since the program's doing I/O.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266:313,perform,performance,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298973266,3,"['cache', 'perform']","['cache', 'performance']"
Performance,"@Horneth:. - With 100 samples, is the tool always running out of memory, or is it only running out of memory with certain buffer sizes?. - If I'm interpreting your document correctly, you found that with a buffer size of 0 or 1, performance degrades by 10%, is that right?. - Can you post the file sizes involved here? Both the size of the original GVCF inputs, *and* the size of the data from those inputs that overlaps your interval (you can find out the latter by running GATK4 SelectVariants on the GVCF using the same interval, and recording the resulting file size). I'd also like to know the sizes of the index files. - It would be good if you could post your profiling results directly in this ticket, rather than in a Google doc, so that @jean-philippe-martin (who has implemented the GCS support in GATK) can easily see them. @jean-philippe-martin Could you chime in here to remind us of your profiling results with NIO buffer sizes and the use case of a single query interval? Didn't you find that there was a very large performance difference between running with and without buffering?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579:229,perform,performance,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298927579,2,['perform'],['performance']
Performance,"@LeeTL1220 - I addressed in the next commit the documentation issues; let me know if you have more suggestions on that. If you have any suggestion for an already implemented tool in GATK for the performance and some data for profiling, I can do a couple of runs in my local computer for look at them. If not, I can implement an small example tool for profiling when splitting by sample is needed (I can also modify `ExampleLocusWalker` for that in a different PR to being able to profile this after this PR too).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961:195,perform,performance,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332227961,1,['perform'],['performance']
Performance,"@LeeTL1220 A few minor remaining comments. Do what you will. How much of a performance impact does the change have? You said it slows it down, is it significant? It might be faster if you make it a long instead of an atomic long which should be safe it it's single threaded and you don't use parallel streams anywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330,1,['perform'],['performance']
Performance,"@MartonKN I've labeled the update of the caller as a ""reach"", so I'm not expecting that it gets done before release. However, I expect that the tutorial data should be updated well before release. The tutorial data runs quickly (~1 hr for coverage collection, which is mostly limited by the slowest samples or cloud preemptions, and then ~minutes once collection has been call cached), so we should have plenty of time. Whether or not the actual tutorial itself will be ready depends on whether @sooheelee has available bandwidth and if it is a high priority for comms.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988:377,cache,cached,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826#issuecomment-353730988,1,['cache'],['cached']
Performance,"@Neato-Nick ~~No, it's very same.~~ Sorry, typing too fast. I tried it again and it crashes:. Using GATK jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz -new-qual; 21:10:39.975 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:10:43.152 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:10:43.153 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 21:10:43.153 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:10:44.334 INFO GenotypeGVCFs - Initializing engine; 21:10:44.849 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 21:10:44.979 INFO GenotypeGVCFs - Done initializing engine; 21:10:45.057 INFO ProgressMeter - Starting traversal; 21:10:45.057 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:10:45.344 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 21:10:47.780 INFO GenotypeGVCFs - Shutting down engine; [2. Äervence 2018 21:10:47 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.13 minutes.; Runtime.totalMemory()=501219328; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928:559,Load,Loading,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928,1,['Load'],['Loading']
Performance,"@RWilton The `HaplotypeCaller` performs two separate filtering passes on the read mapping qualities:. The first is in the `MappingQualityReadFilter`, and occurs before the `HaplotypeCaller` even sees the reads. This filtering pass can be controlled via the `--minimum-mapping-quality` argument you're using (and I can confirm that this argument is hooked up and does work as intended). The second pass is in `HaplotypeCallerEngine.filterNonPassingReads()`. This happens later (just before genotyping), and is hardcoded to filter out reads with mapping quality < 20. There is no way to control the mapping quality threshold in this second pass, as allowing reads with low mapping qualities into the genotyper would result in large numbers of false positive variant calls. You can confirm that the `--minimum-mapping-quality` argument for `MappingQualityReadFilter` works as intended by setting it to a value higher than 20, and inspecting the differences in the output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008:31,perform,performs,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701446008,1,['perform'],['performs']
Performance,"@SHuang-Broad . Nice hack using the cluster name. I don't see any other way to pass an arg to an initialization action. I have one suggestion to consider, but if you think it's too much work or not worth it feel free to skip: what if we separated out the reference bundle to copy from the data by specifying them both in the cluster name? That way we could, say, load either the hg19 or hg38 reference depending on the data we might be working with. So you could say ""cluster-hg38"" or ""cluster-hg19"" or ""cluster-hg19-na12878"". . Carrying it further, if we had a special convention for specifying data, like ""data-$SAMPLE"", we could just map $SAMPLE to a subdirectory on the bucket. That would provide a ton of flexibility. One minor note while you are messing with these scripts: the createCluster.sh script comment says ""This script deletes a Google Dataproc cluster used for running the GATK-SV pipeline."" Could you change to say it creates rather than deletes a cluster?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038:363,load,load,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2456#issuecomment-286876038,1,['load'],['load']
Performance,@SHuang-Broad where does this optimization around soft clipping come from? How does it get turned on or off?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699:30,optimiz,optimization,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294157699,1,['optimiz'],['optimization']
Performance,"@SaintBacchus Yes, this work is in progress. The first step was to add a ""strict"" mode to `HaplotypeCallerSpark` that causes it to match the output of the regular `HaplotypeCaller` very closely (this was done in https://github.com/broadinstitute/gatk/pull/5416). The next step will be to improve the performance of the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406:300,perform,performance,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-451206406,1,['perform'],['performance']
Performance,"@SusieX . Marking duplicates: I do recommend removing duplicates (we run MarkDuplicates from Picard). . BQSR: The pipeline we're developing is for Whole Genome data, so our bams have gone through BQSR in the whole genome pipeline. We're using those recalibrated base qualities. I haven't tested running BQSR only on the mitochondria so I don't know how well that would work. . If you do need to run BQSR only on the mitochondria I'd start by using the phylotree sites as `--known-sites`, but you'd need to have those sites in vcf format. Again, I haven't tested this so I don't know how well it will perform. If you end up using BQSR I think you're pipeline (BAM -> remove dup -> BQ recalibrate -> Mutect2 call -> FilterMutectCalls) is correct. Good luck!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996:600,perform,perform,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996,1,['perform'],['perform']
Performance,"@akiezun @droazen @lbergelson Found the issue with FTZ being cleared. The FTZ setting only applies to the thread where FTZ is set. When running tests in gradle/testng, each test is run in a new thread. However, the pairHMM native library is only loaded for the first HaplotypeCaller test, since code in `VectorLoglessPairHMM.java` prevents the library from being loaded more than once in the same JVM. This means only the first test uses FTZ. A code change that loads the pairHMM native library for each test resolves the issue, and all `HaplotypeCallerIntegrationTest` tests pass. Another option to explore is setting FTZ in `main`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701:246,load,loaded,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771#issuecomment-216259701,3,['load'],"['loaded', 'loads']"
Performance,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:79,cache,cached,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,2,"['cache', 'perform']","['cached', 'performance']"
Performance,"@akiezun thanks. I think this is ready for review now. It would be good to merge something that works, even if there are future performance and usability improvements we can do later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750#issuecomment-219460978,1,['perform'],['performance']
Performance,"@akiezun... because you are working in the performance of LIBS (#2032), could you have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235542188,1,['perform'],['performance']
Performance,"@ashwini06 . This bam appears to be malformed and it fails Picard ValidateSamFile. I think you'll need to examine the earlier stages of your pipeline that produce your bam to ensure you get a correctly formed bam. I'm going to close this ticket now since this doesn't appear to be an issue with Mutect2. (base) wm462-624:Downloads fleharty$ java -jar $PICARD ValidateSamFile I=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam ; INFO	2020-07-14 11:25:52	ValidateSamFile	. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; ********** https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** ValidateSamFile -I concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name U",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:931,Load,Loading,931,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['Load'],['Loading']
Performance,"@asmirnov239 I think that some of the optimizations that @vruano made to the postprocessing step concern the config JSONs, gCNV version, and interval list output added in #5176. These take a lot of time to localize when the number of shards is large but, aside from the interval list, aren't really used for anything, correct?. Were these just added for debugging purposes, or for reproducibility/provenance? Let's revisit whether it's necessary to pass these files on when we merge @vruano's changes into the canonical WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393:38,optimiz,optimizations,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-472862393,1,['optimiz'],['optimizations']
Performance,"@bbimber @mlathara Here is a pretty good article for optimizing the GenomicsDBImport [https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GDBI-usage-and-performance-guidelines] There is some advice about handling many small contigs that may be useful. . To troubleshoot the GenomicsDBImport high memory issue my script have, I reran the script on chr1 to narrow down the source of the high memory issue. These are running on reblocked gvcfs. . 1. Without --bypass-feature-reader and -consolidate; 2. With --bypass-feature-reader; 3. With --consolidate without --bypass-feature-reader (This ended up on a node with 384gb.) The other ran on 256GB nodes. . Test 2 ran the fastest with the lowest memory requirements (Wall clock 76 hours); Test 1 ran slower and required more memory 40-50% of 256GB (Wall Clock 94 hours); Test 3 ran initially faster with less memory than test 1 but by batch 65 it was using 75% of 384 GB. This job has not finished and appears stuck on importing batch 65. So the consolidate option appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reade",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:53,optimiz,optimizing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,4,"['optimiz', 'perform']","['optimizing', 'performance-guidelines']"
Performance,"@bbimber Let me see if I can dig up the old branches where I started this work. I'm not sure what state they're in, and I'm sure they're very out of date wrt/master, but they're probably at least worth looking at. I recall thinking that https://github.com/broadinstitute/gatk/issues/5441 would be quite a lot of work. Have you done any profiling to see where the performance issues actually are for your use case? I'd highly recommend doing that first before embarking on any big changes like this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553:363,perform,performance,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720514553,1,['perform'],['performance']
Performance,"@bbimber The -Xmx option controls only the maximum size of the *Java* heap. It does not limit the size of the C heap used by the GenomicsDB library. This is why we always advise leaving extra memory available for C when using GenomicsDB by selecting an -Xmx value that is comfortably under the amount of physical memory available. Even though your GenotypeGVCFs command is not importing into a GenomicsDB, it is reading out of a GenomicsDB and performing an on-the-fly merge, which will cause the native library to use nontrivial amounts of memory. Having said that, ~80 GB for the native heap does seem like a lot. Do you have lots of highly multi-allelic records in your callset? @nalinigans @mlathara Thoughts on this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342:444,perform,performing,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-963504342,1,['perform'],['performing']
Performance,"@bbimber There are still a lot of unaddressed comments from my review. Eg.,. https://github.com/broadinstitute/gatk/pull/4495#discussion_r189690850; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189693827; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189694685; https://github.com/broadinstitute/gatk/pull/4495#discussion_r189697161; https://github.com/broadinstitute/gatk/pull/4495#discussion_r193207051; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195236794; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242052; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195242436; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243668; https://github.com/broadinstitute/gatk/pull/4495#discussion_r195243837. You also missed a bunch of @cmnbroad 's comments from his earlier review. I'm guessing what's going on here is that you might not be expanding the `N Hidden Conversations...Load More` box that github inserts into the middle of the reviews to hide comments when there are more than a certain number of them?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678:970,Load,Load,970,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-397359678,1,['Load'],['Load']
Performance,"@bbimber There is a defrag operation that you can perform on the DB. My understanding is that it's only necessary if you have >100ish input batches. . When you say similar size combine gVcfs do you mean similar size of the input data, or similar size of the stored database compared to the merged gvcf? If you mean the later then that's expected because a genomicsdb will have much more data than an equivalent gvcf. I assume you mean the former though. I believe it is also expected that running from a genomicsdb should be slower than from a combined gvcf (up to a certain size of gvcf) because the combination operation is done at at read-time so it slows down the genotyping operation. The thing to look at is the sum of GenomicsDBImport + GenotypeGvcfs vs CombineGVCFS + GenotypeGvcfs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616:50,perform,perform,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669343616,1,['perform'],['perform']
Performance,"@bbimber You may not want to restart at this point, but the latest release (https://github.com/broadinstitute/gatk/releases/tag/4.1.8.0) has some optimizations targeted towards shared posix filesystems -- a knob called `--genomicsdb-shared-posixfs-optimizations`. It also reduces the storage space requirements for the genomicsdb workspace substantially. You could also try to check on the size of the workspace where that scatter job is writing. The import should be writing to an ""invisible"" directory (i.e., one starting with a ""."") so it may not be visible under the contig directory. But if the process is making progress, the size should increase over time.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920:146,optimiz,optimizations,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-655897920,2,['optimiz'],['optimizations']
Performance,"@bbimber question for you - what is the primary motivation for wanting to merge the scattered workspaces back into a single workspace here? I'm assuming the scatter is because you have a large enough dataset that you need multiple nodes to run the import in parallel. (side note: we're planning on enabling reading vcfs through native htslib in GenomicsDBImport which should drive down memory usage for cases that are able to take advantage of that route. This might make it more feasible to use `--max-intervals-to-import-in-parallel` for multiple threads on a single node ). If the large dataset is the primary reason, wouldn't you want the benefits of distributed processing on the query side as well? You mentioned in the previous thread that you saw the fact that a single workspace is a valid GenomicsDB workspace as a benefit...and that's certainly true - but if performance is the driving factor then it might be worth it to keep the workspace separate. @droazen Could you elaborate on what you envision we should do here? This approach should work as long as the same command line is used for each import with a different/unique interval list each time. Are you asking for a test case to be run just for sanity? Or add test cases to GATK? Or add a tool to do this...?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015:870,perform,performance,870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635081015,1,['perform'],['performance']
Performance,"@bbimber, I have placed another version of consolidate_genomicsdb_array [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array). This allows for batch-wise consolidation with the `--batch-size` or `-b` option. The tool also does better reuse of the consolidation buffers between reads, so might work a little better. Be aware that the total time to consolidate increases with the batch option and the final batches require almost as much memory as consolidating all the fragments at once. Please try consolidating any one GenomicsDB array to see how it functions as we are still working at making this scalable in a better way. This is just a draft version and if you can share some of the resulting logs, that will be very helpful. Please do let me know the total size of all the `__book_keeping.tdb.gz` files in your fragments. Just a back-of-envelope calculation, you probably need about 40 times that total size of memory to successfully consolidate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205:649,scalab,scalable,649,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081029205,1,['scalab'],['scalable']
Performance,"@bbimber, did you use `GenomicsDBImport` from v4.2.5.0 as well? How large is your /home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz file? Anyway, you can share it with us?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821:116,cache,cachedData,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042028821,1,['cache'],['cachedData']
Performance,"@bbimber, sorry that the import with consolidate did not complete. If you are amenable to using a native tool, please download the tool from [here](https://github.com/GenomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array) for consolidation. This executable will consolidate a given array in a GenomicsDB workspace, it has been instrumented to output memory stats to help tune the segment size. Note that the executable is for Centos 7, if you find any unresolved shared library dependencies during usage, please let me know and I will work on getting another one to you. For usage from a bash shell:; ```; ~/GenomicsDB: ./consolidate_genomicsdb_array; Usage: consolidate_genomicsdb_array [options]; where options include:; 	 --help, -h Print a usage message summarizing options available and exit; 	 --workspace=<GenomicsDB workspace URI>, -w <GenomicsDB workspace URI>; 		 Specify path to GenomicsDB workspace; 	 --array-name=<Array Name>, -a <Array Name>; 		 Specify name of array that requires consolidation; 	 --segment-size=<Segment Size>, -z <Segment Size>; 		 Optional, default is 10M. Specify a buffer size for consolidation; 	 --shared-posixfs-optimizations, -p; 		 Optional, default is false. If specified, the array folder is not locked for read/write and file handles are kept open until a final close for write; 	 --version Print version and exit; ```. ```; ~/GenomicsDB.: ./consolidate_genomicsdb_array -w /Users/xxx/WGS.gdb/ -a ""1\$1\$249250621"" -z 1048576 -p; 21:09:47.100 info consolidate_genomicsdb_array - pid=30881 tid=30881 Starting consolidation of 1$1$249250621 in ws; Using buffer_size=1048576 for consolidation; 21:9:47 Memory stats(pages) beginning consolidation size=45821 resident=18998 share=1824 text=3530 lib=0 data=16810 dt=0; 21:9:47 Memory stats(pages) after alloc for attribute=END size=45821 resident=19009 share=1835 text=3530 lib=0 data=16810 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=REF size=46788 resident=19743 share=18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354:393,tune,tune,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354,1,['tune'],['tune']
Performance,"@bbimber, thanks to @mlathara and @kgururaj, here is a suggestion. With `~36 attributes+offsets` and `~80 fragments` in GenomicsDB parlance getting stored, with `--genomicsdb-segment-size=1048576(1M default)`, we are looking at memory requirements in the range of 10G of memory for reading during consolidation. Just wondering, if you could try GenomicsDBImport with the following options to see if it helps.; ```; java heap options of say -Xmx100g -Xms 100g; --genomicsdb-update-workspace-path WGS_1852_consolidated.gdb \; --batch-size 10 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations \; --bypass-feature-reader \; --genomicsdb-segment-size 32768 \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333:588,optimiz,optimizations,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1051157333,1,['optimiz'],['optimizations']
Performance,"@bbimber, there was an issue with `max-alternate-alleles` getting passed to the GenomicsDB layer from `GenotypeGVCFs`. That has been fixed in this [branch](https://github.com/broadinstitute/gatk/tree/ng_genomicsdb_args) if you would like to try. See this related [GenomicsDB Issue](https://github.com/GenomicsDB/GenomicsDB/commit/3c7d1ead0110fec26d56ff85a5871d8df673504d). This will hopefully help with memory usage. On another note, there is a [performance/bug fix ](https://github.com/broadinstitute/gatk/pull/7520) while reading/writing GenomicDB bookkeeping artifacts in the latest release and in this branch as well. This may help with memory while incrementally adding new batches and using the `--consolidate` option with import.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707:446,perform,performance,446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-974487707,1,['perform'],['performance']
Performance,"@bbimber, we are investigating some scalable solutions for you. Meanwhile, can you provide the following information?; 1. What is the total and free amount of memory available to say consolidate_genomicsdb_array on your individual nodes?; 2. Sizes of all files under any one fragment say in 9$1$134124166?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294:36,scalab,scalable,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1062349294,1,['scalab'],['scalable']
Performance,"@bbimber, your approach should mostly work, this is exactly what I am going to allow with the standalone tool, a new arg for `batch-wise consolidation`. The tool is also better optimized with memory allocations and you can specify the batch size to the tool for batch-wise consolidation which should clamp down the memory use. Still testing out the tool, hopefully I can get some version to you over the weekend or on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553:177,optimiz,optimized,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1073040553,1,['optimiz'],['optimized']
Performance,"@bhanugandham As a side note, you shouldn't be running GATK4 using `java -jar` directly. You should use the included `gatk` launcher script, which sets a lot of important configuration settings, some of which have a major effect on tool performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403:237,perform,performance,237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900#issuecomment-485873403,1,['perform'],['performance']
Performance,"@bhanugandham It looks like the user is not using our provided resource file for `GetPileupSummaries`. It seems that the user selected only biallelic sites from gnomAD by hand, without removing all the extraneous info fields and restricting the exonic variants as we do in our resource. This means that the tool needs to load a huge amount of gnomAD into memory at any given time and is probably causing the crash. . If the user follows our best practices I expect the problem to go away.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994:321,load,load,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918#issuecomment-495288994,1,['load'],['load']
Performance,"@ccastane9, looks like a memory issue. Some questions -. 1. What are the sizes of the book-keeping files in your GenomicsDB workspace? Try running `find /ECA3_GenomicsDB_260 -name __book_keeping.tdb.gz -ls`.; 2. Is /ECA3_GenomicsDB_260 on NFS or another shared Posix FS? Can you try running GenotypeGVCFs with `--genomicsdb-shared-posixfs-optimizations` turned on?; 3. What does your hardware configuration look like, memory wise?; 3. What are your `-Xmx` and `-Xms` java options?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432:339,optimiz,optimizations,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-754244432,1,['optimiz'],['optimizations']
Performance,"@chandrans Can you please ask the user to test with the `4.0.0.0` (or `4.0.1.0`) release, as there were major performance improvements to the `HaplotypeCaller` just before the release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361988531,1,['perform'],['performance']
Performance,"@chandrans Hmn, I don't really know what's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdow",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:787,Perform,Performance,787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,1,['Perform'],['Performance']
Performance,"@chandrans Please ask the user to repeat the test, with the following adjustments:. * Run both GATK 3.7 and GATK 4.0.1.1 (latest release) on the same machine, one right after another, on chromosome 20 only (using `-L` in both cases), and ensure that there are no other expensive processes running on this machine during the tests. Run each version 3 times, and take the average of the results.; * Add `-pairHMM AVX_LOGLESS_CACHING` to both the GATK3 and GATK4 command lines, to guarantee that the native PairHMM will be used in both cases.; * Get rid of the `--native-pair-hmm-threads 32` in the GATK 4 command line. Too many threads can sometimes make performance worse by introducing too much contention.; * Check both the GATK3 and GATK4 output to ensure that the Intel inflater and deflater were used in both cases.; * Check both the GATK3 and GATK4 command lines to be sure they are equivalent (eg., if one is running with -ERC GVCF, the other one should as well).; * Compute wall-clock time by running the Unix `time` command, if the user is not already doing so (eg., `time ./gatk HaplotypeCaller....`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464:653,perform,performance,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4361#issuecomment-363583464,1,['perform'],['performance']
Performance,"@chapmanb We were able to reproduce a failure with your command line. This looks like an issue related to JNI and garbage collection that is exposed by setting `-Xmx46965m` and `-XX:+UseSerialGC`, but it needs further debugging. To confirm, can you please try running without specifying these javaOptions? Something like this:; ```; ./gatk-launch --javaOptions '-Djava.io.tmpdir=$TEMP_DIR' \; ApplyBQSRSpark \; --sparkMaster local[16] \; --input $BAM_IN \; --output $BAM_OUT \; --bqsr_recal_file $BQSR_RECAL \; -- \; --conf spark.local.dir=$SPARK_LOCAL_DIR; ```. FYI, we see better performance from Spark when using an SSD for spark.local.dir. The `--conf ` option above shows how to set the spark.local.dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070:582,perform,performance,582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-332370070,1,['perform'],['performance']
Performance,"@cmnbroad . The non-spark version of CountReads runs fine...... ```; gatk-4.0.12.0/gatk CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar CountReads --input /restricted/projectnb/adsp/wgs.hg38/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 15:18:43.541 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:18:45.267 INFO CountReads - ------------------------------------------------------------; 15:18:45.267 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 15:18:45.268 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:18:45.268 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 15:18:45.268 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 15:18:45.269 INFO CountReads - Start Date/Time: January 2, 2019 3:18:43 PM EST; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.269 INFO CountReads - ------------------------------------------------------------; 15:18:45.270 INFO CountReads - HTSJDK Version: 2.18.1; 15:18:45.270 INFO CountReads - Picard Version: 2.18.16; 15:18:45.270 INFO CountReads ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210:925,Load,Loading,925,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-450983210,1,['Load'],['Loading']
Performance,"@cmnbroad @lbergelson Looks like `SparkContextFactory.DEFAULT_TEST_PROPERTIES` is currently initialized statically at class-loading time, resulting in a call to `getGcsHadoopAdapterTestProperties()` even when we're not running the test suite.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481:124,load,loading,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330902481,1,['load'],['loading']
Performance,"@cmnbroad Thanks for pointing to the conda environment file. I tried to install load it but the first error it gave me was ; ```; NoPackagesFoundError: Package missing in current linux-64 channels:; - intel-openmp 2018.0.0*; ```; so I installed this package by hand with Â´conda install -c anaconda intel-openmpÂ´ and afterwards tried to install the gatkcondaenv.yml again with Â´conda env create -n gatk -f gatkcondaenv.ymlÂ´ . Unfortunately I run into the next error which says:. ```; root@k-hg-srv1:/BioinfSoftware# conda env create -n gatk -f gatkcondaenv.yml; Using Anaconda API: https://api.anaconda.org; Solving environment: done. Downloading and Extracting Packages; mkl-service 1.1.2: ################################################################################################################################################################################################################## | 100%; libgcc-ng 7.2.0: #################################################################################################################################################################################################################### | 100%; mkl 2018.0.1: ####################################################################################################################################################################################################################### | 100%; intel-openmp 2018.0.0: ############################################################################################################################################################################################################## | 100%; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Requirement 'build/gatkPythonPackageArchive.zip' looks like a filename, but the file does not exist; Processing ./build/gatkPythonPackageArchive.zip; Exception:; Traceback (most recent call last):; File ""/BioinfSoftware/Anaconda/envs/gatk/lib/python3.6/site-packages/pip/basecommand.py"", lin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460:80,load,load,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357188460,1,['load'],['load']
Performance,"@cmnbroad The results look good to me. Ideally though, shouldn't we be downloading and baking these images into our bundle instead of fetching them at page load? It seems bad to rely on an external webservice for documentation to render.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551:156,load,load,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6606#issuecomment-631708551,1,['load'],['load']
Performance,"@cmnbroad on a related note -- it might be worthwhile to setup the Docker to include a dynamic BLAS library and pass it to theano. I will test how it affects the performance. NumPy is usually either linked against MKL or OpenBLAS. If theano has no dynamic BLAS lib available to link the compiled graph against, it will fall back to NumPy for linalg ops. It is not too bad since the only cost is the c++/python communication overhead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350264808,1,['perform'],['performance']
Performance,"@cmnbroad right, at the time when we impemented the pedigree annotations we made the decision that we didn't really expect the `PossibleDeNovo` to be used in the same way as the other pedigree annotations, as it seemed to be a specific case. It would be possible to add the ability for `PossibleDeNovo` to perform generate a SampleDB from its pedigree file at construction so it will be compatible. We still couldn't use the variant annotator for `CalculateGenotypePosteriors` though as we still can't have the same argument name on the path in multiple places",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495:306,perform,perform,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495,1,['perform'],['perform']
Performance,"@cwhelan . The soft-clip -> hard-clip optimization is in lines 47-63 in `BwaMemAlignmentUtils.applyAlignment()`, which was called by `AlignedAssemblyOrExcuse.writeSAMFile()` in our discovery pipeline (BwaSparkEngine calls it as well but not affecting what we are talking about here).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235:38,optimiz,optimization,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595#issuecomment-294160235,1,['optimiz'],['optimization']
Performance,@cwhelan Can you please provide some of these calls? I am considering filtering the mapping/alignments and want to evaluate the filter's performance. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090:137,perform,performance,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3225#issuecomment-313876090,1,['perform'],['performance']
Performance,"@david-wb Interesting... what version of spark / spark submit are you using? I was pretty sure that at the point where setMaster() was called by gatk the spark context was already created by yarn and setup with the appropriate master. I wonder if it changed from a previous version of spark... Alternatively, I may be misremembering and relying on the fact that our wrapper script supplies `--sparkMaster yarn` as a gatk argument which would probably override what's being set. Could you try:; ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; --sparkMaster yarn; ```. or with the wrapper: ; ```; GATK_SPARK_JAR_ENV_VARIABLE=/home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar. ./gatk-launch \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true \; -- \; --sparkRunner SPARK \; --sparkMaster yarn; ```. Our wrapper script sets a number of properties which we think are important for running our spark tools. If running directly on spark you might want to set them explicitly. Things like `-Dsamjdk.compression_level=1` have MAJOR performance implications. . Also, be aware the BwaSpark is not in the best health at the moment may have issues. If you're running it you may want to run the version that's in this branch which is currently under review https://github.com/broadinstitute/gatk/pull/2494.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138:1498,perform,performance,1498,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718#issuecomment-301925138,1,['perform'],['performance']
Performance,"@davidbenjamin . - Downsampling is definitely a concern for us, but as you have suggested, I have utilized this parameter to not limit ourselves to 50 reads per alignment start. After running a range of values, I landed on 500 as not being too much of a burden computationally, but still allowing us to fully digest alignments at each start site in a given region. For us, I have estimated a value of 500 would cover us to a depth of about 2000 or so, since we expect to see a bias at the projected amplicon start site.; - The read filters you list don't have a huge effect on the majority of our regions. Generally, I would not expect to see more than a 5% loss based on the mapping quality filter. The read size filter should never be triggered, as we input only reads larger than 30 bases to M2. We perform duplicate removal, as we are working with UMIs, so this also should not be an issue. ; - I wouldn't expect the realignment stage to cause the type of effect I see, and as you say, it really just corrects the data anyways. **Example:**; The following was called:; ```; 1	12919623	.	C	T	.	.	DP=741;ECNT=4;POP_AF=1.000e-03;P_GERMLINE=-2.169e-04;TLOD=743.86; 	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:PGT:PID:SA_MAP_AF:SA_POST_PROB	0/1:520,210:0.291:0,0:520,210:36:153,132:57:47:0; |1:12919623_C_T:0.232,0.283,0.288:0.835,2.995e-04,0.165; ```; Also called in a different variant caller:; ```; 1	12919623	.	CC	TG	6989.54	.	AB=0.308968;ABP=423.639;AC=1;AF=0.5;AN=2;AO=410;CIGAR=2X;DP=1327;DPB=1327;DPRA=0;EPP=55.973;EPPR=139.678;GTI=0;LEN=2;MEANALT=8;MQM=50.4732;MQMR=56.3933;NS=1;NUMALT=1;ODDS=1609.4;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=13156;QR=30163;RO=900;RPL=282;RPP=128.617;RPPR=256.291;RPR=128;RUN=1;SAF=177;SAP=19.6194;SAR=233;SRF=377;SRP=54.4404;SRR=523;TYPE=mnp;technology.ILLUMINA=1	GT:DP:AD:RO:QR:AO:QA:GL	0/1:1327:900,410:900:30163:410:13156:-725.008,0,-2308.15; ```; Yes, M2 also calls the neighboring C>G substitution, these are just being represented differently between the ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595:802,perform,perform,802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3808#issuecomment-344740595,1,['perform'],['perform']
Performance,"@davidbenjamin @droazen unfortunately the new PON does not make up for the precision loss introduced in v4.1.9.0.; In v4.4.0.0 we get just 2 fewer FP SNVs in our performance evaluation, compared to the old PON.; Benchmarking results in WES tumor-normal mode, HCC1395 benchmark, and:. - v4.1.8.1 (last release with high SNV precision), v4.1.9.0 (first release affected by precision drop), v4.4.0.0 (current release); - oldPON: 1000g_pon.hg38.vcf.gz, newPON: mutect2-hg38-pon.vcf.gz; ![FD_TN_4181_FD_TN_4181_oldPON_FD_TN_4181_newPON_FD_TN_4190_FD_TN_4190_oldPON_FD_TN_4190_newPON_FD_TN_4400_FD_TN_4400_oldPON_FD_TN_4400_newPON](https://user-images.githubusercontent.com/15612230/236126940-9fc26627-260a-43c2-b409-69fbcec6ad47.png). Any chance to get this issue fixed? With Mutect3 not being available and v4.1.8.1 being affected by the log4j vulnerability, it is quite regrettable to be stuck with inferior precision. Extended methods, code, and data to reproduce the issue are here: ; [https://github.com/ddrichel/Mutect2_calling_performance_bug](https://github.com/ddrichel/Mutect2_calling_performance_bug)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043:162,perform,performance,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1534177043,1,['perform'],['performance']
Performance,"@davidbenjamin I feel like we need to put good, continuous *performance* regression tests in place for the `HaplotypeCaller` so that we can make changes of this nature without fear. Testing for a performance regression in the `HaplotypeCaller` is currently very non-trivial -- you have to run with and without intervals, with and without -ERC GVCF, on both exome and genome to be confident that you haven't killed performance in a certain mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184#issuecomment-358777173,3,['perform'],['performance']
Performance,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:107,Load,Loading,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,1,['Load'],['Loading']
Performance,"@davidbenjamin Let me do a bit of profiling/optimization on the new code path to see if I can narrow the gap. The patch was intended to address memory use rather than runtime, and could use a profiling pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547:44,optimiz,optimization,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330324547,1,['optimiz'],['optimization']
Performance,"@davidbenjamin The bug here is that we misinterpreted what a TreeSet in java does. The actual behavior for this method was that it only took the FRIST variant at each start position that it saw from the ordering of the haplotypes it saw. This meant if a SNP and INDEL started at the same position then there was a chance that site only looks like a SNP to the subsequent trimming code and we trim incorrectly. . See the TreeSet docs:; ```; <p>Note that the ordering maintained by a set (whether or not an explicit; * comparator is provided) must be <i>consistent with equals</i> if it is to; * correctly implement the {@code Set} interface. (See {@code Comparable}; * or {@code Comparator} for a precise definition of <i>consistent with; * equals</i>.) This is so because the {@code Set} interface is defined in; * terms of the {@code equals} operation, but a {@code TreeSet} instance; * performs all element comparisons using its {@code compareTo} (or; * {@code compare}) method, so two elements that are deemed equal by this method; * are, from the standpoint of the set, equal. The behavior of a set; * <i>is</i> well-defined even if its ordering is inconsistent with equals; it; * just fails to obey the general contract of the {@code Set} interface.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965:888,perform,performs,888,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6661#issuecomment-645528965,1,['perform'],['performs']
Performance,@davidbenjamin You like minor optimizations...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363:30,optimiz,optimizations,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6439#issuecomment-581622363,1,['optimiz'],['optimizations']
Performance,"@davidbenjamin looks that it works for CombineVariants now (not on the full set of data). However, I am getting a lot of random errors for HaplotypeCallerSpark (4.1.8.1):. > 20/09/15 21:46:45 ERROR Executor: Exception in task 14.0 in stage 5.0 (TID 464); > java.util.ConcurrentModificationException; > 	at java.util.ArrayList.sort(ArrayList.java:1456). after rerunning with the same parameters for some runs problems disappeared, for some doesn't, and I must rerun them once again. There was no this kind of issue when I was using 4.1.3.0 HaplotypeCallerSpark O_o. I am confused O_o for what version tools works and for what doesn't. [H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229907/H1_1.2.gatk.spark.HaplotypeCaller.gvcf.log); [H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229908/H1_2.5.gatk.spark.HaplotypeCaller.gvcf.log). in contrast to the working processes; [H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log](https://github.com/broadinstitute/gatk/files/5229912/H1_2.3.gatk.spark.HaplotypeCaller.gvcf.log). exactly same HPC infrastructure O_o. ________________; I think I have stuck once again somewhere in #5680 and #6730",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541:267,Concurren,ConcurrentModificationException,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-693185541,1,['Concurren'],['ConcurrentModificationException']
Performance,"@davidbenjamin ready for re-review. I'm going to do a little performance benchmarking in the meantime. It takes ~40min to call the whole contig for my 4000X bams, which isn't terrible, but it isn't great. Anecdotally it seems like the AF thresholding slowed things down, but I'll collect some numbers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449458286,1,['perform'],['performance']
Performance,"@davidbenjamin take note: the changes to `AssemblyRegionWalker` here have a major effect on `HaplotypeCaller` performance when run with a large (eg., whole-exome) interval list. I would expect there to also be a dramatic effect on `Mutect2` performance when run with such an interval list -- would you have time to do a quick check on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4031#issuecomment-354902432,2,['perform'],['performance']
Performance,"@davidbenjamin the benchmark data is still not ours, it is the current somatic ""gold standard"" HCC1395 from [https://www.nature.com/articles/s41587-021-00993-6](https://www.nature.com/articles/s41587-021-00993-6), based on data from multiple WGS sequencing runs with combined 1,500x coverage, etc.... Apart from that, we are on the same page here. Now that the change leading to the apparent differences in performance is found, and holds up to scrutiny so far (thanks for looking into it), it is a more real possibility than ever that this is not a bug but a case of Mutect2 outperforming the ""gold standard"". Let's keep the issue open for now, I am still investigating and would like to have a place to report my progress.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454:407,perform,performance,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1543422454,1,['perform'],['performance']
Performance,"@davidbenjamin, @lbergelson reminded me about this bug. Unlikely, but any chance the sign error in the digamma implementation (which kicks in for x >= 49) affects M2 or anything else that uses the Dirichlet class? Looks like there are also a few calls in VQSR. I'm still primarily interested in any possible performance/runtime improvements that could be gained by updating to a more recent package, perhaps one more actively developed than Apache Commons Math (e.g. Apache Commons Numbers). Seems to be some complications regarding release policies across those projects that preclude them from being updated frequently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-547988573,1,['perform'],['performance']
Performance,"@davidbenjamin, et al. I have two recommendations:. 1) Though I prefer to work symbolically and through proofs, it might be nice to first expand on the validation by proof in the JavaDoc - including for the specific function's header - and anywhere else where necessary across the GATK code, just for sanity's sake, and for tying things together neatly and properly. This process of always going through the mathematical steps alerts me when I code that I have not missed anything. . 2) When dealing with multiple levels of transformations, it probably would be good to formulate a collection of complete set of simple tests. Since like you mentioned {phased} is a subset (âŠ‚) of {unphased}, then the paths of phased genotypes one works with would also be ideal to test on. Does this function have any validation tests confirming the correct likelihoods, which would be performed for both phased and unphased genotypes? These can be generated tests, if original files do not exists. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221:869,perform,performed,869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019#issuecomment-236759221,1,['perform'],['performed']
Performance,"@davidbernick Thanks for looking into this. ; 1. It looks like we have a big 200G called /app disk but we're only writing to the root for some reason. Can we write there instead? ; 2. ðŸ‘ to removing most of the old jobs. It would be nice if we could save the job status logs but delete the artifacts and temp files. If that's hard to do though it's fine to just rotate out the old ones. Can we keep two months of jobs? That would be enough buffer so we can look back and see when something went wrong. For the performance tests you wrote, if there's any way we can keep their historical run times that would be best since they're mostly useful to see trends against the repositories history.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308:509,perform,performance,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2180#issuecomment-248319308,1,['perform'],['performance']
Performance,"@ddrichel It is regrettable that the tradeoff between precision and recall wasn't beneficial in your data, but in other data it has been. I can't call it a bug because mathematically speaking it is an improvement. At worst it can be considered an arbitrary movement along the ROC curve. Regardless, we have always developed Mutect2 and its successor under the principle that theoretical soundness is the best long-term policy. That is not to deny or disregard that some changes harm performance on some data. There is a chance that adjusting the `-f-score-beta`, which controls the relative contribution of recall and precision to the weighted F score that FilterMutectCalls seeks to optimize, will be able to reverse this shift and give up the gained recall in exchange for the lost precision. I can't promise that this will help, but fortunately you have the patched 4.1.8.1 that @droazen has kindly produced.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085:483,perform,performance,483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1538858085,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"@droazen +1 for being affected by this issue in production. As this is in production (same as @schelhorn , with big pharma which have very strict security requirements), and as 4.1.8.0 contains critical security vulnerabilities that were mitigated in subsequent releases, we are in a serious pickle here. @jhl667 how did you conclude that 4.1.8.0 performs better than newer versions? What we see is that it emits more variants, but after filtering and intersecting with other callers (i.e. Strelka), we get more variants and a ""better"" result (we can't really define ""better"" - it's merely an observation) with 4.2.4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000:347,perform,performs,347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407439000,1,['perform'],['performs']
Performance,"@droazen - That won't be solved by the current #3447, because there is no way of fine-tune the codecs: I require to being able to add/remove concrete classes, and exclude codecs from a concrete package. An example is a custom codec implementation for some feature, to provide extra-validation for the downstream toolkit. This would be even more useful if HTSJDK is moving to an interface-based library...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596:86,tune,tune,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2139#issuecomment-337622596,1,['tune'],['tune']
Performance,"@droazen - sorry for the compilation error, it was just an early optimization. Can you have a look to it? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799:65,optimiz,optimization,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799,1,['optimiz'],['optimization']
Performance,"@droazen @cmnbroad @mbabadi I generally agree with the sentiments expressed in #4127, except that I think it's OK to require a conda environment (or even use of the Docker) for these particular tools. How we should validate this requirement is another question. We can discuss more with @vdauwera. @stefandiederich Hopefully once you get the conda environment set up you will be able to run the tools. We would definitely appreciate any feedback you might be able to provide. Note that the gCNV model is relatively sophisticated, so there may be some parameters (which control the priors for the model as well as how inference is performed) that you will need to adjust for your data. Depending on the number of intervals/bins you are using and your memory constraints, you may also need to scatter across multiple GermlineCNVCaller runs; see how things are done in the WDLs here: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. As you noted, this pipeline is still in beta. We are currently running several evaluations and hope to soon release some Best Practices recommendations for the aforementioned parameter values that should work well for various data types generated at the Broad. We will also have some blog or forum posts that explain the new CNV pipelines in more detail coming soon---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364:630,perform,performed,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4125#issuecomment-357034364,4,"['perform', 'tune']","['performed', 'tuned']"
Performance,"@droazen I didn't really look at runtime scientifically, so I can't comment on that. I pasted this table and explanation into the picard issue, but I think it had already been closed at that point, so I'm not sure how many people saw it:. > I have some _small_ test BAMs that are constructed by extracting reads overlapping a few hundred kb of genome from WGS samples. I made the table below using such a BAM made from the 1KG PCR-free WGS data from NA19625. Not ideal, but I would _think_ would perform fairly similar to a full WGS bam for compression purposes. What I see is that at compression level 1 the intel deflator produces a significantly larger BAM that the JDK deflator at level=1. . Compression Level | Intel Deflater File Size | Intel Deflater % of JDK l=5 | JDK Deflater File Size | JDK Delfater % of JDK l=5; ---|------------|----------|---------------|---------; 1 | 54,840,445 | 175.23% | 38,543,684 | 123.16%; 2 | 35,782,642 | 114.33% | 36,745,494 | 117.41%; 3 | 34,989,899 | 111.80% | 35,262,326 | 112.67%; 4 | 31,815,698 | 101.66% | 32,549,560 | 104.00%; 5 | 31,240,892 | 99.82% | 31,296,433 | 100.00%; 6 | 30,675,174 | 98.01% | 30,577,906 | 97.70%; 7 | 30,379,699 | 97.07% | 30,380,325 | 97.07%; 8 | 30,124,200 | 96.25% | 30,124,375 | 96.25%; 9 | 30,064,322 | 96.06% | 30,064,325 | 96.06%. That does seem to suggest that the intel deflator at `level=2` produces a BAM that is smaller than the JDK deflator at either level 2 or 1, and if it is also faster in your testing, that sounds pretty good for intermediate files. It's still ~15% bigger than a `level=5` BAM though, so unless the vast majority of users are switching over to CRAM for storage, I'd hesitate to change the default compression level in any of the toolkits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661:496,perform,perform,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-323840661,1,['perform'],['perform']
Performance,"@droazen I hacked one of the TrainVariantAnnotationsModelIntegrationTest cases to run in your Docker (only necessary because it seems like `gradlew test --tests *TrainVariantAnnotationsModelIntegrationTest` doesn't recognize tests that use a `DataProvider`, but perhaps I did something wrong). Here are the differences:. ```; (gatk) root@a87e0994889e:/repo# h5diff -v /repo/src/test/resources/large/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/train/expected/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5 /repo/extract.nonAS.snpIndel.posUn.train.snpIndel.posOnly.IF.snp.trainingScores.hdf5. file1 file2; ---------------------------------------; x x / ; x x /data ; x x /data/scores . group : </> and </>; 0 differences found; group : </data> and </data>; 0 differences found; dataset: </data/scores> and </data/scores>; size: [445] [445]; position scores scores difference ; ------------------------------------------------------------; [ 60 ] -0.419202 -0.419202 5.55112e-17 ; 1 differences found; ```. Looks pretty negligible to me! :stuck_out_tongue_closed_eyes: Probably a result of the native code being called by the python/ML packages used in these tools; even minor changes in the compilers across Ubuntu versions might introduce differences like these. A quick fix might be to replace all system calls to `h5diff` in these tests with `h5diff --use-system-epsilon`; seems to do the trick here. But if that doesn't fix all test cases, then perhaps you can relax things with `h5diff -p EPSILON`, where `EPSILON` is a relative threshold. Probably OK to pick something like `1E-6`. OK if I leave it to you to try this or otherwise check the rest of the cases?. Sorry for the inconvenience! I think the exact-match test worked as intended here, but I probably could've put in better messaging originally. Unfortunately, it's a bit awkward to grab the output of system commands. And thanks for dealing with conda again (a necessary evil, unless we want ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931:448,scalab,scalable,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1848796931,2,['scalab'],['scalable']
Performance,@droazen I have pushed the cache removal step down to a more testable point in the code and added the assertion to the existing testing infrastructure. Can you take a quick look at this branch so it can go in at some point?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930:27,cache,cache,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-491914930,1,['cache'],['cache']
Performance,"@droazen I posted the complete command line I used (the version is above). I posted a test.sam that reproducibly fails on my machine (OSX). And below is the log from my machine:. ```; 22:42:22.298 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Aug 01, 2020 10:42:22 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; 22:42:22.412 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:42:22.412 INFO HaplotypeCaller - Executing as nhomer@ip-192-168-7-102.ec2.internal on Mac OS X v10.14.6 x86_64; 22:42:22.412 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:42:22.412 INFO HaplotypeCaller - Start Date/Time: August 1, 2020 10:42:22 PM MST; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 22:42:22.413 INFO HaplotypeCaller - Picard Version: 2.22.8; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:224,Load,Loading,224,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"@droazen I ran #4314 and it did not solve the problem. When I reverted the ADAM patch from the #4314 branch I got the normal performance. @fnothaft I wish I knew. @lbergelson said he saw more logs being produced. Another (untested) theory is that the Kryo registrations changed somehow. GATK only uses the 2bit code from ADAM, so it is surprising that it is having such an effect. I'm not sure how to track down the problem at this point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366294101,1,['perform'],['performance']
Performance,"@droazen I ran the latest version but the message about google is still there!. 14:08:05.607 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1; .9.0-GCCcore-8.3.0-Java-8/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 14, 2020 2:08:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241:120,Load,Loading,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241,1,['Load'],['Loading']
Performance,@droazen I think the BCF code is broken here too. The problem is fundamental to htsjdk. CombineVariants almost certainly has the same or similar problems because it's fundamental to combining vcfs and the fact that htsjdk doesn't handle partially empty lists. Bcftools likely has similar issues. Or loading the correct output from bcftools will recreate the issuue. What about fixing the combine operation so it can substitute default missing values with a per attribute configuration for what value to substitute?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646:299,load,loading,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677814646,1,['load'],['loading']
Performance,"@droazen I thought you might say that. We might be able to come up with an intermediate caching solution where not the entire index is cached. Although the index isn't really that big, so I'm not sure if it's a big deal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244:135,cache,cached,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2366#issuecomment-275425244,1,['cache'],['cached']
Performance,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['perform'],['performance']
Performance,"@droazen Off the top of my head, we. * cache `log10(n)` and `log10(n!)` up to some large value.; * have a fast version of `log10SumLog10(double a, double a)` that works as follows: we want to compute `log10(10^a + 10^b)`. WLOG `a < b`, so this comes out to `a + log10(1 + 10^(a - b))`. I believe we cache the values of `log10(1 + 10^(x)` over a finely-spaced grid and round `a-b` to the nearest cached `x`. . There might not be anything else. There's a lot of stuff to keep calculations in log space for numerical stability but those don't avoid `log10()` and `Math.pow()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322:39,cache,cache,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322,3,['cache'],"['cache', 'cached']"
Performance,"@droazen Responded to comments, note that i added a further escape condition where getMatchingPriors is avoided altogether if the VCpriors list is empty. Remember that this method is in a performance sensitive part of the code so every little bit of speed counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381:188,perform,performance,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381,1,['perform'],['performance']
Performance,"@droazen Thank you for the confirmation that HaplotypeCaller performs separate filtering passes on the read mapping qualities, and that the code on line 729 of HaplotypeCallerEngine.java (method ```filterNonPassingReads()``` ) is indeed executing subsequent to the ```MappingQualityReadFilter```. May I suggest, however, that MAPQ values less than 20 might not necessarily lead to an increase in FP variant calls? My understanding is that HaplotypeCaller uses MAPQ values only in a nonparametric rank sum test, in which case MAPQ is treated as an ordinal. This seems appropriate since the magnitude of a MAPQ value depends both on the data and on the computational model the read aligner uses to calculate it. With this in mind, a set of mappings with MAPQ in a lower range (e.g., ```--minimum-mapping-quality 10``` and a correspondingly lower ```--maximum-mapping-quality``` as well) might very well be appropriate for variant calling. So changing the semantics of ```MappingQualityReadFilter``` or parameterizing the currently-hardwired MAPQ range would enable additional control without affecting performance. @jamesemery I will watch for the HaplotypeCaller update that implements that functionality. And if you have a moment, could you please point me to the code that might be adversely affected by decreasing the low-end MAPQ threshold? I might have some ideas about that (or not!)... Thanks again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278:61,perform,performs,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6854#issuecomment-701512278,2,['perform'],"['performance', 'performs']"
Performance,"@droazen The thought was that we would fail at the point we try to load bases which is usually very close to the start. It moves the error back a bit which isn't great, but would allow for crams with embedded references.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983:67,load,load,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645474983,1,['load'],['load']
Performance,"@droazen Yes, it's tied for next in my queue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267:39,queue,queue,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2067#issuecomment-256740267,1,['queue'],['queue']
Performance,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:277,cache,cache,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666,1,['cache'],['cache']
Performance,"@droazen here are the error messages with gatk4.1.8.1 and gatk4.1.4.1:. 15:01:44.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.4.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:01:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine. 14:28:22.786 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.8.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 2:28:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229:112,Load,Loading,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229,2,['Load'],['Loading']
Performance,"@droazen ideally, the executor must be able to fire up one (or more) python kernels and keep it (them) alive as long as the user decides to keep it (or the GATK session terminates). Here's an example why this is desirable: the compilation of a complicated theano computational graph can take a significant portion of the total computation time. The compiled graph is a function of data dimensions, which in my use case, varies from loci to loci. My current solution to this is to pre-compile and cache a number of theano computational graphs with different sizes, pad the data to fit it to the closest matching computational graph, and re-use the same compiled graph(s) as required. To this end, one needs to keep the python kernel w/ the compiled graphs alive.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324:496,cache,cache,496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3501#issuecomment-325040324,1,['cache'],['cache']
Performance,"@droazen thanks for noticing that PR. Looks like thereâ€™s quite a bit more going on there than is covered by this issue, although I didnâ€™t take a close look. We should certainly make sure that any assumptions on SW parameters, etc. there are checked as well, if it does end up going inâ€”seems quite stale, no?. Also, hope you donâ€™t mind if I unassign myself from thisâ€”whatâ€™s the point of punting on something if it just gets reassigned to you? ;) Happy to review a PR if someone else is convinced that the original optimization is worth restoring, though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477:513,optimiz,optimization,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7441#issuecomment-908616477,1,['optimiz'],['optimization']
Performance,"@droazen this behavior hasn't changed in the most recent GenomicsDB release. . Short recap: this happens because bcf codec doesn't support the 64 bit values that GenomicsDB is returning. Running with `--genomicsdb-use-vcf-codec` will resolve it. From our discussions in the office hours, I thought we had decided to change the behavior in htsjdk so that it doesn't try to decode the type if it doesn't recognize it. (maybe I should have filed https://github.com/broadinstitute/gatk/issues/6548 in htsjdk instead? I thought I was told to do in GATK, but its been long enough that I can't remember). Another possibility is to make `--genomicsdb-use-vcf-codec` the default - though I recall you had some potential performance concerns about that. Lastly, we could change GenomicsDB to throw out a warning if a 64 bit value is needed and we're using bcf codec. Of course, this would still require the user to (re)run with `--genomicsdb-use-vcf-codec` to avoid hitting the NPE (or whatever other failure would be hit if the NPE was changed to something a bit more meaningful).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430:711,perform,performance,711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430,1,['perform'],['performance']
Performance,"@droazen this branch wasn't STRICTLY dependent on #5607, so I removed it from this branch to make reviewing easier. Its worth noting that the performance numbers and observed speedup were seen when this branch did hang off of #5607.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597:142,perform,performance,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-460423597,1,['perform'],['performance']
Performance,"@droazen would you mind reviewing/assigning? Or perhaps @jamesemery can take a look, since he expressed interest in doing similar Bayesian optimizations for an HMM? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278:139,optimiz,optimizations,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-904536278,1,['optimiz'],['optimizations']
Performance,"@droazen you were asking for a check of the performance. I ran the following two commands:. ```; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O gs://$MYBUCKET/pr.bam; $ ./gatk-launch PrintReads -I gs://$MYBUCKET/CEUTrio.HiSeq.WGS.b37.ch20.1m-2m.NA12878.bam -O /tmp/pr.bam; ```. output to: | local disk | GCS; --|--|--; run 1 | 0.12 min | 0.68 min; run 2 | 0.12 min | 0.29 min; run 3 | 0.12 min | 0.28 min; **median** |**0.12 min** | **0.29 min**. So it looks like there's a significant performance difference. For what it's worth, copying the output file to GCS from my desktop takes 3.5s. The log when running PrintReads indicates:. ```; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:06:13.011 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-336978915,2,['perform'],['performance']
Performance,"@droazen, I will address this in #2041. As you suggested this when I implemented `LocusWalker`, I would like to have some idea about why `DownsamplingMethod` is used as a parameter in the constructor. I think that this is misleading, because independently of the method for downsampling the one that is used by `SamplePartitioner` is a `ReservoirDownsampler` (if downsampling is performed), so API users could think that they are performing a different downsampling in LIBS that the actual one. I will keep the constructor in the PR, but I would like some feedback for this either here or in #2041.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006:379,perform,performed,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1879#issuecomment-235530006,2,['perform'],"['performed', 'performing']"
Performance,"@droazen, yes for luster, we should pass the `--genomicsdb-shared-posixfs-optimizations` especially with GenomicsDBImport.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698:74,optimiz,optimizations,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1032004698,1,['optimiz'],['optimizations']
Performance,"@dwuab, we are making some performance improvements with GenomicsDB and still are in the testing stage. Just wondering if you could try gatk from this branch `https://github.com/broadinstitute/gatk/tree/genomicsdb_142` to import large intervals(the ones that were problematic before) and let us know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647:27,perform,performance,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-930343647,1,['perform'],['performance']
Performance,"@erniebrau That's the performance of the pairhmm, but not the entire haplotype caller, there's definitely diminishing returns at some point. I could be a bit off on the core count before it starts leveling out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332917726,1,['perform'],['performance']
Performance,"@fleharty sorry, each curve is a different set of parameters run on the same sample, and I'm plotting all curves generated over the course of optimizing those parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444:142,optimiz,optimizing,142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712333444,1,['optimiz'],['optimizing']
Performance,@fnothaft I tried reverting 1eed8e8 and the performance was back to normal! So it would be worth reverting in ADAM for the next release if possible.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267:44,perform,performance,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-368564267,1,['perform'],['performance']
Performance,"@gspowley Yes, defining the env. var. avoided crash. It just says unable to load GKL. thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530:76,load,load,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530,1,['load'],['load']
Performance,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:319,perform,performance,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['perform'],['performance']
Performance,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:240,load,load,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188,2,['load'],['load']
Performance,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:291,perform,performance,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['perform'],['performance']
Performance,"@jamesemery While you're in the `HaplotypeCallerEngine` doing optimizations, you should profile peak memory usage as well and see if we can get it down to < 3 GB. This would reduce costs by allowing us to use cheaper instances on the cloud.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699:62,optimiz,optimizations,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2591#issuecomment-460407699,1,['optimiz'],['optimizations']
Performance,@jberghout If you post your actual output we might be able to track down your variant of the problem (the error message in the original post looks to me somewhat like a corrupt gradle cache: error reading /vsc-hard-mounts/leuven-user/304/vsc30484/.gradle/caches/modules-2/files-2.1/org.spire-math/spire_2.11/0.11.0/998b1c1d841baf4fc5d1b119ea55f165f6684ef5/spire_2.11-0.11.0.jar; error in opening zip file). Is it the `gatkTabComplete` task that is failing for you as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345:184,cache,cache,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566793345,2,['cache'],"['cache', 'caches']"
Performance,"@jean-philippe-martin I think we should do the comparison in https://github.com/broadinstitute/gatk/issues/995 before porting the ApplyBQSR optimizations, actually. If it turns out that we decide to go with the simpler broadcast approach we'd then need to figure out how the dataflow ApplyBQSR changes fit in. So it probably makes sense to spin out a separate ticket for the ApplyBQSR changes and close this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446:140,optimiz,optimizations,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/970#issuecomment-148758446,1,['optimiz'],['optimizations']
Performance,"@jean-philippe-martin In our initial tests with the latest gatk, we're still getting errors like this at a rate of ~2%:. ```; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; com.google.cloud.storage.StorageException: 503 Service Unavailable; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Remote host closed connection during handshake, for input source:; ```. Now, I know that you put in an explicit retry for 503's, so I'm wondering what could be going on. I've asked the person running the tests to check that they're using an up-to-date GATK jar, but I'm wondering if we're setting all the right retry options on the GATK side. Eg., your PR https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083 says ""but only when OptionMaxChannelReopens is set"" -- are we setting this properly? Any other thoughts on things we could try?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607:555,concurren,concurrent,555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-306944607,2,['concurren'],['concurrent']
Performance,"@jean-philippe-martin Yup, reading hundreds of different files in the same tool. We're trying to optimize the buffer size for that case, that's what we're looking at in that other thread you were commenting on. It seems like we may have some memory leak somewhere else, you seem to need much more memory than we would expect if everything is working as expected. . This branch should fix the thread leak we were seeing. ðŸ‘ Merge when tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867:97,optimiz,optimize,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2643#issuecomment-299038867,1,['optimiz'],['optimize']
Performance,"@jean-philippe-martin do you think it would be possible to fix this? The workaround is to set `GOOGLE_APPLICATION_CREDENTIALS`, but it would be preferable to have gcloud-java-nio fail when a `gs://` path is specified and the credentials are not set, rather than when the filesystem providers are loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337:296,load,loaded,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2110#issuecomment-241670337,1,['load'],['loaded']
Performance,"@jhl667 I'm looking into this. It looks like I neglected to set the sqlite connection to read only mode when connecting to the db file. I'm going to update it to do so. I'm not sure this applies when a read-only connection is created, but it looks like sqlite has some issues with NFS / distributed file systems:; - https://stackoverflow.com/questions/9907429/locking-sqlite-file-on-nfs-filesystem-possible ; - https://github.com/CGATOxford/CGATPipelines/issues/. One post in the github thread above mentions using `-o flock` when mounting Lustre partitions so that they all have concurrent locks. This _may_ be a workaround in the meantime. . I'll try to look at it on our NFS mounts - I don't have access to a Lustre fs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015:580,concurren,concurrent,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4413#issuecomment-366009015,1,['concurren'],['concurrent']
Performance,"@jjfarrell . After talking with @cmnbroad this afternoon, we'd like to ask you to perform an experiment to limit the scope where hunt down the issue. Is it possible for you to run `PrintReadsSpark` on the same cluster? That is, something similar to . ```bash; gatk --java-options ""-Djava.io.tmpdir=tmp"" \; PrintReadsSpark \; -R $REF \; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.cram \; -- \; --spark-runner SPARK \; --spark-master yarn \; --deploy-mode client \; --executor-memory 85G \; --driver-memory 30g \; --num-executors 40 \; --executor-cores 4 \; --conf spark.yarn.submit.waitAppCompletion=false \; --name ""$SAMPLE"" \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208:82,perform,perform,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208,1,['perform'],['perform']
Performance,"@jjfarrell Glad you found that article useful!. In general, `--consolidate` will be memory and time intensive. It's not intuitive, but as you already figured out if `--consolidate` is enabled, we do it on the very last batch. If you only have on the order of a few hundred batches total, not having specified consolidate shouldn't affect read performance much. The only other thing that would help scale here would be to break up your intervals so that larger contigs are split up into multiple regions. Less memory required and you can throw more cores at it (if you have them). What sort of performance did you see on `GenotypeGVCFs` or `SelectVariants`? That could be the other issue with these large intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003:343,perform,performance,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252834003,4,['perform'],['performance']
Performance,"@jjfarrell You don't need splitting index for cram. The index works around a bam specific problem which makes it hard to find good split points in the file. Cram is designed in a way that makes it easier to find the split points so the index is unnecessary. . I don't have good numbers for how long it takes to find the split points for bam. It depends on your filesystem. If you have a low latency file system like a local disk or hdfs setup than finding split points takes very little time (~seconds), but if you have a high latency file system like something backed by google object store then finding split points may take a long time (on the order of minutes to tens of minutes depending on latency and file size).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015:391,latency,latency,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371224015,3,['latency'],['latency']
Performance,"@jjfarrell and @mlathara: thanks for running these tests and sorry I havent been able to do anything yet with our data. I'm under some grant deadlines until into Oct, but I do hope to add to this. A couple comments:. 1) that broadly matches my experience. 2) My sense is that we were caught between and rock and a hard place with GenomicsDb and GenotypeGVCFs. Our workflow until this summer involved creating a workspace (running per-contig), which involved importing >1500 animals at first. This would execute OK when using a reasonable --batch-size on GenomicsDbImport. However, when we had large workspaces that were imported in lots of batches, GenotypeGVCFs (which we execute scattered, where each job works on a small interval) tended to perform badly and was a bottleneck (i.e. would effectively stall). Therefore we began to --consolidate the workspaces using GenomicsDBImport during the append process. Initially --consolidate worked; however, as @jjfarrell noted, that's memory intensive and once our workspace was a certain size, this basically died again. Therefore we even worked with @nalinigans to their the standalone GenomicsDB consolidate tool. This was a viable way to consolidate the workspaces and we successfully aggregated and consolidated all our data (which took a while). However, these massive, consolidated workspaces seem to choke GenotypeGVCFs. Therefore this process is still basically dead. 3) As I noted above, I'm currently giving up on trying to maintain permanent data in genomicsDB. There's so many advantages to not doing so, and letting the gVCFs exist as the permanent store. Notably, there are many reasons we would want/need to remake a gVCF (like the introduction of reblocking). Whenever any one of the source gVCFs changes, the workspace is basically worthless anyway (which is a massive waste of computation time). We've had great success running each GenotypeGVCFs scattered, where each job runs GenomicsDbImport on-the-fly, to make a transient workspace",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852:744,perform,perform,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1256246852,2,"['bottleneck', 'perform']","['bottleneck', 'perform']"
Performance,"@kdatta @kgururaj We have some questions about this pull request. It looks like maybe you forgot some commits, (and included some extra ones that you didn't intend to). . 1. The tests are a direct copy and paste of `PrintReadsIntegrationTests` and couldn't possibly run. Did you forget to push the commit with the actual test code? We really need some tests especially because it's not totally obvious how to use this tool and what needs to be in the jsons. . 2. The tool currently takes a partition index and a json with stream ids. The way we had envisioned this working was that it would take a `-V` argument with a list of vcfs and `-L` argument specifying what chunk of the genome to load. Can you explain why it is designed this way and if it is possible to change to use the more idiomatic input style? . 3. Can you let us know when 0.4.0 is available on maven? . We have additional review comments, but it seems premature to get into those details until the above are answered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677:689,load,load,689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277310677,1,['load'],['load']
Performance,"@kdatta I think the reason its failing is because the dylib has an unresolved transitive dependency on openssl (and possibly other things). When I debug locally, I can see that it has streamed the dylib out to a local folder and its trying to load it, but then I get this (see the highlighted text):. /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib, 1): Library not loaded: **/usr/local/opt/openssl/lib/libssl.1.0.0.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb2535884808429708562.dylib; Reason: image not found. It looks TileDB does have such a dependency ([here](https://github.com/Intel-HLS/TileDB/blob/master/CMakeLists.txt#L79)). Do you know if thats new ? I think we need to figure out how many of these there are and resolve them somehow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131:243,load,load,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294230131,2,['load'],"['load', 'loaded']"
Performance,"@kdatta Looks like the integration tests passed on travis after clearing the cache! Once you address comments, squash, and rebase onto the latest gatk master the unit tests should pass as well, since you just need the TestNG fix that got merged into master. This means we can merge this today in all likelihood!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829:77,cache,cache,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296301829,2,['cache'],['cache']
Performance,"@kdatta Note that a requirement of this feature is that the batch size should limit the number of simultaneous `FeatureReaders` open at any given time. Each `FeatureReader` has an NIO buffer around its byte stream to make queries over GCS performant, and maintaining too many of such buffers at once would likely exceed any reasonable memory limits.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686:239,perform,performant,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2613#issuecomment-296683686,1,['perform'],['performant']
Performance,"@kdatta The .so doesn't seem to load on travis, any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896:32,load,load,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296267896,1,['load'],['load']
Performance,"@kdatta Won't using a `VCFCodec` instead of a `BCF2Codec` hurt performance? Can we try to understand why the problem arises when using a `BCF2Codec` by having a look in a debugger?. Also, can you explain why those particular INFO fields are not supported? Many of them are default annotations of the `HaplotypeCaller`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226:63,perform,performance,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-293593226,1,['perform'],['performance']
Performance,"@kgururaj As I start to think about upgrading exome joint calling to use GenomicsDBImport the 100 interval threshold seems like it might be problematic. I've been working with WGS data, so I don't have much intuition for benchmarking with missing data. Is there any performance downside to running over larger intervals that include missing data? For example, if we want to scatter the exome 50 ways, each subset of the exome interval list will have ~4000 intervals, but the GVCFs won't have data outside those intervals. Does it make sense to pass to GenomicsDBImport a single interval encompassing all of those?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462:266,perform,performance,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-409956462,2,['perform'],['performance']
Performance,"@kgururaj I ran the commands you suggested. . > [user@cedar5 bin]$ bash -x TestGenomicsDBJar/run_checks.sh; > + [[ hB != hxB ]]; > + XTRACE_STATE=-x; > + [[ hxB != hxB ]]; > + VERBOSE_STATE=+v; > + set +xv; > + unset XTRACE_STATE VERBOSE_STATE; > ++ uname -s; > + osname=Linux; > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.so; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + jar xf genomicsdb--jar-with-dependencies.jar libtiledbgenomicsdb.dylib; > java.io.FileNotFoundException: genomicsdb--jar-with-dependencies.jar (No such file or directory); > at java.util.zip.ZipFile.open(Native Method); > at java.util.zip.ZipFile.<init>(ZipFile.java:219); > at java.util.zip.ZipFile.<init>(ZipFile.java:149); > at java.util.zip.ZipFile.<init>(ZipFile.java:120); > at sun.tools.jar.Main.extract(Main.java:1004); > at sun.tools.jar.Main.run(Main.java:305); > at sun.tools.jar.Main.main(Main.java:1288); > + '[' Linux == Darwin ']'; > + LIBRARY_SUFFIX=so; > + ldd libtiledbgenomicsdb.so; > ldd: ./libtiledbgenomicsdb.so: No such file or directory; > + md5sum libtiledbgenomicsdb.so; > md5sum: libtiledbgenomicsdb.so: No such file or directory. I'm using a compute canada server, so I don't have root access. The version of gatk4 I'm using was installed by their support team, and I load it using 'module load gatk'. I had that module loaded when I ran this test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071:1697,load,load,1697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-357005071,3,['load'],"['load', 'loaded']"
Performance,"@kgururaj I ran with batch size 50, using a sample map, 5 reader threads:. `java -jar GATK_GDBfork.jar GenomicsDBImport --genomicsdb-workspace-path forkTest --batch-size 50 -L chr20:45840744-45870555 --sample-name-map gnarly_reblocked_all.sample_map --reader-threads 5`. That sample map has 80K genomes because that's the project I'm working on now. Log was as follows:; ```; 16:27:48.831 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/GATK_GDBfork.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 16:27:48.947 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.948 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.3.0-24-g8804e16-SNAPSHOT; 16:27:48.948 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:48.949 INFO GenomicsDBImport - Executing as gauthier@gsa5.broadinstitute.org on Linux v2.6.32-642.15.1.el6.x86_64 amd64; 16:27:48.949 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 16:27:48.950 INFO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:416,Load,Loading,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['Load'],['Loading']
Performance,"@kgururaj Right, but we want the extra performance provided by using the BCF2Codec, if at all possible..",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092:39,perform,performance,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2632#issuecomment-297985092,1,['perform'],['performance']
Performance,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:905,Load,LoadOperatorException,905,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031,2,['Load'],['LoadOperatorException']
Performance,"@kvn95ss Can you clarify whether the performance was **faster** in the newer version (4.2.5) compared to the older version? We did do some performance work in `SelectVariants` between 4.1.8 and 4.2.5. Also, there are reports that `SelectVariants` is **much** slower if the sample names in the header are not sorted, because the tool has to reorder the genotypes on output if they're not sorted. Can you check whether your sample names are sorted?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066:37,perform,performance,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1074271066,2,['perform'],['performance']
Performance,"@kvn95ss It looks like you're using `--exclude-non-variants` with SelectVariants, which can negatively impact performance. It would be interesting to know how much leaving that out changes things, if thats an option for you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7671#issuecomment-1075132292,1,['perform'],['performance']
Performance,"@laserson the `SAMRecord` vs. Google `Read` is a loooooong story.; The super-short version:; We had a bunch of utilities written for `SAMRecord` that @droazen refactored over months to take the GATKRead interface. As it happens, the SAM spec and the GA4GH spec are not 100% compatible. So, it's not possible to losslessly convert from A -> B -> A (where A is `SAMRecord` or Google `Read`). The cases where it doesn't work are edge cases, but they exist. Second, @jean-philippe-martin found that converting to Google `Read` was fairly expensive. Between those two points, I think we're probably better off with SAM-backed reads. (Also, right now the Google `Read` is serialized via JSON, so it's not that small anyway.). @tomwhite and @jean-philippe-martin, I think adding the header back will be fine for us engineers working on the engine, but it will make for a poorer user experience for newcomers and Comp Bios to burden them with having to care about what happens with shuffles (when they just want to prototype something). . That said, I think this is probably the best approach we have at our disposal. If we do, we need to do an excellent job of throwing errors if users try to perform actions that would require the header. The error message should explain what really happened and ideally point to some documentation we write explaining the stripping of the header and how to fix it. If this error occurs, it needs to be simple for anyone to fix it. @droazen @lbergelson, what do you two think? (also @laserson, do you have any ideas or thoughts on the header since we're probably stuck with `SAMRecord`?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025:1186,perform,perform,1186,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/900#issuecomment-141086025,1,['perform'],['perform']
Performance,"@lbergelson Can you add a summary of the performance improvements (eg., runtime % speedup) introduced in this branch as a comment to the master Funcotator performance ticket (https://github.com/broadinstitute/gatk/issues/4586)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240:41,perform,performance,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240,2,['perform'],['performance']
Performance,"@lbergelson Do you have an opinion on the best way to pip install the gcnvkernel python package and dependencies for Travis testing? I've verified that the pip install works within a basic conda environment with python=3.6. We'll need to load this environment both for unit/integration tests as well as WDL tests. As long as this is the only python environment we need, I think we can simply use the base environment in the Docker. If more environments are required (e.g., for @lucidtronix), then maybe we'll need to be more clever for unit/integration tests, but we can still load them manually in the scripts that kick off the WDL tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948:238,load,load,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-348073948,4,['load'],['load']
Performance,"@lbergelson I am trying latest release, I use following command:. ```; ./gatk-4.1.3.0/gatk --java-options ""-Xmx4g"" FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R ~/human.fa/ucsc.hg19.fasta; ```. and got following Info:. ```; Using GATK jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar FilterMutectCalls -O Filtered.vcf -V Try.vcf.gz -R /home/imp/human.fa/ucsc.hg19.fasta; 09:44:27.763 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/md0/DataProcess/Ranshi/Mutect2/gatk-4.1.3.0/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 21, 2019 9:44:29 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:44:29.499 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.500 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.3.0; 09:44:29.500 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:44:29.500 INFO FilterMutectCalls - Executing as imp@imp-WorkStation on Linux v4.15.0-55-generic amd64; 09:44:29.500 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10; 09:44:29.501 INFO FilterMutectCalls - Start Date/Time: 2019å¹´8æœˆ21æ—¥ ä¸Šåˆ09æ—¶44åˆ†27ç§’; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.501 INFO FilterMutectCalls - ------------------------------------------------------------; 09:44:29.502 INFO FilterMutectCalls - HTSJDK Version: 2.20.1; 09:44:29.502 INFO FilterMutectCalls - Picard Version: 2.20.5; 09:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338:714,Load,Loading,714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6102#issuecomment-523262338,1,['Load'],['Loading']
Performance,"@lbergelson I completely agree. GenomicsDB has been a big step up from CombineGVCFs, but it's been a struggle. Here are some notable characteristics of our data, in case anything seems relevant:. - The core data is ~2K WGS (not WXS) rhesus macaque datasets. WXS tends to perform better.; - We imported them into a workspace using GenomicsDB. ; - We regularly append new batches. As the data has grown, we found we need to lower the batch size (maybe 50-100) to make GenomicsDbImport practically work. It's possible a --consolidate type step might help us; however, our earlier efforts to run this resulted in hung jobs.; - We execute these jobs scattered across a cluster that is using a lustre filesystem. We use the non-posix optimization flag.; - We include the unplaced contigs, so our genome is ~2,900 contigs.; - We currently ask GenotypeGVCFs to call sites genome-wide. I've been pondering whether we should mask repetitive regions. This might also have the effect of removing sites with incredibly high numbers of distinct alleles . I'd appreciate any ideas you or the GATK team has.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707:271,perform,perform,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964343707,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"@lbergelson I think it doesn't actually need to implement max and min at all because those are only used in unit tests. Furthermore, the `kmerCounts` in `AssemblyResultSet` themselves are only used in such methods i.e. they don't need to be members at all. You could delete `CountSet` entirely without replacing it. And even if you keep it there it is definitely not crucial for performance and could easily be any old `SortedSet`. So, the options are 1) replace with `TreeSet` or whatever; 2) delete entirely. Which would you like?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169:379,perform,performance,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2890#issuecomment-397398169,1,['perform'],['performance']
Performance,@lbergelson I updated this branch with the new key representation. After some performance runs it appears that these lead to a slightly faster mapping operation and approximately 15% less serialization for the step where they are used. Can you take a look?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222,1,['perform'],['performance']
Performance,"@lbergelson I was referring more to the middle part of the StackOverflow by Daniel Chapman - specifically the [4.1 The ObjectStreamClass Class](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a5082) and [4.6 Stream Unique Identifiers](http://docs.oracle.com/javase/8/docs/platform/serialization/spec/class.html#a4100):. _If not specified by the class, the value returned is a hash computed from the class's name, interfaces, methods, and fields using the Secure Hash Algorithm (SHA) as defined by the National Institute of Standards._. Now when I look at the `java.io.ObjectStreamClass.java` file for 64-bit JDK7 and JDK8 - from src.zip - both have the same code for the following parts after performing a `diff` - I didn't list all of the lines of code since they are quite long:. ```; public long getSerialVersionUID() {; // REMIND: synchronize instead of relying on volatile?; if (suid == null) {; suid = AccessController.doPrivileged(; new PrivilegedAction<Long>() {; public Long run() {; return computeDefaultSUID(cl);; }; }; );; }; return suid.longValue();; }; ... private static long computeDefaultSUID(Class<?> cl) {; ...very long code which can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:721,perform,performing,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,"@lbergelson If you query for output after you've terminated the process, the query will fail immediately because the Futures will have been completed with a CancellationException when the pipes were broken by the termination. But I think even that might be subject to a race condition. Previously we were dependent on stdout/stderr for synchronization and error detection, but with the ack fifo and the python exception handler installed, we really aren't anymore. We do need to fix https://github.com/broadinstitute/gatk/issues/5100, and have a better logging integration strategy, but in general I think we should seek to eliminate all use of stdout/stderr except for advisory purposes. On a separate tangent, what I'd really like to do is unify the two PythonExecutors into a single one. All of these features I'm adding like profiling, version checking, logging integration etc., will have to be done in both of them otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698:270,race condition,race condition,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698,1,['race condition'],['race condition']
Performance,"@lbergelson In your opinion, how likely is this feature to cause problems? We do still call `QueryInterval.optimizeIntervals()` to merge intervals in `ReadsDataSource` before starting an iteration, and I think that's the main example of an HTSJDK query interface that can't handle overlapping intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113:107,optimiz,optimizeIntervals,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887#issuecomment-567634113,1,['optimiz'],['optimizeIntervals']
Performance,@lbergelson Performance hit is not enough to worry about.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316495018,1,['Perform'],['Performance']
Performance,"@lbergelson The purpose of `HomogeneousPloidyModel` is so that `getPloidy(int sample)` could return `ploidy` instead of `ploidies[sample]`. The speed difference ought to be negligible, I think. I suppose there's an O(# samples) memory cost, but that can't conceivably matter, can it?. Do you think it's okay to have a single `PloidyModel`, implemented as `HeterogeneousPloidyModel` is currently? That is, do you see any point in the ""optimizations"" in `HomogeneousPloidyModel`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634:434,optimiz,optimizations,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6082#issuecomment-519354634,1,['optimiz'],['optimizations']
Performance,"@lbergelson These changes are dependent on a Barclay [PR](https://github.com/broadinstitute/barclay/pull/17) that is not merged yet, so we should at least wait for that snapshot. Ideally, the other two (tiny) Barclay PRs in the queue should also be reviewed/merged, then we could do a release and upgrade to that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891:228,queue,queue,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2327#issuecomment-271991891,1,['queue'],['queue']
Performance,"@lbergelson did you get to the bottom of why the `hdfs` provider is loaded, but the `gs` one isn't?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182:68,load,loaded,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-267992182,1,['load'],['loaded']
Performance,"@lbergelson everything I know I learned from there:; http://stackoverflow.com/questions/28939166/error-submitting-a-cloud-dataflow-job. Mine was also in the 4MB range, I switched to loading that file at the worker instead of the client and it worked. So the size limit is probably somewhere between 3 and 4MB.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863:182,load,loading,182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595#issuecomment-114594863,1,['load'],['loading']
Performance,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesnâ€™t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542,2,"['load', 'queue']","['load', 'queue']"
Performance,@lbergelson thank you for the comment and sorry for my bit late response. I excluded the dependency to the jsr203-s3a and tested that both local- and spark-gatk can access s3a files by dynamically loading it. I also added a new directory `scripts/s3a` for documentation and simple tests for s3a demonstration.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597:197,load,loading,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-665484597,2,['load'],['loading']
Performance,"@lbergelson yes, i meant the same number of WES samples as input (either merged into one gVCF or imported into one workspace), not size of data on the disk. I did not use --consolidate because your docs seemed to recommend against it. For WES, we performed the GenomicsDB import with default options, which I assume means no batching. We are doing batching on our WGS data, but that is not complete yet and we havent yet tried this for GenotypeGVCFs. I didnt realize --genomicsdb-shared-posixfs-optimizations was an option for GenotypeGVCFs, but will try this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551:247,perform,performed,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669369551,2,"['optimiz', 'perform']","['optimizations', 'performed']"
Performance,"@lbergelson, I don't think that this solution will help in this case, because another error when trying to use `CommandLineProgramTest`is that it extends `BaseTest`, which loads directly a `GenomeLocParser` for a reference that is not present and it blows up in every test. Regarding the `Main` class, because you point it out here, I would like to have some control over `Main` and how it manages things like errors or logging header. Basically all the things that I'm facing at the moment are, apart of this error using the testing framework, is that the framework have tons of mentions to the GATK itself (error messages pointing to the GATK manual page or bundle tools), and little control over which of them should be expose to the final user. Only as an example, I would like to output a line with the name and version of my software and a short notice about the usage of the GATK framework and which version I'm using (for easier maintenance, and contribution if a bug is found).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278:172,load,loads,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-242802278,1,['load'],['loads']
Performance,"@ldgauthier ; My thinking on not doing the larger tests was that the native code hasn't changed for this support, so performance and functionality shouldn't see anything unexpected. Additionally, we technically do ""incremental import"" whenever the import is batched currently. We're just extending that same paradigm to extend beyond the case where the initial GenomicsDBImport command is used. Of course, all of this is not to say I don't want to do the larger tests...just wondering if we could capture that in a separate issue? @droazen mentioned that there's a tentative plan for a new GATK release this week and we would like to have this feature in there, if you agree. We'll work in parallel on the performance testing you requested. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-518327043,2,['perform'],['performance']
Performance,"@ldgauthier ; Performance for single import is the same, and we have CI tests on the genomicsdb side as well. Admittedly we haven't done a lot of large scale performance testing using GenomicsDBImport. Can you elaborate on what you mean by accuracy for large scale imports?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985:14,Perform,Performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-517850985,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"@ldgauthier @mbabadi Revisiting this in detail in preparation for a Methods meeting presentation, I discovered that much of the initial poor model performance and the issue with the strange GQs on chr1 was actually fixed in #4335 via a seemingly innocuous change. . Prior to the change, the ploidy model used *lexicographical* sorting to determine contig order (see #4374), but the contig order of the per-contig counts matrix was determined by the *sequence dictionary*. This understandably leads to shenanigans, since the model needs to know things like the number of intervals on each contig. After the change, the sequence-dictionary order is properly used everywhere and most of the bad behavior seems to be resolved (not all of the unusual sex genotypes are resolved, but this may be partly due to mosaicism in those samples) . (@mbabadi, not sure if we actually realized this before? I don't recall nor do I see it discussed elsewhere, but if so, then consider this a note of it!). So the problems with this model are not as severe as we initially thought, and thus we can keep this issue at relatively low priority. Nevertheless, the improved model should still yield additional benefits, such as better depth estimates and ploidy qualities, as well as more robustness to unusual sex genotypes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311:147,perform,performance,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-414852311,1,['perform'],['performance']
Performance,"@ldgauthier I think it's probably best to try and get to the bottom of why this variant's qual isn't being adjusted as it's reduced from 7 alleles in the gVCF to 2 alleles in the called VCF. This is, as you guessed, all single-sample. I've attached a reduced gVCF that just includes the variant in question, and the resulting genotyped VCF from running the command line below (and their indices) ; [here](https://github.com/broadinstitute/gatk/files/3059414/gatk-5793-testcase.tar.gz). ```; gatk GenotypeGVCFs \; -R /Work/refseq/hg19/hg19.fasta \; -V HG02568.g.vcf \; -O HG02568.vcf \; -L chr11:6637700-6637800 \; -stand-call-conf 30; ```. A few observations from running the above command but varying the `-stand-call-conf` at that locus, all performed with GATK 4.1.1.0:; - Running with `-stand-call-conf 30` results in a reduction from 7->2 alleles but no change at all in QUAL; - Running with `-stand-call-conf 0` results in the same genotype and QUAL, but another allele squeaks through even though it's not referenced in the genotype; - Running with `-stand-call-conf 100` results in no variants being emitted. Circling back to one of my original statements, I believe the least confusing way for this to work would be to think of it this way:; - If you run with `-stand-call-conf 0` you should see all variants; - If you run with `-stand-call-conf n` you should only lose variants that were previously emitted with `-stand-call-conf 0` that had QUAL < n. That said, it sounds like maybe the problem is less with the filtering on QUAL and more to do with the calculation of the final QUAL that ends up in the VCF?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026:744,perform,performed,744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793#issuecomment-481273026,1,['perform'],['performed']
Performance,"@ldgauthier I'm in complete agreement, I don't think we should optimize outside the high confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510:63,optimiz,optimize,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714549510,1,['optimiz'],['optimize']
Performance,"@ldgauthier Indexing inputs on the fly is prone to race conditions, so we decided early on to only index outputs on-the-fly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459:51,race condition,race conditions,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3837#issuecomment-345064459,1,['race condition'],['race conditions']
Performance,"@ldgauthier It actually looks like the PairHMM one might be a bug. It should be skipping the subset of ones that aren't loadable, but it seems like it think's it's succesfully loading the library but then failing when it actually tries to compute it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481:120,load,loadable,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592720481,2,['load'],"['loadable', 'loading']"
Performance,"@ldgauthier It does, but you should always run via the launch script and `--java-options`, since the script sets a number of important system properties, some of which affect performance. Do you know whether the GATK3 HC is able to run on the same bams without running out of memory in 4G/2G/1G?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993:175,perform,performance,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4272#issuecomment-385489993,1,['perform'],['performance']
Performance,"@ldgauthier This is why the `--disable-sequence-dictionary-validation` argument exists in `GATKTool`. If you're confident in the compatibility of your inputs, and the checks are too expensive, you can run with that option and (optionally) perform some less strict validation of your own in your `onTraversalStart()` method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653:239,perform,perform,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6589#issuecomment-625366653,1,['perform'],['perform']
Performance,"@ldgauthier so I think this is a different issue with overlapping deletions. The original bug was with queries, this is manifesting on loading. This seems to be an issue specific to dealing with the * allele and the fact that the deletion spans the interval specified. We're working on a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791:135,load,loading,135,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484951791,1,['load'],['loading']
Performance,"@ldgauthier's points are well-taken. I'm not quite ready to close this issue, but I believe there are other optimizations that don't have the downsides of this one.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504:108,optimiz,optimizations,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-310475504,1,['optimiz'],['optimizations']
Performance,"@ldgauthier, I've performed a pass. I think the _Caveat_ section of the AS_StrandOddsRatio doc could use some attention. Here it is currently (I did not touch it):. ![screenshot 2019-03-07 15 37 39](https://user-images.githubusercontent.com/11543866/53987567-60fc0000-40ef-11e9-8415-d52403f01f83.png). Here is what the rendered javadocs look like now. Let me know what you think.; ![screenshot 2019-03-07 15 37 21](https://user-images.githubusercontent.com/11543866/53987525-475ab880-40ef-11e9-8f0e-24a57a84586a.png). ![screenshot 2019-03-07 15 37 32](https://user-images.githubusercontent.com/11543866/53987535-4d509980-40ef-11e9-835b-65ceee2cdc06.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949:18,perform,performed,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5703#issuecomment-470685949,1,['perform'],['performed']
Performance,"@lucidtronix @mbabadi @samuelklee I think the best solution would be to establish a single, common Python environment, with a single set of dependencies, that all GATK Python tools depend on. We would establish a single docker image that has all of these dependencies pip installed, and could also include a conda env for the GATK environment for users who don't want to use the docker image. If we could do that, it would eliminate the need load per-tool conda environments. From what I've seen so far based on existing branches, the two environments we need (gCNV and CNN-VQSR) don't look that far apart in terms of dependencies. gCNV is using Theano, and CNN Tensorflow, but the rest looks [pretty close](https://docs.google.com/a/broadinstitute.org/spreadsheets/d/1RV7--uBQ0ctlXzMH09cmr0VimpZYIU68DdxJzE60y-c/edit?usp=sharing). So a strawman proposal for the main components for a common environment would be:. Python 3.6; Numpy >= 1.13.1; Scipy 1.0.0; Theano .0.9.0; Tensorflow 1.4.0; Pymc3 3.1; Keras 2.1.1. Can you all chime on on whether you think we can converge in a single environment ? If so, it would greatly simplify things, and we can start with getting a docker image built for running travis tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451:442,load,load,442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3692#issuecomment-348188451,2,['load'],['load']
Performance,"@lucidtronix Any suggestions on this? I don't recall ever seeing this before, but we've hit it about 5 times in tests in the last week or so. It looks like its coming from this [jit assembler](https://github.com/intel/caffe/blob/a3d5b022fe026e9092fc7abc7654b1162ab9940d/xbyak/xbyak.h#L229) code used by the intel-optimized tensorflow ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578:313,optimiz,optimized,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6307#issuecomment-566108578,1,['optimiz'],['optimized']
Performance,"@magicDGS . I am afraid this is not easy. I didn't write the binding (@tedsharpe did), but I would asseme the limitation comes from bwa mem itself, not the binding, as the binding is a thin wrapper that delegates the loading of the index files (or the image that combines all 5 index files in this case) to bwa. . The SV team here have a script (`scripts/sv/default_init.sh`) that when the Spark cluster is created and initialized, the image file is distributed to all walker nodes. Spark clusters other than Google's Dataproc would probably allow you to provide scripts as initialization actions as well. On the other hand, there seem to be a `--files` argument that you can append to your cmd line arguments which yarn will parse and distribute the provided local file to all nodes, though in this case it will be very inefficient considering the image file's size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074:217,load,loading,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312362074,1,['load'],['loading']
Performance,"@magicDGS @heuermh It sounds like there are definite advantages to switching to SLF4j. Our fear is that the switch will end up resulting in confusing class loading issues in different spark environments. We're just beginning a round of spark performance testing and evaluation under a pretty tight deadline, and we don't want to introduce any surprises. . We'd be happy to look at a pull request, but we might delay until the end of quarter to fully test / merge it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892:156,load,loading,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-261547892,2,"['load', 'perform']","['loading', 'performance']"
Performance,@magicDGS Doesn't the static block run when a subclass of Main is loaded as well?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854:66,load,loaded,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324388854,1,['load'],['loaded']
Performance,@magicDGS I provide some example data for a tutorial at <https://gatkforums.broadinstitute.org/gatk/discussion/7156/howto-perform-local-realignment-around-indels>. Search the page for ` tutorial_7156.tar.gz`. I showcase illustrative sites within the tutorial and also in <https://software.broadinstitute.org/gatk/blog?id=7847>. I'm actually new to test data so what cases are you hoping to test with the data? The snippet in the tutorial data is much larger than you need so it would be good narrow down the test case.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000:122,perform,perform-local-realignment-around-indels,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-314886000,1,['perform'],['perform-local-realignment-around-indels']
Performance,"@magicDGS I'm still missing why this wouldn't work for downstream projects (as long as they load Main or some Main-derived class). I think the owner config issue is different; for locale, we need to always force US. Can you verify this, or maybe provide more details about what case doesn't work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945:92,load,load,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3483#issuecomment-324622945,1,['load'],['load']
Performance,"@magicDGS Overall, the documentation changes are good. A few more minor comments: ; - If these are classes that are really pegged to the usage of LIBS, then you may want to mention that in the javadocs for the class. A comment along the lines of ""these classes are fairly low-level, developers should probably confirm that their changes do not belong in a higher-level calss such as LIBS"". ; - I am okay with an informal test of the speed, even if you just look at some logs. From the review, it looks like behavior of higher-level API calls will be unchanged. >I think that because the LocusIteratorByState is already splitting by sample, that can improve even more performance, because it will come directly in the state where it can be used by-sample in an efficient way. And maybe, if the tool does not require to split by sample at all, we can add an option to disable that behavior while creating the tracker. Agreed. I do not think you need to worry about the flag, for now. If you'd like, file an issue, but I think it is low priority for us. I tend to be worried about new developers or contributors getting lost in the codebase. And many do not have experience w/ GATK3. Hence, the documentation about when to use the class and why it exists. A couple additional things:; - I found a typo (see my comment); - Can you document the `presorted` parameter? Make sure to mention that if a developer specifies false, and sorting is needed, it will be done under-the-hood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187:667,perform,performance,667,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332215187,1,['perform'],['performance']
Performance,"@magicDGS Sorry for the delayed reply, I had to see what direction the `HaplotypeCaller` branch would take before I could answer your post above. In order to get the `HaplotypeCaller` performance up to acceptable levels we've had to make some changes to the traversal that have caused it to diverge quite a bit from the idea of a `SlidingWindowWalker` in this branch. Also, the way `SlidingWindowWalker` handles the `intervalsForTraversal` (using them to select fixed-size windows) is not compatible with what the `HaplotypeCaller` currently requires. As a result, I recommend that we merge your `SlidingWindowWalker` in as a separate traversal rather than trying to reconcile it with the `HaplotypeCaller` branch and mutate it into something that might not be as useful to you. Fortunately, walkers in GATK4 are simple enough that it's perfectly fine to have several similar-but-subtly-different walker types, provided they all serve actual use cases. I'll",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447:184,perform,performance,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-204134447,1,['perform'],['performance']
Performance,"@magicDGS The HaplotypeCaller traversal has undergone some changes in the past few weeks to improve performance and bring the output of the tool closer to GATK3. There is now an `AssemblyRegionWalker` that divides the intervals into active and inactive regions, in a greatly simplified version of the GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:100,perform,performance,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,1,['perform'],['performance']
Performance,"@mbabadi Ah, well file-based I/O would be the simplest option of all, of course, and should definitely be considered as a candidate solution to this ticket, particularly if you've already tried it and found the performance penalty to be minimal for your use case (@cmnbroad take note). The division of labor you describe between Java and Python sounds great, by the way -- exactly the sort of approach I was hoping you'd implement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853:211,perform,performance,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337319853,2,['perform'],['performance']
Performance,"@mbabadi I've updated my PR to use miniconda3. @mbabadi @lucidtronix @samuelklee I think we should aim for tools that at least run out-of-box, without depending on any out-of-band configuration other than the conda env. On top of that we can provide guidance/configs for users on how to enable further optimizations, like g++. Does that sound like an achievable goal ?. As for the docker, we're going to have strike the right balance between image bloat and performance(including test performance). I think we're around 4+ gig now, and counting. Before the Python integration we were at 1.9G, and trying to find ways to reduce it. So lets see where we wind up but keep that in mind. Finally, we need to find a way to install the (GATK) python package(s) without depending on access to the GATK repo. Right now I think the gCNV branch has a ""pip install from source"" added to the conda env .yml. That will work on the docker at the moment (and thus on travis), but that won't work for non-docker users how don't have source/repo access. Also, one of the proposals to reduce the size of the docker is to remove the repo clone that is currently there. My proposal is that we change the gradle build to create an archive/zip of the python source (this would include the VQSR-CNN package code as well as gCNV kernel). We can then copy that on to the docker image, and pip-install it from the copy. That would retain the ability to always run travis tests based on the code in the repo, and also keep the nightly docker image in sync. We'll also have deliver the archive as an artifact somehow (perhaps including PyPi) for non-docker users.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277:302,optimiz,optimizations,302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-350303277,6,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,@meganshand - FYI a small bug fix and there is another one (in t0 parsing) to follow. This one has a very tiny positive effect on indel performance,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050:136,perform,performance,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8171#issuecomment-1403552050,1,['perform'],['performance']
Performance,"@meganshand Here's a quick example:. ![image](https://user-images.githubusercontent.com/11076296/158385742-20a3303b-d8ce-4335-b42f-622da9bfa8d3.png); ![image](https://user-images.githubusercontent.com/11076296/158385777-6174f8b8-7abb-4b31-92d1-11cc8064854b.png). Note that the malaria data used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian con",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:473,tune,tuned---a,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['tune'],['tuned---a']
Performance,@mishaploid and @spikebike. 4.1.8.0 has a new option - `--genomicsdb-shared-posixfs-optimizations` for GenomicsDBImport that disable file locking and minimize writes to NFS. We are interested to know if this option helps your use case even as we continue making performance improvements. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677:84,optimiz,optimizations,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-651343677,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"@mishaploid, I am assuming the 295 to be single sample vcfs. What do the `data/processed/scattered_intervals/0004-scattered.intervals` look like? Are the intervals very small? Have you tried larger intervals?. @spikebike, thanks for the systemtap output. We do have large internal buffers to help with this type of usage, but will revisit the code to figure out the behavior you are seeing. We do have some experimental optimizations not rolled out yet for writing minimally to shared filesystems. Would you be able to run your tests if we create a gatk branch next week with those changes?. @ldgauthier, I think the Hail team used multi sample vcfs as well. We do have some optimizations(work-in-progress) for importing multi sample vcfs that will get rolled out in the next GenomicsDB release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471:420,optimiz,optimizations,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595405471,2,['optimiz'],['optimizations']
Performance,"@mlathara As suggested, I removed all MNPs from normal vcf files and changed the bed file format and it worked. But it is not going beyond chromosome 1. Here are the stack trace:. 15:44:02.495 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2021 3:44:02 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:44:02.750 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.750 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0; 15:44:02.750 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:44:02.750 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64; 15:44:02.751 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~16.04-b01; 15:44:02.751 INFO GenomicsDBImport - Start Date/Time: January 16, 2021 3:44:02 PM IST; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 15:44:02.751 INFO GenomicsDBImport - Picard Version: 2.23.3; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:220,Load,Loading,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['Load'],['Loading']
Performance,"@mlathara I think we're talking a bit in circles. The main use case I foresee for a generic split/merge tool would be to allow parallelized processing. I cant say there wouldnt be other uses I'm not seeing now (in the VCF world, SelectVariants is an extremely useful tool), but i dont have a specific use-case for GenomicsDB subsetting today beyond this. . I would point out this rapidly gets into specifics and quirks of any one user's infrastructure. I dont actually mind copying the GenomicsDB workspace prior to appending to it, because processing occurs on shared lustre space, while our permanent data lives on other disk space. Therefore we would probably do a copy no matter what. I agree you dont want to develop our one person's infrastructure. . The only aspect that gives me pause on your plan regarding split jobs is that GATK doesnt provide the scheduler. Sure there used to be queue and I gather GATK pushes WIDL/Cromwell (unless this changed), but we never used these. If GATK is not trying to provide the scheduler (which is better), does this really just look like: . 1) kick off X independent jobs for GenomicsDB/append; 2) each job specifies the interval(s) on which to operate; 3) Each job has no knowledge of the other jobs; 4) each job writes it's output to the same workspace; 5) Presumably there is something in place so jobs can run concurrently. This must be the new feature?. I imagine this could work. It does obligate one to have/use some kind of shared disk space, which we can handle, but could be a negative for some.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405:892,queue,queue,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641469405,4,"['concurren', 'queue']","['concurrently', 'queue']"
Performance,"@mlathara Let's discuss our options at our next meeting...if the problem is common enough, and a proper fix is not coming in the short term, we might want to consider making the VCF codec the default despite the performance hit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210:212,perform,performance,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646232210,1,['perform'],['performance']
Performance,"@mlathara The nodes have considerably more (256 or so). is there any rule or thumb or guidance on expected memory needs based on number of gVCFs and/or type of input (WES vs WGS)?. I do think you might be onto something though. Out default cluster submission code takes our slurm job memory request, subtracts only a few GB and passes the remainder to -Xmx/Xms. I will update to leave more buffer as you suggest. Our cluster happens to be undergoing maintenance this week, so this particular job was killed. I'll update the GATK version, add --genomicsdb-shared-posixfs-optimizations, and adjust the memory. One other thing: i noticed GenomicsDBImport is not nearly as verbose in logging as typical GATK tools. Is that expected, or a symptom of whatever problem we're having?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475:570,optimiz,optimizations,570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656259475,2,['optimiz'],['optimizations']
Performance,"@nalinigans Mixed results so far. I'm running the new consolidate tool, per chromosome as before. I ran it with defaults (no custom arguments). It is running longer than previously, but chr 1, the largest, died after consolidating 2 attributes. This job had 248G of RAM allocated. Are there optimizations you'd suggest?. ```; 02 Apr 2022 16:34:31,433 DEBUG: 	[April 2, 2022 4:34:31 PM PDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 5,993.06 minutes.; 02 Apr 2022 16:34:31,438 DEBUG: 	Runtime.totalMemory()=178017796096; 02 Apr 2022 16:34:31,443 DEBUG: 	Tool returned:; 02 Apr 2022 16:34:31,448 DEBUG: 	true; 02 Apr 2022 16:34:34,663 INFO : Will consolidate the workspace using consolidate_genomicsdb_array; 02 Apr 2022 16:34:34,723 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb/1$1$223616942; 02 Apr 2022 16:34:34,748 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942; 02 Apr 2022 16:34:34,754 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 02 Apr 2022 16:34:35,059 DEBUG: 	16:34:35.059 info consolidate_genomicsdb_array - pid=34848 tid=34848 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 02 Apr 2022 16:34:36,091 DEBUG: 	Using buffer_size=10485760 for consolidation; 02 Apr 2022 16:34:36,097 DEBUG: 	Number of fragments to consolidate=26; 02 Apr 2022 16:34:36,101 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975:291,optimiz,optimizations,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975,1,['optimiz'],['optimizations']
Performance,"@nalinigans OK, so most of these jobs are still going (we run per contig); however, one just died as follows:; ```; 03 Mar 2022 12:49:28,390 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb --shared-posixfs-optimizations --segment-size 32768 -a 3$1$185288947; 03 Mar 2022 12:49:28,444 DEBUG: 	12:49:28.444 info consolidate_genomicsdb_array - pid=147768 tid=147768 Starting consolidation of 3$1$185288947 in /home/exacloud/gscratch/prime-seq/workDir/0950f56c-7565-103a-a738-f8f3fc8675d2/Job3.work/WGS_1852_consolidated.gdb; 03 Mar 2022 12:58:29,209 DEBUG: 	Using buffer_size=32768 for consolidation; 03 Mar 2022 12:58:29,222 DEBUG: 	12:58:29 Memory stats(pages) beginning consolidation size=16404102 resident=16377455 share=1814 text=3530 lib=0 data=16375282 dt=0; 03 Mar 2022 12:58:29,228 DEBUG: 	12:58:29 Memory stats(pages) after alloc for attribute=END size=16404138 resident=16377465 share=1821 text=3530 lib=0 data=16375318 dt=0; 04 Mar 2022 16:03:57,025 WARN : 	process exited with non-zero value: 137; ```. Is there any information from this, or information i could gather, that's helpful here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021:351,optimiz,optimizations,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1059616021,1,['optimiz'],['optimizations']
Performance,"@nalinigans Tests passed, but it's worth pointing out that the new `testWriteToAndQueryFromGCS()` test took 9 minutes to complete, which seems very slow. Possible performance issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216:163,perform,performance,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5197#issuecomment-430760216,1,['perform'],['performance']
Performance,"@nalinigans This iteration is on a smaller input (~600 WGS samples). Based on the info below, do you suggest changing --buffer-size or --batch-size?. To your questions:. In chromosome 1's folder (1$1$223616942), there are 26 fragments (the GUID-named folders). The sizes of book_keeping files are:. 168M; 131M; 155M; 149M; 136M; 142M; 216M; 147M; 150M; 134M; 127M; 75M; 172M; 122M; 207M; 122M; 581M; 149M; 150M; 141M; 149M; 143M; 143M; 165M; 160M; 163M. The last job failed an OOM error (the job requested 256GB and the slurm controller killed it). This is the command and output (with timestamps):. ```; 10 Apr 2022 01:56:21,275 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942. 10 Apr 2022 01:56:21,556 DEBUG: 	01:56:21.556 info consolidate_genomicsdb_array - pid=146087 tid=146087 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 10 Apr 2022 01:56:22,385 DEBUG: 	Using buffer_size=10485760 for consolidation; 10 Apr 2022 01:56:22,391 DEBUG: 	Number of fragments to consolidate=26; 10 Apr 2022 01:56:22,396 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=371MB dt=0; 10 Apr 2022 01:56:22,400 DEBUG: 	Sun Apr 10 01:56:22 2022 Memory stats Start: batch 1/1 size=503MB resident=379MB share=6MB text=13MB lib=0 data=391MB dt=0; 10 Apr 2022 01:59:22,970 DEBUG: 	Sun Apr 10 01:59:22 2022 Memory stats after alloc for attribute=END size=25GB resident=25GB share=7MB text=13MB lib=0 data=25GB dt=0; 11 Apr 2022 03:36:27,065 WARN : 	process exited with non-zero value: 137; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878:838,optimiz,optimizations,838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1095228878,1,['optimiz'],['optimizations']
Performance,"@nalinigans We have a very large WGS dataset (<2K subjects). We incrementally add to it over time. After each new batch of data is added, we typically run GenotypesGVCFs. We have run GenotypesGVCFs on prior iterations of this GenomicsDB workspace; however, we have never run it on this particular workspace, after the addition of new samples. You can get those files here: . https://prime-seq.ohsu.edu/_webdav/Labs/Bimber/Collaborations/GATK/%40files/Issue7674/. I think I was mistaken above. According to the job logs, we did run GATK GenomicsDBImport v4.2.5.0 when we did our last append. However, prior append operations would have used earlier GATK versions. I believe we have 79 fragments. We rarely do --consolidate, primarily because those jobs essentially never finish. We've had a lot of issues getting GATK/GenomicsDB to run effectively on this sample set. We have settled on doing the GenomicsDBImport/append operation with a moderate batch size. I realize newer GATK/GenomicsDB versions have been addressing performance, and it is possible we should re-evaluate this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042101804,1,['perform'],['performance']
Performance,"@nalinigans, to answer your questions as best as I can (sorry, I'm a bit of a novice); 1. the size of the book keeping file is: 20,779,823bytes (~21Mb); 2. I believe it is a shared Posix FS; Running this option created a similar error except this time there was: ""cannot load book-keeping: Reading MBR failed"" (output below) ; 3. available memory ~89Gb; 4. I am running -Xmx16g java option. Newest output:; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:26:34.912 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:26:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:26:35.417 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35.418 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:26:35.418 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:26:35.420 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:26:35.421 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:26:35.421 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:26:34 PM CST; 16:26:35.421 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:26:35",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:271,load,load,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,3,"['Load', 'load', 'optimiz']","['Loading', 'load', 'optimizations']"
Performance,@nh13 I wrote a test for your branch (its very simple it just reruns the gvcf mode tests with --disable-optimizations enabled) that should work for your branch. Its in the branch je_addTestForDisableOptimizations. Since you submitted this PR from your own clone of the GATK I cannot push this onto the branch as it stands. Would you be able to copy it into this branch?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846:104,optimiz,optimizations,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7125#issuecomment-793077846,2,['optimiz'],['optimizations']
Performance,"@nvnieuwk, have you tried the following to circumvent the inode limit issue?. - use of the `merge-contigs-into-num-partitions` option with GenomicsDBImport; ```; --merge-contigs-into-num-partitions <Integer>; Number of GenomicsDB arrays to merge input intervals into. Defaults to 0, which disables; this merging. This option can only be used if entire contigs are specified as intervals.; The tool will not split up a contig into multiple arrays, which means the actual number of; partitions may be less than what is specified for this argument. This can improve; performance in the case where the user is trying to import a very large number of contigs; ```; Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072:564,perform,performance,564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8297#issuecomment-1557405072,1,['perform'],['performance']
Performance,"@owensgl Sorry you're running into problems. We typically speed up the process by running multiple GenotypeGVCF processes in parallel, subsetting by genomic intervals. GenotypeGVCFs isn't really multicore, you'll probably be best off giving each process 1 or 2 cores. (You'll see better performance with 2 since java has parallel garbage collection, but it might be more cost effective to run twice as many slower processes...) If you do that you'll want to run with `--only-output-calls-starting-in-intervals` enabled in order to avoid problems on the edges of intervals. . Things tend to bog down with many highly multi-allelic sites. If you have a population with very high diversity you may be hitting lots of sites like that. I'm not sure why it's as slow as you say it is though. It should be faster than 800bp / 30 minutes even with old qual. If you could provide a subset of your data we might be able to profile and see if there's some pathological case we're not handling well. I believe new-qual handles multi-allelic sites more efficiently which I suspect is why it's going faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994,1,['perform'],['performance']
Performance,"@rhysnewell The default settings for the GATK downsampler are optimized for germline calling on typical (ie., Illumina) short reads data. The goal of the downsampler is to control memory usage at dubious sites where the aligner happens to place huge numbers of low-mapping-quality reads, without harming the quality of the variant calls in any meaningful way. By default, if there are more than 50 reads starting at the exact same alignment start position, GATK will start to randomly eliminate reads -- this gives a maximum total coverage for each locus of `50 * read_length`, which is more than sufficient for typical whole-exome or genome germline calling. For other applications, such as amplicon sequencing (where the reads inherently all tend to start at the same positions) or microbial calling, these defaults will not work well, and you are likely better off disabling the built-in downsampler completely, and using a different downsampling approach if memory use is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006:62,optimiz,optimized,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139176006,1,['optimiz'],['optimized']
Performance,"@rsasch my thought was to keep it around for now, but in the future we could add a step that removes it once we know the samples are safely loaded. It could also be that we get rid of the ""is_loaded' flag in sample_info instead since this data is more detailedâ€¦",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451:140,load,loaded,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451,1,['load'],['loaded']
Performance,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:380,perform,perform,380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,4,['perform'],['perform']
Performance,"@samuelklee I wrote some benchmarks for the exact combinatorics and you were right, my optimization was pointless. Although the `CombinatoricsUtils` method does explicitly multiply out instead of using cached factorials 1) the number of multiplications is only min(ploidy, (allele count - 1)), and 2) it actually takes quite a while (much larger than reasonable ploidy and allele count) for multiplication to take longer than the memory access of stored factorials. . I have removed this error in judgment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168:87,optimiz,optimization,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066873168,2,"['cache', 'optimiz']","['cached', 'optimization']"
Performance,"@samuelklee I'd say lets leave 2.1 base image up there for now, and yes on the cache clearing. Once tests pass with the cache cleared it should be good to merge. Feel free to squash and rebase if you like.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109:79,cache,cache,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408453109,4,['cache'],['cache']
Performance,"@samuelklee It wasn't just a rebase, it was a complete rewrite because the old code had since become completely entangled with DRAGEN code. But I did it! Everything is passing, the code is dramatically simpler, and it's even a bit faster. I have done my best to make a coherent commit history. I would recommend reviewing one commit at a time in side-by-side diff mode. Note that some commits rip out old code and replace it with pseudocode, deferring the new code to a later commit. Other commits tell a story of what all the different caches meant in order to motivate the simpification of later commits. The baroqueness of the old code was motivated by three considerations:; * cache-friendliness -- traversing all arrays by incrementing the innermost index, reads. This is absolutely essential.; * flattening 3D arrays into 1D arrays. This was a premature optimization.; * Precomputing addition operations -- this was misguided. The DRAGEN code relied on these caches in a rather complex way, which fortunately turned out not to be necessary and which could be dramatically simplified. My notes on tracking all the variables from the parent genotype calculator down to the DRAGEN calculator are in this google doc: https://docs.google.com/document/d/1v6s57mUAwfj38nL3VdktjA059kYBkJfokq18IDy79E8/edit?usp=sharing. Good luck and don't hesitate to ask me to explain anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476:537,cache,caches,537,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1023647476,8,"['cache', 'optimiz']","['cache-friendliness', 'caches', 'optimization']"
Performance,"@samuelklee My plan for the deprecation was to do it concurrently with (or just before) the merge of this PR. As this gets closer to a final merge, we can coordinate on that aspect.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690:53,concurren,concurrently,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1939712690,1,['concurren'],['concurrently']
Performance,"@samuelklee Now it's back to you. I agreed with and implemented all of your suggestions. `GenotypeIndexCalculator`, `GenotypeAlleleCounts`, `GenotypeLikelihoodsCalculator` and `GenotypeLikelihoodsCalculator` (renamed to `GenotypesCache`) now have clearly-defined roles. A lot of premature optimization is gone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779:289,optimiz,optimization,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1068695779,2,['optimiz'],['optimization']
Performance,"@samuelklee Performance was comparable on one exome bam using the 5M sites file. Each took ~15 minutes. Sample size of 1, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153:12,Perform,Performance,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3203#issuecomment-313156153,1,['Perform'],['Performance']
Performance,"@samuelklee The module can now save and load everything, including the state of the optimizer. This allows to making interesting inference pipelines. Here's a decent strategy for obtaining the global optimum (it works flawlessly on simulated data every time):. - In the first pass, one disables annealing and obtains the variational parameters in a thermal state. The temperature needs to be _high enough_ to allow most/all local minima to merge, though, not too high to allow copy numbers to travel too far away from baseline copy numbers. If this occurs, one must anneal very slowly in the next stage (see below). The results are checkpointed once converged. - In the second pass, one makes another call to the CLI tool, this time w/ annealing enabled (starting from the same temperature) and starting from the checkpointed thermal results (model params, posteriors, adam(ax) state). The annealing rate must be slow enough to prevent thermal fluctuations from getting quenched (i.e. the evolution must be quasi-isothermal). One must look for a steady and linear rise of ELBO, such that when the annealing protocol ends, SNR quickly drops to values below 1. In both runs, the learning rate must be very small (in the rate 0.01-0.05) such that we wouldn't have to worry about controlling stochastic noise. Adam(ax) quickly adjusts its moment estimates and compensates for the small learning rate, so this doesn't increase the training time significantly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020:40,load,load,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-347369020,4,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"@samuelklee The performance issues were a mirage. It turned out that there was a misconfiguration in the Carrot tests so it was pegging the ""control"" version to an older GATK release from a year ago. There was a slight regression in the past year the we haven't caught but that is a separate discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210:16,perform,performance,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1551819210,1,['perform'],['performance']
Performance,"@samuelklee This is a very dangerous/misleading check! Most of the files in your list are actually in use in the test suite, and their presence is inferred from the path to the file(s) they are associated with. For example, vcf index files are loading indirectly by our readers, using the path to the vcf to infer the path to the index.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768:244,load,loading,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905#issuecomment-348591768,1,['load'],['loading']
Performance,"@samuelklee Yeah, you did [see this issue there](https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546) (as well as the concurrent modification exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423:140,concurren,concurrent,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6649#issuecomment-640866423,1,['concurren'],['concurrent']
Performance,"@schelhorn Sorry for the long wait on this issue, which we do take seriously! Unfortunately as mentioned above we have extremely limited resources for Mutect2 maintenance at the moment due to our focus on Mutect3, which we see as the long-term solution to this and other issues. You should definitely continue to run 4.1.8.0 until this is resolved, if at all possible. Having said that, we are generating a new M2 PON now, and should know soon whether it resolves this issue as @davidbenjamin hopes. Stay tuned for an update on the outcome of this evaluation within the next week or two.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738:505,tune,tuned,505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1405524738,1,['tune'],['tuned']
Performance,"@schelhorn We have a large clinical ""truth"" set we utilize during workflow validations. We also utilize spike-in samples from SeraCare and perform dilutions using a couple of the common Coriell cell lines. We noticed the Mutect2 calling inconsistency while validating a small targeted panel.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027:139,perform,perform,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171672027,1,['perform'],['perform']
Performance,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:449,load,load,449,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054,1,['load'],['load']
Performance,"@shisheng-1 Hello, thank you for your contribution! . I tried running with and with-out your change enabled and I didn't see any performance improvement when compiling. I didn't do a very scientific test, but running the following a few times on master vs your branch didn't show any performance advantage for using fork = true.; ```; ./gradlew clean compileTestJava; ```. | fork = | false | true |; | --- | ----- | ---- |; | run 1 | 77 | 93 |; | run 2 | 67 | 70 |. From my quick tests it looked like the forking version might actually be a bit slower but I suspect that's just noise. . Do you see different results?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911:129,perform,performance,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7561#issuecomment-967263911,2,['perform'],['performance']
Performance,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:541,perform,performed,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387,2,['perform'],['performed']
Performance,"@sooheelee I found that on FC, it was only necessary to make the File -> String change at the task level (i.e., in CollectCounts). Not sure if this works due to the particular version of Cromwell on FC. In any case, I don't think you should stress too much about getting NIO working on your VM. Again, I'd expect the current non-NIO gCNV WDL to only take a few hours on FC, and then less than that once coverage collection is call cached. No use spending more time getting NIO working than it would save you, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263:431,cache,cached,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263,1,['cache'],['cached']
Performance,"@sooheelee We've decided to delete the old tools. I will issue a PR soon. Hopefully this lightens the load on everyone a bit!. Incidentally, I would be fine with adding the tools `CollectFragmentCounts` and `CollectAllelicCounts` to the `CoverageAnalysis` category, as they are performing relatively generic tasks that fall in that category. (with the caveat that the output formats contain column headers that are specific to the new CNV workflows).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382:102,load,load,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349145382,2,"['load', 'perform']","['load', 'performing']"
Performance,@spatel-gfb Can you try running `GenomicsDBImport` with the `--genomicsdb-shared-posixfs-optimizations` argument and see if that helps? . @nalinigans @mlathara any other suggestions?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101:89,optimiz,optimizations,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1031830101,1,['optimiz'],['optimizations']
Performance,"@spatel-gfb another option that should improve performance is to run the GATK GVCF postprocessing tool ReblockGVCF on all of your input GVCFs. That tool merges reference blocks (which BAF calculation doesn't need) and throws out uncalled alleles (which BAF calculation doesn't need), both of which should speed up your import. For the purposes of GATK-SV you can use the default GQ bands ([0,20], and (20, 99]).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926:47,perform,performance,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088703926,1,['perform'],['performance']
Performance,"@spikebike, just started looking at this issue again. We are benchmarking operations with NFS and will put out an optimized library soon. But, GenomicsDB does use filesystem locking to allow for simultaneous reads/writes. - Did you try `export TILEDB_DISABLE_FILE_LOCKING=1` ?; - Are you using the `--consolidate true` option with GenomicsDBImport explicitly? If yes, would it be possible to set that option to false which is also the default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603:114,optimiz,optimized,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-616657603,1,['optimiz'],['optimized']
Performance,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:55,optimiz,optimizations,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741,3,"['optimiz', 'tune']","['optimizations', 'tuned']"
Performance,"@takeshi-yoshimura One more thing to note. You should be able to use this library with an existing version of gatk by including this in your classpath since filesystem providers should be dynamically loaded. ( something along the lines of`--java-options ""-cp <pathtothisjar>' should work.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091:200,load,loaded,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698#issuecomment-662035091,1,['load'],['loaded']
Performance,"@takutosato had a good suggestion: to stratify to low-complexity regions in the high-confidence regions. Not sure how many variants are there, but will take a look. EDIT: looks like it's ~5k / ~54k on chr22 in CHM. More generally, I think that defining the appropriate loss function for optimization to set ""default"" parameter values obviously has no unique answer. The problem is also made a little more complicated by our current strategy of sensitive calling + non-trivial filtering. But it would be great to come up with some hard constraints (e.g., we never want runtime/cost to exceed X, we always want to maintain Y metrics in these regions on these samples) and general procedures, then apply them as equitably as possible across all method/parameter changes. Also generally, I'm a bit wary of focusing too hard on the high-confidence regions, as this might lead to overfitting or could understate the potential of method/parameter changes in more difficult regions. But probably we'll have to downweight the loss or do more manual checks in low-confidence regions until we improve truth resources there. One naive question, just want to double check: is it correct that the overall scaling of each set of SW parameters is inconsequential? E.g., if I multiply each by a constant, should I expect the same results? I would expect this to be the case (unless my hazy recollection of the details of SW scoring is off) and simple experiments bear this out, but I'm not sure if there are some edge cases or idiosyncrasies in our implementation or use of the scores that I might be missing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055:287,optimiz,optimization,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714570055,2,['optimiz'],['optimization']
Performance,@tedsharpe How's the performance of this new binding? I'm assuming it's better and less broken than the old one. Do you have any numbers or even just anecdotes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2367#issuecomment-275511690,1,['perform'],['performance']
Performance,"@tedsharpe I've addressed some of your comments here -- all the simpler stuff plus:. - I now only make distal targets for split reads with one supplementary alignment. We can make a ticket to handle more complex cases at some point.; - I renamed the concept of strand in the `EvidenceTargetLink` and related classes -- I'm now calling it `evidenceUpstreamOfBreakpoint`.; - I canonicalize `EvidenceTargetLinks` and only create them when the source is upstream of the target. This allowed me to get rid of the de-duplication code, so thanks for the suggestion. It seemed tricky to me to try to cluster these links during the initial pass over the reads while at the same time keeping track of coherent evidence. In my testing it doesn't seem like it is slow to run over the `EvidenceRDD` again to do this, but we could think about trying to change this sometime if we're looking for optimizations. . Want to take another look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806:881,optimiz,optimizations,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-328300806,2,['optimiz'],['optimizations']
Performance,"@tedsharpe So we've addressed a bunch of the issues you've mentioned in the new htsjdk.beta reader apis. In particular we now optimize the decoding checks so that we only scan the beginning of the files exactly once and the give the necessary bytes to all available codecs. That should work in combination with your optimization to push down the filtering which makes a lot of sense as well. Lazily loading the indexes when you need to query for them is a good idea. I think the thought behind aggressively loading them was probably to handle potential errors up front instead of waiting until later, and it's confounded by the bad choice of automatically discovering indexes by default so the engine can't tell if there SHOULD be an index until it tries and fails to load it. We've also addressed that in the new APIs to give the client more control over how indexes are discovered which should allow for cleaner lazy loading and things like that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181:126,optimiz,optimize,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1364325181,6,"['load', 'optimiz']","['load', 'loading', 'optimization', 'optimize']"
Performance,"@tedsharpe Thanks for checking. In general I've seen the CPB tends to help a lot when reading through long contiguous stretches of BAM file and less when doing anything on smaller or fragmented data. I'm surprised it didn't make any difference here, but seems like it doesn't so that's fine. I've seen catastrophic interactions between insufficiently buffered index inputs with it disabled where it ended up performing an http request for every byte, but hopefully that's avoided just by using a buffered reader for it. . I have a plan to someday enable the more intelligent -L aware prefetcher that will use the list of actual positions of interest to buffer more intelligently, but that's not happening on any specific schedule.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320:408,perform,performing,408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320,1,['perform'],['performing']
Performance,"@tedsharpe, I have changed the implementation to use a lazy non-cached recreation of the read mapping information. . I have not added the additional mapping information in your branch like the MQ value. I would do that in a separate commit. . Please take a look and perhaps run your benchmark.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369:64,cache,cached,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-311404369,1,['cache'],['cached']
Performance,@tfenne The gatk3 behavior when using intervals was that it only loaded data that started within the interval. We change that in gatk4 so queries get all overlapping data. This means that for GenoytpeGVCFs at least you should be getting all upstream deletions that overlap. If that's *NOT* the case then it's a bug we should fix. The side effect was getting duplicate calls in adjacent shards which is why we added `--only-output-calls-starting-in-interval`. . The HaplotypeCaller behavior is probably different and I don't know for sure what it is. ![HaplotypeCaller leading deletion behavior](https://user-images.githubusercontent.com/4700332/72105530-b569a080-32fb-11ea-9fc5-1adaee2ac039.png). It seems like there are 3 possible outcomes for a deletion that starts outside the interval in HaplotypeCaller. @davidbenjamin Do you know which we do? James said you were re-writing related code recently. A toggle to change between behavior 1 and 2 would be idea like you're suggesting @tfenne. I'm a bit afraid that we do 3 and emit nothing in these cases though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496:65,load,loaded,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572762496,1,['load'],['loaded']
Performance,@tomwhite From what I saw the hdfs provider wasn't loaded (at least on my local cluster). Only the default filesystem and the jar file system.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239:51,load,loaded,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312#issuecomment-268009239,1,['load'],['loaded']
Performance,"@tomwhite I think the general goal of unifying the Spark/non-Spark tool hierarchies is worthwhile, but I don't like the idea of all tools having `if (sparkArgs.useSpark) {} else {}` boilerplate. If we do this, we should have separate abstract methods that get called automatically in the Spark/non-Spark cases. I also think we should wait to perform this refactoring until later in the quarter, after the Spark evaluation, and after we've standardized more on `Path` for inputs/outputs, as it will break a lot of downstream code in gatk-protected -- and we don't want to break all the tools more than once if we can help it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946:342,perform,perform,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2217#issuecomment-254228946,1,['perform'],['perform']
Performance,"@tomwhite I'm looking into the performance issues now with the new code path -- it brings the output much closer to GATK3, but clearly needs some profiling work. Can you tell me what kind of difference you saw in the runtime on Spark? Eg., was it on the order of 20-30%, or was it worse than that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-330905564,2,['perform'],['performance']
Performance,"@tomwhite Ignore my previous message, sorry -- I see that you commented above that you tested https://github.com/broadinstitute/gatk/pull/4314 and it had no effect. I've opened https://github.com/broadinstitute/gatk/pull/4428 to revert the ADAM upgrade for now until we understand the underlying cause of the performance regression. @fnothaft It seems to me that the serializer registrations in ADAM could in theory affect the GATK, since we both register serializers for core classes in htsjdk. It seems worth investigating as a possibility, at least, as it's the only candidate mentioned so far that seems to have the potential to cause such a massive performance difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808:309,perform,performance,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367069808,2,['perform'],['performance']
Performance,"@tomwhite In your `--files` branch, you might try enabling feature caching on the `FeatureDataSource` (assuming your sequential queries are forward-only, increasing). Without that it will execute a separate index query for each `query(interval)` to find the overlaps. If you pass a non-zero `queryLookaheadBases` value (we usually use 100000) to the FeatureDataSource constructor in `BroadcastJoinReadsWithVariants`, it will use lookahead and the feature cache to satisfy queries - I would expect it to speed it up a lot.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472:455,cache,cache,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412917472,1,['cache'],['cache']
Performance,"@tomwhite Like we discussed this morning, we can and should get rid of the broadcast code but we should ideally first get some sort of plot we can point to in order to justify the change. This will also be useful for future presentations of our performance improvements. The plot would ideally be a comparison between the new distribution strategy and broadcasting compared across a variable number of cores, so the performance improvement can be better understood.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416327226,2,['perform'],['performance']
Performance,"@tomwhite OOC, do you have any comment as to why bumping the ADAM version caused a performance regression? I'm not aware of any changes we've made in ADAM that would have impacted either BQSR or HC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491:83,perform,performance,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366072491,1,['perform'],['performance']
Performance,"@tomwhite Please review, and confirm that this resolves the performance issues you encountered.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878:60,perform,performance,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4428#issuecomment-367067878,1,['perform'],['performance']
Performance,"@tomwhite That's interesting -- @lbergelson 's profiling on `HaplotypeCallerSpark` in isolation did not show any noticeable slowdown as a result of the switch to the new code path (in addition to the output being much more correct than before), and the walker version which uses the same codepath is as fast or faster than GATK3 at this point in all modes. We should try to see whether we can replicate your results on our end. Can you provide more details on how you ran the tools?. The new `HaplotypeCaller` code path does result in the tool doing more work in some cases as compared to the early betas, particularly for genomic locations not covered by any reads -- but this work is necessary in order to produce correct output without boundary effects, and the fact that the early betas didn't do it was an oversight. I wonder if that could explain what you're seeing -- see the full discussion in https://github.com/broadinstitute/gatk/issues/4169. There are also several ways in which `HaplotypeCallerSpark` currently makes suboptimal use of the new code path introduced in https://github.com/broadinstitute/gatk/pull/4278:. 1. It uses only a single interval per shard, instead of many intervals as the walker version does (https://github.com/broadinstitute/gatk/issues/4299). 2. It materializes all of the assembly regions in a shard at once (https://github.com/broadinstitute/gatk/issues/4301), as opposed to the walker version which materializes the regions in a shard as lazily as possible. 3. The default read shard size may need adjusting (https://github.com/broadinstitute/gatk/issues/4298). The walker version creates one shard per contig, and lazily creates the assembly regions within that shard. That's obviously not possible for the Spark version, but I don't think that the effect of the shard size on performance has been measured yet for the Spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033:1821,perform,performance,1821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364179033,1,['perform'],['performance']
Performance,@tomwhite This seems like an easy way to get a big performance boost in `HaplotypeCallerSpark` -- what do you think?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4296#issuecomment-363201577,1,['perform'],['performance']
Performance,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:746,perform,performance-sensitive,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,2,['perform'],['performance-sensitive']
Performance,"@tomwhite When you get a chance, could you please test this branch to check whether the performance regression you reported earlier is resolved?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397:88,perform,performance,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397,1,['perform'],['performance']
Performance,@tomwhite and/or @laserson should have a look at this and give JP high-level feedback on his approach to optimization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889:105,optimiz,optimization,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/987#issuecomment-146905889,1,['optimiz'],['optimization']
Performance,"@ury the performance evaluation is based on the HCC1395 benchmark, as described in my original report (see above). More variants in newer versions is in line with our analysis, but the new variants are likely to be mostly false positives.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1407452410,1,['perform'],['performance']
Performance,"@vdauwera The tools are categorized and listed in the Google Spreadsheet above. It is waiting for you to assign tech leads to tools for documentation. One thing that @chandrans brought to my attention is that for BaseRecalibrator one of the parameters (`-bqsr`) actually causes an error. One can no longer generate the 2nd recalibration table with correction on the fly and instead must use the recalibrated BAM through BaseRecalibrator to generate the 2nd recal table for plotting. **This type of information is missing from the tool docs.** Furthermore, updates I made to the BQSR slidedeck (that showcase this `-bqsr` parameter) are based on information from a developer and this information turns out to be incorrect now (perhaps correct at some point in development?). Soooo, I think it may be prudent that those responsible for tool docs test the commands on data. - [4] Make sure the doc content enables Best Practices, e.g. plotting BQSR recalibration, and ; - [5] Test example commands to ensure they work. If they do not, make corrections and notate the change in application in the documentation.; - [6] Remember @vdauwera's plan to change the representation of parameters from camel to KEBAB case. Issue is <https://github.com/broadinstitute/gatk/issues/2596>. Geraldine would like your help to do this for the tools you are responsible for. Remember to change the integration tests too. . ### What the gatkDocs look like as of commit of `Mon Nov 20 17:30:46 2017 -0500` where we upgraded htsjdk to 2.13.1: . # [gatkdoc.zip](https://github.com/broadinstitute/gatk/files/1492593/gatkdoc.zip). Download and load the `index.html` into a web browser to click through the docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583:1617,load,load,1617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346060583,1,['load'],['load']
Performance,"@vruano @mwalker174 Any estimates on cost that could help determine the priority of issue 1? Specifically, is the disk cost required to localize the entire count file for all samples a determining factor? If we can drastically reduce this cost, then we can dedicate more to increasing resolution, etc. Here is a minimal set of fixes that could enable the querying of intervals for GermlineCNVCaller (and also for DetermineGermlineContigPloidy without too much extra work, since we also subset intervals there) *only in the gCNV WGS pipeline*, without disrupting other interfaces:. 1) Write a Tribble SimpleCountCodec for the `counts.tsv` extension. I've already done this in a branch.; 2) Change GermlineCNVCaller and DetermineGermlineContigPloidy tools to accept paths.; 3) If an index is present for each count path, create a FeatureDataSource, merge the requested -L/-XL intervals, and query to perform the subset. We will also need to stream the SAM header metadata. It should not require much code to extract all this to a temporary IndexedSimpleCountCollection class. (Caveat: for now, this will work with the current gCNV convention of providing bins via -L/-XL. Technically, it will also work with the more conventional use of -L/-XL to denote contiguous regions, but we may have to perform checks that bins are not duplicated in adjacent shards if they overlap both, since querying a FeatureDataSource will return any bins that overlap the interval---rather than only those that are completely contained within it.); 4) Index read-count TSVs in the gCNV WGS pipeline after collection and modify the DetermineGermlineContigPloidy and GermlineCNVCaller tasks to take read-count paths and indices, if necessary. These changes could be confined in the gCNV WGS WDL for now. I think that should do the trick. If this is high priority, I can implement now. In the future, we might be able to promote all Locatable CNV Records to Features and write code to automatically pass the columns/encoders/de",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082:898,perform,perform,898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082,1,['perform'],['perform']
Performance,"@vruano The HGDP crams also trigger this error. . ftp:/Â­/Â­ftp.Â­1000genomes.Â­ebi.Â­ac.Â­uk/Â­vol1/Â­ftp/Â­data_collections/Â­HGDP/Â­data/Â­Brahui/Â­HGDP00001/Â­alignment/Â­HGDP00001.Â­alt_bwamem_GRCh38DH.Â­20181023.Â­Brahui.Â­cram. ```; 14:32:34.745 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 17, 2021 2:32:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:32:34.877 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.877 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:32:34.877 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:32:34.877 INFO CalibrateDragstrModel - Executing as farrell@scc-gh3.scc.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 14:32:34.877 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_172-b11; 14:32:34.877 INFO CalibrateDragstrModel - Start Date/Time: April 17, 2021 2:32:34 PM EDT; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 14:32:34.878 INFO CalibrateDragstrModel - Picard Version: 2.25.0; 14:32:34.878 INFO CalibrateDragstrModel - Built for Spark Version: 2.4.5; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:261,Load,Loading,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['Load'],['Loading']
Performance,@wujh2017 Great! Let us know if you have any more feedback. Please be aware that both DetermineGermlineContigPloidy and GermlineCNVCaller are still in beta. There are some parameters that may need to be tuned appropriately for your data. We are currently running evaluations and will release some recommendations that we find suitable for data generated at the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401:203,tune,tuned,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4457#issuecomment-369254401,2,['tune'],['tuned']
Performance,"@wujh2017 thank you for trying out this beta tool! As @samuelklee mentioned above, your issue stems from not having the proper python conda env. I guess you have a python 2.x interpreter in your system environment which fails to parse GATK's python code (which requires python 3.6+). Please either follow the instructions in the README.md file, or use the official Docker image instead. Also, I would like to add that the default parameters for running the Germline CNV calling pipeline are currently being fine-tuned separately for WES and WGS data. The default parameters shipped with GATK 4.0.0.0 are preliminary and are not expected to yield optimal results.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170:512,tune,tuned,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4389#issuecomment-365117170,1,['tune'],['tuned']
Performance,"@yfarjoun I've built docs from your branch to see what the changes look like. Here's the PicardDoc bundle I've zipped for you: [picarddoc_yossi_test.zip](https://github.com/broadinstitute/gatk/files/1535680/picarddoc_yossi_test.zip). View the rest by loading `index.html` into a browser. Here is one of the tool docs where you can see your changes:; <img width=""1099"" alt=""screenshot 2017-12-06 10 18 34"" src=""https://user-images.githubusercontent.com/11543866/33668948-1c286b90-da6f-11e7-91d6-3b368375cbab.png"">. ---; # Steps to build docs to see what changes look like:; [1] Download and switch to branch with changes, e.g. ; ```; git branch yf_documentation_update origin/yf_documentation_update; git checkout yf_documentation_update; ```; [2] Generate the docs. GATK and Picard do this differently:; ```; ./gradlew clean gatkDoc; ```; or; ```; ./gradlew clean picardDoc; ```. [3] Open `index.html` from a browser. Again, GATK and Picard have different locations for their respective docs:; > build/docs/gatkdoc; > build/docs/picarddoc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765:251,load,loading,251,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349675765,1,['load'],['loading']
Performance,"@yfarjoun for small arrays, `FastMath.log` is pretty much as fast as it gets. It is highly optimized. If you are dealing with giant arrays, however, an alternative is to cast the Java arrays into [NDArray](www.nd4j.org) and execute Nd4j native ops. Here's how the timings look:; ```N = 1, Apache = 0.001625 ms, Nd4j = 0.113038 ms; N = 10, Apache = 0.002112 ms, Nd4j = 0.029833 ms; N = 100, Apache = 0.011660 ms, Nd4j = 0.028464 ms; N = 1000, Apache = 0.049915 ms, Nd4j = 0.052455 ms; N = 10000, Apache = 0.348786 ms, Nd4j = 0.430606 ms; N = 100000, Apache = 3.572483 ms, Nd4j = 1.810641 ms; N = 10000000, Apache = 323.021844 ms, Nd4j = 175.421305 ms; ```; The nd4j times include the overhead of creating NDArrays and pulling back the results to JVM heap. The break even point is around N ~ 1000. If accuracy is not a concern, (1) a native implementation of log using half-precision floats or (2) caching, tabulation, and linear interpolation could help. ps> I just realized that the Nd4j call was using 4 threads. if you replace for loops in Java with parallel streams in Java with the same number of threads, Apache always beats Nd4j in this specific test:; ```N = 1, Apache = 0.073910 ms, Nd4j = 0.122488 ms; N = 10, Apache = 0.086508 ms, Nd4j = 0.056924 ms; N = 100, Apache = 0.067022 ms, Nd4j = 0.051674 ms; N = 1000, Apache = 0.081751 ms, Nd4j = 0.075098 ms; N = 10000, Apache = 0.202142 ms, Nd4j = 0.514030 ms; N = 100000, Apache = 1.190085 ms, Nd4j = 1.990945 ms; N = 10000000, Apache = 96.536308 ms, Nd4j = 210.331251 ms; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609:91,optimiz,optimized,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292602609,1,['optimiz'],['optimized']
Performance,@yfarjoun this is a memory optimization but I have confirmed it uses less memory and runs at the same speed,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475:27,optimiz,optimization,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3445#issuecomment-323204475,1,['optimiz'],['optimization']
Performance,A couple more minor comments. I will take your word that your changes improve performance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060,1,['perform'],['performance']
Performance,"A couple of thoughts after doing a little more reading on this. Depending on the source it would appear that each arena will allocate either 64MB or 128MB of virtual memory (i.e. address space). So it would probably also be fine to set this limit a bit higher. Secondly, while there's lots of discussion online that setting this doesn't negatively impact Java code running on the JVM, it is possible that native code invoked using JNI could see a modest reduction in performance _if_ a) it's highly multithreaded and b) it's doing lots of heap allocations. I don't know enough about the native pair-HMM and other native code, but it would be helpful if someone who knows more about that could weigh in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401:467,perform,performance,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849#issuecomment-478574401,1,['perform'],['performance']
Performance,"A few minor issues:. - [x] Change `--resource <blah>` to `--resource:<blah>` in tool-level documentation. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [x] The VCF writer in VariantRecalibrator has a few conditionals to allow for VCF headers without contig lines, we could do the same for the writer in LabeledVariantAnnotationsWalker. EDIT: Added to the sl_lite_overlap branch mentioned below.; - [ ] Double check whether we should worry about any differences in extraction on test data (provided via email) from https://gatk.broadinstitute.org/hc/en-us/community/posts/7974912707099-VariantRecalibrator-IndexOutOfBoundsException. Probably nothing to worry about, and at least the error messaging in the new tools is more informative.; - [x] We could change the strategy for checking for resource overlaps to require allele-level matching (rather than only matching on start position, as was inherited from VQSR). A quick test on malaria shows that this can reduce the number of overlaps by O(10%), but performance doesn't really change too much. Branch is already open at https://github.com/broadinstitute/gatk/tree/sl_lite_overlap; - [ ] Expand the exact-match tests to cover some of these strategies, which were added separately in #8049 and merged to make a release deadline.; - [x] Catch the exception in https://github.com/broadinstitute/gatk/blob/fd782504d18b56dbc266c2b3bb4eb32f21916776/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/scalable/LabeledVariantAnnotationsWalker.java#L389 and throw the same message that is thrown in AS mode. Added in #8074.; - [x] Add message to the score tool that the scores HDF5 file will not be out when the input VCF is empty (such a message is already emitted about the annotations HDF5 file). Added in #8074.; - [ ] Megan suggested in the review of #8074 that dynamic disk sizing could be added to the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946:1020,perform,performance,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1222787946,2,"['perform', 'scalab']","['performance', 'scalable']"
Performance,"A quick look at the code results in the following finding:. The error message : `""SA-BWT inconsistency: seq_len is not the same.""` is generated in function `bwt_restore_sa()` defined in `bwt.c`, and resulted in an `abort()` call.; `bwt_restore_sa()` itself was called in `bwa_idx_load_bwt()` defined in `bwa.c`, when trying to restore the suffix array from a file with extension "".sa"". This function call is issued when bwa mem (in its main) tries to load the reference information. This happens before input are read. Because of the `abort()` call, letting the `BwaMem` class handle the error is difficult, so I would propose two possible solutions:; 1. changing the behavior of `BwaIndex` class, so that it eagerly loads the reference, and throws an `Java.lang.IOException` when this error is encountered. Of course this must lead to code duplication, i.e. copying a large part of index loading code from bwa itself and walk around`abort()`ing.; 2. ask @lh3 to change the behavior so that it will not `abort()` any more, but return a null pointer. But the null pointer return error covers so many error cases that this might not be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189,3,['load'],"['load', 'loading', 'loads']"
Performance,"ACKTRACE_ON_USER_EXCEPTION=true --jar /Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar -- PrintReadsSpark -I gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam -O hs37d5cs.reads.txt --apiKey XXXXXXXXXXXXXXXXXXXXX --verbosity DEBUG --sparkMaster yarn; Copying file:///Users/markw/IdeaProjects/gatk/build/libs/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar [Content-Type=application/java-archive]...; - [1 files][ 95.3 MiB/ 95.3 MiB] 9.0 MiB/s; Operation completed over 1 objects/95.3 MiB.; Job [5b3d4225-0547-4aa9-8a83-ab26460aa2d2] submitted.; Waiting for job output...; 21:42:45.768 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/tmp/5b3d4225-0547-4aa9-8a83-ab26460aa2d2/gatk-package-4.alpha.2-157-g7d7c5ec-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 21:42:45.791 DEBUG IntelGKLUtils - Extracted Intel GKL to /tmp/root/libgkl_compression6493251482684327282.so. 21:42:45.792 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [February 6, 2017 9:42:45 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output hs37d5cs.reads.txt --input gs://mw-pathseq-test/hs37d5cs.reads.sorted.bam --apiKey XXXXXXXXXXXXXXXX --sparkMaster yarn --verbosity DEBUG --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [February 6, 2017 9:42:45 PM UTC] Executing as root@mw-test-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: Version:4.alpha.2-157-g7d7c5ec-SNAPSHOT; 21:42:45.795 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; 21:42:45.795 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; 21:42:45.795 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929:2645,load,loaded,2645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2394#issuecomment-277823929,1,['load'],['loaded']
Performance,"AD_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:44:02.752 INFO GenomicsDBImport - Requester pays: disabled; 15:44:02.752 INFO GenomicsDBImport - Initializing engine; 15:44:07.262 INFO FeatureManager - Using codec BEDCodec to read file file:///home/akansha/vivekruhela/gatk_bundle/hglift_genome1.bed; 15:44:07.274 INFO IntervalArgumentCollection - Processing 2759468497 bp from intervals; 15:44:07.276 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 15:44:07.307 INFO GenomicsDBImport - Done initializing engine; 15:44:07.591 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 15:44:07.592 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/akansha/vivekruhela/pon_db/vidmap.json; 15:44:07.592 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/akansha/vivekruhela/pon_db/callset.json; 15:44:07.592 INFO GenomicsDBImport - Complete VCF Header will be written to /home/akansha/vivekruhela/pon_db/vcfheader.vcf; 15:44:07.592 INFO GenomicsDBImport - Importing to workspace - /home/akansha/vivekruhela/pon_db; 15:44:07.592 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial VCF reader initialization.; 15:44:07.592 INFO ProgressMeter - Starting traversal; 15:44:07.593 INFO ProgressMeter - Current L",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:2610,perform,performance,2610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['perform'],['performance']
Performance,"AD_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller - GCS max retries/reopens: 20; 22:42:22.413 INFO HaplotypeCaller - Requester pays: disabled; 22:42:22.413 INFO HaplotypeCaller - Initializing engine; 22:42:22.705 INFO IntervalArgumentCollection - Processing 2001 bp from intervals; 22:42:22.710 INFO HaplotypeCaller - Done initializing engine; 22:42:22.712 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibrar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:2641,Load,Loading,2641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:185,load,loaded,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,3,"['load', 'perform', 'throughput']","['loaded', 'performed', 'throughput']"
Performance,"Actually, if you have the time, an even more valuable test would be to repeat your comparison with latest master vs. a rebased copy of this branch onto latest master. That would tell us whether the performance difference you saw is due to the downsampling, or due to the differences in the traversal code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307:198,perform,performance,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330921307,1,['perform'],['performance']
Performance,"Additionally. You may wish to compare your variant depths and allelic balances to those captured in gnomAD 4.1 ; [Variant page here](https://gnomad.broadinstitute.org/variant/15-93002203-G-GA?dataset=gnomad_r4); Looking at the balances of heterozygous calls your sites don't seem to display any inconsistencies compared to those available in gnomAD sample set. . Artificial Haplotypes are produced by HaplotypeCaller are available only for debugging purposes. Bamout also includes those informative reads used to produce these haplotypes to provide additional debugging support. Soft clipped bases, realignment, overlapping pairs, duplicates and reads that are available to HaplotypeCaller due to interval restrictions all affect the way you observe these bamouts. . HaplotypeCaller by design is not 100% deterministic in performing its calls. But this does not mean it is inconsistent in making calls when provided all the available reads and reference for its function. That is why we don't recommend restricting HaplotypeCaller when making variant calls with short intervals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950:822,perform,performing,822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8959#issuecomment-2304607950,1,['perform'],['performing']
Performance,"After more experimentation, one issue I was running into with the ApproxKernSeg method was failure on small and ""epidemic"" events. This is because 1) the segment cost function used in that paper is extensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:526,perform,performance,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,2,['perform'],['performance']
Performance,Agreed @samuelklee this is likely due to changes in default parameters but still concerning to see differences like this. The defaults were changed in accordance with the gCNV Nature paper but are designed for exomes. @Stikus you may want to look at the [GATK-SV default settings](https://github.com/broadinstitute/gatk-sv/blob/dc92e9f4bc46a1ab19459b515c24c9747a73a967/wdl/GermlineCNVCohort.wdl#L495) which are tuned for genome calling.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583:411,tune,tuned,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8628#issuecomment-1856621583,1,['tune'],['tuned']
Performance,"Ah, I thought that might be the case, to be honest. . My vote is for ripping out the old. If for whatever reason you find an edge case where the new doesn't perform as well, then in my view that just means the new can be improved. . Ultimately it seems to me that @ldgauthier is the one with the most experience in this space and should make the call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795:157,perform,perform,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258170795,1,['perform'],['perform']
Performance,"Aha, after clearing the travis cache for the PR build it passed! Merging",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733:31,cache,cache,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5194#issuecomment-422499733,2,['cache'],['cache']
Performance,"Aha, good -- that's consistent with @skwalker's results showing a ~30%-40% performance regression vs. GATK3 when using a large interval list + the latest GATK4 HC. I think this is something we can resolve through profiling -- I'll update you in a few days with my progress. In the mean time, it would be valuable to me to know whether you see the same performance difference in Mutect2 when running *without* an interval list (latest master vs. this downsampling branch). Perhaps you could create a large-but-not-too-large bam snippet to test that out?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227:75,perform,performance,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3238#issuecomment-330910227,2,['perform'],['performance']
Performance,Almost looks like there is a buffer overrun somewhere. Most of our testing has been on `nfs` and have not encountered a tcache(thread local cache) issue. Is `gpfs` available as open source?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685:140,cache,cache,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1935180685,1,['cache'],['cache']
Performance,"Also as part of the mock-up, we should actually package the mock config files inside of our jar, load them off the classpath, and test file-based override ability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353:97,load,load,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3126#issuecomment-309543353,1,['load'],['load']
Performance,Also fill in some cost optimizations for some tasks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645:23,optimiz,optimizations,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3940#issuecomment-351478645,1,['optimiz'],['optimizations']
Performance,"Also, FeatureDataSource.getGenomicsDBFeatureReader requires the reference, but it's never used, apparently:. https://github.com/broadinstitute/gatk/blob/bc0994c180312cdca7afbe45b410b2c6fc312043/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java#L387. Should something related to GenomicsDBFeatureReader be getting the reference? FeatureData source has an error if it's not provided: ""You must provide a reference if you want to load from GenomicsDB"".",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285:451,load,load,451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-747448285,1,['load'],['load']
Performance,"Also, and again not part of this PR, does the concept of trying to run VariantEvalEngine over an interval, serializing its state to disk somehow (which would involve doing something with StratificationManager), and then writing a tool that loads/aggregates those per-interval objects seem reasonable as a way to support scatter/gather?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232:240,load,loads,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-754946232,1,['load'],['loads']
Performance,"Also, if we can't figure this out, then I really think it's worth kicking it up to Cromwell to see if call caching can be made more scalable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675:132,scalab,scalable,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632309675,1,['scalab'],['scalable']
Performance,"Also, interestingly, if you look at the pipeline we run nightly in jenkins, there hasn't been any performance regression there. https://gatk-jenkins.broadinstitute.org/view/Performance/job/gatk-perf-test-spark-readpipeline/buildTimeTrend",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364188535,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"Also, it seems that if the input VCF and the dbSNP are not sorted in the same way, for example:; Input VCF; >chr22; >chrX; >chrY; >chrM. dbSNP; >chr22; >chrM; >chrX; >chrY. again, the comparison analysis will fail with the error; > java.lang.IllegalStateException: The elements of the input Iterators are not sorted according to the comparator htsjdk.variant.variantcontext.VariantContextComparator. where the workaround is again to sort both the Input VCF and the dbSNP according to a specific dict, in order to make sure they are matching (if, for example, you don't know how the input VCF and dbSNP were handled upstream), thus leading to always having to perform sorting before VariantEval, which can be 'expensive' if using the recent dbSNP databases (which are around 80GB in size). (Reposting this, originally posted from a wrong account)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612:659,perform,perform,659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6855#issuecomment-710908612,1,['perform'],['perform']
Performance,"An update: apparently asking the user to regenerate their index and dictionary files for their references has resolved the issue. We should soften the blow of null pointers in the future. Apparently the dictionary was present (as it wouldn't work otherwise) but was corrupt somehow, a proposal from @droazen would be to add to the sequnce dictionary check we already perform a check that the file is readable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237:367,perform,perform,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6142#issuecomment-529558237,1,['perform'],['perform']
Performance,"Another development, a new inference method claiming improved performance over ADVI:. https://arxiv.org/abs/1706.02375. We can think of such inference methods (along with the three with implementations available in Stan, namely MAP, ADVI, and NUTS) as interchangeable black boxes that take the optimization target specified by the modeling language and the corresponding automatically generated derivative as inputs. Whatever JNI layer we implement would ideally allow us to build such boxes in pure Java that call out to the JNI for each target/derivative evaluation, as the amount and complexity of code for such boxes should be relatively manageable (in comparison to that required for the autodiff/autotransformation/modeling infrastructure). However, it remains to be seen whether the overhead of such calls will be acceptable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138:62,perform,performance,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-311091138,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Another library that just popped up on my radar---and this one is actually in Java!. http://www.amidsttoolbox.com/; https://arxiv.org/abs/1704.01427; https://arxiv.org/abs/1604.07990. The approach is quite different from the other libraries we have been considering; here, the focus seems to be on streaming/parallel data and inference via message passing. I think our models can be expressed in their framework (although, at a glance, the modeling language is not as nice as Stan---it looks like you have to build DAGs explicitly), but I have to admit that I am not familiar with message passing and how performance compares to MCMC, ADVI, etc. @mbabadi @davidbenjamin any thoughts?. I still think it's worth playing around with this library. We could investigate how quick it would be to implement a streaming/minibatch version of VQSR, for example, which might be useful for @eitanbanks @ldgauthier.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946:605,perform,performance,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2746#issuecomment-318674946,1,['perform'],['performance']
Performance,"Any update on this? I just ran into a form of this problem in the context of some pipeline unit tests. I have a task that runs the following:. ```; gatk GenomicsDBImport \; --sample-name-map ${sample_map} \; --genomicsdb-workspace-path ${cohort_name}_gdb \; --genomicsdb-shared-posixfs-optimizations \; -L ${interval_list}. gatk GenotypeGVCFs \; -R ${ref_fasta} \; -V gendb://${cohort_name}_gdb \; -O ${cohort_name}.joint.vcf \; -L ${interval_list} ; ```. Which runs fine, but if I re-run the test suite the system complains it can't delete the gdb workspace. I have to manually `sudo rm` which is gross. I can work around this by adding either `chmod 777 -R ${cohort_name}_gdb` or `rm -r ${cohort_name}_gdb` as a cleanup step, but that seems gross too. . My use case is just a toy example for training purposes, but I worry about what this could mean for a production environment. Am I missing something?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120:286,optimiz,optimizations,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-2078126120,1,['optimiz'],['optimizations']
Performance,"Are gradle dependencies cached anywhere for the Travis build? Pretty sure they aren't, but I'm out of ideas why Travis is failing for this repo.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145:24,cache,cached,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296293145,1,['cache'],['cached']
Performance,"Are there any updates on this feature? The use of soft-clipping is not only confusing, but can negatively affect the performance of other tools that use this sort of information. Ignoring soft-clipped reads altogether, if possible at all, is not a good solution. We are forced to use GATK3 because the output of the GATK4 version does not work well with others tools we need for the detection of certain variants in RNA-seq.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589:117,perform,performance,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589,1,['perform'],['performance']
Performance,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:61,tune,tune,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068,2,['tune'],['tune']
Performance,"Are you sure that there is enough space? Could you run something like:. ```; df -h /storage/home/data/gendb/; ```; And check that available space is in line with your expectations? Also, is there any user specific quota being enforced on your system?. Not completely sure, but another possibility is that you have too many (fairly small) intervals in your `chr13.bed` file. Unless you are specifically trying to exclude intervals not in the bed file, you should get a lot better performance/efficiency by specifying fewer intervals (think <100 intervals, and in your case since you are doing a single chromosome, maybe much less than that). . If you don't want to manually specify fewer intervals, you could try setting `--merge-input-intervals` to `true` which will return one interval for the contig spanning all the intervals you've specified. Having many smaller intervals results in many small files created which will definitely be bad for performance, and may also cause quota issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409:479,perform,performance,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6950#issuecomment-726416409,2,['perform'],['performance']
Performance,"As @nalinigans suggested, the `--genomicsdb-shared-posixfs-optimizations` should help, though probably mostly for import. Similarly, I would highly recommend `--bypass-feature-reader` [link](https://gatk.broadinstitute.org/hc/en-us/articles/13832686645787-GenomicsDBImport#--bypass-feature-reader) for the import as well. As I mentioned before, reblocking will help import and query - mainly because it reduces the input GVCF size by 5x-8x. Shouldn't be necessary for the number of samples you indicate, but will become more important as number of samples scales up (and does help at any number of samples, I should add). That doesn't seem to the crux of your problem though...you note that running serially does better than trying to parallelize across many cores. I don't have a lot of insight into Lustre specifically, but do you have any metrics on how the IOPS looks for the Lustre FS in each case? Also, the bit about the the first set of variants taking a while - does that time look different when running serially versus in parallel?. One experiment to consider - maybe try to copy the workspace to the `$PBS_JOBFS` folder on the compute node before running `GenotypeGVCFs`. Not sure it is feasible in terms of amount of storage, etc but it would at least rule out possible Lustre issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821:59,optimiz,optimizations,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879964821,1,['optimiz'],['optimizations']
Performance,"As discussed in https://github.com/broadinstitute/gatk/issues/1203, we are keeping SHUFFLE so that we have a baseline during performance testing. Closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2231#issuecomment-288534664,1,['perform'],['performance']
Performance,"As far as I can tell, getting that error message means that BaseTest is being loaded at runtime, and running it's static initializer block which calls `SparkContextFactory.enableTestSparkContext();`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299:78,load,loaded,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330658299,1,['load'],['loaded']
Performance,"As for TableReader.... we could make it a bit more efficient by reusing DataLine instances. Currently it creates one per each input line, but same instance could be reused loading each new line data onto it before calling ```createRecord```. . We are delegating to a external library to parse the lines into String[] arrays (one element per cell) .... we could save on that by implementing it ourselves more efficiently but of course that would be take one of our some of his/her precious development time... In any case I don't know what the gain would be considering that these operations are done close to I/O that typically should be dominating the time-cost. . The DataLine reuse may save some memory churning and wouldn't take long to code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131:172,load,loading,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316489131,1,['load'],['loading']
Performance,"As we discussed on Slack, this will fix the NaNs, but I'm not convinced that we should allow the single-contig use case without at least a warning. The ploidy step will essentially perform no inference, since I think the per-contig bias and ploidy factors will cancel out with the way the likelihood is written---it will simply return the prior, and all samples will be guaranteed to have ploidy = 2. @asmirnov239 is going to do some more testing to make sure we understand this right and perhaps add a warning/documentation. The current likelihood is a bit confusing (I tried to address some of these issues in the unmerged ploidy-model update), but in any case, the problem is degenerate and it's hard to define appropriate behavior without additional priors and model structure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589:181,perform,perform,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6613#issuecomment-631693589,2,['perform'],['perform']
Performance,"BI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 18:01:51.712 INFO SortSam - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 18:01:51.712 INFO SortSam - Defaults.REFERENCE_FASTA : null; 18:01:51.712 INFO SortSam - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:01:51.713 INFO SortSam - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:01:51.713 INFO SortSam - Defaults.USE_CRAM_REF_DOWNLOAD : false; 18:01:51.713 INFO SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:1812,Load,LoadSnappy,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,1,['Load'],['LoadSnappy']
Performance,"BTW, we could actually get a more accurate DP estimate by using the INFO field instead of the FORMAT DP (so we wouldn't have to touch the genotypes at all), but I'm still thinking through the implementation details. It would introduce batch effects, but it would be an improvement in results and rescue your performance optimization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302:308,perform,performance,308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377568302,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Based on testing performed on google cloud, this problem seems to have resolved itself after we updated to the newest googleCloudJava package in this #5135. Something fixed by switching off of our ancient fork must have been responsible for the file being misread by spark. I suggest we close this issue as it appears to be resolved unless @droazen you want to do a post-mortem to figure out what the relevant change must have been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317:17,perform,performed,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5133#issuecomment-419539317,1,['perform'],['performed']
Performance,"Btw - it is interesting (as in, unexpected, at least to me) that appending new data is slowing down as the workspace grows. Adding new data is sorta fire-and-forget in that it shouldn't care much about what already exists...offhand, I'm not sure why you're seeing a slowdown. You mentioned needing to use smaller batch sizes...were you otherwise seeing larger memory overheads than before? Or just general slower performance?. There is a new `--bypass-feature-reader` option in the latest release that should help with dramatically lowering memory usage, and potentially offering a slight speedup for imports. Might be worth a shot. edit: you mentioned consolidate, so I should also add that consolidate is expected to help with read performance (shouldn't affect import) after we've had a fair number of batches imported (left intentionally vague. As a guess, ~100 or so?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955:413,perform,performance,413,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964432955,2,['perform'],['performance']
Performance,"Build is failing because of a warning. It's odd that we didn't see this before, it seems unrelated to my change:. ```; :compileJava/home/travis/.gradle/caches/modules-2/files-2.1/com.google.cloud/gcloud-java-nio/0.2.8/57e30d28f80ab3a320e560e7b4aaac07baf98c4/; gcloud-java-nio-0.2.8-shaded.jar(com/google/cloud/storage/contrib/nio/CloudStorageFileSystemProvider.class): warning:; Cannot find annotation method 'value()' in type 'AutoService': class file for shaded.cloud-nio.com.google.auto.service.AutoService not found; error: warnings found and -Werror specified; 1 error; 1 warning; FAILED; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597:152,cache,caches,152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-257033597,1,['cache'],['caches']
Performance,"By default we only support OSX and modernish linux on amd64. We don't have a plan to test or officially support aarch64. However, it's mostly java so it might mostly work? The things I can think of off the bat that won't work are the optimizations for intel hardware, i.e. the fast pairhmm and deflate/inflate optimizations will definitely not work. They **should** degrade to java implementations automatically though. . IBM has a fork that works on power8 which reimplements some of the optimizations for PairHMM, so it's definitely possible to implement the various native optimizations for other architectures although not a small project. . GenomicsDB won't work since we only bundle libs for amd64/osx. You could in theory compile it yourself and pass the library on the library path. Other things that will need special attention would any of the library wrappers, for bwa-mem, fermlite, and hdf5. Those will similarly need custom builds supplied to the gatk. . I suspect that you could get the reads pipeline working, but newer tools with weirder native dependencies will be tricky.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548:234,optimiz,optimizations,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6118#issuecomment-524959548,4,['optimiz'],['optimizations']
Performance,"CRAM + NIO looks to be ~3 cents per sample. This essentially includes disk optimizations, since the disk size is determined by the CRAM size; this is not too large, so this results in disk costs of ~0.3 cents per sample. Note that I ran on the CRAMs in gs://broad-sv-dev-data/TCGA_blood_normals.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484:75,optimiz,optimizations,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467608484,1,['optimiz'],['optimizations']
Performance,"CRAM w/o NIO is also ~3 cents per sample (it was marginally more expensive than CRAM w/ NIO, but within the noise). CRAM w/o NIO w/ SSD is ~5 cents. So I'd say CRAM w/ or w/o NIO is fine. Strictly speaking, we can't directly compare the BAM and CRAM costs, since they were done on different sets of TCGA samples. But both are well under the goal of ~15 cents per sample, so I think it's safe to say that we can turn our attention to optimizing inference costs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453:433,optimiz,optimizing,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453,1,['optimiz'],['optimizing']
Performance,"Can someone explain this pseudoinverse business to me? We standardize the counts to give a matrix C, calculate the SVD to get the left-singular matrix U and the pseudoinverse of C, but then perform *another* SVD on U to get the pseudoinverse of U. Why do we need these pseudoinverses? @LeeTL1220 @davidbenjamin?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546:190,perform,perform,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316267546,1,['perform'],['perform']
Performance,"Can you please test this change with the `HaplotypeCaller` in protected and make sure nothing changes? In particular, can you run `HaplotypeCallerIntegrationTest` and `HaplotypeCallerEngineUnitTest` and make sure they pass with this change? This PR makes me a little nervous given the centrality of the classes touched, even though the optimization itself is simple enough...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202:336,optimiz,optimization,336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795#issuecomment-216595202,1,['optimiz'],['optimization']
Performance,Can you provide a summary of the optimizations included here @pnvaidya ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682:33,optimiz,optimizations,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364256682,1,['optimiz'],['optimizations']
Performance,Chaning the holding collection to a Vector which is synchronized. There is no performance impact as this add and single collection iteractiion should happen just a finite number of times.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899839756,1,['perform'],['performance']
Performance,"Chiming in here -- the default output for GenotypeGVCFs should be a VCF that follows the spec. An important consideration here is that many users/pipelines usually only perform _site level_ filtering. Genotype level filtering is rarely performed. This causes problems in cases where most genotypes are of high quality at a site and a small number are missing but called as ""0/0"". Those 0/0 calls would then slip by. Coding missing data as 0/0 also breaks common site level filters such as vcftools' --max-missing, which relies on missing sites being coded per the spec.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780:169,perform,perform,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8328#issuecomment-1932170780,2,['perform'],"['perform', 'performed']"
Performance,Closing -- we now have a `--strict` mode that matches the regular `HaplotypeCaller` at the expense of speed. Next step is to work on performance (https://github.com/broadinstitute/gatk/issues/5263),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930:133,perform,performance,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5265#issuecomment-460398930,1,['perform'],['performance']
Performance,Closing because of bad travis cache for builds. Will submit identical PR with cloned branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377:30,cache,cache,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5119#issuecomment-414426377,1,['cache'],['cache']
Performance,"Closing this issue, since the performance issues that the `HaplotypeCaller` had in the early betas are believed to be resolved in the 4.0 release. @chandrans please open a new ticket if the user can replicate the performance issue in the 4.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230:30,perform,performance,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-361989230,2,['perform'],['performance']
Performance,"Consolidated with #2498. Now that #6885 is done, I'm going to kick off a Bayesian optimization (using the pipeline-optimizer from https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer I presented on long ago) over all 3 sets of SW parameters using unfiltered HaplotypeCaller -> vcfeval F1 on NA12878 chr22 (with F1 optimized on GQ threshold and enabling decomposition of variants) as the target. This is probably not what we want to ultimately do in practice---we may want to more heavily weight sensitivity after calling, hook up variant filtering, stratify on high/low confidence regions or variant characteristics, etc.---but I'm just curious to see what happens. I see two potentially useful outcomes: 1) we demonstrate that parameters don't have much of an effect and can be consolidated, or 2) we find more optimal sets of parameters. Potentially we could also show that 3) our parameters are already optimal (I'd say this would be by pure dumb luck), in which case we could at least demonstrate and document some justification for them. If the parameters don't have much of an impact on NA12878, I'm curious to see whether this holds for low coverage or messier data---and ultimately, in malaria. Just starting with NA12878 because of the availability of truth and the potential impact for the primary use case of calling in human data. Some preliminary results: I ran the aforementioned comparison on chr22 with 1) 4.1.8.1 master and 2) 4.1.8.1 with haplotype-to-reference SW parameters changed from `NEW_SW_PARAMETERS` to `STANDARD_NGS` on two replicates of NA12878 (O1D1 and O2D2 from the 2018 NovaSeq snapshot experiment). On each replicate, 2) demonstrated slightly lower performance, but it was well within the sample-to-sample variation between these two replicates. Here are the corresponding vcfeval summaries:. ```; ::::::::::::::; NA12878/O1D1/4.1.8.1/summary.txt; ::::::::::::::; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566:82,optimiz,optimization,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566,4,['optimiz'],"['optimization', 'optimized', 'optimizer']"
Performance,"CopyGCSDirectoryIntoHDFSSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:1899,load,loaded,1899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,1,['load'],['loaded']
Performance,Could I have some feedback about the efficiency of the Fisher's Exact Test implemented here? I'm planning to use it in other context where the performance could be reduced and I think that this is a good opportunity to have some information about it. Thanks in advance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486:143,perform,performance,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2307#issuecomment-267036486,2,['perform'],['performance']
Performance,"Couldn't open hdf5 files.; ![Screenshot_2020-10-29_17-20-17](https://user-images.githubusercontent.com/29140765/97586406-459fe000-1a0b-11eb-86cf-d70a28c55637.png); Running without the optional --sequence-dictionary argument also causes an error.; `17:00:00.556 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 5:00:00 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:00:00.683 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 5:00:00 PM MSK; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.23.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Picard Version: 2.23.3; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:00.685 INFO PostprocessGermlineCNV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:288,Load,Loading,288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['Load'],['Loading']
Performance,"Current status of this: The tool can physically run on 11k samples, but with a 1-5% failure rate, depending on the combination of arguments used. The failures are almost all due to https://github.com/broadinstitute/gatk/issues/2685 (see the stack trace in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727 for a representative example error). . One possibility is that we are being throttled in a way that GATK itself can't recover from. GATK is retrying in the face of these SSL errors 20 times, with increasing wait times between each attempt, and still running out of retries. See @jean-philippe-martin 's latest hypothesis in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876. @kcibul @Horneth take note.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736:406,throttle,throttled,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736,1,['throttle'],['throttled']
Performance,D.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10381,concurren,concurrent,10381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"DBImport, I randomly select 50 samples from our history samples(using the same probe set) along with the current batch.; time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport \; --tmp-dir /paedyl01/disk1/yangyxt/test_tmp \; --genomicsdb-update-workspace-path ${vcf_dir}/genomicdbimport_chr${1} \; -R ${ref_gen}/ucsc.hg19.fasta \; --batch-size 0 \; --sample-name-map ${gvcf}/batch_cohort.sample_map \; --reader-threads 5; check_return_code. # For GenotypeGVCFs; time ${gatk} --java-options ""-Xmx8g -Xms2g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenotypeGVCFs \; -R ${ref_gen}/ucsc.hg19.fasta \; -V gendb://${vcf_dir}/genomicdbimport_chr${1} \; -G StandardAnnotation \; -G AS_StandardAnnotation \; -L chr${1} \; -O ${bgvcf}/all_${seq_type}_samples_plus_${sample_batch}.chr${1}.HC.vcf. # These are log records:; 02:07:51.286 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 02:07:51.321 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl; Nov 06, 2020 2:07:56 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 02:07:56.529 INFO GenotypeGVCFs - ------------------------------------------------------------; 02:07:56.529 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 02:07:56.530 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 02:08:01.543 INFO GenotypeGVCFs - Executing as yangyxt@paedyl01 on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 02:08:01.543 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 02:08:01.543 INFO GenotypeGVCFs - Start Date/Time: November 6, 2020 2:07:51 AM HKT; 02:08:01.543 INFO GenotypeGVCFs - ----------------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059:1257,Load,Loading,1257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-722764059,1,['Load'],['Loading']
Performance,Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMete,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2052,Load,Loading,2052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"Date/Time: March 7, 2019 9:49:03 AM EST; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 09:49:06.937 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; 09:49:07.007 INFO ProgressMeter - Starting traversal; 09:49:07.007 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 09:49:17.023 INFO ProgressMeter - 1:139173 0.2 480 2875.7; 09:49:27.704 INFO ProgressMeter - 1:763661 0.3 2590 7508.3; 09:49:38.001 INFO ProgressMeter - 1:958723 0.5 3290 6369.0; 09:49:49.182 INFO ProgressMeter - 1:981050 0.7 3380 4808.5; 09:50:02.383 INFO ProgressMeter - 1:988991 0.9 3440 3727.3; 09:50:13.586 INFO ProgressMeter - 1:1227096 1.1 4290 3866.1; 09:50:23.594 INFO ProgressMeter - 1:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:1817,Load,Loading,1817,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['Load'],['Loading']
Performance,"Dear @droazen I might now have an idea.; Since we use the container in a restricted are, we pull the container from docker hub, save it as tar.gz, transfer it to the server and load it to the docker local image library. On this system we had the latest version of docker installed that was available from the ubuntu repository (20.something). This might lead to the error.; Best,; Daniel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398:177,load,load,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1710415398,1,['load'],['load']
Performance,"Deflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not inst",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5156,load,loaded,5156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,Did you reproduce the issue after using `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484:69,optimiz,optimizations,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761265484,1,['optimiz'],['optimizations']
Performance,"Discussed with @LeeTL1220 just now, and we decided that an approach based on the vanilla `LocusWalker` with calls to `splitContextBySampleName()` would result in a cleaner and more easily-parallelizable implementation, without the need for maintaining state across calls. So this branch is getting put on hold for now until we see whether the overhead of having to merge bams and then pull apart the pileups in the single-pass implementation is a significant performance issue. @LeeTL1220 Let me know the outcome of that evaluation, and we'll decide whether to resurrect or kill this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013:459,perform,performance,459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2374#issuecomment-275748013,1,['perform'],['performance']
Performance,Dispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClas,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:1938,concurren,concurrent,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"Do you manually download the files in src/main/resources/large from git-lfs, you need those in order to perform the build. I'm not sure a good way to get those without using git. You could of course pull them and then tar them though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437:104,perform,perform,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584454437,1,['perform'],['perform']
Performance,"Do you mean in production? In the ~2.92Gbp of the WGS calling regions, I see all ~2.44Gbp of the GIAB HCR, as well as ~485Mbp / ~778Mbp of the GIAB ""LCR"" (naively, just the complement of the HCR). Or did you mean in another context?. I'm kicking off runs with the CHM now. Hopefully, optimizations of sensitivity outside the CHM HCR will be more meaningful, since I see a decent amount of calls outside of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389:284,optimiz,optimizations,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713785389,1,['optimiz'],['optimizations']
Performance,"Don't mean to derail the conversation, but I just wanted to chime in about this:; > A good setting for `--nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. This doesn't sound right. We have informally tested how the performance of PHMM changes with this parameter, and we've observed roughly linear performance improvement as you increase the value of `nativePairHmmThreads`, up to the number of physical cores in the system. These performance tests were done a few months ago, mind you, but I can't think of anything that would have altered this behavior since then, at least in the GKL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917:247,perform,performance,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332916917,3,['perform'],['performance']
Performance,"Done, the problematic commit was https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1. Below are the results with versions suggested by git bisect.; Calling performance was evaluated with chr22 in the query vcf and full truth vcf, therefore the calculation of the recall is invalid (but irrelevant for the purpose):; ![FD_TN_4181_chr22_FD_TN_-4 1 8 1-28-g6d8cdfc_chr22_FD_TN-4 1 8 1-42-g851c840_chr22_FD_TN-4 1 8 1-49-g4e8b73e_chr22_FD_TN-4 1 8 1-50-ga304725_chr22_FD_TN-4 1 8 1-51-g66570bf_chr22_FD_TN-4 1 8 1-52-g1238565_chr22_FD_TN_4190_chr22](https://user-images.githubusercontent.com/15612230/236503899-e922abb0-5d9f-44d3-bf44-a801652744ea.png). As stated in the pull request https://github.com/broadinstitute/gatk/pull/6821, the change was evaluated with the DREAM3 benchmark and made sense at the time. To be fair, in the HCC1395 benchmark there is still a gain in recall (<2%, see the first figure in my original report above), which I assume to be mostly due to this commit, but the recall/precision tradeoff looks different now with the better benchmark callset. I have not figured out what exactly the code in the commit does, maybe we can fine-tune the changes @davidbenjamin ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624:191,perform,performance,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1536484624,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"Done. Full results are on the internal presentation slides. The summary table follows:. | speedup vs async on a 1-core machine | slice | whole | intervals |; |-------------|----------|---------|----------|; | vcf | 0.91| 1.40| 0.57|; | bam (exome)| 40.42| 1.06| 1.02|; | bam (wgs)| 111.18| 1.21| 0.99|. This table compares the execution time of a single machine running PrintReads or SelectVariants, getting its input either directly from the Google bucket (NIO), or by first copying it with gsutil and then running off the local disk (with the async option turned on, allowing eager decompression of the stream - a feature the NIO code does not have). Each experiment is run four times, and each number here represents the ratio of two such experiments. Numbers larger than 1 indicate that NIO was faster. Each row is a different input file: vcf, small bam (exome), large bam (whole genome). Each column is a selection of what to read from the file (via the `-L` argument): a megabase slice, the whole file, or a long list of intervals. The NIO code relies heavily on prefetching, so it doesn't perform well with the many disjoint accesses of the right column. When processing only a small part of the (already small) vcf file, NIO loses out to copy + local processing. Everywhere else the direct-to-bucket ""NIO"" code performs quite well, up to 111x faster than the ""copy then process"" approach. I also ran the full set with async disabled. It makes a difference but NIO still wins and loses at the same places by similar margins (in particular the 111x win remains).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956:1096,perform,perform,1096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2424#issuecomment-284056956,2,['perform'],"['perform', 'performs']"
Performance,"Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.di$able=true --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --conf spark.cores.max=720 --executor-cores 20 --executor-memory 50g --conf spark.driver.memory=50g /home/myname/gatk4/gatk$build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar BwaAndMarkDuplicatesPipelineSpark -I hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam -O hdfs://ln16/user/myname/gatk4test/B$aAndMarkDuplicatesPipelineSpark_out.bam -R hdfs://ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --disableSequenceDictionaryValidation --sparkMaster spark://$n16:7077; 16:55:20.195 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/myname/gatk4/gatk/build/libs/gatk-package-4.alpha.2-1125-g27b5190-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 29, 2017 4:55:20 PM CST] BwaAndMarkDuplicatesPipelineSpark --bwamemIndexImage /hadoop/myname/GRCh37.fa.img --output hdfs://ln16/user/myname/gatk4test/BwaAndMarkDuplicatesPipelineSpark_out.bam --reference hdfs$//ln16/user/myname/genomes/Hsapiens/GRCh37/seq/GRCh37.2bit --input hdfs://ln16/user/myname/NA12878/wes/NA12878-NGv3-LAB1360-A.unaligned.bam --disableSequenceDictionaryValidation true --sparkMaster spark://ln16:7077 --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --help fal$e --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters fals",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:2468,Load,Loading,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['Load'],['Loading']
Performance,"Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2175,load,load,2175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"EF ALT QUAL FILTER INFO FORMAT CGAAGAGGTAGGTGCGAG-1; chr19 55910646 . AC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910648 . AAATCCCCC A,<NON_REF> 352.6 . . GT:AD:DP:GQ:PGT:PID:PS 0|1:6,9,0:15:99:0|1:55910646_AC_A:55910646; chr19 55910653 . CCCCAT *,C,<NON_REF> 227.84 . . GT:AD:DP:GQ:PGT:PID:PS 2|1:0,9,6,0:15:99:1|0:55910646_AC_A:55910646; chr19 55910675 . T C,<NON_REF> 30.64 . . GT:AD:DP:GQ 0/1:13,2,0:15:38; ```. ```; Using GATK jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar GenotypeGVCFs -R chr19.fa -V output.g.vcf -O output.vcf; Picked up JAVA_TOOL_OPTIONS: -Djava.net.useSystemProxies=true; 13:40:59.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:40:59.636 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.653 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.4.0.0; 13:40:59.653 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:40:59.654 INFO GenotypeGVCFs - Executing as gleixner@odcf-worker02 on Linux v3.10.0-1160.76.1.el7.x86_64 amd64; 13:40:59.654 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v17+35-2724; 13:40:59.655 INFO GenotypeGVCFs - Start Date/Time: October 26, 2023 at 1:40:59 PM CEST; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.655 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:40:59.658 INFO GenotypeGVCFs - HTSJDK Version: 3.0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:17114,Load,Loading,17114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,"EVEL : 2; > 17 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 18 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 19 15:07:52.442 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 20 15:07:52.442 INFO Mutect2 - Deflater: IntelDeflater; > 21 15:07:52.442 INFO Mutect2 - Inflater: IntelInflater; > 22 15:07:52.442 INFO Mutect2 - GCS max retries/reopens: 20; > 23 15:07:52.443 INFO Mutect2 - Requester pays: disabled; > 24 15:07:52.443 INFO Mutect2 - Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 INFO ProgressMeter - Starting traversal; > 36 15:07:53.314 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; > 37 15:07:54.410 INFO VectorLoglessPairHMM - Time spent in setup for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:3105,Load,Loading,3105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,E_FOR_TRIBBLE : false; 11:47:51.057 INFO Mutect2 - Deflater: IntelDeflater; 11:47:51.057 INFO Mutect2 - Inflater: IntelInflater; 11:47:51.057 INFO Mutect2 - GCS max retries/reopens: 20; 11:47:51.057 INFO Mutect2 - Requester pays: disabled; 11:47:51.057 INFO Mutect2 - Initializing engine; 11:47:51.372 INFO FeatureManager - Using codec VCFCodec to read file file:///home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz; 11:47:51.457 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0; 11:47:52.683 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0; 11:47:52.683 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.24 sec; 11:47:52.684 INFO Mutect,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:3853,Load,Loading,3853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"Each scatter of ModelSegments will run as before, aside from skipping the segmentation step in favor of using the joint segmentation. We will repeat the het-genotyping step, but this is cheap and it's probably better to repeat it to make sure filtering is applied consistently. It would also require more changes to the command line to specify where to output the hets for each sample during multisample segmentation and to skip genotyping in each scatter, if we were to go that route. There are many possible combinations of inputs that need to be tested, but the same is already true of the current ModelSegments. Furthermore, there are slight wrinkles when running in tumor-only mode (i.e., when `--normal-allelic-counts` are not available). Because each sample is genotyped indiviudally, each may yield a different set of hets (in contrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as op",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:3026,perform,performing,3026,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['performing']
Performance,"Essentially using preclustering of samples (e.g., by performing clustering on a subset of the coverage bins) to automate building of multiple PoNs when appropriate. Proof-of-principle was implemented for germline (#5633) in https://github.com/broadinstitute/gatk-evaluation/tree/sl_mega_wdl, but it hasn't been merged yet nor do we have plans to put it in production or heavy use anytime soon---sticking with manual preclustering when needed, for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707:53,perform,performing,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5634#issuecomment-525798707,1,['perform'],['performing']
Performance,"Evaluation of THCA/STAD/LUAD TCGA WGS/WES CR concordance with SNP arrays was implemented on FC last summer and showed good performance. For WES, comparisons against GATK CNV and CODEX showed comparable to highly improved performance, respectively, with minimal parameter tuning. WGS comparisons were unavailable due to limitations of competing tools. This evaluation will be expanded to include CR/MAF concordance against PanCanAtlas ABSOLUTE results. Some curation of the samples could be performed; some batch effects were observed in some LC WGS LUAD samples. Comparisons to other tools will probably be removed for ease of maintenance. Will be adapted to fit into whatever framework arises from #4630; same goes for HCC1143 and CRSP validations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697:123,perform,performance,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-459833697,3,['perform'],"['performance', 'performed']"
Performance,"Even if we had default methods, `Locatable` should be simple (like `Comparable`) and shouldn't be polluted with every possible operation you might want to perform on an interval.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026:155,perform,perform,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/305#issuecomment-79198026,1,['perform'],['perform']
Performance,"Excellent, thank you @davidbenjamin and @ddrichel!. 1. Since it has been three weeks, are there any news on whether the regenerated PoN works, and where from it is available?; 2. Also, since there are likely many people (some of which have spoken up in the thread) who have been using the affected releases throughout the years, possibly affecting patients' diagnoses, does the Broad have a communication plan to notify Mutect2 users of this serious performance degradation in the specific cases you outlined?; 3. Last, are there plans to incorporate fully functional benchmark regression tests such as @ddrichel's into the Mutect2 (and later Mutect3) CI pipeline, or at least into the build process for each version that is released to the public?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315:450,perform,performance,450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1225309315,1,['perform'],['performance']
Performance,"Executive summary: . My main concern is that the amount of unsupported code is continuing to grow. Adding this PR would bring the total to about ~5k lines of WDL, Java, and test code. In comparison, the amount of corresponding supported CNV code clocks in around ~33k---this includes all of gCNV, as well! Development time has also been non-negligible and dates back to pre-4.0 release. Another concern is that the number of users of this unsupported code is also growing. In fact, it seems like we are actively pointing users to it. This seems unsustainable going forward. Finally, I don't think we have satisfactorily demonstrated which of the functions accomplished by this code (format conversion, post-hoc blacklisting, germline/""CNLOH"" tagging and imputation) are necessary or cannot be performed by existing code or more streamlined and principled methods. (Some of these functions, such as IGV conversion, are already performed by existing code.) Of those functions, I think format conversion is the only one we should retain from this code in an unsupported fashion. So if this PR introduces a useful GISTIC conversion, no harm in merging that. This all sounds like a decision for the new tech lead! @mwalker174 any thoughts? . More detailed responses follow:. > Users are already using this branch and giving me positive feedback (definitely more positive than adjusting num_changepoints_penalty_factor). I suggest merging mostly for practical reasons. It buys us more time to put in a principled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. While it's great that users are giving positive feedback, I refer you to CellBender team's manifesto at https://github.com/broadinstitute/CellBender/commit/28f02f8dbd716aff922bb8da1e56da29347b245b. Can these users help us definitiv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:793,perform,performed,793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,4,['perform'],['performed']
Performance,Executor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSyst,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2064,concurren,concurrent,2064,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,FO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:57:50.897 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:57:50.897 INFO GenomicsDBImport - Inflater: IntelInflater; 11:57:50.897 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:57:50.898 INFO GenomicsDBImport - Requester pays: disabled; 11:57:50.898 INFO GenomicsDBImport - Initializing engine; 11:57:52.588 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/isilon/experiment/resources/final_mm10_exon.bed; 11:57:53.791 INFO IntervalArgumentCollection - Processing 199895151 bp from intervals; 11:57:53.825 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. It is recommended that intervals be aggregated together.; 11:57:53.837 INFO GenomicsDBImport - Done initializing engine; 11:57:54.037 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vidmap.json; 11:57:54.037 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/callset.json; 11:57:54.037 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/vcfheader.vcf; 11:57:54.037 INFO GenomicsDBImport - Importing to array - /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db/genomicsdb_array; 11:57:54.038 INFO ProgressMeter - Starting traversal; 11:57:54.038 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 12:05:27.926 INFO GenomicsDBImport - Starting batch inpu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820:3182,perform,performance,3182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820,1,['perform'],['performance']
Performance,"FO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:50:16.819 INFO HaplotypeCaller - Initializing engine; 14:50:18.950 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:50:18.965 INFO HaplotypeCaller - Done initializing engine; 14:50:19.021 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 14:50:19.280 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.481 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 14:50:19.776 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_utils.so; 14:50:19.795 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 14:50:19.847 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 14:50:19.848 INFO IntelPairHmm - Available threads: 48; 14:50:19.848 INFO IntelPairHmm - Requested threads: 4; 14:50:19.848 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 14:50:19.926 INFO ProgressMeter - Starting traversal; 14:50:19.926 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 14:50:30.309 INFO ProgressMeter - chr17:740224 0.2 3010 17395.5; 14:50:41.016 INFO ProgressMeter - chr17:1675683 0.4 7020 19973.4; 14:50:51.041 INFO ProgressMeter - chr17:2415218 0.5 10100 19477.4; 14:51:01.041 INFO ProgressMeter - chr17:3591332 0.7 14920 21773.6; 14:51:11.059 INFO ProgressMeter - chr17:4574538 0.9 19100 22412.6; 14:51:21.089 INFO ProgressMeter - chr17:5",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:6822,Load,Loading,6822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,"FYI @sooheelee I pointed you at the docs recently, but realized they're slightly out of date. CreatePanelOfNormals currently expects proportional coverage, upon which it initially performs a series of filtering steps. The docs state that these steps are performed on integer counts, which is incorrect. The fact that filtering yields different post-filter coverage for the two types of input ultimately results in slightly different denoising. Not a big deal, but we should decide what the actual method should be doing and clarify in the code/docs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956:180,perform,performs,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-314529956,2,['perform'],"['performed', 'performs']"
Performance,"Fair points. I agree that legacy tools/versions that are part of a canonical or relatively widely used pipeline should have good documentation. However, there are many of the CNV tools that are basically prototypes---they have never been part of a pipeline, have no tutorial materials, and the chances that any external users have actually used them are probably extremely low. The sooner they are deprecated, the less the overall burden on both comms and methods---I don't think comms should need to feel protective of code or tools that developers are willing to scrap wholesale!. I'd like to cordon off or hide such tools so the program group doesn't get too cluttered---if we can do this in a way that doesn't require @cmnbroad to add more categories, that would be great. For example, we will have 5 tools that one might reasonably try to use for segmentation (PerformSegmentation, ModelSegments, PerformAlleleFractionSegmentation, PerformCopyRatioSegmentation, and PerformJointSegmentation). The first two are part of the legacy and new pipelines, respectively, but the last 3 were experimental prototypes. I think it's definitely confusing to have these 3 presented in the program group, and treating them the same as the other tools in terms of documentation is just extra work for everyone. In any case, I definitely think an additional program group to separate the legacy and new tools is warranted, since many of the updated tools in the new pipeline have very similar names to the legacy tools. If this is OK with everyone, I'll just add a ""LegacyCopyNumber"" program group, which I don't think should require extra work on anyone else's part.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604:866,Perform,PerformSegmentation,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346187604,4,['Perform'],"['PerformAlleleFractionSegmentation', 'PerformCopyRatioSegmentation', 'PerformJointSegmentation', 'PerformSegmentation']"
Performance,FeatureDataSource.java:326); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.initializeDrivingVariants(VariantLocusWalker.java:76); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:67); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.VariantLocusWalker.onStartup(VariantLocusWalker.java:63); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.io.IOException: GenomicsDB JNI Error: VariantQueryProcessorException : Could not open array 3$1$121351753 at workspace: /data1/EquCab/GenomicsDB/ECA3_GenomicsDB_260/3; TileDB error message : [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading MBR failed; 	at org.genomicsdb.reader.GenomicsDBQueryStream.jniGenomicsDBInit(Native Method); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:209); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:182); 	at org.genomicsdb.reader.GenomicsDBQueryStream.<init>(GenomicsDBQueryStream.java:91); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.generateHeadersForQuery(GenomicsDBFeatureReader.java:200); 	at org.genomicsdb.reader.GenomicsDBFeatureReader.<init>(GenomicsDBFeatureReader.java:85); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getGenomicsDBFeatureReader(FeatureDataSource.java:407); 	... 12 more,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:8446,load,load,8446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,2,['load'],['load']
Performance,Filed #2331 to add prefetching. As expected this has a big positive impact on performance.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561:78,perform,performance,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-270988561,1,['perform'],['performance']
Performance,"Finally ready to close for two further reasons:. * After David R taught me how to use the profiler I was surprised to see that assembly in M2 is twice as expensive as pairHMM, so post-assembly optimizations aren't so valuable.; * Any issue of spurious or wasteful haplotypes could be addressed more elegantly using a string graph for assembly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611:193,optimiz,optimizations,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2945#issuecomment-381082611,1,['optimiz'],['optimizations']
Performance,"Fine but this is clearly premature optimization. How about a class called Intervals or intervalutils for this sort of random; utility ?. On Thursday, February 18, 2016, droazen notifications@github.com wrote:. > @akiezun https://github.com/akiezun What will actually happen is that; > someone will need that functionality months from now, forget that it; > already exists (embedded in some random tool), and re-implement it. It; > should be moved back now before this is allowed to happen.; > ; > â€”; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185886728. ## . Sent from Gmail Mobile",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065:35,optimiz,optimization,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1497#issuecomment-185888065,1,['optimiz'],['optimization']
Performance,"Finished, [2D CNN inference](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java) and training merged, many new issues spawned.:; - [X] A cool(?) name CNNScoreVariants; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [x] Currently just re-filtering. Still no joint calling solution...; - [x] Hyperparameters optimized for small 2d model similar performance but .25 params.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706:663,optimiz,optimized,663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-377623706,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Finishing up a tieout using old MalariaGEN Pf7 CNV genotyping runs, and I think things look good from this perspective, at least. This tieout uses a subset of the Pf7 samples containing 300 cohort and 1683 case samples (which were indeed treated as a cohort-case cluster in the original Pf7 CNV genotyping analysis). ~4k genomic bins are covered. We compare this branch against 4.5.0.0, as well as this branch against itself (checking for reproducibility). Costs for this branch ($10.92) and 4.5.0.0 ($10.96) were quite comparable. Note that a small portion of these costs derives from Pf7-specific genotyping steps, which I did not bother to remove from the workflow. Runtime for the ploidy modeling and postprocessing steps were comparable. Interestingly, **runtime for the gCNV was ~20-25% longer with this branch than with 4.5.0.0, but memory usage fell by a factor of ~3 (~6GB to ~2GB)!** I am not sure if we could recoup the runtime with some more tweaking of the environment (perhaps double checking that optimized BLAS/MKL/etc. packages are properly used, changing environment variables/flags, etc.), but I think the decrease in memory usage is quite nice. Concordance was checked for the following quantities (4.5.0.0 is on the x-axis and this branch is on the y-axis in all plots below):. 1) Variational posterior means (`mu_*`) and standard deviations (`std_*`) for all analogous variables in the ploidy and gCNV models. There were some slight changes to the gCNV model in this branch (e.g., the functional form of the ARD prior was changed), which means some variables are no longer directly comparable. Furthermore, some variables (such as the bias factors W) are degenerate and cannot be immediately compared. Otherwise, there is good concordance between the remaining variables, e.g.:. ![image](https://github.com/broadinstitute/gatk/assets/11076296/614cf501-ca31-4199-badb-3194b7f78154); ![image](https://github.com/broadinstitute/gatk/assets/11076296/f615084d-d0bf-44e9-bcf5-98abd26ce",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268:1012,optimiz,optimized,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2079695268,1,['optimiz'],['optimized']
Performance,"First comment, I would like to see what happens when there are 2 concurrent failing commits being run by travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443:65,concurren,concurrent,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6247#issuecomment-550468443,1,['concurren'],['concurrent']
Performance,"For evaluations, we don't look at calls outside the high confidence regions. For production we deliver calls over the whole genome (except for Y in WGS, which is another story). Even for CHM, I wouldn't optimize outside the high confidence regions. For example, seg dupes are excluded from the high confidence regions. There likely will be calls there, but there's no guarantee that even a real variant should map to that copy of the segment. Of course if either @davidbenjamin or @fleharty has a differing opinion I'm open to discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769:203,optimiz,optimize,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-714493769,1,['optimiz'],['optimize']
Performance,"For production we use a different execution engine (Zamboni, not Queue). The number of scatters is a static part of the workflow and Zamboni has the responsibility of splitting the interval list per scatter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261:65,Queue,Queue,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270436261,1,['Queue'],['Queue']
Performance,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:146,perform,performance,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030,2,['perform'],['performance']
Performance,"For reference, plots of memory usage for a 50x10000 shard:. ![gcnv-10k-mem](https://user-images.githubusercontent.com/11076296/53906618-6be46100-4019-11e9-8d70-5971fd5d2e35.png). It appears that the 0.9.0 leak only occurs during denoising + sampling. Not sure if differences in runtime are indicative of OpenBLAS vs. MKL performance or within the noise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552:321,perform,performance,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764#issuecomment-470236552,1,['perform'],['performance']
Performance,"For some performance comparisons with a small dataset:. Smith-Waterman runtime without AVX: `256.88`s; Smith-Waterman runtime with AVX: `34.09`s (`7.5`x faster ðŸ˜±). It's not surprising, but it is a huge difference.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845:9,perform,performance,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8485#issuecomment-1684147845,1,['perform'],['performance']
Performance,"For the following variants: . `chr1 819955 . TTCACGAATT T . PASS .`; `chr1 819979 . CATGAGCAT C . PASS .`. _VEP_ seems to produce **incorrect** data: ; ```; ##fileformat=VCFv4.1; ##VEP=""v94"" time=""2018-10-30 19:13:50"" cache=""/nfs/public/release/ensweb-data/latest/tools/grch37/e94/vep/cache/homo_sapiens/94_GRCh37"" db=""homo_sapiens_core_94_37@hh-mysql-ens-grch37-web"" 1000genomes=""phase3"" COSMIC=""81"" ClinVar=""201706"" ESP=""20141103"" HGMD-PUBLIC=""20164"" assembly=""GRCh37.p13"" dbSNP=""150"" gencode=""GENCODE 19"" genebuild=""2011-04"" gnomAD=""170228"" polyphen=""2.2.2"" regbuild=""1.0"" sift=""sift5.2.2""; ##INFO=<ID=CSQ,Number=.,Type=String,Description=""Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|TSL|APPRIS|ENSP|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|gnomAD_AF|gnomAD_AFR_AF|gnomAD_AMR_AF|gnomAD_ASJ_AF|gnomAD_EAS_AF|gnomAD_FIN_AF|gnomAD_NFE_AF|gnomAD_OTH_AF|gnomAD_SAS_AF|CLIN_SIG|SOMATIC|PHENO|PUBMED|MOTIF_NAME|MOTIF_POS|HIGH_INF_POS|MOTIF_SCORE_CHANGE"">; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	819955	.	TTCACGAATT	T	.	PASS	CSQ=-|splice_acceptor_variant&coding_sequence_variant&intron_variant|HIGH|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3|2/2|||?-38|?-38|?-13|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; chr1	819979	.	CATGAGCAT	C	.	PASS	CSQ=-|coding_sequence_variant&3_prime_UTR_variant|MODIFIER|AL645608.2|ENSG00000269308|Transcript|ENST00000594233|protein_coding|3/3||||54-?|54-?|18-?|||||1||Clone_based_ensembl_gene||||ENSP00000470877|||||||||||||||||||||||||||; ```. There is no cDNA string / amino acid change and the positions have `?` characters in them (not 100% sure the question marks are wrong - there is no real spec for these fields). _Oncotator_ seems to produce **incorre",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224:218,cache,cache,218,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4307#issuecomment-434440224,2,['cache'],['cache']
Performance,For the new GATK framework the multi-thread support is through Spark (see https://github.com/broadinstitute/gatk/issues/2345 for more details).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760:31,multi-thread,multi-thread,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4448#issuecomment-367984760,1,['multi-thread'],['multi-thread']
Performance,"For this ticket, we need to modify `ReferenceUtils.loadFastaDictionary()` to throw a `UserException` if the header returned from its `SAMTextHeaderCodec` contains no sequence dictionary",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587:51,load,loadFastaDictionary,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2609#issuecomment-305017587,1,['load'],['loadFastaDictionary']
Performance,"For those who might be interested, I finally found a good set up to run gcnvkernel outside a conda environment. Modules loaded: GATK/4.2.4.0 and python/3.8.2. In my python virtualenv:. pip install --no-index --upgrade pip; pip install --no-index --ignore-installed numpy==1.21.0; pip install --no-index scipy==1.2.0; pip install pymc3==3.1; pip install Theano==1.0.4; pip install --no-index tqdm==4.19.5; pip install --no-index PyVCF==0.6.8. git clone https://github.com/broadinstitute/gatk.git; cd gatk/src/main/python/org/broadinstitute/hellbender; python setup_gcnvkernel.py install; python setup.py install",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762:120,load,loaded,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387#issuecomment-1613468762,1,['load'],['loaded']
Performance,For various reasons that backport would be too awful to contemplate even if we wanted to backport. The question remains for GATK4 whether we want to preserve the old qual in case there's some edge case where it performs better. I consider this highly unlikely but others might disagree.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008:211,perform,performs,211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258167008,1,['perform'],['performs']
Performance,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:67,load,loading,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206,1,['load'],['loading']
Performance,"From what I have heard from Dmitriy, this loss in Mutect2 precision between Mutect2 v.4.1.8.1. and v.4.1.9.0 seems to be perfectly reproducible using default parameters on the HCC1395 somatic benchmark gold-standard from the [Sequencing Quality Control Phase II Consortium](https://pubmed.ncbi.nlm.nih.gov/?term=Somatic+Mutation+Working+Group+of+Sequencing+Quality+Control+Phase+II+Consortium%5BCorporate+Author%5D). The drop in performance seems to still persist in the current Mutect2 version. If true, then this could have dramatic affects on Mutect2 clinical variant calling quality since v.4.1.9.0 until now. It would appear that isolating the root cause of this drop in performance has high importance for maintaining the clinical validity of Mutect2. It seems quite a number of Mutect2 code details have [changed between v.4.1.8.1. and v.4.1.9.0](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0), as for instance [this commit concerning quality filtering](https://github.com/broadinstitute/gatk/commit/a304725a60f5000ec6381040137043a557fc3dc1), or [this one concerning soft-clipped bases](https://github.com/broadinstitute/gatk/commit/4982c2fa60e89f699a81150116d058aeac2f7573). Perhaps the circumstance of the drop in precision affecting WES but not WGS data may point to the culprit?. Also, may I ask whether the GATK team is doing real-world regression test on gold-standard variant callsets between version releases, and have these tests passed between v.4.1.8.1. and v.4.1.9.0?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509:429,perform,performance,429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1169735509,2,['perform'],['performance']
Performance,"GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5178,load,load,5178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,GJlbmRlci91dGlscy9HQVRLUHJvdGVjdGVkTWF0aFV0aWxzLmphdmE=) | `69.531% <Ã¸> (-13.802%)` | `51 <0> (-8)` | |; | [...stitute/hellbender/utils/icg/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvRHVwbGljYWJsZU51bWJlci5qYXZh) | `80% <Ã¸> (Ã¸)` | `5 <0> (?)` | |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9Ob3JtYWxpemVTb21hdGljUmVhZENvdW50cy5qYXZh) | `79.167% <Ã¸> (Ã¸)` | `6 <0> (Ã¸)` | :arrow_down: |; | [...institute/hellbender/tools/exome/CallSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxsU2VnbWVudHMuamF2YQ==) | `83.333% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/icg/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ2FjaGVOb2RlLmphdmE=) | `80.645% <Ã¸> (Ã¸)` | `9 <0> (?)` | |; | [...ntationbiasvariantfilter/OrientationBiasUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9vcmllbnRhdGlvbmJpYXN2YXJpYW50ZmlsdGVyL09yaWVudGF0aW9uQmlhc1V0aWxzLmphdmE=) | `84.956% <Ã¸> (Ã¸)` | `56 <0> (Ã¸)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `59.686% <Ã¸> (Ã¸)` | `49 <0> (Ã¸)` | :arrow_down: |; | [...tute/hellbender/tools/exome/TargetTableReader.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911:2713,Cache,CacheNode,2713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911,1,['Cache'],['CacheNode']
Performance,"GenomicsDB loads now, and I can run the tests. The MIN_DP issue is happening because the BCF codec creates the VariantContext with an Integer value for the attribute since it has the type descriptor at hand, whereas the VCF codec just creates it as a String (it doesn't consult the header, which I might call a bug). The comparator doesn't compare these correctly, and ironically, its the string that gets converted to a double. There also seem to be other problematic type differences between the VCs created from BCF vs. VCF. I need to do think a bit about how to properly fix this; I'm not sure just making the comparator more tolerant of these differences is a good idea, since it could wind up masking real problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429:11,load,loads,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294962429,1,['load'],['loads']
Performance,"Getting closer; the GenomicsDB tests all pass locally for me now, but we seem to have a few other issues. All of the GenomicsDB tests are failing on travis when trying to load GenomicsDB, and the failure looks somewhat similar to the stack we got when we had the UnsatisfiedLinkException (manifests as java.lang.NoClassDefFoundError: Could not initialize class com.intel.genomicsdb.GenomicsDBImporter). Not sure if its the same thing or not; as I said I can run this branch fine. Maybe the dylib works, but not the so ?. There is a seemingly unrelated unit test failure in AlleleFrequencyCalculatorUnitTest, which I'll look at. Also, we probably shouldn't have the getGCPTestInputPath() calls in static initializers in GenomicsDBImportIntegrationTest, since if it throws it can fail badly (I don't think thats affecting travis though).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917:171,load,load,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296190917,1,['load'],['load']
Performance,"Glad to see we're not the only one's noticing this. We have been using 4.0.11.x for quite a while with success. During a recent revalidation effort, we have been working with 4.2.x versions and missing quite a few calls we consider ""truth"", while making these calls with other variant callers. We have since verified v4.1.8.0 is performing acceptably, so our data supports the issue occurring during the 4.1.9.0 update.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473:329,perform,performing,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171550473,1,['perform'],['performing']
Performance,"Good news. User says this is no longer an issue:. `We found the reason why HaplotypeCaller 4.0.0.0 performed worse than 4.beta.2. We are scattering tools using intervals from a custom BED file. Before, each instance of HaplotypeCaller received a BAM file from ApplyBQSR that was produced using a specific interval, but not the interval itself. This worked for 4.beta.2, but was causing poor performance for 4.0.0.0. We now pass both the BAM and the interval to HaplotypeCaller 4.0.0.0 and it performs just as well as 4.beta.2 with the same amount of memory.`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183:99,perform,performed,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4169#issuecomment-359944183,3,['perform'],"['performance', 'performed', 'performs']"
Performance,"Great! And yes, LL will be optimized separately for SNPs and INDELs. How about this for a first workflow to target?. 1) Run ExtractVariantAnnotations on a training set of chromosomes. You can keep training/truth labels as in Best Practices, for now.; 2) Run TrainVariantAnnotationsModel on that. We'll use the truth scores generated here for any sensitivity conversions---i.e., we'll be calibrating scores only to the truth sites that are contained in the training set of chromosomes.; 3) Use the trained model to run a single shard of ScoreVariantAnnotations on a validation set of chromosomes.; 4) Run some variation of the above script on the resulting outputs to determine SNP and INDEL score thresholds for optimizing the corresponding LL scores. We can also add some code to the script to use the truth scores from step 2 to convert these score thresholds into truth-sensitivity thresholds.; 5) Provide these truth-sensitivity thresholds to ScoreVariantAnnotations and use them to hard filter. Evaluate on a test set of chromosomes. If all looks good, we can later move steps 3-4 into the train tool and automate the passing of sensitivities in 5 via outputs in the model directory. This will let us keep the basic interface of ScoreVariantAnnotations the same, but we'll have to add a few basic parameters to TrainVariantAnnotationsModel to control the train/validation split. So I think all this branch is missing is step 5---we'll simply need to add command-line parameters for the SNP/INDEL sensitivity thresholds and then do the hard filtering in the VCF writing method highlighted above. Do you think you can handle implementing that in this branch, and then the rest at the WDL level? I can help with the python script for the LL stuff (or anything else), if needed. Not sure if you got a chance to check out what your collaborators are doing in the methods you're looking to compare against, but it would be good to understand if this basic scheme for train/validation/test splitting can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084:27,optimiz,optimized,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1068345084,4,['optimiz'],"['optimized', 'optimizing']"
Performance,"HI @sooheelee I did try using BWA to index the 2-bit reference, but it doesn't work as well. @lbergelson Do you think the reference loading issue can be fixed soon? like this month?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251:132,load,loading,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251738251,1,['load'],['loading']
Performance,HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:11:57.326 INFO FilterAlignmentArtifacts - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:11:57.326 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 19:11:57.326 INFO FilterAlignmentArtifacts - Inflater: JdkInflater; 19:11:57.326 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 19:11:57.326 INFO FilterAlignmentArtifacts - Requester pays: disabled; 19:11:57.326 WARN FilterAlignmentArtifacts - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:3880,Load,Loading,3880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,HaplotypeCaller does not resume from where it stopped. If you need to perform the same task again restart the whole task using the very same commandline.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900:70,perform,perform,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7454#issuecomment-918232900,2,['perform'],['perform']
Performance,"Hello @bharathramh thank you for your question. We just recently merged the bulk of the code for DRAGEN-GATK (#6634) which means that we are slated to release the DRAGEN-GATK for the next release of GATK. When that happens we will also be releasing an official wdl workflow to use all the new features together. . As for STR variants, one of the code improvements in the new genotyping engine is to better model STR errors by performing a per-sample pre-processing step where a table of empirically generated STR priors is generated by the tools `ComposeSTRTableFile` and `CallibrateDragstrModel`. These work by modifying the priors for genotying to be more in line with the sample PCR/Sequencing errors and can't really be thought of as a feature to ""find STRs"" (except `ComposeSTRTableFile` which is used to find STRs in the reference). These will be featured in the published wdls with appropriate commands. . I would recommend in the future that you direct questions like this one to our forums since this ticket tracker is used to track bugs/ongoing development on the GATK and our Comms Team is better equipped to answer questions there: https://gatk.broadinstitute.org/hc/en-us/community/topics",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266:426,perform,performing,426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6912#issuecomment-716747266,1,['perform'],['performing']
Performance,"Hello @droazen and @nalinigans , we tried out the parameter you suggested and did improve the performance but not as great as compared to writing the data locally and then moving to FSx for luster. Using the parameter, the job completed in ~2 hrs 52 mins, whereas when we run to write on EBS volumes it completes within 1 hr.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049:94,perform,performance,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039658049,1,['perform'],['performance']
Performance,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didnâ€™t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:208,optimiz,optimizations,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,3,['optimiz'],['optimizations']
Performance,"Hello,. I did encounter the same behaviour that I reported one year ago with the official Docker GATK 4.1.0.0 container converted into a singularity image. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.1.0.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. ### Actual behavior; ```; 14:39:21.762 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:39:24.079 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.079 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.1.0.0; 14:39:24.080 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:24.081 INFO DetermineGermlineContigPloidy - Executing as tintest@dahu38 on Linux v4.9.0-8-amd64 amd64; 14:39:24.081 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:39:24.081 INFO DetermineGermlineContigPloidy - Start Date/Time: May 26, 2019 2:39:21 PM UTC; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.082 INFO DetermineGermlineContigPloidy - HTSJDK Version: 2.18.2; 14:39:24.082 INFO DetermineGermlineContigPloidy - Picard Version: 2.18.25; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:387,Load,Loading,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081,1,['Load'],['Loading']
Performance,"Here is a recap of what we discussed today during the CNV meeting:. For the first round of evaluations we decided to run Germline CNV pipeline on TCGA exomes using a range of key hyperparameters (namely psi-t-scale and p-alt) and establish the base level performance metrics using output of GenomeSTRiP on matched WGS samples as ground truth. . @mbabadi could you come up with a good range of hyperparameter values that you think should be cross-validated?. In particular we need to:. - Dockerize tools we will be evaluating against (XHMM, CODEX2, CLAMMS, GenomeSTRiP); - Write a WDL that runs Germline CNV that scatters across range of key hyperparameters and outputs array of VCFs; - Write VCF processors for output of CLAMMS and CODEX2 ; - Write WDLs for running XHMM, CODEX2, CLAMMS and GenomeSTRiP that output VCFs; - Write WDL that takes results of the above and uses @mbabadi 's gCNV evaluation python modules(located here /dsde/working/mehrtash/gCNV_theano_eval) to output performance metrics; - Decide on an automatic evaluation framework. For the next round of evaluations we need to:. - Decide on appropriate metrics for evaluating performance on trios and write scripts that implement them; - Expand the range of hyperparameters in search space (possibly include different bin sizes, GC vs no GC correction, and fragment mid point coverage collection vs largest fragment overlap coverage collection); - Use gnomAD subset of matched WES/WGS pairs for validation",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071:255,perform,performance,255,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-362075071,3,['perform'],['performance']
Performance,"Here is a script I ran to run the import on 3500 unblockedÂ gvcfs. The script imports one chromosomeÂ per workspace.Â  As the chromosomesÂ get larger --more and more memory is needed.Â  chr4 through 22 ran fine. The chr3 (see log below) ends without an error BUT with theÂ callset.json NOT being written out.Â  Â I could split the chr1-3 at the centromere to try it again. Any other suggestions? Would increasing -Xmx150g to 240g help? . For chromosome 1, which is still running, top indicates is using aboutÂ  240g (after importing the 65 batches).; ```; PID USER Â  Â  Â PR Â NI Â  Â VIRT Â  Â RES Â  Â SHR S Â %CPU %MEM Â TIME+ COMMAND; 21698 farrell Â  20 Â  0Â  Â  Â  443.7g 240.3g Â  1416 S Â 86.7 95.5 Â  7398:14 java; ```. ```; #!/bin/bash -l; #$ -l mem_total=251; #$ -P casa; #$ -pe omp 32; #$ -l h_rt=240:00:00; module load miniconda/4.9.2; module load gatk/[4.2.6.1](http://4.2.6.1/); conda activate /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1](http://4.2.6.1/install/gatk-4.2.6.1). CHR=$1; DB=""genomicsDB.rb.chr$CHR""; rm -rf $DB; # mkdir -p $DB; # mkdir tmp; echo ""Processing chr$CHR""; echo ""NSLOTS: $NSLOTS""; # head sample_map.chr$CHR.reblock.list; head sample_map.chr$CHR; wc Â  sample_map.chr$CHR; gatk --java-options ""-Xmx150g -Xms16g"" \; Â  Â  Â  Â GenomicsDBImport \; Â  Â  Â  Â --sample-name-map sample_map.chr$CHR \; Â  Â  Â  Â --genomicsdb-workspace-path $DB \; Â  Â  Â  Â --genomicsdb-shared-posixfs-optimizations True\; Â  Â  Â  Â --tmp-dir tmp \; Â  Â  Â  Â --L chr$CHR\; Â  Â  Â  Â --batch-size 50 \; Â  Â  Â  Â --bypass-feature-reader\; Â  Â  Â  Â --reader-threads 5\; Â  Â  Â  Â --merge-input-intervals \; Â  Â  Â  Â --overwrite-existing-genomicsdb-workspace\; Â  Â  Â  Â --consolidate. ```; End of log on chr3. ```; 07:19:44.855 INFO Â GenomicsDBImport - Done importing batch 38/65; 08:05:11.651 INFO Â GenomicsDBImport - Done importing batch 39/65; 08:49:12.112 INFO Â GenomicsDBImport - Done importing batch 40/65; 09:32:39.526 INFO Â GenomicsDBImport - Done importing batch 41/65; 10:23:36.849 INFO Â GenomicsDBImport - Done importing batch 42/65; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:800,load,load,800,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,2,['load'],['load']
Performance,Here is another one; ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:100); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:539); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:441); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:322); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:215); 	at htsjdk.tribble.readers.TabixReader.readIndex(TabixReader.java:269); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:161); 	at htsjdk.tribble.readers.TabixReader.<init>(TabixReader.java:125); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:84); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getF,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:64,concurren,concurrent,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"Here is the command line. I'm using funcotator_dataSources.v1.2.20180329.tar.gz.; /omics/chatchawit/gatk/gatk Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 22:56:54.836 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:56:55.064 INFO Funcotator - ------------------------------------------------------------; 22:56:55.065 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 22:56:55.065 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:56:55.065 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 22:56:55.066 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 22:56:55.066 INFO Funcotator - Start Date/Time: April 27, 2018 10:56:54 PM ICT; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.067 INFO Funcotator - HTSJDK Version: 2.13.2; 22:56:55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:859,Load,Loading,859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157,1,['Load'],['Loading']
Performance,"Here is the command line. ```; ./gatk Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; Using GATK jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar Funcotator --variant /home/deepak/software_library/gatk-4.1.7.0/SAMPL3_VARIANTFIL.vcf --reference /media/deepak/EXTRA/Genomedir/hg38/hg38.fasta --ref-version hg38 --data-sources-path /media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES --output variants.funcotated.vcf --output-file-format VCF; 16:01:36.165 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/deepak/software_library/gatk-4.1.7.0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 12, 2020 4:01:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:01:36.870 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.7.0; 16:01:36.871 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:01:36.871 INFO Funcotator - Executing as deepak@ngs on Linux v5.3.0-26-generic amd64; 16:01:36.871 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_241-b07; 16:01:36.871 INFO Funcotator - Start Date/Time: 12 May, 2020 4:01:35 PM IST; 16:01:36.871 INFO Funcotator - ------------------------------------------------------------; 16:01:36.871 INFO Funcotator -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:989,Load,Loading,989,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['Load'],['Loading']
Performance,"Here is the equivalent output for the master branch:. ```; 2022-08-16T22:45:53.6066834Z Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.help.DocumentationGenerationIntegrationTest > documentationSmokeTest2 [31mFAILED[39m[0K; 2022-08-16T22:45:53.6067689Z java.lang.AssertionError: Loading source files for package javadoc...[0K; 2022-08-16T22:45:53.6068266Z programName: warning - No source files for package javadoc; 2022-08-16T22:45:53.6068997Z Loading source files for package org.broadinstitute.hellbender.tools.walkers.variantutils...; 2022-08-16T22:45:53.6079174Z Constructing Javadoc information...; 2022-08-16T22:45:53.6079564Z [search path for source files: src/main/java]; 2022-08-16T22:45:53.6082788Z [search path for class files: /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/classes,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/zipfs.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunjce_provider.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/icedtea-sound.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/nashorn.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/java-atk-wrapper.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunec.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/cldrdata.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/jaccess.jar,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/localedata.jar,/gatk/gatk-package-unspecified-SNAPSHOT-local.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSHOT-testDependencies.jar,/jars/gatk-package-4.2.6.1-50-g40182c7-SNAPSH",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370:300,Load,Loading,300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217253370,2,['Load'],['Loading']
Performance,"Here the error log ; Using GATK jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar IndexFeatureFile -I output/called/final/allsites.filtered.vcf; 00:57:08.257 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2023 12:57:08 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:57:08.789 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.789 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.4.1; 00:57:08.789 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:57:08.790 INFO IndexFeatureFile - Executing as [ychrysostomakis@compute-0-3.local](mailto:ychrysostomakis@compute-0-3.local) on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 00:57:08.790 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_231-b11; 00:57:08.790 INFO IndexFeatureFile - Start Date/Time: 11. September 2023 00:57:07 MESZ; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Version: 2.21.0; 00:57:08.790 INFO IndexFeatureFile - Picard Version: 2.21.2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:447,Load,Loading,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['Load'],['Loading']
Performance,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:503,perform,performance,503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['perform'],['performance']
Performance,Here's the performance benchmarking; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2000000 0.4121864 0.9387755 0.8521127 345 279 98 142; 2 0.4598540 0.6086957 0.9647059 0.8602941 137 184 85 136; 3 0.6352941 0.7181208 0.9565217 0.8650794 85 149 69 126; 4 0.7796610 0.7744361 0.9636364 0.8606557 59 133 55 122; 5 0.8913043 0.8715596 0.9583333 0.8495575 46 109 48 113; 6 0.9024390 0.9019608 0.9750000 0.8545455 41 102 40 110; 7 0.9428571 0.9263158 0.9722222 0.8666667 35 95 36 105; 8 0.9310345 0.9534884 0.9666667 0.8736842 29 86 30 95; 9 0.9285714 0.9518072 1.0000000 0.9213483 28 83 26 89; 10 0.9565217 0.9487179 1.0000000 0.9259259 23 78 23 81. ### NEW gCNV docker; sen_del sen_dup ppv_del ppv_dup sen_del_hist sen_dup_hist ppv_del_hist ppv_dup_hist; 1 0.2034884 0.4107143 0.9387755 0.8581560 344 280 98 141; 2 0.4705882 0.6086957 0.9647059 0.8676471 136 184 85 136; 3 0.6547619 0.7248322 0.9565217 0.8730159 84 149 69 126; 4 0.7931034 0.7819549 0.9636364 0.8677686 58 133 55 121; 5 0.9111111 0.8807339 0.9583333 0.8558559 45 109 48 111; 6 0.9250000 0.9019608 0.9750000 0.8584906 40 102 40 106; 7 0.9705882 0.9263158 0.9722222 0.8712871 34 95 36 101; 8 0.9642857 0.9534884 0.9655172 0.8791209 28 86 29 91; 9 0.9629630 0.9518072 1.0000000 0.9186047 27 83 25 86; 10 1.0000000 0.9487179 1.0000000 0.9113924 22 78 22 79,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329:11,perform,performance,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2187034329,1,['perform'],['performance']
Performance,"Hey @mwalker174 ,. Do you have any suggestions about how to perform step 1? I naively tried to use picard's `MergeBamAlignment` using the PathSeq output BAM as the aligned bam and the PathSeq input BAM as the unmapped BAM but I get the following error message. `IllegalArgumentException: Do not use this function to merge dictionaries with different sequences in them. Sequences must be in the same order as well. Found [NZ_DS990135.1, NZ_AJSY01000035.1, ... `. I tried sorting both BAM files by queryname and removing the alignment for the input BAM using `RevertSam` but neither of these worked. I suspect that it's because of the PathSeq output BAM given the references to the microbial sequences. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370:60,perform,perform,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6655#issuecomment-644426370,1,['perform'],['perform']
Performance,"Hey @mwalker174,. Setting `--host-min-identity 20` lowered the `READS_AFTER_PREALIGNED_HOST_FILTER` to 131467, which is closer to what I expected (albeit still 12% of the reads) so thanks for the suggestion. I'm still unsure why the preferred approach is to use this fairly arbitrary metric that doesn't incorporate both mates instead of leveraging the known alignments from STAR, which does. Given what I've learned today, I think the better approach (for my use case) would be to filter the unique mappers, the multi-mappers and the chimeric reads (which in my data set represent 97.5% of the reads) and then apply the QUALITY_AND_COMPLEXITY_FILTER and the DUPLICATE_READS_FILTER. Would you agree?. To put myself in your shoes, I would guess that PathSeq is designed for general purpose use cases (which is probably `--is-host-aligned false`) and that performing such a filtering would require specific handling for each supported aligner, which would be a lot of work. Moreover, my use case with short paired-end reads is also probably not common. So I understand why you use the existing approach but are there any reasons that I'm missing as to why you'd suggest to stick with the basic approach for my use case instead of the one that I proposed above?. Best, Welles",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772:854,perform,performing,854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6687#issuecomment-652524772,2,['perform'],['performing']
Performance,"Hi @Hemantcnaik ,. This is a much better question for the GATK forum. We try to limit github issues to bugs and feature requests, while questions about tool usage should go to the forum where they're triaged by another team. We haven't done much RNA development in a long time, though hopefully that's about to change. In the meantime, a lot of other places in the GATK code we use 20 as a minimum base quality. If you're really concerned about optimizing this value, you could probably come up with an in silico experiment similar to what we do for BQSR. If you assume that every site in your sample that's in dbSNP is a real variant and everything that's not is a false positive (not true, but to a good approximation), then you can look at the base quality distributions for true positives and false positives and try to decide what's a good tradeoff between sensitivity and precision.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871:445,optimiz,optimizing,445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6753#issuecomment-678345871,1,['optimiz'],['optimizing']
Performance,"Hi @cccnrc, glad you found this discussion interesting and apologies for the very late reply. Unfortunately, there hasn't been much movement on this front, as I think we decided that the current model sufficed. I think we are moving in a direction so that we can obviate the need for a separate step to fit global (e.g., depth) and per-contig (e.g., contig ploidy) parameters for each sample. Recall that this is only necessary because each gCNV genomic shard needs these quantities to perform inference, but cannot infer them from the data they are responsible for fitting (which typically covers less than a contig). We are looking to reimplement gCNV in a more modern inference framework that could allow us to do away with such sharding entirely. We could thus fit global/per-contig quantities of interest jointly with the rest of the gCNV model. The timeframe for this work may be relatively long (~year), but I think it'll be worth it. That said, I think a key takeaway from this work is that genotype priors can be more powerful for breaking degeneracies than contig-ploidy priors, so we will probably try to incorporate that insight in future work. To answer your questions:; 1) There are no additional results much further beyond what is shown above.; 2) You can see snippets/comparisons of the genotype and contig-ploidy prior file above. If you're just looking for information about the contig-ploidy prior table used in the current pipeline, see an example at https://gatk.broadinstitute.org/hc/en-us/articles/360051304711-DetermineGermlineContigPloidy and feel free to modify it to be more/less strict as desired.; 3) Unfortunately I believe the samples discussed above are not publicly available. If you are having difficulties with the current model, it would probably be useful to hear about them!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129:486,perform,perform,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-894432129,1,['perform'],['perform']
Performance,"Hi @cmnbroad, we can do better than crash - you'll notice that in my pull request, I've added a check to `CNNScoreVariants.java` to test whether AVX is present (the method was checking if command-line arguments are valid, so I stretched a point and used it to test the environment). . ~. You could definitely offer your own build of TensorFlow, but do you _want_ to? Apart from anything else, it'll be 10X slower than the default. My understanding is that inference using CNNScoreVariants was taking ~ 50 hours on a typical real-world file before optimization. Wouldn't it be more cost-effective just to use instances that at least have AVX?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785:547,optimiz,optimization,547,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429323785,1,['optimiz'],['optimization']
Performance,"Hi @davidbenjamin ,. this ticket is already open for a while, but I haven't found a sensible multi-sample wdl yet, so I've spent some time putting a multi-sample calling workflow together and tested it extensively: https://github.com/phylyc/gatk4-somatic-snvs-indels. It's somewhat optimized for resource needs, at least much more so than the published gatk mutect2 wdl.; One option that I added is to run the realignment filter only on a subset of called variants, which is especially helpful for tumor-only calling to reduce costs if we are hard filtering variants afterwards based on some thresholds anyways (who doesn't?). One todo is still to choose the best normal sample for the contamination model based on which has the highest sequencing depth, as you had mentioned in some old gatk forum post. Happy to have some contributions there :); I also brushed up the PoN wdl, which is also tested. How important is cram input support? I took that part out, but it's easy to plug it back in, I suppose.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113:282,optimiz,optimized,282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6022#issuecomment-1113847113,1,['optimiz'],['optimized']
Performance,"Hi @davidbenjamin and @ldgauthier,. I'm running into this same problem with version 4.4.0.0 in tumor only mode (actually creating a panel of normal). As far as I understand ""--emit-ref-confidence GVCF"" is required for compatibility with GenomicsDBImport? Is there some workaround?. > 1 Using GATK jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar; > 2 Running:; > 3 java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar Mutect2 -R /ref_nobackup/Homo_sapiens_assembly38.fasta -I /BQSR/BSSE_QGF_229563_bqsr.bam --emit-ref-confidence GVCF --germline-resource /ref_nobackup/af-only-gnomad.hg38.vcf.gz -max-mnp-distance 0 -O /output/BSSE_QGF_229563_pon.g.vcf.gz --tmp-dir /scratch; > 4 15:07:52.399 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 5 15:07:52.435 INFO Mutect2 - ------------------------------------------------------------; > 6 15:07:52.437 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.4.0.0; > 7 15:07:52.438 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; > 8 15:07:52.438 INFO Mutect2 - Executing as X on Linux v3.10.0-1160.66.1.el7.x86_64 amd64; > 9 15:07:52.438 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10; > 10 15:07:52.438 INFO Mutect2 - Start Date/Time: June 19, 2023 at 3:07:52 PM CEST; > 11 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 12 15:07:52.438 INFO Mutect2 - ------------------------------------------------------------; > 13 15:07:52.439 INFO Mutect2 - HTSJDK Version: 3.0.5; > 14 15:07:52.440 INFO Mutect2 - Picard Version: 3.0.0; > 15 15:07:52.440 INFO Mu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:958,Load,Loading,958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"Hi @droazen ,; I am sorry for the late response. I used the docker container from dockerhub broadinstitute/gatk:4.4.0.0 .; The which python command produces the correct path. I re-loaded the container and tried it again without modification. Again the pipeline crashed with BaseRecalibrator and the above mentioned error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235:180,load,loaded,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8402#issuecomment-1691458235,1,['load'],['loaded']
Performance,"Hi @droazen! Again, I'm skeptical that it would be a serialization change as we have only made one change to serializer registration for htsjdk classes since 0.20.0:. ```; $ git diff adam-parent_2.10-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala | grep htsjdk; kryo.register(classOf[scala.Array[htsjdk.variant.vcf.VCFHeader]]); ```. Additionally, the [Kryo version used in ADAM has been unchanged since 0.20.0](https://github.com/bigdatagenomics/adam/commit/295fb5cf7bf1f089e82a1624e2056053e817b5f3). Since we do not shuffle `SAMRecord`s, we do not register a serializer for `SAMRecord`, which would presumably be the main record that HaplotypeCaller is serializing. The GATK's implementation of BQSR doesn't perform any shuffles other than when reducing the recalibration tables, correct? If so, any performance regression in GATK's BQSR would be exceedingly unlikely to be caused by serializer registrations in ADAM.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827:755,perform,perform,755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367112827,2,['perform'],"['perform', 'performance']"
Performance,"Hi @fpbarthel,. You have a few options, but I think they would all require a bit of manual processing on your part. For example, you could use CollectReadCounts with your initial list of bins, but then you'd have to do the averaging step manually. Hopefully, it should not be too trouble to put together your own script to do this; however, since it is somewhat of a custom operation, it's unlikely we'll support it as a feature. In any case, note that the rest of the downstream tools in both our germline and somatic pipelines are only set up to work with integer counts. I don't know your reasons for wanting to average counts over multiple bins, but since 1) averages contain less information than the full resolution bins, 2) the BAM I/O to perform count collection is expensive (so we don't want to discard information during the CollectReadCounts step that we might need later), and 3) we want to build generative models of the counts, our philosophy is to always retain the integer counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644:746,perform,perform,746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5432#issuecomment-439892644,1,['perform'],['perform']
Performance,"Hi @jamesemery Thanks for your email.; Actually, I was successful with calculation of ""DepthOfCoverage"" with following procedure:; 1. I found the exact reference fasta file which is used for mapping procedures in my pipeline + .dict and .fa.fai; 2. I made a bed file out of the reference fasta file, using the following command:; faidx -i bed genome.fa > out.bed; 3. Finally I did this for performing the ""DepthOfCoverage"" tool:; gatk DepthOfCoverage -R reference.fa -O result -I s269825.haplotagged.bam -L out.bed. So everything works fine but there is another problem: the size of output file is very huge (GBs)! Should it be like that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891:390,perform,performing,390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7453#issuecomment-971402891,1,['perform'],['performing']
Performance,"Hi @lbergelson ; Thanks for the response!; I tried building the current gatk from the master branch, and now the error message says something a little different. java.lang.ClassCastException: class htsjdk.samtools.BAMRecord cannot be cast to class java.lang.Comparable (htsjdk.samtools.BAMRecord is in unnamed module of loader 'app'; java.lang.Comparable is in module java.base of loader 'bootstrap'); 	at java.base/java.util.Arrays$NaturalOrder.compare(Arrays.java:105); 	at java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355); 	at java.base/java.util.TimSort.sort(TimSort.java:234); 	at java.base/java.util.ArraysParallelSortHelpers$FJObject$Sorter.compute(ArraysParallelSortHelpers.java:145); 	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746); 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); 	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408); 	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736); 	at java.base/java.util.Arrays.parallelSort(Arrays.java:1183); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:247); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:182); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:202); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); 	at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); 	at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:123); 	at java.base/java.lang.Thread.run(Thread.java:829); 	Suppressed: htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; 		at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); 		at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:58); 		at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SA",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087:320,load,loader,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452463087,6,"['concurren', 'load']","['concurrent', 'loader']"
Performance,"Hi @lbergelson I added **--disableAllReadFilters**, the log still says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableAllReadFilters --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 12:08:44.856 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 12:08:44.905 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 17, 2016 12:08:44 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --disableAllReadFilters true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [September 17, 2016 12:08:44 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 12:08:44.930 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_INDEX : false; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_MD5 : false; 12:08:44.930 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 12:08:44.930 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MAS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408:525,load,load,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson Thanks for the quick response! I just used the fasta as reference, and it still doesn't work. The log only says ""null"". Using GATK wrapper script /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk; Running:; /home/kh3/Softwares/gatk/build/install/gatk/bin/gatk BwaSpark -I /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam -R /home/kh3/Resources/genome_b37/genome.fa --disableSequenceDictionaryValidation true -t 16 -O /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam; 16:55:32.261 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/home/kh3/Softwares/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.so; 16:55:32.310 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [September 16, 2016 4:55:32 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark --output /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.aligned.bam --threads 16 --reference /home/kh3/Resources/genome_b37/genome.fa --input /home/kh3/data/Illumina/GATK4/Platinum/TEST/test.spark.bam --disableSequenceDictionaryValidation true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 4:55:32 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 16:55:32.335 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_INDEX : false; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_MD5 : false; 16:55:32.335 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 16:55:32.335 INFO BwaSpark - Default",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200:557,load,load,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200,2,['load'],"['load', 'loaded']"
Performance,"Hi @lbergelson and @AJDCiarla ,. Thank you for your working!. I am the one who reported this bug and I had given it a try as @lbergelson suggested. I performed tests in two different scenarios:. 1. Using full path without any non-ascii characters as tmp path and it succeeded:; ```; /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest"" BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:35:32.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:32.890 INFO BaseRecalibrator - ------------------------------------------------------------; 13",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:150,perform,performed,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['perform'],['performed']
Performance,"Hi @lbergelson,. We experienced the related issue in GATK 4.1.8 (it persisted since 4.1.5 or early version as far as we know) when running `FilterAlignmentArtifacts` in one of our cluster but not the other. We narrowed down the issue, using the CPU differences (the working one does not support AVX2), to `libgkl_smithwaterman.so`. Paths are shortened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO Fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:933,Load,Loading,933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"Hi @ruslan-abasov,. I believe your GermlineCNVCaller results should have inherited the correct dictionary from the count files. The issue is you created some GermlineCNVCaller shards (e.g., shard 4) with inappropriately ordered intervals (since these were instead ordered w.r.t. to the idiosyncratic dictionary you attached). However, I think if you just reshard and rerun GermlineCNVCaller for any such shards, you may be able to reuse most of your results. For example, you could take your shard 4 interval list, which contains intervals from chr18, chr19, and chr1, and reshard these intervals into two shards: 4a containing chr18-19 intervals, and 4b containing chr1 intervals. After rerunning 4a and 4b through GermlineCNVCaller, you should be able to use PostprocessGermlineCNVCalls to stitch together shards 4b, 1, 2, 3, and 4a, since these will be ordered w.r.t. the correct dictionary from the count files (i.e, they will contain intervals in the order chr1, chr10-19). Of course, you will want not want to perform this exact procedure; you'll want to generalize it to whatever will yield the correct order for all 10 of your shards across all contigs. Again, this may be error prone and I can't guarantee that it will be successful, since I haven't tried it myself. I would personally just rerun the pipeline. You might be able to cut down on runtime by using smaller shards (I believe we typically shard the entire genome into far more than 10 shards, which we usually run in parallel) and making sure you set parameters appropriately for WGS. @mwalker174 has the most experience running on WGS and should be able to provide you the latest recommendations, or you might be able to find them by searching GitHub or the GATK Forums. Your point is well taken about failing earlier, and I think I've outlined the best strategy above. It is impossible to catch all possible errors early, but for some we can certainly fail before the GermlineCNVCaller step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802:1016,perform,perform,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720091802,1,['perform'],['perform']
Performance,"Hi @shengqh, tools such as you describe are still under development. We are also still working to tune hyperparameters in the main gCNV pipeline, so thanks for trying it out! Please let us know if you encounter any issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953:98,tune,tune,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5373#issuecomment-434275953,1,['tune'],['tune']
Performance,"Hi @tedsharpe, . A quick follow-up question on this â€“ how does the function currently handle the Q-scores from overlapping portions of paired-end reads? @BenjaminWehnert1008 noticed that read pre-merging and dual Q-score integration can help improve our performance for 1nt variants. Best wishes,; Max",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456:254,perform,performance,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8995#issuecomment-2416095456,1,['perform'],['performance']
Performance,"Hi @tomwhite! The serializer registrations in ADAM shouldn't effect any GATK serialization, unless you're serializing classes from ADAM (such as `org.bdgenomics.format.avro.AlignmentRecord`); additionally, we haven't seen any performance issues with serialization in ADAM 0.23.0 outside of the GATK. We actually made a number of patches to eliminate logging in 0.23.0 (relative to 0.22.0), so I'd doubt that is the culprit. The one exception to this is logging when writing Parquet out to disk, which greatly increased sometime between ADAM 0.21.0 and 0.23.0, due to a change in Parquet versions upstream in Spark. However, this would only impact you if you were writing Parquet, and additionally this issue was resolved with the release of Spark 2.2.0, so you should see the logging go away with #4314. If I had to hypothesize anything, I'd suggest that the 2bit file change is the one thing that could be biting you. That said, we've been running with this 2bit file code quite frequently on AWS and Azure for at least the last 6 months using GATK HC on WES and WGS data and haven't seen any performance issues. I show that the changes in the 2bit file code between ADAM 0.20.0 and 0.23.0 are API compatible, so if you check out ADAM 0.23.0 and then `git checkout adam-parent-spark2_2.11-0.20.0 adam-core/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala` and build the ADAM JAR (and then package that jar into GATK), you should be able to test that hypothesis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897:226,perform,performance,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366308897,2,['perform'],['performance']
Performance,"Hi @zaneChou1,. Thanks for filing the issue---I'm actually working on this at the moment. The gCNV python code requires some updating, since the APIs for inference changed after the pymc3 version we use (which is the primary reason we stuck with it). Because the changes required go beyond those in the PR you opened, I'll go ahead and close it, but thanks for making the effort to help us keep things updated! We certainly appreciate it. I would be surprised if updating numpy led to drastic performance improvements (1.17.5 was only released in 1/2020), but it's possible there have been improvements in the pymc3 python code. However, we are planning to move much of the python inference code over to pyro, for which the numpy upgrade is also necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543:493,perform,performance,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978#issuecomment-733001543,1,['perform'],['performance']
Performance,"Hi Adam,. Maybe I'm thinking naively here - and I don't have access to a complete and proper Spark cluster for rigorous testing - but just as a simple test of loading a VCF via Spark, I took [PrintReadsSpark.java](https://github.com/broadinstitute/gatk/blob/030858bc08328200b9df287db2571b907189ec66/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/PrintReadsSpark.java) and performed following updates:; - Copied `./src/test/resources/org/broadinstitute/hellbender/utils/SequenceDictionaryUtils/test.vcf` into the local directory.; - Renamed the copy of `PrintReadsSpark.java` as `PrintVCFSpark.java`; - Added `import org.broadinstitute.hellbender.utils.variant.Variant;`; - Added `import org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSource;`; - As a test, I changed to the `runTool` method with the following to print the information in the first element in the RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:159,load,loading,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Hi Adam,. That is the verbosity level that is performed by `bwa`. So since [BWASpark](https://github.com/broadinstitute/gatk/blob/c9807a1a94a60b69e8184c1fcf3083516bb0a78b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark.java#L47) launches `BwaSparkEngine` as follows:. ```; final BwaSparkEngine engine = new BwaSparkEngine(bwaArgs.numThreads, bwaArgs.fixedChunkSize, referenceFileName);; ```. and launches the alignment as follows:. ```; final JavaRDD<GATKRead> reads = engine.alignWithBWA(ctx, unalignedReads, readsHeader);; ```. That in turn calls the `align()` method within [BwaSparkEngine.java](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L72):. ```; final JavaRDD<String> samLines = align(shortReadPairs);; ```. which then instantiates a new [BwaMem](https://github.com/broadinstitute/gatk/blob/9fb4d756afbbad58a7e709e2e9fd308983ad255b/src/main/java/org/broadinstitute/hellbender/tools/spark/bwa/BwaSparkEngine.java#L101) object:. ```; final BwaMem mem = new BwaMem(index);; ```. Since the BWA implementation at [lindenb/jbwa](https://github.com/lindenb/jbwa) is basically a direct call to Heng's BWA as a library, the BWA option for verbosity is set by the `-v` argument as noted [here](http://bio-bwa.sourceforge.net/bwa.shtml#3):. ```; -v INT Control the verbose level of the output. This option has not been fully supported ; throughout BWA. Ideally, a value 0 for disabling all the output to stderr; 1 for ; outputting errors only; 2 for warnings and errors; 3 for all normal messages; 4 or ; higher for debugging. When this option takes value 4, the output is not SAM. [3] ; ```. This is used in Heng's [bwamem.c](https://github.com/lh3/bwa/blob/5961611c358e480110793bbf241523a3cfac049b/bwamem.c#L1224-L1226) file - and several other places - which generates the printout you see, as follows:. ```; if (bwa_verbose >= 3); fprintf(stderr, ""[M::%s] P",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398:46,perform,performed,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2054#issuecomment-235675398,1,['perform'],['performed']
Performance,"Hi David,. 1. Here, we must distinguish between the WES minimal example (default parameters, command lines in my original bug report) and our ""production pipeline"". In the minimal example, we do not use PON. In the production pipeline, we use the exome PON file from: gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz as recommended here: https://gatk.broadinstitute.org/hc/en-us/articles/360035890631-Panel-of-Normals-PON-; In the production pipeline, we also use gnomAD 3.1.2, filtered for AF>0 and FILTER=PASS as ""germline resource"". Some more data from the production pipeline, just in case it could help. ; Since the PON is exome-only, we were not sure whether including the PON would improve the performance of our WGS workflow in the ""production pipeline"" with gatk version 4.1.7.0, so we ran experiments:; - calling without germline resource and without PON; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v04_gnomAD_only_WGS_FD_tumor-normal_reference_workflow_v04_no_gnomAD_no_PON](https://user-images.githubusercontent.com/15612230/182359001-a173e711-748b-49ec-b03f-71e5a8293c51.png). We also conducted experiments with a subset of the WES FD sample (FD_1, 1/3 of the full ~100x FD dataset):. - calling without germline resource and without PON; - calling with PON only, without germline resource; - calling with germline resource only, without PON; - calling with both germline resource and PON. The results (please note that the labeling conventions are now different compared to WGS experiments, apologies for the inconvenience):. ![FD_1_T_tumor-normal_WES_muTect2_FD_1_tumor-normal_muTect2_PON_FD_1_tumor-normal_muTect2_gnomAD_FD_1_tumor-normal_muTect2_PON_gnomAD](https://user-images.githubusercontent.com/15612230/182358963-97f04d12-94c6-4f77-acef-c3ebcd78a98f.png). 2. The reference is GRCh38.primary_assembly.genome.fa; The",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705:712,perform,performance,712,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1202344705,1,['perform'],['performance']
Performance,"Hi again,; I tried installing java8 and switching to this version prior to running gatk. It runs and looks to be running the right Java, but spits out roughly the same error:. Thoughts?. /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; --tmp-dir /thing -O thing.bam; Using GATK jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar SplitNCigarReads -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:35:00.220 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.3.0.0-44-g227bbca-SNAPSHOT; 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:35:00.226 INFO SplitNCigarReads - Executing as drichard@illuvatar on Linux v5.19.0-32-generic amd64; 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 3:34:59 PM EST; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.226 INFO SplitNCigarReads - ------------------------------------------------------------; 15:35:00.227 INFO SplitNCigarReads - HTSJDK Version: 3.0.1; 15:35:00.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485:940,Load,Loading,940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452525485,1,['Load'],['Loading']
Performance,"Hi everyone. I try to run gatk 4.2.5.0 VariantAnnotator using gnomAD data. However I get this error message java.lang.IllegalStateException: Allele in genotype C not in the variant context [C*, CT] can you maybe advise whats going on? . java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx30G -jar /run/media/riadh/One Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar VariantAnnotator -V PE69_chr3.vcf -R /run/media/riadh/One Touch/Reference_data_b38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta --resource:gnomad /run/media/riadh/One Touch/Reference_data_b38/gnomad.genomes.v3.1.2.sites.chr3.vcf.bgz -E gnomad.nhomalt -E gnomad.ALT -E gnomad.AF -O PE69_ch3_vep_cadd_gnomad.vcf --resource-allele-concordance; 10:58:19.715 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/run/media/riadh/One%20Touch1/Analysis/gatk-4.2.4.1/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 17, 2022 10:58:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:58:19.796 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.796 INFO VariantAnnotator - The Genome Analysis Toolkit (GATK) v4.2.5.0; 10:58:19.796 INFO VariantAnnotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:58:19.797 INFO VariantAnnotator - Executing as riadh@ikm-unix-1012.uio.no on Linux v5.16.12-200.fc35.x86_64 amd64; 10:58:19.797 INFO VariantAnnotator - Java runtime: OpenJDK 64-Bit Server VM v11.0.14.1+1; 10:58:19.797 INFO VariantAnnotator - Start Date/Time: March 17, 2022 at 10:58:19 AM CET; 10:58:19.797 INFO VariantAnnotator - ------------------------------------------------------------; 10:58:19.797 INFO VariantAnnotator - -------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053:881,Load,Loading,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6689#issuecomment-1070784053,1,['Load'],['Loading']
Performance,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:610,Load,Loading,610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance,"Hi, . I am experiencing exactly the same issue. I also run gatk on a cluster using singularity. When using --include-non-variant-sites the same error message is reported for all but the first chromosome. When exluding non variant sites it works fine. . ```; 05:04:16.405 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 05:04:16.556 INFO GenotypeGVCFs - ------------------------------------------------------------; 05:04:16.559 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.5.0.0; 05:04:16.559 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 05:04:16.563 INFO GenotypeGVCFs - Initializing engine; 05:04:16.929 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.5.1-84e800e; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 16:04:16.979 INFO NativeGenomicsDB - pid=680685 tid=680686 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 05:04:17.059 INFO GenotypeGVCFs - Done initializing engine; 05:04:17.104 INFO ProgressMeter - Starting traversal; 05:04:17.105 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 05:04:17.124 INFO GenotypeGVCFs - Shutting down engine; [June 26, 2024 at 5:04:17 AM GMT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1126170624; java.lang.IllegalStateException: There are no sources based on those query parameters; at org.genomicsdb.reader.GenomicsDBFeature",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079:298,Load,Loading,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2191214079,1,['Load'],['Loading']
Performance,"Hi, @jamesemery .I downloaded the `GTF` file for `basic gene annotation` in the CHR regions from gencode. The version I obtained was `Release 43 (GRCh38.p13)`. After making some modifications as follow, I managed to run SVAnnotate without encountering any errors. ; ```; sed 's/; tag ""Ensembl_canonical""//g' gencode.v43.basic.annotation.gtf|sed 's/; tag ""overlaps_pseudogene""//g'|sed 's/; tag ""readthrough_gene""//g'|sed 's/; tag ""artifactual_duplication""//g' > gencode.v43.basic.modified_annotation.gtf; ```; However, the tool still fails to provide meaningful annotation information. The output file remains unchanged compared to the original file. Based on the SVAnnotate output as follow, I suspect that the issue might be caused by 'Current Locus unmapped.' ; ```; 16:11:10.044 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Division/1user/2_Exome/Tools/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:11:10.072 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.074 INFO SVAnnotate - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:11:10.074 INFO SVAnnotate - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:11:10.074 INFO SVAnnotate - Executing as user@localhost.localdomain on Linux v3.10.0-1160.90.1.el7.x86_64 amd64; 16:11:10.074 INFO SVAnnotate - Java runtime: Java HotSpot(TM) 64-Bit Server VM v17.0.7+8-LTS-224; 16:11:10.075 INFO SVAnnotate - Start Date/Time: 2023å¹´7æœˆ5æ—¥ CST ä¸‹åˆ4:11:10; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - ------------------------------------------------------------; 16:11:10.075 INFO SVAnnotate - HTSJDK Version: 3.0.5; 16:11:10.075 INFO SVAnnotate - Picard Version: 3.0.0; 16:11:10.076 INFO SVAnnotate - Built for Spark Version: 3.3.1; 16:11:10.076 INFO SVAnnotate - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:11:10.076 INFO SVAnnotate - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138:809,Load,Loading,809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8394#issuecomment-1621377138,1,['Load'],['Loading']
Performance,"Hi, I am encountering a similar error attempting to run `GenotypeGVCFs` in `gatk v4.1.2.0`. It runs very briefly and writes a handful of variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:331,perform,performs,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,2,['perform'],['performs']
Performance,"Hi, I encountered a similar error attempting to run GenotypeGVCFs in gatk v4.1.4.1. ; It ran very briefly and writed few variants to the output file but then exited with java.lang.ArrayIndexOutOfBoundsException. . I solved the problem (I created a merged vcf) by performing bgzip (and tabix) of each gvcf file before running GenomicsDBImport and GenotypeGVCFs.; I don't know if it could be a solution also for your issues. command:; for P in my_gvcf.list; do; bgzip -c $P.g.vcf > $P\.g.vcf.gz. tabix -p vcf $P.g.vcf.gz; done. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path my_database --sample-name-map sample_map --batch-size 50 --consolidate true --tmp-dir=tmp/ --reader-threads 5 -L file.bed. gatk --java-options ""-Xmx12g -Xms12g"" GenotypeGVCFs -R hg19.fa -V gendb://my_database -O global.vcf --new-qual --tmp-dir=tmp/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728:263,perform,performing,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-583260728,1,['perform'],['performing']
Performance,"Hi, you have probably deleted out the previous comment with the `No space left on device` error, but was wondering if you could check if you have enough space, import to a new workspace and turn on `--genomicsdb-shared-posixfs-optimizations` with GenomicsDBImport?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424:227,optimiz,optimizations,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-759831424,1,['optimiz'],['optimizations']
Performance,"Hi,; I am trying to generate vcf using GATK pipeline from bam file, but everytime, I am getting the following exception:; 01:13:15.801 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/ngs/programs/gatk-4.0.0.0/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 01:13:16.075 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.075 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.0.0.0; 01:13:16.075 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:13:16.076 INFO HaplotypeCaller - Executing as shashank@grande on Linux v3.13.0-79-generic amd64; 01:13:16.076 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_72-internal-b15; 01:13:16.076 INFO HaplotypeCaller - Start Date/Time: January 18, 2020 1:13:15 AM IST; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.076 INFO HaplotypeCaller - ------------------------------------------------------------; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Version: 2.13.2; 01:13:16.077 INFO HaplotypeCaller - Picard Version: 2.17.2; 01:13:16.077 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:13:16.078 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:13:16.078 INFO HaplotypeCaller - Deflater: IntelDeflater; 01:13:16.078 INFO HaplotypeCaller - Inflater: IntelInflater; 01:13:16.078 INFO HaplotypeCaller - GCS max retries/reopens: 20; 01:13:16.078 INFO HaplotypeCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 01:13:16.078 INFO HaplotypeCaller - Initializing engine; 01:13:17.087 INF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947#issuecomment-575601220,1,['Load'],['Loading']
Performance,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:963,load,loadNextAssemblyRegion,963,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140,1,['load'],['loadNextAssemblyRegion']
Performance,"Hm. Even though I totally expected to be able to reproduce this, I can't. I tried quite a few different combinations. I think this one captures the steps that were reported, but it works:. ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; Using GATK jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; 16:00:13.443 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Mon Jun 22 16:00:13 EDT 2020] MergeVcfs --INPUT data/calling/my.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon Jun 22 16:00:13 EDT 2020] Executing as cnorman@WMCEA-78B on Mac OS X 10.13.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:00:13 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=373293056; Tool returned:; 0; ```; The only way I can reproduce it is to delete one of the files so it *really* doesn't exist at the specified location:; ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk Merge",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321:814,Load,Loading,814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321,1,['Load'],['Loading']
Performance,"Hm. Just yesterday we updated from TF 1.4 to 1.9. Although this makes it more compelling to switch the default to Intel-optimized, we may still have an issue for the reasons outlined in the previous PR (academic users, not all GCS zones guaranty AVX hardware, and its still unclear to me if Travis, which uses both GCS and EC2, makes such a guaranty). It [sounds like](https://github.com/tensorflow/tensorflow/issues/18689) the failure mode is to crash. For running inference at least (training may be a different story), we may need something better. Another option is that it sounds like its possible to build our own distribution without AVX dependencies to use as a fallback.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465:120,optimiz,optimized,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429316465,1,['optimiz'],['optimized']
Performance,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:79,load,loaded,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205,5,"['concurren', 'load']","['concurrency', 'load', 'loaded', 'loading']"
Performance,"Hmm, guess I didnâ€™t realize how important it was to subset to high-confidence regions. Most of the false positives above are coming from the low-confidence regions, so lower precision may actually indicate higher sensitivity to any real variants that may be there and missing in the truth set. When optimizing parameters in the past, have we always just focused on the high confidence regions and called it a day?. Also, do we typically run HC/M2 identically in low/high confidence regions? Iâ€™m noticing that itâ€™s taking much longer to get through the low confidence regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743:299,optimiz,optimizing,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-713168743,1,['optimiz'],['optimizing']
Performance,"Hmm, it looks like we could perform more checks upstream in GermlineCNVCaller as well. I think it might be also possible to introduce inconsistencies there by using read counts and sharded interval lists with different dictionaries or contig orders (which might have happened here). The canonical dictionary is taken from the first read count file, but the intervals pass through the `-L` machinery and I think any dictionary information is not checked (probably because it will not be present if Picard interval lists were not used as input); intervals are also just (re)sorted by the read-count dictionary to use in subsetting counts. EDIT: Seems like checking intervals for the dictionary may be something we should properly add at the engine level. See e.g. the following `TODO`:. ````; /**; * Returns the ""best available"" sequence dictionary or {@code null} if there is no single best dictionary.; *; * The algorithm for selecting the best dictionary is as follows:; * 1) If a master sequence dictionary was specified, use that dictionary; * 2) if there is a reference, then the best dictionary is the reference sequence dictionary; * 3) Otherwise, if there are reads, then the best dictionary is the sequence dictionary constructed from the reads.; * 4) Otherwise, if there are features and the feature data source has only one dictionary, then that one is the best dictionary.; * 5) Otherwise, the result is {@code null}.; *; * TODO: check interval file(s) as well for a sequence dictionary; * ...; * @return best available sequence dictionary given our inputs or {@code null} if no one dictionary is the best one.; */; public SAMSequenceDictionary getBestAvailableSequenceDictionary() {; ````. We could also just require a sequence dictionary for the relevant tools and/or somehow try to reconcile with dictionaries from other inputs, but I think it's better to check and fail.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218:28,perform,perform,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718792218,1,['perform'],['perform']
Performance,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:338,tune,tune,338,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,2,"['optimiz', 'tune']","['optimized', 'tune']"
Performance,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:772,load,load,772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['load'],['load']
Performance,"Hmm, thanks for suggesting the addition of a regression test @fleharty. This caused me to realize that I actually missed another gap in the previous filtering logic that might have yielded NaNs (resulting from division by zero interval medians) in this particular edge case, which actually takes effect before the rounding error I originally fixed. However, because of how HDF5 writes NaN values as 0, this apparently doesn't lead to any catastrophic failures. We should definitely check that behavior is reasonable in this case (i.e., when interval medians are zero); I've filed #6878. In the end, I added a regression test that only passes with the changes to address the rounding error. This was a bit of a pain because we use simulated data in the tests that cover this code, and the filters are applied in sequential order only on those elements that passed the previous filter. Note that there are many other possible filtering combinations that would be impractical to test. I think that all of this filtering logic was ported over from GATK CNV (I only rewrote the code to perform the filtering in-place to improve memory usage), and I'm not sure that all edge-case behavior was well defined by the original logic (which probably implicitly assumed typical, well formed data, i.e., using more than one sample, without too many uncovered intervals). Fortunately, these edge-case usages (i.e., using a single sample to build the PoN, mistakenly including too many uncovered intervals, and/or disabling various filters) are probably not too common.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882:1081,perform,perform,1081,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6624#issuecomment-705843882,1,['perform'],['perform']
Performance,"Hmmm....as an alternate proposal, what if we implemented a custom serializer for `SimpleInterval` that does the contig name -> index and index -> contig name conversion transparently at serialization/deserialization time? Would that address the performance issue with shuffles and allow us all to share the same interval class?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374:245,perform,performance,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418509374,1,['perform'],['performance']
Performance,"Hmn, we could be seeing quota issues. We just dramatically increased the size of our build matrix and there's a ton of work going on today. It's hard to tell because there's no way to view the queue...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745:193,queue,queue,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305020745,1,['queue'],['queue']
Performance,How about I close the PR for now and open a ticket to investigate caching and its performance implications? I'll keep the branch so you can always compare etc.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855:82,perform,performance,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235939855,1,['perform'],['performance']
Performance,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:716,optimiz,optimizing,716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,4,"['optimiz', 'tune']","['optimizing', 'tune']"
Performance,"How to reproduce:. Modify the code in ```AlignmentIntervalUnitTest.testConstructionFromSAMRecord``` to perform a validation of the read returned by ```applyAlignment```:. ```; final SAMRecord samRecord = BwaMemAlignmentUtils.applyAlignment(""whatever"", SVDiscoveryTestDataProvider.makeDummySequence(expectedContigLength, (byte)'A'), null, null, bwaMemAlignment, refNames, hg19Header, false, false);; if (samRecord.isValid() != null) {; throw new IllegalStateException(samRecord.isValid().stream().map(s -> s.getMessage()).collect(Collectors.joining("", "")));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384:103,perform,perform,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3459#issuecomment-323218384,1,['perform'],['perform']
Performance,Huh... That's not good. I wonder if it's because we removed the jcenter repository resolver from the build. Maybe local caches are hiding the problem for us. . Could you try checking out the branch `lb_add_back_jcenter` and see if that fixes the problem?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184:120,cache,caches,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7636#issuecomment-1012346184,1,['cache'],['caches']
Performance,"I also forgot to mention that this change needs an update for Hadoop-BAM due to a change in the way filesystems classes are loaded by Spark submit. @cmnbroad, would you be able to take a look at https://github.com/HadoopGenomics/Hadoop-BAM/pull/120, so I can merge it and do a new release?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089:124,load,loaded,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-257583089,1,['load'],['loaded']
Performance,"I also just got this ConcurrentModificationException in HaplotypeCallerSparkIntegrationTest locally when running tests on one of my branches (this time in `testNonStrictVCFModeIsConsistentWithPastResults`, but the rest of the stack looks the same). I also vaguely recall seeing once before on travis on a branch where all I did was try to update miniconda to a newer version. My local branch doesn't have any dependency changes, so I suspect this is an existing issue that is just showing up intermittently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665:21,Concurren,ConcurrentModificationException,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602804665,1,['Concurren'],['ConcurrentModificationException']
Performance,"I am also getting this error on chromosome 11:; ```; 11:49:44.992 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (5/12) (contig name: 9)...; 11:49:44.996 INFO gcnvkernel.postprocess.viterbi_segmentation - Segmenting contig (6/12) (contig name: 11)... Stderr: Traceback (most recent call last):; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.6491270870870970325.py"", line 79, in <module>; viterbi_engine.write_copy_number_segments(); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 234, in write_copy_number_segments; for segment in self._viterbi_segments_generator():; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/postprocess/viterbi_segmentation.py"", line 160, in _viterbi_segments_generator; log_prior_c, log_trans_contig_tcc, copy_number_log_emission_contig_tc); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/gcnvkernel/models/theano_hmm.py"", line 88, in perform_forward_backward; prev_log_posterior_tc, admixing_rate, temperature))); File ""/ngc/projects/gm/data/res",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:926,perform,perform,926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['perform'],['perform']
Performance,"I am also seeing this warning 3x with 4.0.11.0 on a cluster but outside of docker (centos 6). . ```; 18:05:08.861 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.11.0/install/bin/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredential",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:141,Load,Loading,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,1,['Load'],['Loading']
Performance,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:831,Load,Loading,831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['Load'],['Loading']
Performance,I am not sure if any performance gained by having Spark versions of these tools is worth the support cost. Can reopen if we feel otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4198#issuecomment-900613847,1,['perform'],['performance']
Performance,I ask that because for htsjdk defaults they must be system properties and they're final and set statically on load so mucking about resetting system properties after the JVM started already is going to be a bit of a fiddly ordering nightmare. . [Stack overflow](http://stackoverflow.com/questions/6736235/set-java-system-properties-with-a-configuration-file) doesn't seem to think that it's possible to initialize them from a file.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704:110,load,load,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267126704,1,['load'],['load']
Performance,"I brew installed openssl, and now there is a different unresolved dependency:. java.lang.UnsatisfiedLinkError: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib: dlopen(/private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib, 1): Library not loaded: **/usr/local/opt/libcsv/lib/libcsv.3.dylib**; Referenced from: /private/var/folders/cr/16ghvyfj5lvfwxx01rt1k4tdl04sy3/T/libtiledbgenomicsdb6507285380909029818.dylib; Reason: image not found",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254:340,load,loaded,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294232254,1,['load'],['loaded']
Performance,"I can actually replicate this with an interval list with two intervals: ; ```; chr1	11719	18516	+	.; chr2	38664	202755	+	.; ```; throws; ```; org.broadinstitute.hellbender.exceptions.GATKException: Cannot call query with different interval, expected:chr1:11719-18516 queried with: chr2:38664-202755; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport$InitializedQueryWrapper.query(GenomicsDBImport.java:816); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:136); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:563); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. We typically run with `--reader-threads 5` in the pipeline, but if I change it to 1 I can get it to run. That's not a longterm solution, but hopefully it's a good hint and it's a good enough workaround for me for the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966:627,concurren,concurrent,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5300#issuecomment-438818966,3,['concurren'],['concurrent']
Performance,"I can confirm that in my own extensive runs of the tools I've seen it sometimes get hung when out of memory instead of exiting (resulting in bad data for that run). My own benchmarking was with PrintReads on a large file, using various cache buffer sizes. I remember seeing an increase in performance up to about 50MB buffer size and then it flattened out. I would expect a more CPU-intensive (or a more heavily loaded machine) would reduce the impact of the buffer size as I/O ceases to be the bottleneck. I also ran experiments on a 1-cpu machine with VCF and loading a single interval, which I think matches what you are asking about. In that experiment 10MB was enough, adding to the cache did not bring any improvement and of course too large a cache leads to running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380:236,cache,cache,236,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2640#issuecomment-298972380,7,"['bottleneck', 'cache', 'load', 'perform']","['bottleneck', 'cache', 'loaded', 'loading', 'performance']"
Performance,"I can confirm that, GATK 4.1.3.0 with CollectRawWgsMetrics; ```; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; ```. **Cmdline:**; ```; picard -Xms4000m \; CollectRawWgsMetrics \; INPUT=example.bam; VALIDATION_STRINGENCY=SILENT \; REFERENCE_SEQUENCE=Homo_sapiens_assembly38.fasta \; INCLUDE_BQ_HISTOGRAM=true \; INTERVALS=wgs_coverage_regions.hg38.interval_list \; OUTPUT=example.raw_wgs_metrics \; USE_FAST_ALGORITHM=true \; READ_LENGTH=250; ```. **Output:**; ```; 18:48:46.330 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/picard-2.20.7-0/picard.jar!/com/intel/gkl/native/libgkl_compression.so; [Fri Sep 20 18:48:46 GMT 2019] CollectRawWgsMetrics INPUT=example.bam; [Fri Sep 20 18:48:46 GMT 2019] Executing as root@98e13b9f4ef1 on Linux 4.19.44+ amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.7-SNAPSHOT; [Fri Sep 20 18:49:00 GMT 2019] picard.analysis.CollectRawWgsMetrics done. Elapsed time: 0.24 minutes.; Runtime.totalMemory()=4054515712; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; Exception in thread ""main"" java.lang.IllegalArgumentException: The requested position is not covered by this StartEdgingRecordAndOffset object.; at htsjdk.samtools.util.AbstractRecordAndOffset.validateOffset(AbstractRecordAndOffset.java:146); at htsjdk.samtools.util.EdgingRecordAndOffset$StartEdgingRecordAndOffset.getBaseQuality(EdgingRecordAndOffset.java:112); at picard.analysis.FastWgsMetricsCollector.excludeByQuality(FastWgsMetricsCollector.java:189); at picard.analysis.FastWgsMetricsCollector.processRecord(FastWgsMetricsCollector.java:144); at picard.analysis.FastWgsMetricsCollector.addInfo(FastWgsMetricsCollector.java:105); at picard.analysis.WgsMetricsProcessorImpl.processFile(WgsMetricsProcessorImpl.java:93); at picard.an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047:604,Load,Loading,604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6163#issuecomment-533770047,1,['Load'],['Loading']
Performance,"I can't reproduce this yet. I tried downloading the jar, unzipping it, and running the example command you gave, but I can't reproduce what you're seeing. I modified it for my local files:; ```; java -jar gatk-package-4.2.5.0-local.jar \; GenotypeGVCFs \; -R /Users/louisb/Workspace/gatk/src/test/resources/large/Homo_sapiens_assembly19.fasta.gz \; --variant gendb:///Users/louisb/Workspace/gatk/output \; -O out.vcf \; --annotate-with-num-discovered-alleles \; -stand-call-conf 30 \; --max-alternate-alleles 6 \; --force-output-intervals 20 \; -L 20 \; --only-output-calls-starting-in-intervals \; --genomicsdb-shared-posixfs-optimizations; ```; It runs to completion on my machine. ; My md5sum matches yours so that's not the problem. It's not clear to me what's going on here. Are the previous releases working on your cluster still?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522:627,optimiz,optimizations,627,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1042010522,2,['optimiz'],['optimizations']
Performance,"I can, this only happens on 10 of our 2000 samples (only in WES) none of our 600 WGS seems to have the same issue. It is always on some small contig (you can see here range is 544, but all cases are small ranges like this one). Everything is the default mutect2 pipeline and params (e.g. [gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0?prefix=Homo_sapiens_assembly38.fasta&authuser=jkalfon%40broadinstitute.org)) : except the interval file: [gs://ccleparams/region_file_wgs.list](https://console.cloud.google.com/storage/browser/ccleparams?prefix=region_file_wgs.list&authuser=jkalfon%40broadinstitute.org); GATK 4.2.6.1. . Here is the VCF file to annotate `gs://ccleparams/test/CDS-2jucw0.hg38-filtered.vcf.gz`. Here is the stacktrace:. ```; ....; 10:53:39.044 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2145; 10:53:39.249 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/1069225; 10:53:39.520 INFO Funcotator - Shutting down engine; [July 12, 2022 10:53:39 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 115.46 minutes.; Runtime.totalMemory()=2050490368; java.lang.StringIndexOutOfBoundsException: String index out of range: 544; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:293); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:101); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:399); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2054); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFunc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:945,cache,cache,945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['cache'],['cache']
Performance,"I checked in the spark JAR, and `META-INF/services/java.nio.file.spi.FileSystemProvider` contains . ```; com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider; hdfs.jsr203.HadoopFileSystemProvider; ```. So it looks like the service loader file is being created correctly. I tried to reproduce on GCS by running (essentially the same as @vdauwera's command.). ```; time ./gatk-launch ApplyBQSRSpark \; -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R gs://gatk-legacy-bundles/b37/human_g1k_v37.2bit \; -O gs://gatk-demo-tom/TEST/gatk4-spark/recalibrated.bam \; -bqsr gs://gatk-demo/TEST/gatk4-spark/recalibration.table \; -apiKey $GOOGLE_APPLICATION_CREDENTIALS \; -- \; --sparkRunner GCS \; --cluster cluster-tom \; --num-executors 40 \; --executor-cores 4 \; --executor-memory 10g; ```. But I got another error earlier on. Any ideas what this could be? (I can see the input bam with `gsutil cp`). ```; org.broadinstitute.hellbender.exceptions.UserException: A USER ERROR has occurred: Failed to read bam header from gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.bam; Caused by:Error reading null at position 0; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:182); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:376); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:357); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:347); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.insta",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676:246,load,loader,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287#issuecomment-264909676,1,['load'],['loader']
Performance,I cleaned up the mutect2 wdl and added multi-sample support. I also optimized resource usage and exposed the memory parameters: https://github.com/phylyc/gatk4-somatic-snvs-indels,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665:68,optimiz,optimized,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532#issuecomment-1125321665,1,['optimiz'],['optimized']
Performance,"I copied 60Ã— BAM file to an interactive Linux server with 768 GB physical RAM and eighty cores and used version 4.5.0.0. ```; %Cpu(s): 1.3 us, 0.0 sy, 0.1 ni, 98.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; GiB Mem : 754.5 total, 52.1 free, 107.3 used, 600.3 buff/cache ; GiB Swap: 931.3 total, 924.9 free, 6.4 used. 647.3 avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 171365 dario 20 0 35.0g 31.3g 23040 S 100.0 4.1 32:18.12 java ; ```. I removed `-Xmx` and using `top` to see the process is consistently at about 32 GB. So, `-Xmx` is irrelevant to the problem. ```; 12:15:04.531 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 12:57:32.208 INFO GetPileupSummaries - Shutting down engine; [January 13, 2024 at 12:57:32 PM AEDT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 50.36 minutes.; Runtime.totalMemory()=20753416192; java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects; ```. What does ""reallocation of scalar replaced objects"" mean? I don't think it could possibly have run out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060:257,cache,cache,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8654#issuecomment-1890249060,1,['cache'],['cache']
Performance,"I created a panel of normals from 90 WGS TCGA samples with 250bp (~11.5M) bins, which took **~57 minutes** total and produced an **11GB PoN** (this file includes all of the input read counts---which take up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:961,optimiz,optimize,961,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,2,['optimiz'],['optimize']
Performance,I definitely say kill the pretty streaming code if it's harming performance in these critical sections -- no shame in a straightforward loop!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318:64,perform,performance,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257582318,1,['perform'],['performance']
Performance,"I did a quick scalability ministudy - this seems to scale well up to 12 cores and then diminishes due to Amdahl's law I think that's fine. There is no diminished runtime due to OMP overhead when on 1 core. Note that our cluster on which I wan these was not empty (a few of the 48 cores were in use) and do this is just a ballpark estimate of scalability, in particular 24 was worse than 12 probably due to interference. Based on this I think OMP is a good idea and it's going to work on 1 CPU too. limited to 1 OMP thread, using 10GB of RAM. ```; real 2m15.621s; user 3m17.269s; Total compute time in PairHMM computeLogLikelihoods() : 50.964700625000006; ```. ---. limited to 1 OMP thread, using 32GB of RAM . ```; real 1m46.597s; user 3m17.363s; Total compute time in PairHMM computeLogLikelihoods() : 45.797104454; ```. limited to 2 OMP threads, using 32GB of RAM. ```; real 1m26.310s; user 3m24.636s; Total compute time in PairHMM computeLogLikelihoods() : 23.790980359000002; ```. limited to 4 OMP threads, using 32GB of RAM. ```; real 1m15.298s; user 3m29.834s; Total compute time in PairHMM computeLogLikelihoods() : 11.332445694; ```. limited to 6 OMP threads, using 32GB of RAM. ```; real 1m14.015s; user 3m20.876s; Total compute time in PairHMM computeLogLikelihoods() : 7.862075811; ```. limited to 12 OMP threads, using 32GB of RAM. ```; real 1m6.370s ; user 3m42.340s; Total compute time in PairHMM computeLogLikelihoods() : 4.585800097; ```. limited to 24 OMP threads, using 32GB of RAM (clearly, OMP hits the limit here). ```; real 1m8.779s; user 4m15.489s; Total compute time in PairHMM computeLogLikelihoods() : 3.047581173; ```. limited to 48 OMP threads, using 32GB of RAM (worse than 12 threads). ```; real 1m11.535s; user 6m26.100s; Total compute time in PairHMM computeLogLikelihoods() : 4.112299148; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496:14,scalab,scalability,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800#issuecomment-218810496,2,['scalab'],['scalability']
Performance,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:892,perform,perform,892,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['perform'],['perform']
Performance,"I feel like this is going to be problematic in a different way than what @magicDGS is mentioning. We expect many versions of gatk to be compatible with the same python environment. Also for performance reasons we want to start avoid rebuilding the conda environment on every push and bake it into the base docker instead. This change means we definitely have to build it every time. . It feels like we need something more sophisticated. Instead of stamping the conda environment with the gatk version that matches it, maybe we should be stamping the gatk jar and the conda environment with some version based on the conda.env? Maybe we can do something like taking the md5 of the conda.yml and pushing that into both the jar manifest and the conda environment in some way? I'm guessing this scheme has an issue with the actual python code in the gatk since I think that's installed with conda as well? I'd really like to be able to preinstall the various dependencies though and then only update the code that's part of the gatk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721:190,perform,performance,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721,1,['perform'],['performance']
Performance,"I guess I could, but what is GATKTool contract... is it supposed to only perform reference driven analysis? I thought that is what a Walker is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790:73,perform,perform,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6390#issuecomment-577247790,1,['perform'],['perform']
Performance,I have also stumbled over this. I am adding a detailed error log.; I think that the incompatibility of accelerated PairHMM with a tmp directory mounted noexec should be mentioned in ; the installation requirements. I found it well-documented in [the troubleshooting section](https://gatk.broadinstitute.org/hc/en-us/articles/18965297287067-How-to-setup-and-use-temporary-folder-for-GATK-local-execution). But everyone with this setup will experience falling back to the slow implementation for no other reason. . ```; INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/; miniconda2/envs/polyploidPhasing/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils9418239050694741169.so: /tmp/libgkl_utils9; 418239050694741169.so: failed to map segment from shared object: Operation not permitted); WARN IntelPairHmm - Intel GKL Utils not loaded; PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389:545,Load,Loading,545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8453#issuecomment-1905717389,4,"['Load', 'load', 'multi-thread']","['Loading', 'load', 'loaded', 'multi-threaded']"
Performance,"I have more examples of this now (90 and counting, ~1% of jobs) which seems to match with the above numbers:. `htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset, for input source: gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/1e300bb3-6990-4342-8959-118826efb3dd/PairedEndSingleSampleWorkflow/3b32519a-f910-49a6-a5fc-b7ec9700d281/call-GatherVCFs/S153-2.g.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:102); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:86); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:210,concurren,concurrent,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"I have not had time to do any profiling, but I have looked at a lot of commits. I think it's likely that my recent changes cause some haplotypes with leading indels to be kept when previously they may have been dropped. It's hard to believe that this could cause a 10-20% slowdown via a commensurate increase in the number of haplotypes assembled. However, haplotypes with leading indels would have a disproportionate pair HMM cost since they would spoil caching of the read-haplotype pair HMM matrix at the very beginning of the matrix. That is, in addition to being particularly expensive haplotypes because they would diverge from the previous haplotype at the first position and therefore not benefit from caching at all, they would also completely destroy whatever caching the previous haplotype would have gotten. We ought to think about haplotypes that start or end with indels. It seems to me that they are bad news and very likely artifacts of assembly windows and/or reads that end in the middle of an STR. I would worry about discarding them outright, because what if all the real variation is attached to haplotypes like this. Therefore, I think the best thing to do is to choose assembly windows more carefully and increase or decrease padding to avoid ending in an STR. Avoiding assembly windows that end in STRs is a wise thing to do regardless, so how about I make a branch for that and we can see if the performance regression goes away?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595:1421,perform,performance,1421,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595,1,['perform'],['performance']
Performance,"I haven't understood how multi-allele model exactly works in the old GATK, so can't comment on why it does not perform well. In general, I am supportive of making the new model the default going forward. However:. > when we remove the other models. I would suggest retaining the old model if possible. As I said on the method meeting, the old model takes the full power of population information (by full, I mean under the Wright-Fisher and HWE assumptions, you can't derive a more powerful model in theory). My understanding is that David's current model isn't. This is fine as long as the information from sequence data overwhelms the population information, which is usually true for highCov data. However, when data is thin, the population information will play a more important role. Without thorough evaluations in multiple scenarios, it is not clear when the loss of population information in the new model starts to matter. It would be good to keep the old model as a reference point, at least for biallelic SNPs, until we have more comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127:111,perform,perform,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2098#issuecomment-242810127,2,['perform'],['perform']
Performance,"I just got the same error:. ```; Using GATK jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar FilterMutectCalls -V tumor-vs-normal.mutect.temp1.vcf -O tumor-vs-normal.mutect.temp2.vcf; 22:58:25.052 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/anaconda2/share/gatk4-4.0.12.0-0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:58:26.911 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.912 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.12.0; 22:58:26.912 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:58:26.912 INFO FilterMutectCalls - Executing as www-data@SpongeBob on Linux v4.15.0-39-generic amd64; 22:58:26.912 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 22:58:26.912 INFO FilterMutectCalls - Start Date/Time: January 6, 2019 10:58:24 PM SGT; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - ------------------------------------------------------------; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Version: 2.18.1; 22:58:26.913 INFO FilterMutectCalls - Picard Version: 2.18.16; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:58:26.913 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:58:26.914 INFO FilterMutectCalls - De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085:513,Load,Loading,513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5553#issuecomment-451749085,1,['Load'],['Loading']
Performance,I loaded the docker repo GATK v4.1.4.0 and had the same (or similar) error result. ```; 2019-10-30T13:35:51.791637449Z java.lang.IllegalArgumentException: log10 p: Values must be non-infinite and non-NAN; 2019-10-30T13:35:51.792001654Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 2019-10-30T13:35:51.792175325Z 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 2019-10-30T13:35:51.792358868Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-30T13:35:51.792559803Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-30T13:35:51.792736667Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-30T13:35:51.792905235Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-30T13:35:51.793072365Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-30T13:35:51.793261944Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-30T13:35:51.793456807Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-30T13:35:51.793619935Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-30T13:35:51.793810301Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 2019-10-30T13:35:51.794006885Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 2019-10-30T13:35:51.794191116Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-30T13:35:51.794367593Z 	at java.util.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227:2,load,loaded,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821#issuecomment-547909227,1,['load'],['loaded']
Performance,"I may have a lead. The error occurs here (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:976,cache,cached,976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"I might be missing something, but this looks like it iterates over assembly regions, rather than loading them all at once. `Utils.stream` converts an iterator to a stream, but even that doesn't load them all into memory as it basically creates a one-shot `Iterable` that just returns the passed in `Iterator` (this is what `() -> iterator` does).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180:97,load,loading,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4301#issuecomment-368830180,2,['load'],"['load', 'loading']"
Performance,"I ran `FindBreakpointEvidenceSpark` and did some high-level checks to see if there are any opportunities for performance improvements. (cc @tedsharpe @cwhelan). This is the command line I ran. (Earlier I had run more executors with smaller memory settings, but the job didn't complete then.); ; ```bash; ./gatk-launch FindBreakpointEvidenceSpark \; -I hdfs:///user/$USER/broad-svdev-test-data/data/NA12878_PCR-_30X.bam \; -O hdfs:///user/$USER/broad-svdev-test-data/assembly \; --exclusionIntervals hdfs:///user/$USER/broad-svdev-test-data/reference/GRCh37.kill.intervals \; --kmersToIgnore hdfs:///user/$USER/broad-svdev-test-data/reference/Homo_sapiens_assembly38.dups \; -- \; --sparkRunner SPARK --sparkMaster yarn-client --sparkSubmitCommand spark2-submit\; --driver-memory 16G \; --num-executors 5 \; --executor-cores 7 \; --executor-memory 25G; ```. What does FindBreakpointEvidenceSpark do, from the perspective of Spark?. * [runTool] filter out secondary and supplementary alignments; * [getMappedQNamesSet] filter out duplicate reads, reads that failed vendor checks, unmapped reads; * Job 0 [ReadMetadata] mapPartitions to find partition stats; * Job 1 [getIntervals] filter and multiple map partitions to find breakpoint intervals ; * Job 2 [removeHighCoverageIntervals] mapPartitionsToPair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you want",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:109,perform,performance,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,['perform'],['performance']
Performance,"I ran a few Bayesian optimizations, mostly focusing on various subsets/ranges of the read-to-haplotype and haplotype-to-reference SW parameters. I also varied whether I used part/all of chr22 and chose either F1/sensitivity as the optimization target. These experiments show that varying the SW parameters can definitely move the needle in terms of performance, even after variant normalization. For example, here are the SNP precision/sensitivity curves for an optimization of F1 over about half of chr22:. ![snp_f1](https://user-images.githubusercontent.com/11076296/96470421-44490900-11fc-11eb-96f6-83a6833b2b1f.png). And the same for indels:. ![non_snp_f1](https://user-images.githubusercontent.com/11076296/96470468-5034cb00-11fc-11eb-86a7-bbf81bc10122.png). This all raises the question of what the best optimization strategy should be. It looks like the SW parameters aren't the limiting factor for sensitivity (I would guess that might be MQ or other read filters) but can probably help improve precision. So we may want to relax some filters or optimize them jointly. We may also want to look at optimizations that include filtering, as I mention above.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919:21,optimiz,optimizations,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712237919,7,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimize', 'performance']"
Performance,"I rebased this, and responded to code review comments:. - updated comments; - reverted GenotypeGVCFs change; - reverted changing the default lookahead for VariantWalker side inputs. I think changing the default lookahead for VariantWalker side inputs to the new, smaller value will hurt performance for tools like VQSR. I did some crude timing tests using the FeatureDataSource default (1000 bases) proposed in this branch, and the current default (100,000 bases). The following are total times as reported by Gradle for serial runs of the VQSR integration tests:. With 1000 base lookahead:; 1m40s; 1m29s; 1m29s; 1m25s. With 100,000 base lookahead:; 1m29s; 1m17s; 1m16s; 1m18s. Back to 1000 base lookahead again:; 1m36s; 1m26s; 1m26s; 1m29s. It seems pretty consistently slower with the smaller lookahead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848:287,perform,performance,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480#issuecomment-417360848,1,['perform'],['performance']
Performance,"I reproduced various out of memory errors in a Linux VM with 4G of RAM, both with the `IntelInflaterDeflaterIntegrationTest` enabled and disabled. Most resulted in the kernel killing the Java process, like this one (from `dmesg`):; ```; [38425.759992] Out of memory: Kill process 10295 (java) score 747 or sacrifice child; [38425.759998] Killed process 10295 (java) total-vm:7885212kB, anon-rss:3250892kB, file-rss:0kB; ```. Some were caught by the JVM, like this one:; ```; #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 90177536 bytes for committing reserved memory.; # Possible reasons:; # The system is out of physical RAM or swap space; # In 32 bit mode, the process size limit was hit; # Possible solutions:; # Reduce memory load on the system; # Increase physical memory or swap space; # Check if swap backing store is full; # Use 64 bit Java on a 64 bit OS; # Decrease Java heap size (-Xmx/-Xms); # Decrease number of Java threads; # Decrease Java thread stack sizes (-Xss); # Set larger code cache with -XX:ReservedCodeCacheSize=; # This output file may be truncated or incomplete.; #; # Out of Memory Error (os_linux.cpp:2627), pid=20484, tid=139679452493568; #; # JRE version: Java(TM) SE Runtime Environment (8.0_72-b15) (build 1.8.0_72-b15); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.72-b15 mixed mode linux-amd64 compressed oops); # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; ```. Here's my theory of what's happening. The `maxHeapSize` for test JVMs is set to 4G in `build.gradle`:; ```; maxHeapSize = ""4G""; ```. A 4G max heap size is too high for systems with 4G of RAM, because the Java heap grows until the system runs out of memory. If we decrease `maxHeapSize`, the GC should prevent the Java heap from growing too large, with the trade-off of more GC calls. I changed the `maxHeapSize` to `2G` a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316:813,load,load,813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-288423316,2,"['cache', 'load']","['cache', 'load']"
Performance,"I see same problem in `4.0.5.2`:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz --new-qual; 22:45:47.050 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:45:47.648 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:45:47.649 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 22:45:47.649 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:45:47.800 INFO GenotypeGVCFs - Initializing engine; 22:45:48.331 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 22:45:48.467 INFO GenotypeGVCFs - Done initializing engine; 22:45:48.555 INFO ProgressMeter - Starting traversal; 22:45:48.556 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:45:51.038 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 22:45:51.045 INFO GenotypeGVCFs - Shutting down engine; [2. Äervence 2018 22:45:51 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=528482304; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:422,Load,Loading,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028,1,['Load'],['Loading']
Performance,"I see the exception on most chromosomes, here's the one for chr1:. java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:79293873 end:79293872; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:49); at org.broadinstitute.hellbender.engine.AssemblyRegion.add(AssemblyRegion.java:335); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.fillNextAssemblyRegionWithReads(AssemblyRegionIterator.java:230); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:194); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095:782,load,loadNextAssemblyRegion,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4120#issuecomment-356718095,1,['load'],['loadNextAssemblyRegion']
Performance,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:342,tune,tuned,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353,2,['tune'],['tuned']
Performance,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:326,Load,Loading,326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,1,['Load'],['Loading']
Performance,"I suspect it's a different issue than #4062, but probably some similar library incompatibility issue. Unfortunately the stack trace doesn't have enough information in it. We should consider rewriting the error message for this so that we are sure to have the first cause reported, not just the `Could not load genomicsdb native library` message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584:305,load,load,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124#issuecomment-356984584,1,['load'],['load']
Performance,I suspect this was caused by the race condition in #2050 ; will close now - reopen if seen after #2053 is merged,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642:33,race condition,race condition,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039#issuecomment-235660642,1,['race condition'],['race condition']
Performance,"I talked to comms and we agreed that a ""mitochondria-mode"" argument to Mutect2 was the right balance of clarity (you're really running Mutect2 not a wrapper) and simplicity (you don't need a laundry list of arguments to change which mode you're in if you just want to run with optimized defaults). . @ldgauthier @davidbenjamin @takutosato @rcmajovski Could you please take another look? Removing the wrapper tools has cleaned up the code so there are fewer changes now. I also changed TLOD to LOD in this version, but I'm happy to take that out and have that be future work if anyone is worried about it being a breaking change.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077:277,optimiz,optimized,277,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-428195077,2,['optimiz'],['optimized']
Performance,"I think issues with gCNV postprocessing initially stemmed from things like 1) taking large sample x shard transposes (which Cromwell may have had trouble with at some point), 2) localizing more files than necessary for each sample, 3) introduction of lexicographical globbing bugs, etc. I think various people have tried to address this via things like bundling, which we ended up reverting in favor of transposing. Unfortunately, I'm afraid I've lost the plot on this after so long; and not sure I ever had it---is there any documentation (e.g., of things like how bundling improved performance) elsewhere that might help us decide what's left to be done? Seems like I had some more coherent thoughts in https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744. As before, if any remaining issues like the call caching mentioned above would best be addressed at the Cromwell level, I think it's at least worth an investigation of what might be involved.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782:584,perform,performance,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-900624782,1,['perform'],['performance']
Performance,"I think the root cause is the method ensureCapacity of GenotypesCache is not synchronized. So when multiple task threads run into this method, the new added cache is not fully initialized.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902:157,cache,cache,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8961#issuecomment-2306051902,1,['cache'],['cache']
Performance,"I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, https://github.com/dbpedia/distributed-extraction-framework/issues/9.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169:69,load,loader,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-668654169,1,['load'],['loader']
Performance,"I think this issue is very important to me. When I deal with the CAP data (high capture cfDNA data), GATK4.1.0.8's performance is much better than GATK4.2 ...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963:115,perform,performance,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1174725963,1,['perform'],['performance']
Performance,"I think we are also waiting for FC to update, so that NIO can be call cached. Not sure what the status is on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277:70,cache,cached,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277,1,['cache'],['cached']
Performance,I think we discovered at some point that IntervalSkipList has worse performance than htsjdk `OverlapDetector`. We should probably be deprecating and removing it. #3608,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525:68,perform,performance,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3541#issuecomment-331950525,1,['perform'],['performance']
Performance,"I think we should wait for the NIO-based fix rather than merging this, which would be useless for our performance work.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196:102,perform,performance,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2333#issuecomment-271720196,1,['perform'],['performance']
Performance,"I thought I figured this out but I haven't. I'm looking at sample1.md.bam file header, but I can't figure it out. ```. for BAM in sample1.md.bam ; do samtools view -H $BAM; > header.sam; done; @HD	VN:1.5	SO:coordinate; @SQ	SN:1	LN:4918979; @SQ	SN:2	LN:4844472; @SQ	SN:3	LN:4079167; @SQ	SN:4	LN:3923705; @SQ	SN:5	LN:3948441; @SQ	SN:6	LN:3778736; @SQ	SN:7	LN:2058334; @SQ	SN:8	LN:1833124; @PG	ID:hisat2	PN:hisat2	VN:2.1.0	CL:""/usr/local/apps/eb/HISAT2/2.1.0-foss-2016b/bin/hisat2-align-s --wrapper basic-0 -p 8 --no-spliced-alignment -I 100 -x ./ref_tran.dir/ref_tran -S sample1.sam -1 e1_R1.fastq -2 e1_R2.fastq""; @PG	ID:MarkDuplicates	VN:2.16.0-SNAPSHOT	CL:MarkDuplicates INPUT=[sample1.bam] OUTPUT=sample1.md.bam METRICS_FILE=sample1.md.metrics.txt MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 TAG_DUPLICATE_SET_MEMBERS=false REMOVE_SEQUENCING_DUPLICATES=false TAGGING_POLICY=DontTag CLEAR_DT=true ADD_PG_TAG_TO_READS=true REMOVE_DUPLICATES=false ASSUME_SORTED=false DUPLICATE_SCORING_STRATEGY=SUM_OF_BASE_QUALITIES PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates READ_NAME_REGEX=<optimized capture of last three ':' separated fields as numeric values> OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 MAX_OPTICAL_DUPLICATE_SET_SIZE=300000 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false	PN:MarkDuplicates. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611:1174,optimiz,optimized,1174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5372#issuecomment-434156611,1,['optimiz'],['optimized']
Performance,"I tried clearing my caches and rebuilding, but I resolve everything. I noticed that our artifactory website looks much different today than it did yesterday. I wonder if it was down temporarily for an update. Maybe try again now? Unless they put it behind the firewall which would be a disaster... can you access https://artifactory.broadinstitute.org/?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641:20,cache,caches,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2579#issuecomment-292587641,2,['cache'],['caches']
Performance,"I tried the FilterSamReads command using the Picard jar file and the output bam file to stdout had no issues; Command:; `java -jar ~/picard/build/libs/picard.jar FilterSamReads -I subsampled.bam -O /dev/stdout --READ_LIST_FILE read_names.txt --FILTER excludeReadList --VALIDATION_STRINGENCY SILENT --QUIET > picard_stdoutbam.bam`; Log:; ```; 10:33:09.327 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/gbrandt/picard/build/libs/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; INFO	2021-02-23 10:33:09	FilterSamReads	Filtering [presorted=true] subsampled.bam -> OUTPUT=stdout [sortorder=coordinate]; INFO	2021-02-23 10:33:09	SAMFileWriterFactory	Unknown file extension, assuming BAM format when writing file: file:///dev/stdout; INFO	2021-02-23 10:33:09	FilterSamReads	6 SAMRecords written to stdout; ```; Checking the file:; `gunzip -c -d -f picard_stdoutbam.bam | head -n 5`; No issues:; ```; BAM?2@HD	VN:1.6	SO:coordinate; @SQ	SN:1	LN:249250621	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:2	LN:243199373	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:3	LN:198022430	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; @SQ	SN:4	LN:191154276	AS:NCBI-Build-37	SP:Homo sapiens	UR:http://www.bcgsc.ca/downloads/genomes/9606/hg19/1000genomes/bwa_ind/genome/GRCh37-lite.fa; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228:382,Load,Loading,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7080#issuecomment-784427228,1,['Load'],['Loading']
Performance,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ22æ—¥ ä¸‹åˆ05æ—¶49åˆ†50ç§’; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:507,optimiz,optimizations,507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,2,"['Load', 'optimiz']","['Loading', 'optimizations']"
Performance,"I used to have it in there, but once I got copy performance up I assumed it was just clutter. I need to make a minor update to the scripts anyway (option to run the debug program instead of the full pipeline) so I can put back the option to not copy fastq files as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4332#issuecomment-362655529,1,['perform'],['performance']
Performance,I was able to reproduce this problem by running 1000 concurrent read streams for a while. This allowed me to verify that my fix indeed resolves the problem (at least for my repro). I have sent a [PR](https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2083) to gcloud for review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426:53,concurren,concurrent,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303211426,1,['concurren'],['concurrent']
Performance,"I was looking `ReadWindowWalker` and I found it very interesting for iterate over windows, but although it is similar I still think that is not solving the same problem as the `SlidingWindowWalker` for two reasons:; 1. `ReadWindowWalker` requires reads to construct the `ReadWindow`, and it is not general for both reads and variants. My main idea behind the `SlidingWindowWalker` was to perform operation over windows along the genome (or requested intervals) for any kind of source provided to the walker.; 2. On the other hand, the approach to generate the sliding windows in my implementation is different from the one in `ReadWindowWalker`: first, the overlap between windows is only in one direction; second, `SlidingWindowWalker` is more like a reference/interval walker, from the beginning of the reference (or interval) till the end, it walks in overlapping windows. One example is the following (window-size 10, window-step 5, the - represent the window):. ```; Reference: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; Windows1: _ _ _ _ _ _ _ _ _ _; Windows2: _ _ _ _ _ _ _ _ _ _; Windows3: _ _ _ _ _ _ _ _ _ _; Windows4: _ _ _ _ _ _ _ _ _ _; Windows5: _ _ _ _ _ _ _ _ _ _; ```. Of course, after having a look to `ReadWindowWalker` I think that several things could be improved in my implementation for a general `SlidingWindowWalker`:; - Apply function similar to the `ReadWindowWalker`, with `ReadWindow` being empty if reads are not provided.; - Three window options: `windowSize` (the actual size of the window), `windowStep` (how much advance for the following window) and `windowPadding` (how much extend the window in both directions). Using this abstraction, `ReadWindowWalker` could be implemented setting `windowSize=windowStep`, and the problem that I need to solve could be implemented setting `windowPadding=0`. The simplest way to acomplish this is to use the current implementation of `ReadWindowWalker` to develop a `SlidingWindowWalker` adding three abstract ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775:388,perform,perform,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528#issuecomment-198438775,1,['perform'],['perform']
Performance,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:356,perform,performing,356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,1,['perform'],['performing']
Performance,"I was mistaken about this not being faster - I was using a counting function that Spark can optimise by pulling onto the map side so that the records don't go through the shuffle. I changed this to simply dump the processed reads so they have to go through the shuffle, and I got the following timings when processing a 121GB BAM file.; - With shuffle: 27 min; - No shuffle (two scans over input): 24.7 min (8% saving); - No shuffle (one scan over input): 17 min (37% saving). The version that does two scans is faster, but not hugely so. Removing a scan is possible, but requires the use of a sequence dictionary to find the end points of contigs. I've done this in the latest version of my branch (https://github.com/broadinstitute/gatk/compare/tw_overlap_partitioner), but there are more edge cases to test. Before I do this, however, it would be worth trying this approach with the Haplotype Caller to see if it works, and if it is appreciably faster. If the number of reads is filtered significantly so only a fraction go through the shuffle, then the performance gains will be smaller, and may not in fact be worth the increase in code complexity. @droazen, what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040:1057,perform,performance,1057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988#issuecomment-249590040,1,['perform'],['performance']
Performance,"I was outputting to .vcf.gz. . I reran the command to output to just. vcf and it runs without error:; ```; /gatk-launch FilterByOrientationBias --artifactModes 'G/T' -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P ~/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk FilterByOrientationBias --artifactModes G/T -V /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz -P /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics -O test_filterbyorientationbias.vcf; 01:16:16.916 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.4.3.jar!/com/intel/gkl/native/libgkl_compression.dylib; [June 6, 2017 1:16:16 AM EDT] FilterByOrientationBias --output test_filterbyorientationbias.vcf --preAdapterDetailFile /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/gatk_6_T_artifact.pre_adapter_detail_metrics --artifactModes G/T --variant /Users/shlee/Documents/workshop_materials/mutect2_tutorial/mutect2_handson/8_mutect2.vcf.gz --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306384891,1,['Load'],['Loading']
Performance,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:261,perform,performance,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['perform'],['performance']
Performance,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:102,perform,perform,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,2,['perform'],['perform']
Performance,"I wondered how much of the time was due to parsing the VCF file. To test this, I used Kryo to serialize the `IntervalsSkipList` to a binary blob, then tried loading the binary blob directly. This reduced the load time from around 6 minutes to 4.7 minutes - so some speed improvement, but not a lot. See https://github.com/broadinstitute/gatk/tree/tw_known_sites_perf_kryo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602:157,load,loading,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412897602,2,['load'],"['load', 'loading']"
Performance,"I would guess that we actually do get the requested r-backports 1.1.10, but since this is an R package rather than a python package, the python package version check performed by the failing test is not applicable (i.e., you cannot `import r-backports` in python).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652:166,perform,performed,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6955#issuecomment-726977652,1,['perform'],['performed']
Performance,"I would like to eventually port the indel realignment pipeline, but I don't know if I will have time. @sooheelee, maybe you can tell me if people is interested on it? I think that it is still important for Pool-Seq data, where haplotype-based calling is not usually performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736:266,perform,performed,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307734736,1,['perform'],['performed']
Performance,"I wouldn't be surprised if I could optimize the implementation of the likelihoods model quite a bit. Just having a slightly looser threshold for convergence, for example.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104:35,optimiz,optimize,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-449499104,1,['optimiz'],['optimize']
Performance,"I'd also note that in the example command that @tomwhite is running uses just 4GB of memory on the driver while using a broadcast join strategy, which I believe is the default strategy used by BQSR. The [GZIP'ed dbSnp build 138 VCF in the hg18 GATK bundle](ftp://ftp.broadinstitute.org/bundle/hg18/) is ~1.4GB, and the hg18 2bit file is probably going to be ballpark 1GB of data. That's a lot of data to collect onto a driver with just 4GB of memory, so I wouldn't be surprised if the driver is OOMing. @lbergelson you'd mentioned [above](https://github.com/broadinstitute/gatk/issues/4376#issuecomment-364187885) that you haven't seen said performance regression and that it isn't showing up in the regression tests kicked off by CI. Can you confirm what driver memory settings you are using?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206:641,perform,performance,641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367150206,1,['perform'],['performance']
Performance,I'll add that we do already know that the existing caching strategy massively improves Funcotator performance (by about an order of magnitude!) due to an evaluation that @lbergelson did.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210:98,perform,performance,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5143#issuecomment-416986210,1,['perform'],['performance']
Performance,I'll kick off a test run on FC soon (the queue is currently a bit backed up)...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381:41,queue,queue,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5490#issuecomment-450968381,1,['queue'],['queue']
Performance,"I'll open a ticket for ref conf performance optimization, but I'm keen to get a ""draft"" of the MT joint calling pipeline into 4.1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938:32,perform,performance,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5312#issuecomment-451466938,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:310,Perform,Performing,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,1,['Perform'],['Performing']
Performance,"I'm getting the same issue on GATK 4.1.9.0 FilterAlignmentArtifacts. This bug has been present for 1 year. Has this been fixed?; Note: There is no work-around because FilterAlignmentArtifacts does not have a --smith-waterman option. Here is my error:; ```; 20:12:42.724 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:820,cache,cacheCopy,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,2,"['Load', 'cache']","['Loading', 'cacheCopy']"
Performance,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:309,Perform,Performing,309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,1,['Perform'],['Performing']
Performance,"I'm not sure what proportion of users leverage the incremental import functionality...it wasn't available when GenomicsDBImport was first made available, but has been around for ~3 years now. As for workspaces with whole chromosomes -- there is no requirement or performance benefits to using whole chromosomes. As you say, subsetting a chromosome to smaller regions will work and make the import and query parallelizable. (if you remember where the advice about whole chromosomes came from, let us know. That might be something that needs to be updated/clarified). Many small contigs does add overhead to import though and, till recently, multiple contigs couldn't be imported together (i.e., each contig would have it's own folder under the GenomicsDB workspace - which gets inefficient with many small contigs). For WGS, probably the best way to create the GenomicsDBImport interval list is to split based on where there are consecutive N's in the reference genome (maybe using [Picard](https://broadinstitute.github.io/picard/command-line-overview.html#ScatterIntervalsByNs)) and/or regions that you are blacklisting. I think you suggested that some of the blacklisted regions were especially gnarly - maybe ploidy or high alternate allele count? - depending on the frequency of those, we may save a bit on space/memory requirements. That may address your concern about overlap between variants and import intervals. In general, any variant that starts in a specified import interval will show up in a query to that workspace. I'm not sure if the blacklist regions contain any variants that start within but extend beyond the blacklist -- those may not show up if the regions are split up in this way.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548:263,perform,performance,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1212486548,1,['perform'],['performance']
Performance,I'm posting an experimental version of this branch for performance purposes. This is no longer in a state where it should be merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011,1,['perform'],['performance']
Performance,"I'm still not very familiar with the way people have used extensions of; Locatable, or how indexing can be used to boost performance. The stuff I've; worked on is a bit of a corner case, and I didn't write much of the; infrastructure, I've been tacking on features. For now I've just been keeping the gzipped text files from the UCSC; browser. They're tab delimited with two header lines, the first basically; giving info about context of the data (it's genome data for the , hg38) and; the second being a description of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:121,perform,performance,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551,1,['perform'],['performance']
Performance,"I've also seen UnknownHostException: metadata, which seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleT",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:215,concurren,concurrent,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,5,['concurren'],['concurrent']
Performance,"I've been looking at 2bit performance today, comparing ADAM release version 0.20.0 to release version 0.23.0 and to git HEAD (0.24.0-SNAPSHOT), in various use cases, and do not see any performance differences. @tomwhite `loadReferenceFile` in ADAM only supports local 2bit files, what happens in between the file in `gs://` cloud storage to when you load it via ADAM APIs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861:26,perform,performance,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-367159861,4,"['load', 'perform']","['load', 'loadReferenceFile', 'performance']"
Performance,"I've created a third party JAR that depends on GATK4 (4.beta.2). I believe I just hit a similar issue; however, what's odd is that it seems intermittent (though i have n=2 on this tool so far):. [October 2, 2017 6:10:01 AM PDT] com.github.discvrseq.walkers.BackportLiftedVcf done. Elapsed time: 6.27 minutes.; Runtime.totalMemory()=45813334016; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at com.github.discvrseq.walkers.BackportLiftedVcf.apply(BackportLiftedVcf.java:156); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182:422,Load,LoadSnappy,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333579182,1,['Load'],['LoadSnappy']
Performance,I've created https://github.com/broadinstitute/gatk/issues/2625 to separately address a possible performance issue introduced by this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331:97,perform,performance,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2605#issuecomment-297483331,1,['perform'],['performance']
Performance,"I've found at least one optimization that may be invalid if SW parameters are changed (also noted by @yfarjoun https://github.com/broadinstitute/gatk/commit/4ccbaddc069539fa6e3ac2690469c1e0a4b03ccb#r31525284). I'm going to remove this for now, but will leave a TODO that it could be reintroduced if appropriate checks on the SW parameters are performed. Not sure if there are any other similar assumptions lurking...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344:24,optimiz,optimization,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:488,perform,performance,488,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457,1,['perform'],['performance']
Performance,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['perform'],['performance']
Performance,"I've never used these, but https://github.com/jvm-profiling-tools could potentially be a source for java profilers. From reading a bit about hprof, it seems to add a lot of overhead and has questionable accuracy. About workspaces with lots of contigs/smaller contigs -- the performance issue there is mostly during import. In your experiment above to subset the workspace, did the subsetted workspace return faster for SelectVariants? Or use less memory? I'm a bit surprised if so since your query is restricted to just that single array anyway. Regarding @jjfarrell's suggestion of ReblockGVCFs -- I can't speak to any loss of precision, etc there but I would be curious to see if you could run some of your input GVCFs through that, just to see how much smaller they seem to get.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696:274,perform,performance,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211399696,1,['perform'],['performance']
Performance,I've performed a release of 4.0.9.0 using the poms generated with https://github.com/broadinstitute/gatk/pull/5224 and a bit of manual work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910:5,perform,performed,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5212#issuecomment-424502910,1,['perform'],['performed']
Performance,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:653,Load,Loading,653,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363,1,['Load'],['Loading']
Performance,"IBBLE : false; >; > 16:17:05.844 INFO HaplotypeCaller - Deflater: JdkDeflater; >; > 16:17:05.844 INFO HaplotypeCaller - Inflater: JdkInflater; >; > 16:17:05.844 INFO HaplotypeCaller - GCS max retries/reopens: 20; >; > 16:17:05.844 INFO HaplotypeCaller - Requester pays: disabled; >; > 16:17:05.845 INFO HaplotypeCaller - Initializing engine; >; > 16:17:05.928 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:4985,Load,Loading,4985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"IIRC, the main commit in ADAM that would effect BQSR or HC was https://github.com/bigdatagenomics/adam/commit/1eed8e8e464f8f92a6e87afc1d334e751423e810#diff-abb5cd690409453a589fa8aadbfd7151 which _improved_ the performance of extracting regions from a 2 bit file. I run GATK4 HC pretty regularly with this change and haven't seen performance issues on datasets ranging in size from small targeted datasets up to WGS. Anywho, let me know if there's anything I can do to help debug the perf issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617:210,perform,performance,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-366078617,2,['perform'],['performance']
Performance,"ITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2937,multi-thread,multi-threaded,2937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['multi-thread'],['multi-threaded']
Performance,"If I understand correctly, are you saying that the pseudocode for `loadIntervals` in case of `IntervalSetRule.UNION` is:. ```; initialize cumulative interval list to empty list; for (intervalString in intervals) {; parse the next interval; *recalculate* cumulative intervals = union(next interval, cumulative intervals); // as opposed to cumulative intervals.add(next interval); }; ```; which scales quadratically?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635:67,load,loadIntervals,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-341729635,1,['load'],['loadIntervals']
Performance,"If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. If tagging/filtering rare germline is still a concern, then I'd say the next step is to see whether simply changing segmentation parameters to artificially decrease resolution and/or simple length-based filtering suffices. Finally, simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250:997,perform,performed,997,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-458551250,2,['perform'],['performed']
Performance,"In addition, I haven't been very through about how the random number generator is interacting with the multi-threading part, so just to be cautious: https://stackoverflow.com/questions/22313552/contention-in-concurrent-use-of-java-util-random",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217:103,multi-thread,multi-threading,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5146#issuecomment-419549217,2,"['concurren', 'multi-thread']","['concurrent-use-of-java-util-random', 'multi-threading']"
Performance,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:381,load,loading,381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['load'],['loading']
Performance,"In order to get it running though you will need to install the following things on each machine using apt-get: gawk, sysstat, and perf-tools-unstable. Additionally as root, you will have to set the /proc/sys/kernel/perf_event_paranoid variable from 1 to 0. For these tasks it might be possible to automate these steps by updating the system image that is used to setup dataproc clusters. In order to actually run and install PAT, you will need to download it from [here](https://github.com/intel-hadoop/PAT/tree/master/PAT) and add all the machines and ssh ports (including the master) in your cluster to the ""ALL_NODES"" setting in the config.template -> config file. You will also have to setup an SSH key to root on the cluster, which can be done with the command `gcloud compute ssh` and set the ""SSH_KEY"" variable in the config file to point to the google_compute_engine file in roots .ssh directory (public keys should have automatically been distributed to the other nodes). . At this point you need simply input the command line command you wish to run into the ""CMD_PATH"" variable and run ./pat run. I recommend running a spark-submit job using yarn-client as master. NOTE: the output will be a directory containing an excel spreadsheet and a bunch of data for each cluster. You will need to open the spreadsheet on a windows copy of excel and use ""control+q"" to run the macros that load the data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495:1391,load,load,1391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1986#issuecomment-234947495,1,['load'],['load']
Performance,"In particular, can we improve the performance of `--strict` mode to the point where it could become the default?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5263#issuecomment-460399213,1,['perform'],['performance']
Performance,Integration tests look okay (there's a Python one that says to skip if it's in docker):; - org.broadinstitute.hellbender.utils.python.StreamingPythonExecutorIntegrationTest#testRequirePythonEnvironment; Unit tests:; - org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest#testLikelihoodsFromHaplotypes; - org.broadinstitute.hellbender.utils.io.IOUtilsUnitTest#testUnsuccessfulCanReadFileCheck (intended to be skipped); - fixed ; No variant calling tests ignored; No python tests ignored; No R tests ignored. The PairHMM one returns:; ```; 03:44:48.410 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga7585161099923450811.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); ```; I don't know if that's expected or not -- maybe yes because it's looking for an FPGA?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345:601,load,load,601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5339#issuecomment-592086345,1,['load'],['load']
Performance,"IntelliJ seems to be having a bit of difficulting loading GATK these days:; > build.gradle error (413,0); > Received fatal alert: protocol_version",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516:50,load,loading,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2642#issuecomment-298437516,1,['load'],['loading']
Performance,"Interesting, it's definitely possible it's coming from one of the other buckets. I don't think we have fine grained control over WHICH bucket we attempt to read requester pays status from, so it's possible if it's enabled it's necessary to have that permission on every bucket. It's annoying that the error message doesn't say which reader is performing the access. Is there a longer stack trace available?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586:343,perform,performing,343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492#issuecomment-934908586,1,['perform'],['performing']
Performance,"Interesting, that's somewhat disturbing news, I wonder if we're paying for ssd's without actually being able to use them... It's also possible there's a different setting that's configuring the ssd's to be used for shuffle output. . We should investigate this further and 1) see if setting spark.local.dir makes a performance difference 2) ask the dataproc team about this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564:314,perform,performance,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2426#issuecomment-283418564,1,['perform'],['performance']
Performance,"Interesting. At some point we do expect to see diminishing returns when adding more cores, because the parts that parellelize poorly will start to dominate, but I would hope that it's with more than 16 cores... . One thing we've seen is that performance can be harmed by using to many cores / executor. We've seen problems due to lock contention within spark on executors that have more than ~8 cores. I might try running 4 executors with 8 cores each and seeing if that's an improvement vs 2 executors with 16 cores each. . We're currently heavily re-writing MarkDuplicatesSpark which may be an improvement in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207:242,perform,performance,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4516#issuecomment-371856207,1,['perform'],['performance']
Performance,"Interesting. Sorry this is causing so much trouble. From one of your above comments I wasn't clear if the solution using `--conf 'spark.submit.deployMode=cluster'` work correctly or not. . Is it possible that it's correct behavior for it to fail with the linkage error? According to the [mapr doc](https://maprdocs.mapr.com/52/DevelopmentGuide/c-loading-mapr-native-library.html) that command causes it to expect the application to load the library itself, but GATK by default doesn't have a copy of MAPR and won't load it on it's own. Have you included the mapr library somehow into the gatk jar? Or is it provided to spark some other way? I don't really know how maprfs works and how it interacts with hadoop paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653:346,load,loading-mapr-native-library,346,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3933#issuecomment-350315653,6,['load'],"['load', 'loading-mapr-native-library']"
Performance,Is there a way to have java load a config file as system properties on startup?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998:28,load,load,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267124998,1,['load'],['load']
Performance,"Is your GenomicDB named as your contig ? If so it may produce an error if your working directory is the Genomic DB folder, see my comment here : . https://gatk.broadinstitute.org/hc/en-us/community/posts/26901826831899-GenotypeGVCFs-issue-when-using-include-non-variant-sites-parameter. In addition the option -L needs to be set in in both the GenomicDBimport step and the GenotypeGVCFs step, see below : . `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx15g -Xms4g"" GenomicsDBImport --genomicsdb-workspace-path ""${INDIR}GenomicDB/${CONTIG_NAME}"" --batch-size 50 -L $CONTIG --sample-name-map ""${INDIR}aspat_gvcf_clean.sample_map"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --reader-threads 7 --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader true `. `singularity exec --bind /nvme/disk0/lecellier_data:/nvme/disk0/lecellier_data /home/hdenis/gatk_latest.sif gatk --java-options ""-Xmx58g"" GenotypeGVCFs -R $REF_3 -V ""gendb://${INDIR}GenomicDB/${CONTIG_NAME}"" -O ""${OUTDIR}aspat_clean_${CONTIG_NAME}.vcf.gz"" --tmp-dir /nvme/disk0/lecellier_data/WGS_GBR_data/tmp --include-non-variant-sites true -L $CONTIG --only-output-calls-starting-in-intervals true`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735:816,optimiz,optimizations,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415#issuecomment-2212079735,1,['optimiz'],['optimizations']
Performance,Issue solved after updating to the recent version and using the `--genomicsdb-shared-posixfs-optimizations true` flag.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928:93,optimiz,optimizations,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007#issuecomment-748076928,1,['optimiz'],['optimizations']
Performance,"It is not designed to enable you to do that. It uses a scatter-gather approach for parallelism. You'll want to submit multiple tasks that each process a disjoint interval of the genome. Personally, I have wondered how Mutect2 performs for variants at the borders of interval boundaries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449:226,perform,performs,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7688#issuecomment-1046371449,1,['perform'],['performs']
Performance,"It looks like at one point GATK 3.8 was available via the [homebrew science tap](https://github.com/ilovezfs/homebrew-science/blob/master/gatk.rb). I've tried adding their formula to my homebrew formula folder and installing via ``` brew install gatk.rb``` but there's a ton of errors. ```Updating Homebrew...; ==> Downloading https://github.com/broadgsa/gatk-protected/archive/3.8-1.tar.gz; Already downloaded: /Users/timothystiles/Library/Caches/Homebrew/gatk-3.8-1.tar.gz; ==> mvn package -Dmaven.repo.local=${PWD}/repo; Last 15 lines from /Users/timothystiles/Library/Logs/Homebrew/gatk/01.mvn:; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3; @; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR]; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/pom.xml) has 1 error; [ERROR] Non-parseable POM /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /private/tmp/gatk-20180118-71498-skz9cg/gatk-protected-3.8-1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR]; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR]; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586:441,Cache,Caches,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4164#issuecomment-358697586,1,['Cache'],['Caches']
Performance,It looks like it made the tests substantially slower.... I'm not totally clear on why. Maybe because it has to re-optimize code every time it restarts the jvm.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663:114,optimiz,optimize,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093#issuecomment-521372663,2,['optimiz'],['optimize']
Performance,"It looks like the errors are all of the flavor:. ```; Unable to load Maven meta-data from https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml.; > Could not GET 'https://artifactory.broadinstitute.org/artifactory/libs-snapshot/com/github/samtools/htsjdk/2.9.1-34-gd7bae17-SNAPSHOT/maven-metadata.xml'. ; Received status code 401 from server: Unauthorized; ```; Perhaps Maven's temporarily in a bad mood, we'll have to try again later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180:64,load,load,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2879#issuecomment-306264180,1,['load'],['load']
Performance,"It looks like the tests are running very slowly because of a performance regression due to the changes in `IntegrationTestSpec.assertEqualTextFiles`. You can see this on tests that should be unaffected by the core changes in this PR, like the VQSR integration tests, which have no cloud/bucket dependency and usually take about [1 minute](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24775.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html), but took [25 minutes](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_24563.2/tests/test/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.VariantRecalibratorIntegrationTest.html) with this branch. The same thing happens when the VQSR tests are run locally with this branch; all the time is spent in `assertEqualTextFiles`. @jean-philippe-martin I don't want to push to this branch without your ok, but reverting the first few lines of `assertEqualTextFiles` seems to fix the problem locally. (Separately, I will do some profiling to figure out for posterity sake why that change had such a dramatic affect ).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143:61,perform,performance,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-462772143,1,['perform'],['performance']
Performance,It looks like we need to update mockito. ; https://storage.googleapis.com/hellbender-test-logs/build_reports/master_27538.13/tests/test/index.html. ```. java.lang.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassB,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:493,load,loading,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,5,['load'],"['load', 'loading']"
Performance,"It seems like the problem was that the researcher was starting with a coordinate-sorted bam, whereas `MarkDuplicatesSpark` requires a name-sorted bam for good performance. @jamesemery feel free to close once you're satisfied that this is resolved, and once we've made whatever additional documentation clarifications are warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512:159,perform,performance,159,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670#issuecomment-463756512,1,['perform'],['performance']
Performance,"It seems plausible to me, though, that the Google auth library may have been patched to perform checks that it wasn't performing previously. Maybe our project permissions have always been mis-configured :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762:88,perform,perform,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-330940762,2,['perform'],"['perform', 'performing']"
Performance,"It was a feature which we would have loved, but alas this isn't the case. We also relied on ```-ct``` to get percent of bases depending on their coverage (ex. 20x) which has now been dropped in GATK 4+ versions. We came across another tool called [mosdepth](https://github.com/brentp/mosdepth). When compared to DepthOfCoverage - ; - It uses multithreading (albeit only for deflation, so no performance gains when going beyond 4 threads).; - It gives coverage for exome within 5 minutes, and even faster when we don't need the per base coverage output.; - Per base coverage output can be skipped using ```-x``` the output of this matches closely to output from DepthOfCoverage. Do keep in mind, DepthOfCoverage also supports this skipping when using the parameter ```--omitDepthOutputAtEachBase``` which saves massively on I/O and cuts processing time from 50 minutes per sample to 40 minutes per sample. If you do decide to give it a try, we have some tips and suggestions - ; - The tool generates multiple output files. If looking for total coverage, check the last line of file ```output.mosdepth.summary.txt```; - If looking for percent of bases covered at target read depth, this information is present in file ```output.mosdepth.region.dist.txt```. If your target read depth is 20x, you can search this file with ```grep -P ""total\t20\t""``` and the third column should be the percentage (with only one decimal); - By using ```-d4``` switch, they claim the above percentage granularity increases to 4 decimal points.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071:391,perform,performance,391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7890#issuecomment-1153478071,1,['perform'],['performance']
Performance,It was cursed with a bad cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400:25,cache,cache,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5120#issuecomment-423018400,1,['cache'],['cache']
Performance,"It won't be able to run any faster than BWA mem does with a similar number of cores, since it is essentially just running bwamem. It's potentially faster as part of a spark pipeline so you can load and process data once instead of saving the data to disk and reloading it repeatedly. . The complete list of spark configuration parameters is available on the [spark docs](https://spark.apache.org/docs/3.5.0/configuration.html). Many of them are not relevant in local mode. From what I understand the local mode is going to execute as a single executor with the number of cores specified in the `local[#]` block ( or the total number of system threads if it's set to `*`) It will use the available memory that java is configured with. I'm pretty sure it's ignoring the memory and configuration parameters you've set. Those will be relevant if you configure a stand alone spark cluster (potentially one running exclusively on your local machine). . Our spark tools are not being actively developed for the most part. We've moved away from them to use single threaded tools widely sharded and managed by cromwell. The additional complexity of the spark environment made it hard to see much benefit when most of the tools are embarassingly parallel and easily shardable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066:193,load,load,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8897#issuecomment-2214866066,1,['load'],['load']
Performance,It would help performance if we used constants instead of arrays in PairHMM. I'll investigate.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961:14,perform,performance,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2036#issuecomment-288540961,1,['perform'],['performance']
Performance,It's not likely that the reference loading issue will be fixed this month @huangk3. I'm answering in @lbergelson stead as he's out for a few weeks. This is on the team's radar so there is some chance that it could be but we are unable to say for sure.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030:35,load,loading,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-251757030,1,['load'],['loading']
Performance,"It's on my list. Pretty near the bottom, but it's there. Runtime is probably getting up near an hour for big jobs. The memory requirements are horrific, because we load all the variants into memory and then we don't even use them all! For the biggest cohorts we use 104GB. I wish I was joking. If sklearn can minibatch GMMs then that would be amazing. We use a maximum of 2.5M variants for training and number of annotations/dimensions is O(10). The smallest exome cohort would train with about 80,000 variants with about 3GB of memory. That being said this definitely isn't the biggest cost contributor for joint calling, and hopefully all the sporadic failures have been hammered out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198:164,load,load,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6425#issuecomment-594061198,1,['load'],['load']
Performance,"It's possible that GenomicsDB was also optimized for diploids and thusly slower for other ploidies because a lot of the code was ported from GATK. I think `new-qual` is more likely to help, but you can certainly try CombineGVCFs. I don't have a lot of good benchmarks, honestly, but maybe @lbergelson does. GATK3 GenotypeGVCFs for 20,000 human samples took 113.54 hours for about 1.9Mbp, but that was on a CombinedGVCF already extracted from GenomicsDB. With that many samples there are a lot of multi-allelics so that part should be similar to your data. The java options were `java -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx11500m`, so still less memory than your task seems to need. And without `new-qual`. It also seems to be much faster than your run, but it's hard to say how runtime should scale with the number of samples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452:39,optimiz,optimized,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4467#issuecomment-370477452,1,['optimiz'],['optimized']
Performance,It's some sort of race condition; ```; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:80); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:156); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:134); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:110); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:18,race condition,race condition,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['race condition'],['race condition']
Performance,"Its the same optimizations for level as we discussed before, no new optimizations. The previous merge did not include all the optimizations (error on my part)... hence I have to do a new PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035:13,optimiz,optimizations,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379#issuecomment-364265035,3,['optimiz'],['optimizations']
Performance,Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <Ã¸> (Ã¸)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `64.706% <Ã¸> (-13.072%)` | :arrow_down: |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <Ã¸> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `73.529% <45.161%> (-4.248%)` | :arrow_down: |; | [...stering/BayesianGaussianMixtureModelPosterior.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxQb3N0ZXJpb3IuamF2YQ==) | `58.333% <58.333%> (Ã¸)` | |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_cam,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:3585,scalab,scalable,3585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,Jar on Maven central updated - please clear any cached jars,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988:48,cache,cached,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-295584988,2,['cache'],['cached']
Performance,"Java implementation of segmentation is now in the sl_wgs_segmentation dev branch, with a few simple unit tests. I'll expand on these and add tests for denoising in the future, but for now we have a working revised pipeline up through segmentation. The CLI is simply named ModelSegments (since my thinking is that it could eventually replace ACNV). I ran it on some old denoised exomes. Runtime is <10s, comparable to CBS. Here's a particularly noisy exome:. CBS found 1398 segments:; ![cbs](https://user-images.githubusercontent.com/11076296/30165095-cdf6251a-93ac-11e7-91fb-dcc8f48fe07f.png). Kernel segmentation with a penalty given by a = 1, b = 0 found 1018 segments:; ![kern](https://user-images.githubusercontent.com/11076296/30165106-dbbe0b40-93ac-11e7-99ec-5d58d8417d8b.png). Kernel segmentation with a penalty given by a = b = 1 (which is probably a reasonable default penalty, at least based on asymptotic theoretical arguments) reduced this to 270 segments :; ![kern-smooth](https://user-images.githubusercontent.com/11076296/30165113-e2b545a8-93ac-11e7-97a9-a692e43ebbdf.png). The number of segments can similarly be controlled in WGS. WGS runtime is ~7min for 250bp bins, ~30s of which is TSV reading, and there is one more spot in my implementation that could stand a bit of optimization, which might bring the runtime down. In contrast, I kicked off CBS 45 minutes ago, and it's still running... @LeeTL1220 this is probably ready to hand off to you for some WDL writing and preliminary evaluation. ; Although I can't guarantee that there aren't bugs, I ran about ~80 exomes with no problem. We can talk later today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936:1289,optimiz,optimization,1289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-327797936,2,['optimiz'],['optimization']
Performance,"Just a quick test of `--splitMultiallelics` looks good:; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250_splitmultiallelics.vcf.gz --splitMultiallelics; 17:52:19.004 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:52:19 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:52:19.130 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:52:19.131 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:52:19.131 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:52:19.131 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:52:19.131 INFO LeftAlignAndTrimVariants ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971:811,Load,Loading,811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418893971,1,['Load'],['Loading']
Performance,Just adding a note here that `FeatureCache` should eventually be refactored to use the simplified Interval class (when it exists) to track cache boundaries and compute overlap.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630:139,cache,cache,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/100#issuecomment-76229630,1,['cache'],['cache']
Performance,"Just for future reference, note that comments in `testVariantRecalibratorSNPMaxAttempts` are also incorrect or out of date. The test passes even if you limit it to one attempt. ```; // For this test, we deliberately *DON'T* sample a single random int as above; this causes; // the tool to require 4 attempts to acquire enough negative training data to succeed; ```. So again, the tests were already ""broken."" But still, rather than attempt to fix them, I think it's best to follow the principle of not changing both production and test code to the extent that it is possible in this scenario. We've already updated enough exact-match expected results to make me a bit uncomfortable!. Someone else may want to tackle fixing the tests in a separate push, but I think it makes sense for me to focus on avoiding these sorts of issues when writing tests for the new tools. EDIT: For the record, I confirmed that the undesired behavior in this test that the RNG hack was trying to avoid was fixed (and hence, the test was ""broken"") in #6425. Probably wasn't noticed because this is the only non-exact-match test and the test isn't strict enough to check that attempts 1-3 fail, it only checks that we succeed by attempt 4. Again, someone else may feel free to examine the actual coverage of this test and whether it's safe to remove it and/or clean up all the duct tape---but at some point, it becomes difficult to tell which pieces of duct tape are load bearing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628:1444,load,load,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628,1,['load'],['load']
Performance,"Just tested this locally outside of the Docker. I do not see the WARN:; ```; WMCF9-CB5:shlee$ gatk40110 LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv ; Using GATK jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar LearnReadOrientationModel -alt-table 13_tumor-alt.tsv -ref-hist 13_tumor-ref.metrics -alt-hist 13_tumor-alt-depth1.metrics -O 13_tumor-artifact-prior-table.tsv; 12:16:19.960 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.0.11.0/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Nov 26, 2018 12:16:20 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:16:20.176 INFO LearnReadOrientationModel - ------------------------------------------------------------; 12:16:20.177 INFO LearnReadOrientationModel - The Genome Analysis Toolkit (GATK) v4.0.11.0; 12:16:20.177 INFO LearnReadOrientationModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:16:20.177 INFO LearnReadOrientationModel - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:16:20.177 INFO LearnReadOrientationModel - Java run",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615:816,Load,Loading,816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441721615,1,['Load'],['Loading']
Performance,"Just to add - GenomicsDB relies on the filesystem for integrity checks just like any other file on the filesystem. We could add integrity checks at a micro chunk-level, but that would be at the expense of performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866:205,perform,performance,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-713055866,1,['perform'],['performance']
Performance,"Just wanted to add - I just tried installing the GATK docker as described here: https://gatk.broadinstitute.org/hc/en-us/articles/360035889991--How-to-Run-GATK-in-a-Docker-container. As I'd think that all software dependencies and whatnot should be fine. However, I still get the same error message:. /gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads \; > -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam \; > --tmp-dir /gatk/my_data/temp -O thing.bam; Using GATK jar /gatk/gatk-package-4.1.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx25g -jar /gatk/gatk-package-4.1.3.0-local.jar SplitNCigarReads -R Homo_sapiens.GRCh38.dna.primary_assembly.fa -I subset_TINY_rehead.bam --tmp-dir /gatk/my_data/temp -O thing.bam. 21:12:14.158 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Mar 02, 2023 9:12:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:12:16.383 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.384 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK) v4.1.3.0; 21:12:16.384 INFO SplitNCigarReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:12:16.384 INFO SplitNCigarReads - Executing as root@9d399eec0e24 on Linux v5.19.0-32-generic amd64; 21:12:16.384 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 21:12:16.384 INFO SplitNCigarReads - Start Date/Time: March 2, 2023 9:12:14 PM UTC; 21:12:16.385 INFO SplitNCigarReads - ------------------------------------------------------------; 21:12:16.385 INFO SplitNCigarReads - ----------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826:928,Load,Loading,928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452564826,1,['Load'],['Loading']
Performance,LGTM!. Have any numbers on the performance improvement? It wasn't easy to see an old run before Jasix that would be a good 1-to-1 comparison.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843:31,perform,performance,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8133#issuecomment-1355651843,1,['perform'],['performance']
Performance,"Let us take an example. Suppose, we configure GenomicsDB with 3 column partitions - 0-10, 10-100, 100-300 and want to run GenomicsDBImport tool with an interval [0,100]. In this case, the import tool will only write contigs between 0 and 100 into first two partitions (according to the loader JSON file). Is this what you had in mind? The command line will look like:; $ gatk-launch GenomicsDBImport -L 0-100 --loaderJSONfile loader.json --streamIdJSONFile stream.json. This can definitely be done. However, the client still needs to know the column partitions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931:286,load,loader,286,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277372931,3,['load'],"['loader', 'loaderJSONfile']"
Performance,"Let's discuss further before you get too far along. The design of the Collections code was intended to ensure that very strict file formats are adhered to within the CNV pipeline. Making it more flexible to accommodate TSVs with arbitrary column headers, relax requirements for sequence dictionaries, etc. undermines that goal. There are also two other issues to consider:. 1) It looks like @jonn-smith has also been putting considerable effort into building a TSV framework for Funcotator. Perhaps CombineSegmentBreakpoints should consider using that framework instead, if it is more appropriate. We can also discuss bringing the CNV pipeline over into that framework, but this should definitely wait until after release. The end goal is for CNV team to spend as little time as possible writing or maintaining any code related to TSV parsing. 2) @mbabadi has put together some python evaluation code for the new gCNV, which makes use of the IntervalTree python package and PyVCF to accomplish some things that are very similar to what CombineSegmentBreakpoints is doing. Perhaps we could implement a similar approach purely in Java by making use of the IntervalTree implementation in htsjdk. I think for now we should treat CombineSegmentBreakpoints as a one-off tool to be used for internal validations. After release, we should design a more generic evaluation tool. This tool could take as input multiple collections of annotated locatables, with a few rigidly defined formats allowed (e.g., VCF, CNV Collection TSVs, perhaps some TSVs from other tools, etc.), with one designated as ground truth. The regions for evaluation could also be specified via -L (since it is possible this might not completely specified by the ground-truth collection). The appropriate intersections and lookups could then be performed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616:1807,perform,performed,1807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3995#issuecomment-352860616,1,['perform'],['performed']
Performance,"Let's talk about groupReadPairs, pairedReads, and unpairedReads. The groupBy method called by groupReadPairs is very expensive both in time and in memory. It's a full hash shuffle of GATKReads (time expensive), that results in a gazillion 1- and 2-element Lists (memory expensive). So you certainly don't want to do it twice. But the way pairedReads and unpairedReads is set up, you *will* do it twice if you want to process both paired and unpaired reads. (And even if you aren't, someone else might try to use this code to do so.). So my first suggestion is that you remove the call to groupReadPairs from pairedReads and unpairedReads, and let a user groupReadPairs once, and reuse the resulting JavaPairRDD to process paired and unpaired reads. My second suggestion is quite a bit more complicated, but I think it would result in far better performance. I'll sketch it out here, and then I can explain it further in person, if it's a direction you'd like to pursue. The first step is to create a JavaRDD<GATKRead> in which all pairs sharing a template name are in the same partition (but without grouping them). To do that, you temporarily boost the input JavaRDD into a JavaPairRDD<String,GATKRead> by extracting the read name as a key. Then you repartition (to do the shuffle). Then you map back to an ordinary JavaRDD<GATKRead> by keeping just the value. (Note: if the BAM has queryname sort order, you can just skip this step entirely.); Now you can do a mapPartition operation to filter for paired or unpaired reads: Iterate over the reads in the partition, and keep a hash map of [name -> read] of reads that have not yet found mates. To filter for paired reads, whenever you find the name of the current read already in the table, just emit the current read and the read in the map as a pair, and delete the read from the map (you're done with that name -- this keeps the table smaller). To filter for unpaired reads, just delete any map entry that you successfully look up, and insert any ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039:845,perform,performance,845,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-299955039,1,['perform'],['performance']
Performance,Look how much cleaner these tests are with the loader api. It would have saved me so much pain if we'd had it a month ago :/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2106#issuecomment-241125956:47,load,loader,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2106#issuecomment-241125956,1,['load'],['loader']
Performance,"Looking at the LL score cutoff from the plot above (around -.35) shows that the recall is expected to be quite low at this cutoff. This is also the case when looking at NA12878 with GiaB truth as is seen in the ROC plot below. The group of red points to the right is the region where the F1 score is maximized, while the purple points to the left represent the cutoff using the LL score. It looks like the LL score is much too conservative and chooses a sensitivity around .4 even when the overall ROC curve looks pretty good and a more optimal tradeoff exists. . ![image](https://user-images.githubusercontent.com/13020550/176241604-381d3a64-ee7f-4cf7-8ebe-bf18ae967301.png). For now we can afford to use a held out truth sample to tune this threshold so I don't think we need to use the LL score at this point in time. It would still be good to look into using the positive and negative model (this version only used the positive model) and plot the F1 and LL scores on the same plot to dig into what's going on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1169000346:733,tune,tune,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1169000346,1,['tune'],['tune']
Performance,"Looking at the htsjdk code responsible for the original throw (as far as I can see in the stack enclosed in the description) there is a few ""smells"" in the way synchronized is used or not use ReferenceSource.java. It is likely to be the reason behind the error considering that is failing in multi-thread. . Probably adding synchronized to getReferenceBasesByRegion would fix that. Is a htsjdk issue and not a GATK one. Do you want to add a workaround in GATK or press for a fix and update of the htsjdk dependency. @droazen?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392:292,multi-thread,multi-thread,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376298392,1,['multi-thread'],['multi-thread']
Performance,MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:2754,scalab,scalable,2754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"Marissa Powers here -- I'm an Intel engineer on the same team as Ed. It sounds like we all agree on having Intel-optimized TF as the default and figuring out the best intervention for older machines from there. We can add the AVX flag within CNNScoreVariant (and any other AI tool). From there, we can (1) provide a detailed error output describing the issue, (2) provide a non-AVX TF build, and (3) automatically roll back TF to the provided version. @EdwardDixon, it sounds like @cmnbroad is suggesting is options (1), (2), and (3), while you're suggesting just (1). Sound right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667:113,optimiz,optimized,113,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429409667,1,['optimiz'],['optimized']
Performance,"Modifying what I wrote earlier, got confused with another issue. I am not familiar with Lustre and Lustre configuration. Did the excessive file locking from Lustre(FUTEX_WAIT_PRIVATE?) go away with `--genomicsdb-shared-posixfs-optimizations`? . Is there anyway to configure Lustre buffer sizes for writing? If not, can you try setting environment variable TILEDB_UPLOAD_BUFFER_SIZE to something like 5242880(5M) and try `GenomicsDBImport`? Does it help with performance? Is the amount of file locking lower than before?. If the performance is still not acceptable... What version of gatk are you using? Can you use the latest gatk and try using the `--bypass-feature-reader` option with `GenomicsDBImport`? Does this help with performance?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554:227,optimiz,optimizations,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039746554,4,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Most of the changes in GenomicsDB 1.3.0 was import and performance related. However, #6500 did add support for MNPs, not sure if that played a part here and the default streaming of data from GenomicsDB is now VCFCodec whereas it was BCFCodec before. @mlathara, can you think of anything else here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-670816626:55,perform,performance,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-670816626,1,['perform'],['performance']
Performance,"Most of the scaling issues in Cromwell/Terra have been resolved. Terra still has limitations on workflow metadata size, and passing long file arrays to ever task in large scatters (i.e. the full list of counts files is passed into every gCNV shard) can limit our batch sizes for workflows that embed gCNV (e.g. GatherBatchEvidence in gatk-sv). gCNV workflows also don't call cache reliably (presumably due to timeouts) probably again due to the large file arrays, including 2D arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236:375,cache,cache,375,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236,1,['cache'],['cache']
Performance,"Multiple causes can cause closed connections when reading from GCS, almost all of which are outside of our control. This will never be ""completely fixed"" in the sense that even if the code is perfect it's completely possible to send too many requests to GCS, and it'll respond by closing connections. The main factors that I know of are:. - number of concurrent accesses to the GCS bucket in question; - number of concurrent accesses to the GCP project in question; - storage class of the GCS bucket in question (the more expensive ones have more replicas, thus can handle a higher load). If you're running into those difficulties I would suggest trying to reduce the load (reduce the number of concurrent workers or threads) and making sure it's not a single-region storage bucket. If that fails, perhaps try using a different bucket that no one else is also using (to reduce other sources of load). If I understand correctly that you didn't change the version you're using but are suddenly seeing more issues than before, then perhaps the cause is a server-side change from GCS (outside of our control), a change in configuration (are you reading from a bucket of a different class from before), or perhaps just an increase of other activity on the same bucket/project. The current code is very persistent in its retries: as you can see from the messages it spent a whole half hour waiting. If it's an overload situation then you may get better performance by reducing the worker count (as they will have to retry less).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716:351,concurren,concurrent,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631#issuecomment-526270716,7,"['concurren', 'load', 'perform']","['concurrent', 'load', 'performance']"
Performance,"My builds are queued, but not started for more than 4 hours",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305019422:14,queue,queued,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2733#issuecomment-305019422,1,['queue'],['queued']
Performance,My calculations give a runtime estimate of $0.18 for the 1D and... more for the 2D. We'll optimize later. It'll be fine.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363560735:90,optimiz,optimize,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363560735,1,['optimiz'],['optimize']
Performance,"My knowledge about .so files, linkers and architectures is very limited but to me it sounds like the GKL library is part of the jar and GATK tries to load it from there. And reading [this](https://github.com/broadinstitute/gatk/issues/2302) thread it sounds like the GKL library of GATK should work on ppc. Because of the ""no such file or directory"" messages (in the first warnings) I have also tried to point the `--tmp-dir` parameter of HaplotypeCaller to my home directory to make sure that it's not just a permission problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687564287:150,load,load,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687564287,1,['load'],['load']
Performance,Nevermind Chris. I've found the documentation for the feature at https://cromwell.readthedocs.io/en/stable/optimizations/FileLocalization/.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155:107,optimiz,optimizations,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155,1,['optimiz'],['optimizations']
Performance,NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0YS5qYXZh) | `75.510% <100.000%> (+1.283%)` | :arrow_up: |; | [.../hellbender/utils/genotyper/AlleleLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_sourc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:3185,scalab,scalable,3185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:131,optimiz,optimized,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,1,['optimiz'],['optimized']
Performance,"Not sure what your `GenotypeGVCFs` command was, but did you use the `--genomicsdb-shared-posixfs-optimizations` option? This option is available for the import too and may improve your performance.; ```; --genomicsdb-shared-posixfs-optimizations <Boolean>; Allow for optimizations to improve the usability and performance for shared Posix; Filesystems(e.g. NFS, Lustre). If set, file level locking is disabled and file system; writes are minimized. Default value: false. Possible values: {true, false} ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166:97,optimiz,optimizations,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879779166,10,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"Note that GATK3 uniquified the `GATKCommandLine` lines using a scheme like this:. ```; ##GATKCommandLine.SelectVariants.2=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:48:45 EST 2016"",Epoch=1452019725506,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=[3:113005755-195507036] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=true never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=variant source=/humgen/gsa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:962,perform,performanceLog,962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,2,['perform'],['performanceLog']
Performance,"Note that no segmentation or resolution parameters have been tuned yet for either performance or runtime. Some of these are very easy wins. For example, `kernel-approximation-dimension` is set to a default of 100, and the time for segmentation scales roughly linearly with this (documentation erroneously states that the scaling is quadratic, this should be fixed---my bad). In practice, setting this to as little as 2 seems to work OK for some cases, so we should evaluate this more rigorously. This can cut WGS segmentation down from ~10 minutes (out of the total ~60 minutes for 250bp bins, typically) to ~1 minute.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380:61,tune,tuned,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4122#issuecomment-461607380,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,Note to self: move the purging of the cache to the end of execution and test that leftover cache values are never present after invoking referenceConfidenceModel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-489242346:38,cache,cache,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911#issuecomment-489242346,2,['cache'],['cache']
Performance,Note: it looks like this doesn't fix the ConcurrentModificationException during serialization while running the unit tests on my Picard branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581#issuecomment-329793309:41,Concurren,ConcurrentModificationException,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581#issuecomment-329793309,1,['Concurren'],['ConcurrentModificationException']
Performance,"Now run with `--maxIndelSize 250`.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz --maxIndelSize 250 -O zeta_snippet_leftalign_maxindelsize250.vcf.gz; 17:24:16.345 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 5:24:16 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 17:24:16.502 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 17:24:16.502 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 17:24:16.502 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:24:16.502 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 17:24:16.502 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 17:24:16.503 INFO LeftAlignAndTrimV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543:710,Load,Loading,710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418887543,1,['Load'],['Loading']
Performance,"Now that I look at https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer it will take a bit of work to get up to date (it was tied to a particular version of cromwell-tools and Advisor, which is python2-based, has not been actively maintained---exactly why I wanted to move to a more well supported solution...). . If it's not too much trouble, I'll try to get everything running locally on this older stuff for the first round of optimization, but this problem is probably a perfect candidate for the Neptune-based solution that @dalessioluca recently finished at https://github.com/dalessioluca/cromwell_for_ML.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710153874:90,optimiz,optimizer,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710153874,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"Now, it seems like calling `contaminationDownsampling` right after `retainEvidence` could cause problems if both methods remove reads. However, one might correctly point out that although the cache invalidation I mentioned is not handled systematically, the method `removeEvidenceByIndex` _does_ have some code to update the evidence by sample and the evidence index map. It's possible that this code is totally fine and that this lead is a dead end. However, the code looks like it could be simpler and it's tough to parse. For example, try to track the `to` variable, which determines the determination of the outer `for` loop:. ```; for (int etrIndex = 1, to = nextIndexToRemove, from = to + 1; to < newEvidenceCount; etrIndex++, from++) {; if (etrIndex < evidencesToRemove.length) {; nextIndexToRemove = evidencesToRemove[etrIndex];; evidenceIndex.remove(evidences.get(nextIndexToRemove));; } else {; nextIndexToRemove = oldEvidenceCount;; }; for (; from < nextIndexToRemove; from++) {; final EVIDENCE evidence = evidences.get(from);; evidences.set(to, evidence);; evidenceIndex.put(evidence, to++);; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697:192,cache,cache,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625030697,2,['cache'],['cache']
Performance,"O FilterAlignmentArtifacts - Requester pays: disabled; 19:11:57.326 WARN FilterAlignmentArtifacts - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:11:57.326 INFO FilterAlignmentArtifacts - Initializing engine; 19:11:57.666 INFO FeatureManager - Using codec VCFCodec to read file file:///output/sample.FilterMutectCalls.vcf.gz; 19:11:57.757 INFO FilterAlignmentArtifacts - Done initializing engine; 19:11:57.827 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 19:11:57.861 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 19:11:57.862 INFO IntelPairHmm - Available threads: 4; 19:11:57.862 INFO IntelPairHmm - Requested threads: 4; 19:11:57.862 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 19:11:57.862 INFO ProgressMeter - Starting traversal; 19:11:57.862 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; *** glibc detected *** /for/bar/bin/java: double free or corruption (out): 0x00007f450af58700 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x3d01675dee)[0x7f45058afdee]; /lib64/libc.so.6(+0x3d01678c80)[0x7f45058b2c80]; /tmp/libgkl_smithwaterman410767516409374085.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x7f4499f4cfa8]; /tmp/libgkl_smithwaterman410767516409374085.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x7f4499f4cbf8]; [0x7f44f58be6a2]; ======= Memory map: ========; ```. Then we **disabled** AVX2 in the newer cluster using Intels [sde64](https://software.intel.com/en-us/articles/intel-software-development-emulator) with `-ivb`, which directed GATK to use the Java implementation, and the filter worked without core dump. ```; sde64 -ivb -- faa.sh",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:4246,multi-thread,multi-threaded,4246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['multi-thread'],['multi-threaded']
Performance,"OK -- got it. I wasn't sure if the below comment was implying any better performance. . > Using this subset workspace seems to execute just fine as an input for SelectVariants. Sounds like you were just saying it worked, which is expected. As I said, I wouldn't expect the query to work any faster with such a single contig. I think some sort of profiling run, and trying ReBlockGVCFs on a few examples inputs are probably the best next steps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211417392:73,perform,performance,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1211417392,1,['perform'],['performance']
Performance,"OK @droazen @davidbenjamin I think this is ready for review. Note that:. 1) I did not restore the optimization introduced by @davidbenjamin in #5466. Happy to file an issue to restore it later by adding the appropriate parameter check, if we think it's important.; 2) I only added integration tests for HC and M2, since there is only a minimal test for FilterAlignmentArtifacts at this time. But I would think that these tests are enough to show that the exposure preserves behavior (unless I somehow got extremely unlucky with the test data and parameter values...); 3) Apologies to the reviewer for the somewhat complicated commit history, which resulted from introducing/removing the TSV stuff and was more trouble than it might be worth to reorder/resolve in the final rebase. I think it would be easiest for the reviewer to look at the 4 ""bubbled up..."" commits separately when reviewing the exposure of each parameter set, but then look at the overall commit when reviewing more superficial things or the tests.; 4) I'd appreciate it if the reviewer double checked that I did not switch anything up in parameter names, doc strings, default values, etc. when introducing the 12 explicit args in AssemblyBasedCallerArgumentCollection. Lots of copying and pasting there and would be easy to screw things up, as you might imagine! Pretty sure the tests bear out that this was done correctly, but you never know. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-897081017:98,optimiz,optimization,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-897081017,1,['optimiz'],['optimization']
Performance,"OK, I think I accidentally removed the `getopt` dependency in #3935. Not sure why tests didn't fail as expected. EDIT: I think this is because R dependencies are cached on Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359983165:162,cache,cached,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359983165,1,['cache'],['cached']
Performance,"OK, added an integration test to check that MKL is enabled. If we move the conda install of these packages into the base image, then we might need to perform these checks elsewhere, e.g., in the bash script for building the image. Gotta push the base image and update the main Dockerfile. I'll merge after the weekend unless there are any more comments. Again, @droazen please be sure to highlight that R plotting will now require the conda environment in the release notes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-622500992:150,perform,perform,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-622500992,1,['perform'],['perform']
Performance,"OK, relaxed the exact match to a delta of 1E-6 (chosen because doubles are formatted in somatic CNV outputs as `""%.6f""`) and tests pass on Travis (modulo an unrelated intermittent timeout failure). Note also that I was also able to reproduce locally by switching between Java 8 and 11. Had to add some quick test code for doing the comparisons; not actually sure if we have other utility methods to do so somewhere in the codebase. Another interesting note: I tried to clean up the offending use of log10factorial in AlleleFractionLikelihoods, but this introduced numerical differences at the ~1E-3 level. I think all of the round tripping between log and log10 actually adds up. Some digging revealed that this was introduced way back in gatk-protected in https://github.com/broadinstitute/gatk-protected/commit/aeec297e104db9f5196cb8f8e6691133302474bc#diff-34bd76cb2a416a212e25cbfb11298207265fb9cced775918aefcdb6b91ebc247. Despite the fact that we could easily replace the use of log10factorial with a private logGamma cache, at this point I think it makes more sense to freeze the current behavior. But if similar numerical changes are introduced to ModelSegments in the future, then it might make sense to clean this up at that point as well. Anyway, changed the title of the PR to reflect this update. Should be ready to go!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014:1021,cache,cache,1021,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014,1,['cache'],['cache']
Performance,"OK, thanks @droazen, that sounds sensible. Just pushed the rebase and will try to get to the rest of the changes this week. Just to clarify about running future evaluations/optimizations---I might need some pointers from whoever has been running the ""canonical"" evaluations (@michaelgatzen, maybe)? The ones I've run so far are on chr22 on the CHM mix, stratified in a few different ways (high/low complexity/confidence). We'll probably want to do something more thorough (e.g., akin to whatever is done for functional equivalence) to justify changing the defaults, I'm guessing. But we can cross those bridges once we get there!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-892034856:173,optimiz,optimizations---I,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-892034856,1,['optimiz'],['optimizations---I']
Performance,"OK, thanks @ldgauthier, I think I've addressed all the comments but one. A little TODO list for my benefit:. - [x] Updated the GATK version in the ExcessHet documentation to 4.2.2.0, but we'll see if I need to revisit that.; - [x] Not quite sure about the `ReducibleAnnotation` business. Let me know how to make these changes, or else happy to punt and file an issue.; - [ ] Also not sure I've parsed the results of the Jenkins tests, at least in terms of comparing how many sites get hard filtered with/out the change. Where should I be looking at to see the baseline result for that step? Also looks like a lot of results for https://gotc-jenkins.dsp-techops.broadinstitute.org/job/warp-workflow-tests/11755/ were call-cached, is that to be expected? Haven't looked at these tests before, so maybe you can walk me through them at some point. But I guess we can be sure that the overall results don't change too much (at least for 50 samples), which is a good start.; - [x] Didn't quite get to making those plots of the change in decision boundary, will do that tomorrow or later this week. EDIT: Nevermind, took like 5 minutes to throw them together (albeit using the slow python implementation and some for loops...), see below.; - [x] Hmm, looks like my own PR #6885 might've introduced a few more exact match test failures...grr. Here are some plots for N = 50, 100, and 500 samples showing (in black) those counts that previously fell under the 3E-6 threshold with the mid-p correction but now pass without it. As you can see, not much to sweat from these ""theoretical"" plots, but good to convolve with the actual allele frequency spectrum and get an idea of how many sites occupy these black squares in practice (as well as start us down the road of reexamining the threshold itself):. ![image](https://user-images.githubusercontent.com/11076296/132413689-37f3dfeb-e3f5-4869-a803-fe27f3cd79bd.png); ![image](https://user-images.githubusercontent.com/11076296/132413649-d716ee7d-6763-4275-82de-e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907:721,cache,cached,721,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-914612907,1,['cache'],['cached']
Performance,"OK. As a reference, how does GATK deal with max-alternate-alleles for normal human variant calling? Presumably really high alternate alleles would primarily happen in repetitive/index prone-regions? FWIW, When we execute GenotypeGVCFs, we run as ~1000 jobs where each takes an even chunk of the genome, by base pairs. . Yes, I did see the bypass-feature-reader option, but we have jobs in-flight and I'm reluctant to change too many things as once. We will try this when possible though. As far as number of batches imported: I would need to check, but I believe it's only ~5 batches with perhaps 50-100 samples/ea. So I guess it's not that many new batches in the scheme of things, but anecdotally we have noticed that with the last couple rounds of import we needed to reduce batch size to make it work (i.e. not get hung). It is conceivable there is some other factor that is causing that variable performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581:901,perform,performance,901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7542#issuecomment-964442581,1,['perform'],['performance']
Performance,"OK. In GATK3, the sharding size is calculated in `GenomeAnalysisEngine.getShardStrategy`. Since `GenotypeGVCFs` is a RodWalker and does not use input reads (BAM file), the shard size is `1,000,000`. ; This might be more than a thread safety bug (which is easy to fix, by making `GenotypingEngine.calculateOutputAlleleSubset() ` `synchronized`). What worries me is If the cache of upstream deletions spans intervals, this code will not work since the processing is asynchronous. For example, if there are 2 threads and the removed deletion crosses the shard barrier and the downstream interval thread is first to process, it will not see the upstream removed deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197:371,cache,cache,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197,1,['cache'],['cache']
Performance,"OK. StratManager is about 80% wired to do the merging this would require. It would require me to implement some existing methods, like VariantEvaluator.combine(), but the basic idea exists today in StratificationManager.combineStrats(). I'd probably propose to expose this via VariantEvalEngine by adding saveToDisk(File targetFile), and restore(List<File> serializedStratManagers) methods. I'd propose to keep interaction with this process protected and only expose the save/restore methods. Alternately, I could expose save, restore (single-object) and combine methods. Regarding actually saving the state of StratificationManager to disk: some form of Jackson-based serialization would probably be easiest. My initial thought would be to make a new class specifically for serializing: SerializedStratificationManager. This would cache the relevant information and be the object that is serialized/deserialized. I would add a new constructor to StratificationManager that accepts this object and restores the state within StratificationManager. I think this could be all be kept internal to GATK. Does that broad outline seem like a concept you might accept in GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932:832,cache,cache,832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-755351932,1,['cache'],['cache']
Performance,"Oh boy this smacks of being a race condition. More interesting is the fact that its failing specifically when threads = 1 and cram is off? I'm not sure if its also failing elsewhere on other runs though. It looks like its failing when it tries to add a new `final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);` to the datasource pool which sounds particularly nasty since its on an `ArrayList.add()` call. . @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-897021763:30,race condition,race condition,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-897021763,1,['race condition'],['race condition']
Performance,"Oh nice, that seems to be the ticket. All reads are used when setting that parameter. May I ask what the logic behind performing the downsampling is? Isn't there a risk of removing valid alignments that contribute to low abundance variation events? This would maybe only really be a problem when you are analysing sequences from a population of cells/microbes, but maybe the reward is greater than the risk?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543:118,perform,performing,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543,1,['perform'],['performing']
Performance,"Ok @jean-philippe-martin, I have an updated patch that seems to resolve the 503 errors! It's here: https://github.com/droazen/google-cloud-java/tree/dr_retry_CloudStorageReadChannel_fetchSize. Will you have time before you leave on vacation to open a PR against google-cloud-java? If not, let me know and we'll try to sort out our CLA issues and PR it ourselves. I didn't have time to write unit tests, unfortunately, though we're running it now with 1000 concurrent jobs each accessing 11,000 files and not seeing any errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319:456,concurren,concurrent,456,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3253#issuecomment-315447319,1,['concurren'],['concurrent']
Performance,"Okay apparently there is not a version number in the picard.jar file downloaded from https://github.com/broadinstitute/picard/releases and thus if easybuild detects a cache copy, it will use that instead of downloading it which was an older version. They forced it to download and now version is correct. I will re-try and see if the error persists. Thanks for catching that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633:167,cache,cache,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633,1,['cache'],['cache']
Performance,"Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:4560,perform,perform,4560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['perform']
Performance,"On another note, if we really have hundreds of readers in parallel it's possible they're being throttled by GCS and that may be why we're seeing opens fail. GCS is counting on us backing off to reduce its load.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300320552:95,throttle,throttled,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300320552,2,"['load', 'throttle']","['load', 'throttled']"
Performance,"One final thing: i'm happy to try to debug this, and was going to write a test case based on the existing GenomicsDB integration tests. However, when I try to run any integration test involving genomicsdb, I get an exception like the following. I am on windows, so perhaps this is the issue?. 09:03:37.460 FATAL GenomicsDBLibLoader - ; java.io.FileNotFoundException: File /tiledbgenomicsdb.dll was not found inside JAR.; 	at org.genomicsdb.GenomicsDBLibLoader.loadLibraryFromJar(GenomicsDBLibLoader.java:118) ~[genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBLibLoader.loadLibrary(GenomicsDBLibLoader.java:55) [genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBUtilsJni.<clinit>(GenomicsDBUtilsJni.java:30) [genomicsdb-1.3.2.jar:?]; 	at org.genomicsdb.GenomicsDBUtils.createTileDBWorkspace(GenomicsDBUtils.java:46) [genomicsdb-1.3.2.jar:?]; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteCreateOrCheckWorkspace(GenomicsDBImport.java:1005) [classes/:?]; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:661) [classes/:?]; 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1056) [classes/:?]",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-749138102:460,load,loadLibraryFromJar,460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-749138102,2,['load'],"['loadLibrary', 'loadLibraryFromJar']"
Performance,"One note that might be useful (or known already to the team): simply calling `cache()` doesn't cause any action. It seems that one might need to force the computation to be done on the RDD (e.g. `count()`), for caching to work, if the predicate depends on the results of computation. (ref last comment in #1877)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823:78,cache,cache,78,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811#issuecomment-225204823,1,['cache'],['cache']
Performance,"One observation that illustrates the need for care when optimizing metrics: for a few of the F1 optimizations, the haplotype-to-reference match-value parameter gets driven to its minimal value (1). Not 100% sure, but I'm guessing this might effectively boost precision by somehow cutting down on the complexity of proposed haplotypes---it depends on what the exact behavior of our SW algorithm is for negative scores. @davidbenjamin any thoughts on this behavior?. Something I don't quite understand yet is if we can impose some effective constraints on the parameters or otherwise reduce the number of independent dimensions. For example, it seems reasonable to me to fix the gap-extend penalties to -1 and let all other parameters be defined w.r.t. them. But perhaps we can also fix the match values similarly?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193:56,optimiz,optimizing,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712268193,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"One proposal for moving forward would be to have a default properties file with a known name/location that is included in the gatk jar (say, ""gatk.default.properties""), which is always loaded and populates the initial configuration, and then use the classloader getResources method to also load all resources with some other known name (say, ""gatk.properties""). That way any properties files on the classpath with the known name would be automatically discovered and loaded. The apache commons API allows looks like it has good support for handling this using a [composite](http://commons.apache.org/proper/commons-configuration/userguide/howto_compositeconfiguration.html#Composite_Configuration_Details) configuration. We would have to define some rules around override semantics, but it looks like the api provides a lot of control over that as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954:185,load,loaded,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322#issuecomment-274654954,3,['load'],"['load', 'loaded']"
Performance,"One suspects that there's a lot of inefficiency in this approach. If we dissected the ReadDataSource, we could probably figure out a way to separate the parts that need to be reinitialized for a new pass (iterators) from the parts that don't (cached index, e.g.).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499609179:243,cache,cached,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5985#issuecomment-499609179,1,['cache'],['cached']
Performance,"Our current mode of communication in gCNV is disk I/O. In my estimate, I/O is _not_ going to take a significant amount of time relatively. The overall structure is as follows: (java) pre-processing and filtering counts and writing processed counts to disk, (python) reading the processed counts, making CNV calls, writing model parameters and CNV posteriors to disk, (back to java) loading python results from disk and post-processing, i.e. creating .vcf files, .seg files, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337305574:382,load,loading,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3698#issuecomment-337305574,1,['load'],['loading']
Performance,"Overriding the defaults would get us most of the way there. Right now, we perform the following check on the IAC and fail if the defaults aren't changed to values that the CNV tools require, which is awkward:. ```; /**; * Validate that the interval-argument collection parameters minimally modify the input intervals.; */; public static void validateIntervalArgumentCollection(final IntervalArgumentCollection intervalArgumentCollection) {; Utils.validateArg(intervalArgumentCollection.getIntervalSetRule() == IntervalSetRule.UNION,; ""Interval set rule must be set to UNION."");; Utils.validateArg(intervalArgumentCollection.getIntervalExclusionPadding() == 0,; ""Interval exclusion padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalPadding() == 0,; ""Interval padding must be set to 0."");; Utils.validateArg(intervalArgumentCollection.getIntervalMergingRule() == IntervalMergingRule.OVERLAPPING_ONLY,; ""Interval merging rule must be set to OVERLAPPING_ONLY."");; }; ```. If we override defaults, we'd still perform the check to make sure the user didn't muck with them, but it'd still be nicer than forcing the user to change the original defaults on their own. However, there are still two more awkward points: 1) there is no value for `-interval-set-rule` that does *nothing* to the incoming intervals (you must either union or intersect), and 2) we have to specify our own `padding` argument in `PreprocessIntervals` that is distinct from `-interval-padding`, since we want to implement our own padding. The first can be easily addressed by adding an option to do nothing to the intervals; I'm not so sure what the best way to handle the second would be, so we can punt on it if it'd be more work than it's worth---these are relatively minor pain points, in the end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857:74,perform,perform,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4341#issuecomment-363219857,2,['perform'],['perform']
Performance,"Pair to find coverage for each interval, then reduceByKey; * Job 3 [getQNames] mapPartitions; * Job 4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like itâ€™s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so itâ€™s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2301,perform,performing,2301,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,['perform'],['performing']
Performance,"Per comments on the Slack channel, this can also be used as a component in germline tagging for matched tumor-normal pairs. For the same series above:. Here is the 100% normal run through ModelSegments, which yields 36 segments:; ![N modeled](https://user-images.githubusercontent.com/11076296/76631751-931d1a80-6518-11ea-8c18-2a8f41057ce3.png). Joint segmentation of the 100% normal and the 100% tumor yields 241 segments (up from 130 for the 100% tumor alone, as above). Using this joint segmentation for subsequent ModelSegments runs:. For the 100% normal, this yields 88 segments (up from 36):; ![N-SJS modeled](https://user-images.githubusercontent.com/11076296/76632024-ebecb300-6518-11ea-89ff-109c97970ef0.png). For the 100% tumor, this yields 166 segments (up from 130):; ![T-SJS modeled](https://user-images.githubusercontent.com/11076296/76632125-13dc1680-6519-11ea-9901-0c78809d08ba.png). I haven't performed detailed validations, but some spot checking suggests that this actually mitigate oversegmentation while still increasing sensitivity to shared events. For example, there is a small 13-bin deletion in chr19 that is found when running the 100% normal alone, but gets broken up into two adjacent deletions when running the 100% tumor alone (probably just due to statistical noise in the copy ratios). When running jointly, the deletion does not get broken up. However, as discussed over Slack, we should probably run some scenarios with simulated data to check behavior---for example, how robust is the joint segmentation to some of the samples being noisy/oversegmented?. There are lots of options for restructuring the workflow. We could potentially modify ModelSegments to take in the denoised copy ratios from the normal, when available, and add modeling of the normal and germline tagging to that tool. Or we could break things up into separate tools. @fleharty any opinions?. Note that another benefit of using this joint segmentation for germline tagging is that common breakp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477:910,perform,performed,910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598764477,1,['perform'],['performed']
Performance,"Please find the stack trace below. Hopefully helpful. Using GATK jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/test.vcf -O /omics/chatchawit/sm/anno/test.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 23:24:49.725 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:24:49.934 INFO Funcotator - ------------------------------------------------------------; 23:24:49.934 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-34-g2cc7abd-SNAPSHOT-0.0.3; 23:24:49.934 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:24:49.935 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:24:49.935 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:659,Load,Loading,659,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032,1,['Load'],['Loading']
Performance,Please use the template in the WDL GATK repo doc that was shared. Or we can modify that template. I'd like the document to match what is generated automatically. The template in that document includes optimizations and is quite portable.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295:201,optimiz,optimizations,201,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2480#issuecomment-358440295,1,['optimiz'],['optimizations']
Performance,Possibly you are running into the Spark performance regression described in https://github.com/broadinstitute/gatk/issues/4376. This was patched in the latest release (4.0.2.0) -- could you try running with that release and see if the issue is resolved?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369947491:40,perform,performance,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369947491,1,['perform'],['performance']
Performance,"Prior to this, gCNV shard size and VM type have never been optimized. For WES cohort mode, we arbitrarily ran with:. 37 shards of 200 samples by 5000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~2 hours = ~3 cents / sample. This PR gets rid of a memory spike in the sampling of denoised copy ratios, fixes a memory leak by updating theano, and also adds some theano flags that typically yield a factor of ~2 speedup (notably, the OpenMP elemwise flag, although we also get a slight boost from using numpy MKL). This allows us to run, e.g.: . 2 shards of 50 samples by 100000 intervals on n1-standard-8s (8 CPU, 30GB memory, $0.08 / hr) each taking ~5 hours = ~1.6 cents / sample; 4 shards of 50 samples by 50000 intervals on n1-highmem-4s (4 CPU, 26GB memory, $0.05 / hr) each taking ~3.25 hours = ~1.3 cents / sample; 45 shards of 50 samples by 5000 intervals on *n1-standard-1s* (1CPU, 3.75GB memory, $0.01 / hr) each taking ~0.5 hours = ~0.5 cents / sample. For these runs, we used a slightly larger interval list and 1/4 the number of samples than in the first example, but because everything scales linearly, it's probably fair to compare the per-sample-and-interval costs. So we get a factor of ~8 savings if we keep the shard size the same. The cost was already satisfactory, but fixing the leak allows us to more easily run scatters that are not so wide, which may be crucial for running the megaWDL. Adding the OpenMP flag also lets CPU scalability work as intended. We can do a more systematic optimization for cost if desired, and we should also revalidate to make sure performance doesn't vary too much with shard size (from spot checking, it looks like marginal and/or single-bin calls may flicker on and off). Note that we have still not optimized inference for WES, although I believe @vruano has done some optimizations for WGS. @mwalker174 @vruano for WGS with 2kb bins, I would expect the cost of the gCNV step to be ~10 cents in cohort mode before infer",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697:59,optimiz,optimized,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781#issuecomment-471570697,1,['optimiz'],['optimized']
Performance,R_SAMTOOLS : false; 13:48:31.695 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:48:31.695 INFO CountReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:48:31.695 INFO CountReadsSpark - Deflater: IntelDeflater; 13:48:31.695 INFO CountReadsSpark - Inflater: IntelInflater; 13:48:31.696 INFO CountReadsSpark - GCS max retries/reopens: 20; 13:48:31.696 INFO CountReadsSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 13:48:31.696 WARN CountReadsSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: CountReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 13:48:31.696 INFO CountReadsSpark - Initializing engine; 13:48:31.696 INFO CountReadsSpark - Done initializing engine; 18/12/21 13:48:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/12/21 13:48:33 WARN component.AbstractLifeCycle: FAILED ServerConnector@1cba0321{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use; java.net.BindException: Address already in use; at sun.nio.ch.Net.bind0(Native Method); at sun.nio.ch.Net.bind(Net.java:433); at sun.nio.ch.Net.bind(Net.java:425); at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223); at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74); at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:321); at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80); at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236); at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68); at org.eclipse.jetty.server.Server.doStart(Server.java:366); at org.eclipse.jetty.util.component.AbstractLife,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:4088,load,load,4088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['load'],['load']
Performance,"Ran some comparisons between funcotator and oncotator using the following data sources against a set of ~70 variants (see attached):; - achilles; - cancer_gene_census; - clinvar; - cosmic; - cosmic_fusion; - cosmic_tissue; - dna_repair_genes; - familial; - gencode; - gencode_xhgnc (Only Funcotator had this data source); - gencode_xrefseq; - hgnc; - oreganno; - simple_uniprot. While the details of the results were not compared between the two tools (unit tests are designed to do this comparison), the tools runtimes were captured (using the BSD `time` utility) over 10 iterations of annotating this data set (VCF->VCF). Full results set is attached, but the long and short of it is that depending on which timing you count by (real/user/system/user+system) **Funcotator is faster than Oncotator by anywhere from 8% to 57% and has not had any performance tuning.**. I have attached the timing measurements, as well as a gzip containing the inputs and the script I used to collect the timing information:. [benchmarking_funcotator_oncotator.tar.gz](https://github.com/broadinstitute/gatk/files/1601085/benchmarking_funcotator_oncotator.tar.gz); [BENCHMARK_funcotator_oncotator.xlsx](https://github.com/broadinstitute/gatk/files/1601079/BENCHMARK_funcotator_oncotator.xlsx)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3857#issuecomment-355068642:846,perform,performance,846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3857#issuecomment-355068642,1,['perform'],['performance']
Performance,"Ran tests with JOIN query vs. IN query and performance/bytes scanned appeared identical. Going with IN query because it's more readable (to me, at least).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7762#issuecomment-1092333101:43,perform,performance,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7762#issuecomment-1092333101,1,['perform'],['performance']
Performance,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:999,optimiz,optimized,999,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,4,"['load', 'optimiz']","['loads', 'optimized']"
Performance,Re-assigning to @jonn-smith and @jamesemery to assess whether this is worth resurrecting as part of the current HaplotypeCaller performance work.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-453558455:128,perform,performance,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-453558455,1,['perform'],['performance']
Performance,Reader(AbstractFeatureReader.java:106); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at sh,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2768,concurren,concurrent,2768,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,Reading performance is particularly important,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5205#issuecomment-423304679:8,perform,performance,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5205#issuecomment-423304679,1,['perform'],['performance']
Performance,"Rebased and tweaked a few package versions, some of which are slightly different than those currently in the base (as is the version of R). Looks like `build-essential` is all you need in the base image for tests to not fallback on slow implementations, but we might want to double check that native dependencies are correct. Not really sure how urgent this is, and I have to admit I've lost track of the remaining ways R can break our builds. The possibility of a bad Travis cache (which I think was a common issue in the past) is now prevented by #6454, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311:476,cache,cache,476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311,1,['cache'],['cache']
Performance,"Regarding intervals from the command line:. The column partition(s) is/are specified in the loader JSON file - the current functionality allows the user to not have to determine the list of contigs that overlap with a given column partition. All he/she needs to do is specify which column partition is being written to. We are ok with accepting contig intervals from the command line - in that case, it's the user's responsibility to ensure that the contig intervals are correct for the specific column partition being written to and are in the correct order. Regarding the other JSON input file, [this wiki section](https://github.com/Intel-HLS/GenomicsDB/wiki/Java-interface-for-importing-VCF-CSV-files-into-TileDB-GenomicsDB#mode-2-java-api-for-importing-variantcontext-objects) explains the why and what. This goes back to our discussions of meta-data and names. As Kushal mentioned, we can explain in more detail next week.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277348689:92,load,loader,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277348689,1,['load'],['loader']
Performance,"Regarding the non-Docker integration tests failing earlier today, I think this was because the R packages were added to the Travis cache in #3101. @cmnbroad cleared the cache to see if we could reproduce a compiler error introduced in #3934 on Travis (for the record, we could reproduce it on my local Ubuntu machine and gsa5, but not on Travis). This removed the cached getopt dependency, which then caused tests to fail. See #4246.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441:131,cache,cache,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4209#issuecomment-359999441,6,['cache'],"['cache', 'cached']"
Performance,"Reopening -- @ldgauthier reports that this error still occurs even after the patch in https://github.com/broadinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:418,concurren,concurrent,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,4,['concurren'],['concurrent']
Performance,"Requires https://github.com/samtools/htsjdk/pull/724. I tested on a cluster, loading features (VCF) from HDFS, by running:. ```bash; ./gatk-launch PileupSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam \; --output hdfs:///user/$USER/out/pileup \; --reference hdfs:///user/$USER/fasta/human_g1k_v37.20.21.fasta \; -L 20:10000092-10000112 \; -metadata hdfs:///user/$USER/vcf/dbsnp_138.b37.20.21.vcf \; -- \; --sparkRunner SPARK \; --sparkSubmitCommand spark2-submit \; --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-273178186:77,load,loading,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-273178186,1,['load'],['loading']
Performance,"Reviving this. This will essentially be a major refactor/rewrite of CreatePanelOfNormals to make it scalable enough to handle WGS. - [x] CombineReadCounts is too cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:100,scalab,scalable,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['scalab'],['scalable']
Performance,Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `Ã¸` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `37.037%` |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `66.667%` |; | [...e/TrainVariantAnnotationsModelIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbEludGVncmF0aW9uVGVzdC5qYXZh) | `77.778%` |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `100.000%` |; | ... and [2 more](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:5181,scalab,scalable,5181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,Rpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.st,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4570,concurren,concurrent,4570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"ST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFControlSample/Benchmark/8c516721-e955-41d1-907e-fcee92f592d3/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFTestSample/Benchmark/427c5010-a177-42d8-81be-5a387beed653/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:21386,cache,cacheCopy,21386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,2,['cache'],['cacheCopy']
Performance,STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2210,load,load,2210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['load'],['load']
Performance,"SVIntervals are comparable, and provide the same total order as coordinate-sorted BAMs. They can encode any contiguous stretch of bases on the reference, or strand-sensitively on its reverse complement. They can encode 0-length intervals (i.e., locations) to indicate things like the precise, unambiguous location of an insert between two reference bases. They're super light-weight, and cache friendly (no references), and they can be compared and tested for equality quickly and locally. They know how to serialize themselves with Kryo. They have a much more complete set of operations to calculate overlaps and underlaps, total order, etc. How will we implement isUpstreamOf using SimpleIntervals, when SimpleIntervals has no concept of contig order?; I think we'd have a heck of a job retrofitting our code to use SimpleIntervals, and a lot of testing to do to prove that the performance loss isn't significant. I'm sorry we struck out in an incompatible direction, but SimpleInterval just didn't seem up to the job at the time we started the SV project.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418520720:388,cache,cache,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154#issuecomment-418520720,2,"['cache', 'perform']","['cache', 'performance']"
Performance,Sampling method optimized in #5781.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-471632458:16,optimiz,optimized,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-471632458,1,['optimiz'],['optimized']
Performance,Saw it again [here](https://travis-ci.com/broadinstitute/gatk/jobs/180435353) (now restarted). If I'm reading the serialization stack in the right order:. ```; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); ```. it looks like we're trying to serialize a ClassLoader. The FieldSerializer does appear to use a ClassLoader to load classes during serialization.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740:774,load,load,774,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680#issuecomment-467462740,1,['load'],['load']
Performance,"See the ScatterIntervals and related tasks in the gCNV WDLs at the link above. The idea is to performs multiple runs of gCNV in shards of >10,000 intervals each across the exome. These shards can be specified by passing the set of intervals for each shard via the -L argument to GermlineCNVCaller. The gCNV model is independently fit in each shard, using the global depth and contig-level ploidy parameters determined across the whole exome in the DetermineGermlineContigPloidy step. The shards are then stitched together in the PostprocessGermlineCNVCalls step. I believe we typically run shards of 10,000 intervals for cohorts of 100-200 samples using n1-standard-8 machines, which have 30GB of memory (although looking at some recent runs, I think we've used as few as 5,000 intervals per shard). I think as few as several tens of samples could be sufficient to train a batch, but 100-200 is preferable---of course, this will depend on the specifics of your particular data set.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-407750916:94,perform,performs,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053#issuecomment-407750916,1,['perform'],['performs']
Performance,So the null pointer is caused by bwa mem complaining that it cannot load the index from the HDFS.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311699030:68,load,load,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-311699030,1,['load'],['load']
Performance,"Some of the CNV tools are miscategorized: GetHetCoverage + tools in the ""Intervals Manipulation"" category. The latter should probably be considered CNV-specific because they either use the target-file format (which is only used in the legacy CNV + ACNV pipeline) or perform a task that is specific to the CNV pipeline and probably not of general interest (PreprocessIntervals).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-347530234:266,perform,perform,266,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-347530234,1,['perform'],['perform']
Performance,Sorry but it doesn't seem to be resolved.; ```; $ gatk ViewSam -I /data/MCF7_PDS/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true ; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/; tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY; true; 08:51:42.543 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compr; ession.so; [Mon May 07 08:51:42 CEST 2018] ViewSam --INPUT /data/MCF7_PDS.bam --ALIGNMENT_STATUS Aligned --PF_STATUS PF --HEADER_ONLY true --REC; ORDS_ONLY false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH; _CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon May 07 08:51:42 CEST 2018] Executing as [...] on Linux 4.4.0-87-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12; Deflater; : Intel; Inflater: Intel; Provider GCS is available; **Picard version: Version:4.0.4.0**; ```; And if I use `--version true`:; ```; $ gatk ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true --version true; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106:605,Load,Loading,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106,1,['Load'],['Loading']
Performance,"Sorry to be a pain... I'm still getting this problem with 4.0.2.1, which according to the tag [description](https://github.com/broadinstitute/gatk/releases/tag/4.0.2.1) should have this issue fixed. ```; Using GATK jar /home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar FilterMutectCalls --variant gatk4/WW00274.vep.vcf.gz --output test.vcf.gz; 10:00:20.977 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/db291g/applications/gatk/gatk-4.0.2.1/gatk-package-4.0.2.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:00:21.082 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.083 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.0.2.1; 10:00:21.083 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:00:21.083 INFO FilterMutectCalls - Executing as db291g@login01 on Linux v2.6.32-431.23.3.el6.x86_64 amd64; 10:00:21.083 INFO FilterMutectCalls - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 10:00:21.083 INFO FilterMutectCalls - Start Date/Time: March 7, 2018 10:00:20 AM GMT; 10:00:21.083 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.083 INFO FilterMutectCalls - ------------------------------------------------------------; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Version: 2.14.3; 10:00:21.084 INFO FilterMutectCalls - Picard Version: 2.17.2; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:00:21.084 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOL",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4363#issuecomment-371088787:660,Load,Loading,660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4363#issuecomment-371088787,1,['Load'],['Loading']
Performance,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:223,throughput,throughput,223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193,2,['throughput'],['throughput']
Performance,"SortSam - Deflater IntelDeflater; 18:01:51.713 INFO SortSam - Initializing engine; 18:01:51.713 INFO SortSam - Done initializing engine; 18:02:01.512 INFO SortSam - Shutting down engine; [December 7, 2016 6:02:01 PM AST] org.broadinstitute.hellbender.tools.picard.sam.SortSam done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=1911029760; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); 	at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); 	at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); 	at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); 	at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); 	at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); 	at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); 	at org.broadinstitute.hellbender.tools.picard.sam.SortSam.doWork(SortSam.java:52); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:62); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	... 15 more",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924:3117,Load,LoadSnappy,3117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2299#issuecomment-265469924,4,"['Load', 'load']","['LoadSnappy', 'loadClass']"
Performance,"Sounds good. I have not characterized the performance changes (if any). There are some minor differences in output, but the output is now more correct for symbolic alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5834#issuecomment-476358270:42,perform,performance,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5834#issuecomment-476358270,1,['perform'],['performance']
Performance,Sounds like the fix here might kill some/all of our performance gains from implementing this feature... :(,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377291391:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3688#issuecomment-377291391,1,['perform'],['performance']
Performance,"Spot(TM) 64-Bit Server VM 1.8.0_112-b15; Version: 4.alpha.2-1125-g27b5190-SNAPSHOT; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4569,load,loaded,4569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"Standard disclaimer up front that I'm still a bit new and can't speak to; the needs of the GATK outside the small bits I've looked at. The data I've wanted so far had all come from the UCSC genome browser,; which has a reasonable format and good documentation. My personal feeling; is there's little reason to reformat the data or choose a different format; for data we may create. For me the real problem is establishing fast and; reliable look up of that data. I've so far just thrown a couple light; weight tracts into the resources on my git branch and loaded them into an; interval tree. That storage strategy will not scale for all of the UCSC; data, and in any case might antagonize them. On Mon, Apr 30, 2018, 11:59 PM samuelklee <notifications@github.com> wrote:. > I tend to agree. VCF is perhaps a special case of the format we actually; > need, so no point in shoehorning output that doesnâ€™t fit just for the sake; > of standardization. @sooheelee <https://github.com/sooheelee> may feel; > more strongly otherwise.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385593864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCLeOf8aOnroZA0wirz8zeBoeodNsks5tt92xgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418:557,load,loaded,557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418,1,['load'],['loaded']
Performance,"Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.Comple",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3937,concurren,concurrent,3937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,Step.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:76); at org.gradle.i,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9260,Cache,CacheStep,9260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,"Subsampling seems to be the way to go, see #2858. For the record, I did try to implement caching, but this results in excessive cache checking. In general, I think a better solution is to structure code so that expensive global quantities are not unnecessarily recomputed locally. At some point, this sort of undesirable recomputation snuck in during a refactoring of the allele-fraction likelihood code, probably when we tried to make the method for computing site likelihoods pull double duty based on the presence or absence of an allelic PoN. With an allelic PoN, we need to compute a log gamma at each site based on the site-specific bias hyperparameters; without a PoN, we only need to do this once for all sites, since the bias hyperparameters are now global, but the code naively recomputes it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709:128,cache,cache,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2860#issuecomment-335621709,1,['cache'],['cache']
Performance,"Sure, it's a fair point that we could write this genotype-compare operation as a standalone tool. It's convenient to make it work via VariantAnnotator so that we can do it concurrently with other annotation tasks, instead of needed two passes through the VCF. I primarily ask b/c this is a downgrade from GATK3, where the equivalent of FeatureContext was passed. . Would it be reasonable for the FeatureManager to somehow get exposed? In my example I am calling FeatureContext.getValues() without supplying an interval, but I certainly could (and maybe should). If one supplies a specific query interval, the Feautrecontext isnt doing much other than providing an abstraction between FeatureManager, and the boundaries set on Featurecontext (which i agree are tricky to figure out in some cases) dont really matter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795:172,concurren,concurrently,172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6930#issuecomment-754256795,1,['concurren'],['concurrently']
Performance,"TK BaseRecalibrator was run for each sample. The 32 recalibration tables per sample were merged into one table per sample with GATK GatherReports.; > ; > Base-recalibrated BAM files were produced with GATK ApplyBQSR in parallel over each of the 3,366 contigs in the hg 38 + alt reference genome, plus the unmapped reads. These were merged into a final BAM per sample with GATK GatherBamFiles.; > ; > SplitIntervals was used to define 3,200 evenly sized genomic intervals across hg38 + alt contigs, excluding telomeres, centromeres, unplaced contigs, unlocalized contigs, decoy and the Epstein-Barr viral sequence (Supplementary data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was run at the 3,200 pre-defined intervals for each sample and interval VCFs were gathered into GVCFs per sample. The following steps were then performed for the OSCC, TCGA and CSCC cohorts separately. Per sample GVCFs were consolidated with GenomicsDBImport and joint-called with GenotypeGVCFs at the 3,200 pre-defined intervals. Multi-sample interval calls were gathered using GatherVcfs. VariantFiltration and MakeSitesOnlyVcf was used to filter records with excess heterozygosity and retain only the site-level annotations. VariantRecalibrator and ApplyRecal were then applied for SNP and indels separately following GATKâ€™s recommendations to obtain a final, filtered cohort VCF.; > ; > Somatic short variant calling; > ; > Somatic short variant calling was performed for tumour-normal pairs, first by preparing a panel of normals (PoN) for the OSCC, TCGA and CSCC cohorts separately. To create a PoN, Mutect2 was run in tumour only mode for the normal samples in the cohort at the 3,200 pre-defined intervals. GatherVcfs was used to gather interval VCFs into per sample VCFs. GenomicsDBImport and CreateSomaticPON was used to consolidate sample VCFs an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:2694,perform,performed,2694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,"TRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.M",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1651,load,load,1651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"Testing branch `ck_3487_port_LeftAlignAndTrimVariants`, which ports LeftAlignAndTrimVariants from GATK3 to GATK4. ### stdout; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign.vcf.gz; 16:34:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 05, 2018 4:34:35 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 16:34:35.413 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 16:34:35.414 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-24-gb43bc27-SNAPSHOT; 16:34:35.414 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:34:35.414 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 16:34:35.414 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 16:34:35.414 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494:730,Load,Loading,730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-418875494,1,['Load'],['Loading']
Performance,"Testing updated branch with improved messaging.; ```; WMCF9-CB5:shlee$ ./gatk LeftAlignAndTrimVariants -R ~/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V ~/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; Using GATK wrapper script /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk; Running:; /Users/shlee/Documents/branches/hellbender/build/install/gatk/bin/gatk LeftAlignAndTrimVariants -R /Users/shlee/Documents/ref/hg38/Homo_sapiens_assembly38.fasta -V /Users/shlee/Downloads/zeta_snippet_shlee/zeta_snippet.vcf.gz -O zeta_snippet_leftalign_96branch.vcf.gz; 12:55:31.964 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/shlee/Documents/branches/hellbender/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.dylib; Sep 06, 2018 12:55:32 PM shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider warnAboutProblematicCredentials; WARNING: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a ""quota exceeded"" or ""API not enabled"" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.; 12:55:32.083 INFO LeftAlignAndTrimVariants - ------------------------------------------------------------; 12:55:32.083 INFO LeftAlignAndTrimVariants - The Genome Analysis Toolkit (GATK) v4.0.8.1-25-g0c6f06f-SNAPSHOT; 12:55:32.083 INFO LeftAlignAndTrimVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:55:32.083 INFO LeftAlignAndTrimVariants - Executing as shlee@WMCF9-CB5 on Mac OS X v10.13.6 x86_64; 12:55:32.083 INFO LeftAlignAndTrimVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_111-b14; 12:55:32.083 INFO LeftAlignAndTrimVariants - Start Date/Time: September 6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326:671,Load,Loading,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3487#issuecomment-419190326,1,['Load'],['Loading']
Performance,"Thank you @kshakir. What I see there is that the code sets the default NIO option, and as part of this is creates a google cloud `StorageOptions` object. Sadly for us, when this object is created it determines which Google credentials to use, and if nothing was specified by the user it will send some network messages to try to figure out whether it's running on a Google Compute Engine machine. When we wrote the default-setting code we didn't realize that setting the number of retries was going to cause a network message to be sent, with the associated potential retries and delays. We can't change the way Google Compute Engine works, or how the Google authentication works either. Ideally we'd want some way to only search for credentials when we know NIO is going to be used. The point of these defaults is that they're used for anything that uses NIO, including third-party library code. We can't fully replicate this behavior in a different way from the outside. So I think the ""correct"" fix would be to go deep inside the Google NIO library and change it so that instead of providing a default configuration (that the user would have to put together, causing the problem you've seen), we can provide a *callback* that sets the configuration when the Google Cloud NIO provider is loaded. This is harder for future developers to wrap their heads around, but at least it would prevent this delay if NIO is not used. I'd like to think about this some more before doing something quite this drastic, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504:1290,load,loaded,1290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443837504,1,['load'],['loaded']
Performance,"Thank you @mehrzads for your contribution. It would appear that this optimization is aimed to incorporate the knowledge that we could never possibly visit a given vertex more than K times in the first K best paths. Since the graphs are prone to exponential expansion of paths this seems like an important safeguard against this exponential expansion of the graph. . Looking at the code and the algorithm behavior it is intending to copy I see that there is a degenerate case in the current code that can cause the results to be order dependent. My belief is that this code can fall over by virtue of the fact that we refuse to make new incoming edges to a given vertex if there are already too many incoming edges for that vertex. Unfortunately this heuristic doesn't strike me as being valid, because those incoming edges can have any weight, including very high weights because they are bad paths through the graph that we created at a previous step. . I think a more correct optimization would be to limit the number of edges we create LEAVING a given vertex. The logic for this is that while we may not necessarily see all of the incoming edges in the correct weight order we will necessarily see all of the leaving edges in the correct order because those paths are pulled off of the priority queue in the correct order. Thus we can safely ignore any additional paths we see leaving a given edge because by construction as they would necessarily have at least one path that is cheaper than all of the paths leaving the current node.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417:69,optimiz,optimization,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417,3,"['optimiz', 'queue']","['optimization', 'queue']"
Performance,"Thank you for the kind explanation, *@davidbenjamin*. I understand your; rationale. BTW, in my test with a PoN of ~100 sample, this set of changes; makes significant performance improvement regardless of the value of; --genotypePonSites. Could you advise how many samples you used to create; the PoN you test? Thanks!. On Tue, Aug 8, 2017 at 9:28 PM, David Benjamin <notifications@github.com>; wrote:. > *@davidbenjamin* commented on this pull request.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/; > Mutect2Engine.java; > <https://github.com/broadinstitute/gatk/pull/3304#discussion_r132078537>:; >; > > }; >; > - if (hasNormal() && normalContext != null && countNonRef(refBase, normalContext) > normalContext.getBasePileup().size() * MTAC.minNormalVariantFraction) {; > + if (!MTAC.genotypePonSites && !featureContext.getValues(MTAC.pon, new SimpleInterval(context.getContig(), (int) context.getPosition(), (int) context.getPosition())).isEmpty()) {; >; > I deliberately made --genotypePonSites false by default because running; > local assembly and realignment of PoN sites is very expensive, especially; > so because PoN sites are frequently in regions that yield very messy; > assembly graphs, hence many haplotypes. It's true that explicit results can; > be useful, and we frequently want them in the course of development, but a; > tenet of the GATK is to make the tools work as well as possible with; > default settings.; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/3304#discussion_r132078537>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABD6FgXbu1QybkQOkGpGBtjNQINUx13rks5sWRk3gaJpZM4OcvPO>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3304#issuecomment-321154425:166,perform,performance,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304#issuecomment-321154425,1,['perform'],['performance']
Performance,"Thank you for the speedy review @davidbenjamin. I agree with you that the obvious place to trim the alleles is in `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` as it is the place where we actually edit the output. Indeed my first attempt at this fix was to make that change. Unfortunately, because the `readAlleleLikelihoods` object is constructed with the un-trimmed alleles in the `alleleMapper` was causing failures because the Liklihoods object would have mismatching alleles. To fix `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` we would have to edit the alleleMapper object, which would be difficult given that I would prefer to just use the allele trimming library object. . Another proposal would have been to just hold onto the `mergedVC` object before we cull the extra alleles and then just compare the alleles at the end. Unfortunately due to engine code optimizations we have enabled an unsafe allele list copy for these alleles in the HaplotypeCaller (to save ourselves the cost of allocating dozens of identical ArrayLists to store Haplotypes every time we use the VariantContextBuilder). . To clarify, it is possible to move the check to the right place its likely to force me to write a non-library implementation of the trimming code that tracks what edits it made and I was trying to avoid doing that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693:913,optimiz,optimizations,913,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693,1,['optimiz'],['optimizations']
Performance,"Thank you very much for the detailed answer, @lbergelson. I understand the point 1 and 3, but regarding 2: there is also a cost of running a Spark tool with 1 thread? I would love to use the framework for ""sparkify"" my tools, but I would like to be sure that there is no cost for running it locally without multi-thread...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273421641:307,multi-thread,multi-thread,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273421641,1,['multi-thread'],['multi-thread']
Performance,"Thank you!. On Mon, Oct 22, 2018 at 10:33 AM meganshand <notifications@github.com>; wrote:. > @SusieX <https://github.com/SusieX>; >; > Marking duplicates: I do recommend removing duplicates (we run; > MarkDuplicates from Picard).; >; > BQSR: The pipeline we're developing is for Whole Genome data, so our bams; > have gone through BQSR in the whole genome pipeline. We're using those; > recalibrated base qualities. I haven't tested running BQSR only on the; > mitochondria so I don't know how well that would work.; >; > If you do need to run BQSR only on the mitochondria I'd start by using the; > phylotree sites as --known-sites, but you'd need to have those sites in; > vcf format. Again, I haven't tested this so I don't know how well it will; > perform.; >; > If you end up using BQSR I think you're pipeline (BAM -> remove dup -> BQ; > recalibrate -> Mutect2 call -> FilterMutectCalls) is correct. Good luck!; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431852996>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AQ4DHh9X94wZ-4488FohFKnkv1SCe6c2ks5undccgaJpZM4WqV76>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431899805:753,perform,perform,753,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-431899805,1,['perform'],['perform']
Performance,"Thanks @bbimber. Can you run the command without the `--consolidate` option? We will be working on a `--consolidate-only` option for `gatk GenomicsDBImport` soon, but can provide you a standalone tool to perform only the consolidation later today. Is that OK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724313006:204,perform,perform,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724313006,1,['perform'],['perform']
Performance,"Thanks @bbimber. Just tested with the files you shared and the book keeping file uncompresses and loads fine. So, no problem on that count. As you have noticed, consolidation is very resource-intensive in GenomicsDBImport. When fragments are not consolidated on-disk during import, they get consolidated in-memory during queries. In this case, it is possible that with 79 fragments and with 10MB(hardcoded currently) being used per fragment per attribute just for consolidation, with memory fragmentation and other internal buffers, we may have run out of memory. Possible solutions -; 1. Can you run with something less that 178 with java heap options `-Xmx178g -Xms178g`, so the native process gets a little more?; 2. There is no way to perform consolidate only from gatk currently. You could try importing another set with the `--consolidate` and possibly the `--bypass-feature-reader` option.; 3. I am working on a fix for consolidation for Cloud Storage mainly, it may perform a little better generally and can point to a gatk branch when created if you are interested. @mlathara, any other ideas?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042407322:98,load,loads,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042407322,3,"['load', 'perform']","['loads', 'perform']"
Performance,"Thanks @cmnbroad! I optimized the imports, now waiting for Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231:20,optimiz,optimized,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231,1,['optimiz'],['optimized']
Performance,"Thanks @danagibbon, I may know what the issue is. `hdfs` support in GenomicsDB still relies on JVM/Java 11 and we had some workarounds with thread local caches from a while ago. I will create a branch sometime next week without `hdfs` which will hopefully get us past this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936476061:153,cache,caches,153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936476061,1,['cache'],['caches']
Performance,"Thanks @davidbenjamin for the feedback and sorry for the slow response. We have been working on improving PairHMM by adding AVX-512 (#3615) and FPGA (#2725) implementations. . We are also adding AVX2 (#3701) and AVX-512 (future PR) Smith-Waterman, which will improve the performance of Mutect2. We have the data above and will provide benchmarking results of your Mutect2 command with these improvements.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871:271,perform,performance,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2562#issuecomment-338732871,2,['perform'],['performance']
Performance,"Thanks @davidbenjamin, I can try that out. Any other parameters or modes that you feel might be gating any of these metrics/optimizations, which should be explored jointly with the SW parameters?. I guess the same question applies for `linked-de-bruijn-graph`, which is currently marked as experimental: what would be the procedure/criteria for changing the default behavior? Hopefully, we can answer this question for the case of a binary parameter before tackling 12 parameters! In general, I'm interested in establishing clear processes so it's easier for anybody to propose improvements. If there's no clear answer just yet, I'm happy to stop at exposing these parameters, perhaps consolidating defaults to one of the current sets if that is not too disagreeable (which is just slightly more complicated than a binary decision). Don't want to rabbit hole if there's no need. Hopefully, at the least, the blog-like documentation above will provide useful pointers to anyone that might want to tackle similar efforts in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619:124,optimiz,optimizations,124,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715407619,2,['optimiz'],['optimizations']
Performance,"Thanks @drifty914, it sounds like you may need to limit the number of concurrent jobs that Cromwell is allowed to scatter. We typically run gCNV in the cloud and scatter across multiple VMs, so we haven't encountered this issue before. At the same time, you could also try to reduce the total number of shards (by increasing num_intervals_per_scatter), which should be fine if each shard has enough memory. We typically scatter 200 samples x 5000 intervals, which fits comfortably in VMs with 30GB of memory. We haven't gotten a chance to profile how much of this memory is being used in detail, so you might be able to get by with much less. I don't think this is a matter of a memory leak or files being left open by the tool, as it looks like your job fails during the theano compilation step. I'll try to get an idea of how many files theano opens for each compilation, but I don't think this is something we have much control over. We have thought about whether it might be possible to reuse the same compiled theano model for identically sized shards, but haven't gotten a chance to investigate this yet either.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-468502707:70,concurren,concurrent,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-468502707,1,['concurren'],['concurrent']
Performance,"Thanks @droazen & @ldgauthier. I can certainly run a bunch more iterations of the same HC run on the same data. I'm not super hopeful it will turn anything up though. I can also try selecting a bunch of the different PairHMM implementations. I can't share too much, but this issue turned up in a very high throughput (1000s of samples a day) clinical pipeline. We're going back and looking for other instances where we see an excess of that `Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null` message, and re-running those samples to see if, on re-run, they generate different outputs. I realize the AVX-specific hardware issue is perhaps a little far-fetched, though given the volume of the pipeline and the fact that it runs in a cloud environment, I think it's entirely reasonable to suspect we'll run into hardware/instance issues occasionally. And there are AVX or at least SIMD specific registers, so if one of those were to see problems that could cause the PairHMM issues, without causing issues in other software that doesn't leverage the SIMD/AVX instructions. My main question really is this: is anyone familiar enough with the Intel PairHMM implementation and interface that they could weigh in on whether or not unexpected hardware errors could result in the return of empty likelihoods from the PairHMM instead of some kind of error, exception or segfault?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915:306,throughput,throughput,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6889#issuecomment-709555915,1,['throughput'],['throughput']
Performance,Thanks @gbggrant! @droazen @ldgauthier I just pushed a branch that resolves the error by simplifying the evidence-to-index cache.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066:123,cache,cache,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-626448066,2,['cache'],['cache']
Performance,"Thanks @kdatta. The branch builds now, but there are a couple of problems that cause several tests to fail, including some existing tests that used to pass. You can see the results [here](https://travis-ci.org/broadinstitute/gatk/jobs/221534229). - The main issue is that GenomicsDB fails to load. This causes the importer tests to fail, as well as the existing GenomicsDB integration tests. (Note that the importer tests fail with a null pointer exception, but that problem is secondary and only happens when the db fails to load, which is the root problem.) We can fix the NPE in code review, for now the main issue is fix the core problem of why genomics db fails to load. - The changes in OptionalVariantInputArgumentCollection and RequiredVariantInputArgumentCollection are causing argument name collisions in other tools, which is why ExampleIntervalWalkerIntegrationTest tests are failing in this branch. The simplest fix in the short term is to just revert the changes you made to those two classes, and remove the new VariantInputArgumentCollection class. These aren't being used by the importer tool anyway. It should be pretty easy to reproduce load issue, it happens on my laptop and and travis, but let me know if you need help or have questions about any of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791:292,load,load,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-294148791,8,['load'],['load']
Performance,Thanks @lbergelson. I merged your changes from `lb_connect_pairhmmargs` into this PR. This works with the `gatk-protected` changes in https://github.com/broadinstitute/gatk-protected/tree/lb_connect_pairhmmargs. Here's an idea of the performance improvement when running HaplotypeCaller (HC) on 4 CPUs. | PairHMM Threads | PairHMM Time (sec) | HC Time (sec) | HC Speedup |; | --- | --- | --- | --- |; | 1 | 50.7 | 96.6 | 1x |; | 2 | 28.0 | 73.8 | 1.31x |; | 3 | 21.6 | 67.2 | 1.44x |; | 4 | 18.8 | 65.4 | 1.48x |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2259#issuecomment-261353229:234,perform,performance,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259#issuecomment-261353229,1,['perform'],['performance']
Performance,"Thanks @ldgauthier , we will try that option out in few weeks. For now as a work-around for this, we are writing it to local EBS volumes which gives best performance for this job and then we move the files to the shared storage.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088707338:154,perform,performance,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1088707338,1,['perform'],['performance']
Performance,"Thanks @samuelklee , I will incorporate your conda update into this branch, now that we've dealt with the test failures!. I patched the VETS test code to include the h5diff (and diff) output in the exception messages when one of these commands fails, and switched to the existing `BaseTest` methods for running the process and capturing the output. You can see what the output looks like (when we remove the epsilon tolerance) here:. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.ScoreVariantAnnotationsIntegrationTest.html. https://storage.googleapis.com/hellbender-test-logs/build_reports/8610/merge_7165443572.3/tests/testOnPackagedReleaseJar/classes/org.broadinstitute.hellbender.tools.walkers.vqsr.scalable.TrainVariantAnnotationsModelIntegrationTest.html. As you suspected/hoped, all the differences were tiny. When you have a chance, could you please review these changes to the VETS tests and let me know if you spot any issues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977:612,scalab,scalable,612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610#issuecomment-1850563977,2,['scalab'],['scalable']
Performance,"Thanks @samuelklee. I have taken a read through most of the forum posts that I could find that reference the `GermlineCNVCaller` or just `germline cnv`. I've also read through the tutorial dock, and poked around in the published WDL. I've heard good things about the results of the GATK germline CNV caller, and thus would really like to get it performing well in my own hands. I _think_ at this point I have it so that I should be able to generate calls for my 20 samples in 15-20 hours on a 96-core machine. If the results look better than the alternatives then I'll probably end up spending more time trying to figure out how best to run things to minimize runtime/cost while maintaining sensitivity. I'll keep you posted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532371274:345,perform,performing,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532371274,1,['perform'],['performing']
Performance,"Thanks @spatel-gfb. The consolidate argument is what will give you the query performance as the consolidation happens in memory when running `GenotypeGVCFs` or `SelectVariants` otherwise. Working on the fix for the consolidation issue, hopefully will have it in a branch in a day or so. Yes, running queries from a local disk will be the fastest, but running from NFS/Lustre/Cloud Storage should be comparable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039672759:77,perform,performance,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1039672759,1,['perform'],['performance']
Performance,"Thanks a lot for this explanation. For the moment I'm more interested in the toolkit that I'm implementing, which does not have a master script for running. It is good to know that if I implement a Spark tool it could run locally with a shadow jar. I will study a bit more about spark in the near future. So just the last question, and I will close this: why there are some tools that have a Spark version and a normal version in GATK4? If the Spark version could run locally, is there any performance issue related to run it without Spark?. Thanks a lot for all your answers, it is very informative :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273309729:490,perform,performance,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273309729,1,['perform'],['performance']
Performance,Thanks everyone for working this out. Is there any chance we can detect this problem on load and fail rather than silently giving bad output?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105:88,load,load,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105,1,['load'],['load']
Performance,"Thanks for adding this! Incidentally, I noticed a few messages are still emitted by com.github.fommil.jni.JniLoader (which uses a different logger) in CreateReadCountPanelOfNormals when native libraries are loaded by MLlib, but probably more trouble than it's worth to clean those up. Couple of minor comments, looks fine to me but maybe engine team should chime in.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5825#issuecomment-475703103:207,load,loaded,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825#issuecomment-475703103,1,['load'],['loaded']
Performance,"Thanks for all the comments! I've made fixes for the minor ones. I'd like to address the secondary and supplemental reads improvement separately (https://github.com/broadinstitute/gatk/issues/2418), so that it doesn't block progress on testing the intervals optimization. I hope this PR can go in once we have a Hadoop-BAM release (which I'll do soon). (Regarding the ""keep paired reads together"" code - as I've explained in a comment, it's pragmatic to have this code in GATK.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-281688390:258,optimiz,optimization,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2350#issuecomment-281688390,1,['optimiz'],['optimization']
Performance,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:358,tune,tune,358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"Thanks for chiming in!. @davidbenjamin can you give a little more detail on the kind of merging operations you'd need?. @tedsharpe I believe bedGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is most",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:643,load,loading,643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329,2,['load'],['loading']
Performance,"Thanks for comments about the documentation @LeeTL1220 - I fixed them (I hope) trying to explain the logic behind the element tracker. In principle a developer shouldn't care about when to use them, because they should come from a `LocusIteratorByState` or from inside a previous tracker. In addition, the implementation should be (most of the cases) hidden from the API user, which should use `ReadPileup`. The idea of the trackers come from GATK3, so this is a custom port with some design differences. The basic idea is to cache some operations that may be time consuming for large pileups (sorting, split by sample, extract a single sample). I actually haven't test the performance in a proper way, just running some tools in development with the branch and it feels like is faster - in my case I use all the features that are cache: split by sample, retrieving several times single-samples and also calling `fixOverlaps()` (which uses using sorted pileups). I think that because the `LocusIteratorByState` is already splitting by sample, that can improve even more performance, because it will come directly in the state where it can be used by-sample in an efficient way. And maybe, if the tool does not require to split by sample at all, we can add an option to disable that behavior while creating the tracker. Looking forward for your comments and ideas about this...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332144484:526,cache,cache,526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332144484,4,"['cache', 'perform']","['cache', 'performance']"
Performance,"Thanks for looking into this, @vruano. We've not optimized CollectReadCounts for runtime or experimented with how the filters might affect performance (the latter would be easier with the FC-based evaluation in place). Can you first try to use `disable-read-filter` on the offending filters to see if that affects runtime or the resulting counts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425212036:49,optimiz,optimized,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5233#issuecomment-425212036,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Thanks for opening this issue. I hope that this performance issue can be fixed in HTSJDK soon, but I agree a warning would be useful in the intervening time. . To clarify, I believe this applies to any tool that loads a VCF but does not need to parse genotypes - not just SelectVariants. For instance, I saw a 5-10x slowdown in SVAnnotate with unsorted sample IDs for a VCF with ~2500 samples. . Another note: GATK did not reorder the sample IDs in the output VCF during my tests of SVAnnotate, but did reorder IDs during SelectVariants.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7732#issuecomment-1074303682:48,perform,performance,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7732#issuecomment-1074303682,2,"['load', 'perform']","['loads', 'performance']"
Performance,"Thanks for pointing this out, @cxfustc. We should probably update to more recent versions of Apache Commons Math, where this appears to be fixed. We may want to check for any other performance/runtime improvements. @lbergelson what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-527147418:181,perform,performance,181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6133#issuecomment-527147418,1,['perform'],['performance']
Performance,"Thanks for the quick answer, @davidbenjamin! By the moment is just that, but in the future I would plan to perform more complicating modeling. This is just a proof-of-concept that I'm working on as a starting point. Could it be possible to implement addition of new alleles, reads for a sample and likelihoods? Something like `add(String sample, GATKRead read, Allele allele, double likelihood)` method, which takes reads/alleles included or not in the initialization?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250101548:107,perform,perform,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2185#issuecomment-250101548,1,['perform'],['perform']
Performance,"Thanks for the review @jamesemery. I have tried these changes on a cluster with an exome-sized input, with no performance issues, so I'd be comfortable merging. As you say it will be a useful basis for the HaplotypeCallerSpark work we are doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5386#issuecomment-436634110:110,perform,performance,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5386#issuecomment-436634110,1,['perform'],['performance']
Performance,"Thanks for the review James. Regarding keeping Broadcast for BQSR - I'm not sure we need to. The Spark files approach is superior for several reasons: it's a lot faster (~2x), it uses a lot less memory (MB vs GB), it works progressively (conversely, if the reference is read into memory it takes a few minutes and can't be parallelized, so can't be sped up as much due to Amdahl's law), it doesn't require conversion of `fasta` reference files to `2bit`. We don't know about how this performs on single multi-core machines (in Spark local mode), but I think we can examine and optimize that later.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416182781:484,perform,performs,484,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-416182781,2,"['optimiz', 'perform']","['optimize', 'performs']"
Performance,"Thanks for these questions, @tfenne, and glad you are experimenting with the workflow. Several Broad-internal groups are running various WGS and WES analyses and are seeing encouraging performance, so Iâ€™m looking forward to hearing feedback from you as well. However, I am currently indisposed and may be out for the next month or so, but @mwalker174 (who has now taken over from me as CNV tech lead, with a focus on the germline workflows) and @asmirnov239 should be able to point you in the right direction and respond to you in more detail. In the meantime, you may find some pointers in various forum posts or issues here on GitHub. Weâ€™re looking into improving documentation processes for runtime/requirements GATK-wide, which should help with this sort of thing in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246:185,perform,performance,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166#issuecomment-532343246,2,['perform'],['performance']
Performance,"Thanks for these suggestions, @sooheelee. Note that there's already an issue filed to add `@PG` tags at https://github.com/broadinstitute/gatk/issues/4117. As for the `@RG` tag, I was following the example of the SV team, which I saw introduced a custom RG ID (although this may only be used for tagging intermediate files---not sure?) Although not ideal, I think passing the sequence dictionary and sample name in this way allows us to reuse the relevant SAM header code and also prevents the possibility of users mixing up samples. (Recall that the old pipeline required the sample name to be passed in as a separate input to each tool and that no validation that each input came from the same sample was performed.). As for VCF output, we are also planning to add this to the ModelSegments pipeline (it is already in the gCNV pipeline). See https://github.com/broadinstitute/gatk/issues/4114. However, I think it makes sense for this to wait until the improved caller is in. I'll close this as a dupe for now, but we'll be able to reference your comments here from those issues in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4481#issuecomment-369986160:707,perform,performed,707,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4481#issuecomment-369986160,1,['perform'],['performed']
Performance,Thanks for your suggestion @wir963. The `--min-score-identity` and `--host-min-identity` parameters can be used to tune your desired specificity/sensitivity for microbe read detection. The default settings should guarantee that the identified microbial reads have better alignments to the microbe reference than host.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510:115,tune,tune,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510,1,['tune'],['tune']
Performance,"Thanks to @lbergelson and the magic that is git-bisect we have isolated the reproducibility bug on my toy example this commit in master: ; #5554 @davidbenjamin ; It looks like when we added that optimization to restrict the number of outgoing edges for vertexes we may have introduced non-determinism (which looks like it might stem from tie-breaking around the maxHaplotypes number output for the results. Since the priority queue has undefined behavior for tied haplotypes its not surprising that there was some non-determinism in the output, what is surprising is that this never manifested itself (apparently) until this branch. This doesn't quite explain why Google inputs should make a difference at all, that is worth investigating further. . Louis and I propose after some discussion to create an arbitrary tiebreaker for the paths, since the graph code doesn't give paths unique deterministic identifiers we could arbitrarily tiebreak based on the bases. This should only make a difference at the very fringes of the output in sites where we already discover a pathologically large number of haplotypes. Any objections @davidbenjamin @ldgauthier",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6105#issuecomment-523173394:195,optimiz,optimization,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6105#issuecomment-523173394,2,"['optimiz', 'queue']","['optimization', 'queue']"
Performance,"Thanks to a suggestion of mwalker174, I solved the issue. I had to specify, for each input and output file, the full path from the root. All the HPC nodes share the same hdfs, so it worked. What leaves me a bit perplexed, is that the task took 5 minutes to run on a 16 cores nodes, and exactly the same to run on a master-worker setup with 5 workers and 16 cores each (the log cofirms that the tasks were distributed to the workers IP addreses). Is this something expected? Shall I maybe try with alarger input to see the difference in performances? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326:536,perform,performances,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326,1,['perform'],['performances']
Performance,"Thanks very much for your analysis. Job 4 does create a lot of garbage, but that appears to be inevitable whenever you are dealing with a PairRDD: You have to use a Tuple2 to represent key and value rather than using a more memory-conservative custom data object. You end up with a gazillion tiny objects that survive only during the shuffle. Too bad they didn't base PairRDD on an interface like Map.Entry. Also too bad that you cannot force a shuffle on a (plain old, non-Pair) RDD. Why not just treat it as a key-only structure and allow repartitioning? I mention this not merely to whine, but also in the faint hope that you've developed some helpful workarounds. I don't think we have enough memory to persist the reads, but we can revisit that later. Job 5 *is* doing a lot of computation. It's turning each read into kmers and testing each of those kmers to see if they exist in a large hash table. I don't think there's much opportunity for further optimization -- I knew this would be a bottleneck and tried my best to make the code efficient. The skew in task size is definitely a problem, and I'll be looking for opportunities to address that issue. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292230002:957,optimiz,optimization,957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292230002,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimization']"
Performance,"Thanks! This is a good question. I had seen it too and looked into it much greater detail now. For Extract (both for VCF and PGEN) the only BigQuery activity that is being recorded is on the `sample_info` table. Each shard in a multi-sharded extract is querying the entire table to get a map of sample id to sample name (excluding withdrawn). Since every shard is running the same query (again, not sample-specific, it's the whole table!), BigQuery returns the cached result in almost all cases and so, the scanned value is 0. For the run in question, there were two shards which returned values of 54. I think they ran close enough in time that they weren't cached.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8958#issuecomment-2302656350:461,cache,cached,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8958#issuecomment-2302656350,2,['cache'],['cached']
Performance,"Thanks, @cmnbroad!. - You're right about gatkbase-2.1.0, that image is coming from #5026, which needs some more work. We can delete it for the time being if you think it'll cause confusion.; - Correct, I think the import statement for `reshape` in BQSR.R was always incorrect/extraneous. `reshape2` is the correct dependency for `ggplot2` (which is itself imported), and `reshape` is not explicitly used in BQSR.R. So to recap: I removed the installation of this unnecessary package, but failed to remove an unnecessary import statement since it was in an untested code path, which was then caught when users tried to run the tool. Investigation of this issue then revealed that `ggplot2` was not installed correctly in the current base image, due to a completely unrelated dependency issue.; - Good call on clearing the Travis cache. Not actually sure how to do that, do I just delete the cache at https://travis-ci.org/broadinstitute/gatk/caches for this particular branch?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924:828,cache,cache,828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040#issuecomment-408447924,6,['cache'],"['cache', 'caches']"
Performance,"Thanks, @droazen. While I understand the effects of the funding landscape on academic resources, it seems to me this is a full capitulation of the GATK developer team given a serious bug, especially in light of the fact that the team seems to have enough resources to continue working on Mutect3. Mutect2 has been one of the best performing variant callers of the last years and is a major reason for the Broad's good reputation in the oncology bioinformatics field. GATK and Mutect2 are used by hundreds of institutions in clinical practice, affecting thousands of real patients' lives. Almost all of these institutions are likely to use clinical WES assays due to cost reasons and will thus have been directly affected by this issue _for the last three years_. Also, almost all of these institutions will never learn of this bug since they likely trusted in the developers to have proper functional regression tests in place. If this is indeed the best the Broad can do as an institution, then I will take your offer of providing a build of Mutect2 4.1.8.1 with the log4j vulnerability patched out - thank you. The one thing that I am asking for in addition (for the sake of the overall oncology bioinformatics community), however, is that you conduct a best effort to notify organizations (universities, hospitals, and biotechs/pharmaceuticals that you know are using Mutect2) and best-practise workflow owners (Nextflow, Snakemake, WDL, CWL etc. that include Mutect2) of the forced downgrade. Also, I think it makes sense to include a very prominent warning into the Mutect2 READMEs and GATK best practice documentations and guides. I know that this is work, too, but with success comes responsibility, and I can just hope that providing proper warnings uses less developer bandwidth than applying binary search to find out which of these [10 commits between 4.1.8.1 and 4.1.9.0 that are touching variant filtering (see below)](https://github.com/broadinstitute/gatk/compare/4.1.8.1...4.1.9.0) bro",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226:330,perform,performing,330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1535909226,2,['perform'],['performing']
Performance,"Thanks, I found the weights and loaded the 1D model. Would it also be possible to get the original training script that generated this model?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4511#issuecomment-375091383:32,load,loaded,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4511#issuecomment-375091383,1,['load'],['loaded']
Performance,"That sounds like a bad implementation from our side - apologies. We are working on fully fixing the Protobuf implementation. As part of that task, the temp loader/query JSON files will no longer be created (the vid and callset JSONs will still be needed).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4106#issuecomment-356986554:156,load,loader,156,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4106#issuecomment-356986554,1,['load'],['loader']
Performance,"That's correct, @akiezun.; However, it's not just stripping out that code. There are a ton of optimizations that can then be made to the code to simplify it afterwards. These classes were made very bulky to accommodate the indel calibration, and we should really remove that bulk. I can help with that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427:94,optimiz,optimizations,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1056#issuecomment-152047427,1,['optimiz'],['optimizations']
Performance,The Hail team noticed this as well. I think Cotton told me that GDB appeared to be writing 3X more data than he expected. @nalinigans how hard would this be to optimize?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595324294:160,optimiz,optimize,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487#issuecomment-595324294,1,['optimiz'],['optimize']
Performance,"The `ExtractCohortLiteFilterRecordUnitTest` appears to be blowing up b/c it's looking for a ""calibration_sensitivity"" field in the `src/test/resources/org/broadinstitute/hellbender/tools/gvs/extract/ExtractCohortLiteFilterRecord/test_input.avro` file that's not there. Unexpectedly (for VQSR Lite) there is a ""vqslod"" field. ```; $ avro-tools getschema ~/gitrepos/gatk/src/test/resources/org/broadinstitute/hellbender/tools/gvs/extract/ExtractCohortLiteFilterRecord/test_input.avro ; 23/04/11 16:22:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; {; ""type"" : ""record"",; ""name"" : ""Root"",; ""fields"" : [ {; ""name"" : ""location"",; ""type"" : [ ""null"", ""long"" ]; }, {; ""name"" : ""alt"",; ""type"" : [ ""null"", ""string"" ]; }, {; ""name"" : ""ref"",; ""type"" : [ ""null"", ""string"" ]; }, {; ""name"" : ""vqslod"",; ""type"" : [ ""null"", ""double"" ]; }, {; ""name"" : ""yng_status"",; ""type"" : [ ""null"", ""string"" ]; } ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8284#issuecomment-1504046383:540,load,load,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8284#issuecomment-1504046383,1,['load'],['load']
Performance,"The advantage of using SLF4J is that it is a general facade, so it makes simpler to change for one logging system to other if the bound is implemented. For the most common logging systems (log4j, jul, JLC, etc.), there are this implementation and even no-op logging. One of the nice things from slf4j is that it allows to use the logging format set by the software to every library dependency, controlling the verbosity of other libraries too. . After having a look to the gradle dependencies, it seems that ADAM and Spark use slf4j. This will allow better integration with the two libraries: now the `slf4-jdk` is completely removed, and I don't know if this will blow up at some point if some of the ADAM/Spark classes try to load them. In addition, it will make GATK4 more general. Regarding features, I'm not using more that what log4j is providing, but I'm quite familiar with logback and I have a bias to use it if possible, but the GATK framework as it is implemented now ""force"" to use log4j. But anyway, I'm happy also with using log4j and I was only suggesting this for make GATK4 more general (and to come back in my work to logback, but that is just personal taste). @lbergelson, feel free to close the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054:728,load,load,728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2176#issuecomment-259211054,2,['load'],['load']
Performance,The bulk of this issue was addressed in #6266. I believe @samuelklee also did some detailed profiling and determined that the memory bottlenecks were inference-related.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-926158295:133,bottleneck,bottlenecks,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-926158295,1,['bottleneck'],['bottlenecks']
Performance,"The change is because we started using VariantsSparkSink to write VCFs on the cluster, rather than writing them in a single thread from the driver (which doesn't scale). VariantsSparkSink only supports .bgz extensions currently, not .gz. So if you change the output extension to .bgz it will work. Gzip is not splittable so if possible we'd avoid outputting it at all. Hail for example will not load .gz unless the force option is used. (There are actually two force options, one to read it as regular gzip, the other to read it as bgzip, so you have to know which flavour of gzip it is...). We could do one of the following:. 1. Throw an error if the extension is .gz.; 2. Write bgzipped output to the .gz file.; 3. Write regular gzipped output to the .gz file. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307:395,load,load,395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307,1,['load'],['load']
Performance,"The change is good :thumbsup:. You may also want to investigate having fewer parallel reads, so we get throttled less.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307443641:103,throttle,throttled,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307443641,1,['throttle'],['throttled']
Performance,"The command line I use is as /opt/reseq_softwares/gatk-4.1.8.0/gatk GenomicsDBImport --java-options ""-Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/new_20220624/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db.The output log file is as followsï¼ŒUsing GATK jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx100g -Xms100g -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar GenomicsDBImport -R /mnt/nvme1/reference/Gallus_gallus/Ensembl_g6a/Gallus_gallus.GRCg6a.dna.toplevel.fa --sample-name-map samplelist -L chr33.bed --genomicsdb-workspace-path chr33.db; 11:19:39.692 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/nvme1/opt/reseq_softwares/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 04, 2022 11:19:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:19:40.099 INFO GenomicsDBImport - ------------------------------------------------------------; 11:19:40.099 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:19:40.100 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:19:40.100 INFO GenomicsDBImport - Executing as maosong@nygpu on Linux v3.10.0-1160.45.1.el7.x86_64 amd64; 11:19:40.100 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_312-b07; 11:19:40.100 INFO GenomicsDBImport - Start Date/Time: 2022å¹´7æœˆ4æ—¥ ä¸Šåˆ11æ—¶19åˆ†39ç§’; 11:19:40.100 INFO GenomicsDBImport - ----------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460:998,Load,Loading,998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7952#issuecomment-1196217460,1,['Load'],['Loading']
Performance,"The fix here is to stop Barclay from auto-expanding files with a "".list"" extension (the extension will be changed to "".args""). With this change, for the common case of a .list file, the interval merging code will see a single (file) value, which it will load directly, rather than the already expanded list of interval strings. Loading the file directly results in a single call to mergeListsBySetOperator (which re-copies the entire aggregate interval list on each call), rather one call per interval. Note that this doesn't actually change the underlying order of growth of mergeListsBySetOperator; it just minimizes the magnitude of the input size for the common .list case. The same problem would still exist if a "".args"" file was supplied as the intervals file. we may want to address that in a separate PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-342526178:254,load,load,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3788#issuecomment-342526178,2,"['Load', 'load']","['Loading', 'load']"
Performance,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:178,perform,performed,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183,1,['perform'],['performed']
Performance,"The issue is definitely in htsjdk -- GATK is just calling `queryOverlapping()` a single time per file with all of the intervals, having first called `QueryInterval.optimizeIntervals()` on the interval list as required by the API.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356#issuecomment-275424956:164,optimiz,optimizeIntervals,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356#issuecomment-275424956,1,['optimiz'],['optimizeIntervals']
Performance,The main issue with this task was that the query results were being limited to 100 by default. So we use the -n param now in the query. Another issue was that we were running bq show on a table variable $TABLE which is never defined.; I also changed this because the approach (returning all the samples names of the samples that have been loaded) didn't seem scalable. I wanted to only return at most the number of samples we are trying to ingest.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230:339,load,loaded,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7470#issuecomment-921024230,2,"['load', 'scalab']","['loaded', 'scalable']"
Performance,"The memory footprint used by ReadCountCollectionUtils.parse is extremely large compared to the size of the file. I suspect there may be memory leaks in TableReader. Speed is also an issue; pandas is 4x faster (taking ~10s on my desktop) on a 300bp-bin WGS read-count file with ~11.5 million rows if all columns are read, and 10x faster if the Target name field (which is pretty much useless) is ignored. (Also, for some reason, the speed difference is much greater when running within my Ubuntu VM on my Broad laptop; what takes pandas ~15s to load takes ReadCountCollectionUtils.parse so long that I just end up killing it...not sure why this is?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316417271:544,load,load,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316417271,1,['load'],['load']
Performance,The multiple output file problem is fixed in #2350. The ConcurrentModificationException issue needs more investigation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349#issuecomment-277660539:56,Concurren,ConcurrentModificationException,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349#issuecomment-277660539,1,['Concurren'],['ConcurrentModificationException']
Performance,"The original reason for this issue was to investigate why BAM files compressed with GKL level 1 DEFLATE compression are larger than BAM files compressed with JDK level 5 DEFLATE compression. Stating the obvious, level 1 compressed files are expected to be larger file than level 5 compressed files. However, GKL level 1 and 2 trade compression ratio for compression speed, so we ran some experiments on data from #4249 to investigate. The chart below shows compression speed vs. compression ratio for 11 files compressed using GKL (use-jdk-deflater = False) and JDK (use-jdk-deflater = True) for compression levels 1 through 9. The chart shows BAM files with binned quality scores can be compressed more given more CPU effort (i.e. higher compression level). The effect is the same for GKL compression and JDK compression, but the compression ratio difference is higher for GKL due to the ratio vs. speed trade off. ![compression study](https://user-images.githubusercontent.com/10476709/38428795-a5f11e4c-398a-11e8-8cfb-800b0423a95a.png). My conclusion is GATK default compression settings work well for some use models (i.e. minimizing cloud compute cost). The default settings may not be optimal for all use models, so users are free to tune the compression options to minimize their own cost function.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-379290482:1240,tune,tune,1240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-379290482,1,['tune'],['tune']
Performance,The solution is definitely not to run Mutect2 in GVCF mode. It's too different from VCF mode and has a big performance cost.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677022081:107,perform,performance,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-677022081,1,['perform'],['performance']
Performance,"The thing about logarithms is that consistency between processors sacrifices performance because the `Math` library uses on-hardware implementations when possible. . I think the problem here is really our bad criterion for choosing the best haplotypes. I bet the calls would be the same, perhaps with tiny differences in QUALs, using the `--linked-de-bruijn-graph` argument to circumvent the `KBestHaplotypeFinder`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1587997729:77,perform,performance,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1587997729,1,['perform'],['performance']
Performance,"The two main resources limiting how many gVCFs you can import at once will be memory and file handles. . I'm not sure if you mean incremental import or batch size when you mention the iterative approach. I assume the latter, but just want to clarify that there isn't any reason to break up the import using incremental import. The batch size parameter effectively does that, so you might as well import all gVCFs you have available (optionally using batch size if you're running out of memory). I'll throw out batch sizes of 50 or 100 as being reasonable, but the size of the intervals being imported will make a difference. It would be best to try to monitor how much memory is being used with those settings. There won't be a huge difference in import performance depending on the number of batches (ignoring memory issues) but if you have more than a 100 or so batches and you don't enable the `--consolidate` option you may see some query performance degradation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952:754,perform,performance,754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-656376952,2,['perform'],['performance']
Performance,"The underlying issue here is is that the GATK conda env environment isn't established since bioconda doesn't appear to configure it. The NPE needs is fixed by #7816. In this particular case it appears that some of the requirements are satisfied, since the code gets past the initial check to see if the GATK python code is available. But then the actual CNN code can't be loaded.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269:372,load,loaded,372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1110010269,1,['load'],['loaded']
Performance,"Theory from @cmnbroad is below:. ```; I think this is happening because were trying to serialize the class loader sun.misc.Launcher$AppClassLoader), which appears to be reached through the graph by way of via https://github.com/damiencarol/jsr203-hadoop/blob/master/src/main/java/hdfs/jsr203/HadoopFileSystem.java#L82. We probably need to short circuit that with a custom serializer for one of these:. Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager). See, for instance, dbpedia/distributed-extraction-framework#9.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579:107,load,loader,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730#issuecomment-671508579,1,['load'],['loader']
Performance,There are a few other performance fixes for SelectVariants coming in the next release of GATK as well. We discovered a number of really pathologically cases that we are fixing bu haven't released yet.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-680118909:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-680118909,1,['perform'],['performance']
Performance,"There are currently two failing tests, both of which need fixes in htsjdk.; * CountVariantsSparkIntegrationTest. This is a concurrency issue where VCFCodec (which isn't threadsafe) is being shared by multiple tasks in each Spark executor. The fix is for each task (partition) to have its own VCFCodec. This needs a couple of small changes in htsjdk to make it possible to access the codec's version and header fields so the codec can be recreated. See https://github.com/samtools/htsjdk/pull/1176.; * CpxVariantReInterpreterSparkIntegrationTest. I tracked this down to a problem with the buffer in htsjdk's SeekableBufferedStream. See https://github.com/samtools/htsjdk/pull/1175 for the fix.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035:123,concurren,concurrency,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5138#issuecomment-417241035,1,['concurren'],['concurrency']
Performance,"There is still more that could be done to improve performance, we take 18 minutes to do what I'm told took 12 in gatk3, but I'm not sure it's worth it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6672#issuecomment-648227914:50,perform,performance,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6672#issuecomment-648227914,1,['perform'],['performance']
Performance,"There was a confirmed performance regression in `BaseRecalibratorSpark` just before the 4.0 release: https://github.com/broadinstitute/gatk/issues/4376. I suspect that's what's responsible for the results reported above. Until this is patched (most likely in the next GATK release), I'd recommend running the regular `BaseRecalibrator` instead of the spark version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365991967:22,perform,performance,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300#issuecomment-365991967,1,['perform'],['performance']
Performance,"There was an issue with the compressed write of book-keeping files. The import would seem to have succeeded, but the issue would show up during queries sometimes. This was fixed in `4.2.3.0` - see https://github.com/broadinstitute/gatk/pull/7520. If you did use `4.2.3.0`, it should not have been a problem. . Yes please, can you arrange to share the `_book_keeping.tdb.gz` file? Can you also share the `__array_schema.tdb` file if possible. Also, how many fragments(folders starting with __) do you see in /home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942, there may be multiple if you did not use the `--consolidate` option during import?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042059426:541,cache,cachedData,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1042059426,1,['cache'],['cachedData']
Performance,"These all sound like positive improvements. Provided they don't affect performance by dramatically increasing the number of discovered haplotypes, I'm on board. Hopefully this will go a long way towards removing the dependence of calling on the active region boundaries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-447012273:71,perform,performance,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3697#issuecomment-447012273,1,['perform'],['performance']
Performance,"These changes look good to me in terms of reverting to GATK3-matching results. On a separate note though, the optimizations on my branch are largely aimed at reducing unnecessary allocations and boxing, since this (GMM expectation step) is the main hotspot in VQSR, and we iterate through it millions of times. That may conflict with the use of streams as introduced in these PRs (I don't think they introduce any boxing since they use DoubleStreams, but may still result in the same number of array allocation(s) that were in the original code). When I merge in my changes I'll do some more profiling. The stream code certainly looks nicer so it would be nice if we can keep it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257578991:110,optimiz,optimizations,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2241#issuecomment-257578991,1,['optimiz'],['optimizations']
Performance,These now seem to be working on both firecloud and cromwell with intel-optimized tensorflow and t tranche-filtering. Back to you @ldgauthier.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394:71,optimiz,optimized,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394,1,['optimiz'],['optimized']
Performance,"They are retried. You're seeing this message because it failed more than `maxChannelReopens` times. The new version which you really want to use for any test at this point is described there:; https://github.com/broadinstitute/gatk/issues/2685#issuecomment-302798685. Among other things, this new version puts in a message about 'retry failed' when it runs out of retries to eliminate the very confusion that you ran into. GenomicsDBImport opens a large number of parallel connections and as a result is getting throttled fairly heavily (by the host GCE machine if nothing else). This results in timeouts and dropped connections. One way forward is to increase the retry delays, another is to find a way to do the same work with fewer parallel open connections.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753:512,throttle,throttled,512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753,1,['throttle'],['throttled']
Performance,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:162,Load,Loading,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"Things left for later:; * `GenotypeIndexCalculator` sometimes interacts with primitive arrays, sometimes with `GenotypeAlleleCounts`; * `GenotypeLikelihoodCalculator` has extraneous responsibilities and doesn't interact with `GenotypeAlleleCounts` as well as it should.; * `alleleCountsToIndex(final GenotypeAlleleCounts newGAC, final int[] newToOldAlleleMap)` in `GenotypeIndexCalculator` needs refactoring.; * `GenotypeLikelihoodCalculators` is really just a cache of `GenotypeAlleleCounts`.; * `GenotypeAlleleCounts` has some unused and barely-used methods, and it precomputes a lot of quantities that are not often needed and could be computed on-the-fly without difficulty or expense.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217:461,cache,cache,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1066400217,1,['cache'],['cache']
Performance,"This change degrades performance on Spark, so should not be merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-326366040:21,perform,performance,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3533#issuecomment-326366040,1,['perform'],['performance']
Performance,"This error message is related to GATK's ability to load files on Google buckets (""gcs://bucket/file.bam""). This ability is enabled even when running locally (this aspect is intentional, because it's useful to be able to run a local GATK instance to process remote data without having to fire up a VM). As the bucket-reading code (""NIO"") initializes, it looks for credentials to use. Those can be set via an environment variable or via `gcloud auth`, as described in GATK's README. If neither of these are set, it checks whether it's currently running in a Google virtual machine (so it can figure out who owns the virtual machine that it's running on, and use those credentials). Apparently this code throws an exception if it runs out of ways to find credentials, and our code prints it out and moves on. The message is useful, for if we *were* running in a google VM and the credential-finding failed, we'd certainly like to know. Whether we need the full stack trace, now, that's a choice we have to make.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095:51,load,load,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4369#issuecomment-424038095,1,['load'],['load']
Performance,This is easily resolvable without performance profiling since the only remaining use of the `IntervalSkipList` was in `ContextShard` which is now also unused / untested. Removing context shard and rebasing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-461935085:34,perform,performance,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-461935085,1,['perform'],['performance']
Performance,"This is still throwing errors. Importing `chr2	6861126	.	ACCCACACAC	*,<NON_REF>	2.22	.	AS_RAW_BaseQRankSum=|||;AS_RAW_MQ=0.00|0.00|10800.00|0.00;AS_RAW_MQRankSum=|||;AS_RAW_ReadPosRankSum=|||;AS_SB_TABLE=0,0|0,0|3,0|0,0;DP=6;QUALapprox=126;RAW_MQandDP=21600,6;VarDP=3	GT:AD:DP:GQ:PL:SB	1/1:0,3,0:3:10:126,10,0,129,18,168:0,0,3,0`; for interval -L chr2:6861134-7268940 produces the log; ```; 11:11:14.566 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:11:15.838 INFO GenomicsDBImport - Importing batch 1 with 1 samples; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Found cell [ 0, [ 255817547, 255817547 ] ] that does not belong to TileDB/GenomicsDB partition with row_bounds [ 0, 0 ] column_bounds [ 255817555, 256225361 ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863:621,Load,LoadOperatorException,621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449#issuecomment-484558863,2,['Load'],['LoadOperatorException']
Performance,"This looks OK to me. If I understand correctly, the decaying probability to stay in a class/state now only applies to the silent class and the baseline copy-number state?. @asmirnov239 Let's try to do a quick run on GPC2 to see how performance changes. You might want to sweep `p-alt`, `p-active`, and `cnv-coherence-length` again, since I don't think we should expect the optimal parameters to be the same across both HMMs; perhaps just sample the corners of the hypercube. Then let's rerun a SFARI cohort with 1) the filtered target list, 2) this HMM change, and 3) both, using optimal parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454:232,perform,performance,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454,1,['perform'],['performance']
Performance,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:107,perform,performance,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['perform'],['performance']
Performance,This needs a fix in the spark loading code. We need to explicitly account for empty shards when doing the shuffle of the first readname to the shard to the left it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662564530:30,load,loading,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709#issuecomment-662564530,1,['load'],['loading']
Performance,"This should add support for reading bam files with CSI indexes, as well as porting the FastaReferenceWriter to htsjdk and a lot of other changes to htsjdk. @samuelklee This includes the overlap detector optimizations you wanted as well as the changes to let you write interval file to paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651:203,optimiz,optimizations,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651,1,['optimiz'],['optimizations']
Performance,This should be resolved in GATK 4.1.8.0 with the new `--genomicsdb-shared-posixfs-optimizations` argument -- closing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6321#issuecomment-651285797:82,optimiz,optimizations,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6321#issuecomment-651285797,1,['optimiz'],['optimizations']
Performance,"This sounds like a good idea, but it might be tricky because much of the file reading is likely done by native code that we're just wrapping. We could do something like automatically downloading and caching the file locally and handing the cached version to the native library. Would that suit your needs?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710:240,cache,cached,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3178#issuecomment-314147710,1,['cache'],['cached']
Performance,"This will definitely not be part of this PR, but I wonder if you'd be willing to comment on this: it would be quite helpful if I could get VariantEval to run in a scatter/gather mode, in which I submit jobs to the cluster, each saves results for an internal and then I combine them. One way to make this work would be for VariantEvalEngine to save the StratificationManager (or some version of this) to disk, perhaps serialized to JSON with Jackson. There would need to be a separate tool that takes N serialized StratificationManager objects, loads then, merges, and then writes the actual report. It's not completely implemented, but there is already a concept of combineStrats() in StratificationManager. It doesnt work complete, but this doesnt seem all that far from being able to take two StratificationManager objects and returning the merged form. Is this a reasonable idea?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750694321:544,load,loads,544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-750694321,1,['load'],['loads']
Performance,"To add some context, the test file was a file with a single no-call with basically no information at each position in the genome. A very weird (but valid) input. The problem was that we were regenerating the sequence dictionary on every site which is an expensive operation. Caching the sequence dictionary at the beginning improved the speed by 2 orders of magnitude on this file. . There are probably additional performance optimizations available for data like this. We spend a lot of time in overhead of creating empty AlleleLikelihoods objects and things like that. I don't think it's worth pursuing at the moment though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849:414,perform,performance,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6663#issuecomment-648232849,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"To clarify our experimental setup: for the benchmark we are using the latest release (v.1.2) somatic ""ground truth"" of the HCC1395 cell line from . [https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/seqc/Somatic_Mutation_WG/release/latest/). It contains ~40k SNVs and ~2k INDELs in ~2.4Gb high-confidence regions.; In high-confidence regions intersected with WES bed, we still have ~1.1k SNVs and ~100 INDELs. Therefore, even in the WES analysis scenario, the SNV counts should be high enough to draw reliable conclusions when comparing performance between different callers and releases.; For WES INDELs, the counts are indeed rather low and results should be interpreted with caution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032:633,perform,performance,633,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1172101032,1,['perform'],['performance']
Performance,To ensure that our native libraries are all loaded in a consistent way. For @lbergelson,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2408#issuecomment-279831623:44,load,loaded,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2408#issuecomment-279831623,1,['load'],['loaded']
Performance,UkhpZGRlblN0YXRlLmphdmE=) | `100% <100%> (Ã¸)` | `4 <1> (+1)` | :arrow_up: |; | [...tools/exome/segmentation/ClusteringGenomicHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vQ2x1c3RlcmluZ0dlbm9taWNITU0uamF2YQ==) | `100% <100%> (Ã¸)` | `14 <6> (+1)` | :arrow_up: |; | [...a/org/broadinstitute/hellbender/utils/hmm/HMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vSE1NLmphdmE=) | `38.462% <100%> (+11.189%)` | `7 <3> (+3)` | :arrow_up: |; | [...hellbender/utils/hmm/ForwardBackwardAlgorithm.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9obW0vRm9yd2FyZEJhY2t3YXJkQWxnb3JpdGhtLmphdmE=) | `87.766% <100%> (-0.469%)` | `13 <0> (Ã¸)` | |; | [...lbender/tools/exome/segmentation/CopyRatioHMM.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vQ29weVJhdGlvSE1NLmphdmE=) | `100% <100%> (Ã¸)` | `5 <1> (Ã¸)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <100%> (Ã¸)` | `4 <0> (+2)` | :arrow_up: |; | [...r/tools/exome/segmentation/JointAFCRSegmenter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vSm9pbnRBRkNSU2VnbWVudGVyLmphdmE=) | `100% <100%> (+0.99%)` | `32 <5> (Ã¸)` | :arrow_down: |; | ... and [103 more](https://codecov.io/gh/broadinstitute/gatk/pull/3036?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201:3334,Perform,PerformAlleleFractionSegmentation,3334,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3036#issuecomment-306513201,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,"Update:. 1000 samples on one interval, 11GB Xmx to the JVM:; 20 runs for each parameter set. - batchSize at **200** and buffering **disabled** yields: 25% success and 75% OOM; Runtime (when it succeed) is ~ 1.75x longer than batchSize 100 (with or without buffer); ; - batchSize at 100: ; Tried on both OpenJDK8 and Oracle JDK, with buffer disabled and default.; Overall similar performances. Got 2 failures over the 80 total runs:; - One SSL handshake on the ""default buffer OpenJDK""; - One `com.google.cloud.storage.StorageException: 503 Service Unavailable` on the ""buffer disabled OracleJDK""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301153727:379,perform,performances,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301153727,1,['perform'],['performances']
Performance,"Update:; - [ ] A cool name! (Marduk, Clarendon, NeuroVar) ? maybe I'll do a slack poll of dsde methods?; - [x] Model training script (in Python, eventually in Java); - [x] Pretrained model for WGS; - [x] Pretrained model for WEx (still being validated and was only trained on NA12878); - [x] Model inference and VCF annotation (in Java); - [x] Solution for applying filters based on CNN score cutoff (tranches.py script); - [ ] Alternate joint calling WDL? Or for re-filtering? (ideally with a $$$ estimate); - [ ] Performance optimizations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363436039:515,Perform,Performance,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4225#issuecomment-363436039,2,"['Perform', 'optimiz']","['Performance', 'optimizations']"
Performance,"Update:; The problem seemingly was caused by the logging part in Spark (thanks to Carlos H. Andrade Costa from IBM for pointing this out). So a temporary solution that works for now is to add the following lines in the initialization script for spinning up the cluster to turn off logging, . ```bash; sudo sed -i -- 's/log4j.rootCategory=INFO, console/log4j.rootCategory=OFF, console/g'\; /etc/spark/conf/log4j.properties; sudo sed -i -- 's/log4j.logger.org.apache.spark=WARN/log4j.logger.org.apache.spark=OFF/g'\; /etc/spark/conf/log4j.properties; sudo sed -i -- 's/hadoop.root.logger=INFO,console/hadoop.root.logger=OFF,console/g'\; /etc/hadoop/conf/log4j.properties; ```. assuming the cluster was created with . --image-version preview. I test run the tool where I observed the bug (`FindBreakpointEvidenceSpark`) twice on a cluster created & initialized this way and didn't observe any `ConcurrentModificationException`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2277#issuecomment-262640914:891,Concurren,ConcurrentModificationException,891,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2277#issuecomment-262640914,1,['Concurren'],['ConcurrentModificationException']
Performance,"User is reporting a nearly exact 2 minute pause at tool startup. Seems very suspicious, possibly some sort of gcs operation trying and timing out?. ```; 14:33:39.416 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 5; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:46.843 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 14:35:46.844 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:46.844 INFO BaseRecalibrator - Deflater: IntelDeflater; 14:35:46.844 INFO BaseRecalibrator - Inflater: IntelInflater; 14:35:46.844 INFO BaseRecalibrator - GCS max retries/reopens: 20; 14:35:46.844 INFO BaseRecalibrator - Using google-cloud-java patch 317951be3c2e898e3916a4b1abf5a9c220d84df8; 14:35:46.844 INFO BaseRecalibrator - Initializing engine; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756:193,Load,Loading,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-325211756,1,['Load'],['Loading']
Performance,"Using git bisect I found the commit that caused the performance regression: 8a366c7ba570c61338f7109b86c3284b80d5cf47. If I revert this, then both BQSR and HC both take the expected amount of time running on an exome (~15 min, ~10 min, respectively).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-365969330:52,perform,performance,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4376#issuecomment-365969330,1,['perform'],['performance']
Performance,"VD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1915,perform,performed,1915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performed']
Performance,Very excited that this is almost ready! I'm presently going to run some tests on matched BGE/exome samples to assess runtime performance and results with gatk-sv.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2125097844:125,perform,performance,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-2125097844,1,['perform'],['performance']
Performance,W50RXZpZGVuY2UuamF2YQ==) | `0% <0%> (-83.036%)` | `0 <0> (-13)` | |; | [...er/tools/spark/sv/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `8.523% <0%> (-63.352%)` | `0 <0> (-43)` | |; | [...ellbender/tools/spark/sv/ReadsForQNamesFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SZWFkc0ZvclFOYW1lc0ZpbmRlci5qYXZh) | `0% <0%> (-93.548%)` | `0 <0> (-7)` | |; | [...exome/germlinehmm/xhmm/XHMMArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9nZXJtbGluZWhtbS94aG1tL1hITU1Bcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-2%)` | |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...dinstitute/hellbender/utils/svd/OjAlgoAdapter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvT2pBbGdvQWRhcHRlci5qYXZh) | `0% <0%> (-100%)` | `0% <0%> (-10%)` | |; | [...llbender/tools/walkers/bqsr/GatherBQSRReports.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvR2F0aGVyQlFTUlJlcG9ydHMuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-2%)` | |; | [...itute/hellbender/tools/exome/SampleCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3133?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlb,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-310490622:2701,Perform,PerformSegmentation,2701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3133#issuecomment-310490622,1,['Perform'],['PerformSegmentation']
Performance,"WIth the 4.2.2.0 ReblockGVCF it is running fine. This was without rerunning the HaplotypeCaller to create the gvcf just the reblock. . ```; Using GATK jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O g1.test.reblock.g.vcf.gz; 00:54:40.318 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.2.0/install/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 12:54:40 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:54:40.501 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.501 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.2.0; 00:54:40.501 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:54:40.501 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 00:54:40.502 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 00:54:40.502 INFO ReblockGVCF - Start Date/Time: August 25, 2021 12:54:40 AM EDT; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.502 INFO ReblockGVCF - ------------------------------------------------------------; 00:54:40.503 INFO ReblockGVCF - HTSJDK Version: 2.24.1; 00:54:40.503 INFO ReblockGVCF - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643:789,Load,Loading,789,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334#issuecomment-905183643,1,['Load'],['Loading']
Performance,We already cache these values,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2058#issuecomment-494963009:11,cache,cache,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058#issuecomment-494963009,1,['cache'],['cache']
Performance,"We aren't using any multi-threading inside the GenomicsDB jar. If there is a way to reproduce this, we can take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-374035037:20,multi-thread,multi-threading,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-374035037,1,['multi-thread'],['multi-threading']
Performance,"We can work around the TestNG issue, but the `.so` not loading on travis is potentially serious, and prevents us from handing the tool off for profiling. Do we need to regenerate the `.so` @kgururaj @kdatta ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296268544:55,load,loading,55,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-296268544,1,['load'],['loading']
Performance,"We decided to remove the ""conversion"" to AllelicCapseg output from ModelSegments, since this was an ill defined procedure. The models used by AllelicCapseg and AllelicCNV/ModelSegments are simply different, so it's not possible to define a unique conversion between their model parameters. Compounding this, we also had difficulty finding up-to-date documentation about the models used by various versions of both AllelicCapseg and ABSOLUTE. That said, some of this removed functionality can be found in unsupported WDLs at https://github.com/broadinstitute/gatk/tree/master/scripts/unsupported/combine_tracks_postprocessing_cnv (specifically, see the PrototypeACSConversion task in combine_tracks.wdl). These scripts also attempt to perform rudimentary filtering of germline events found in the matched normal; see first link below for some additional caveats. Note that we cannot really answer further questions or otherwise support these scripts (and it's possible that the experimental/beta GATK tools used in the WDLs may be removed in the future), and the developer responsible for them has moved on from the Broad---use them at your own risk. See also https://gatkforums.broadinstitute.org/gatk/discussion/comment/59467 https://github.com/broadinstitute/gatk/pull/5450 https://github.com/broadinstitute/gatk/issues/5804 for additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603:734,perform,perform,734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6685#issuecomment-652407603,2,['perform'],['perform']
Performance,"We don't yet have good regression tests for Spark that run on a cluster and are separate from the jenkins performance tests. https://github.com/broadinstitute/gatk/issues/2298 will satisfy part of the requirements for this ticket once it's done (by catching the most basic regressions before merge), but there's also a need for larger-scale correctness tests whose status is clearly visible on github.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713:106,perform,performance,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2288#issuecomment-287473713,2,['perform'],['performance']
Performance,"We have discussed this and I have shown @lbergelson the error of his ways ;). Admittedly I'm still working on improving the presentation of content on the website -- but user feedback suggests they find the current site far superior to the old wiki. Also, I hate wikis. Also also, Louis was mostly complaining about the dev zone and queue docs, which do suck.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099:333,queue,queue,333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049#issuecomment-151957099,1,['queue'],['queue']
Performance,"We recommend backing up data just because it is the ""cleanest"" way to roll back. If backing up data is really such a pain point, you could skip doing that. Just back up the callset.json file, and don't turn on `--consolidate` when you're doing incremental import. If a failure happens, just roll back the callset.json and re-do the import. The downside is that the failed import will hang around and take up disk space, but hopefully it is a rare enough occurrence that it doesn't matter - and you will have saved yourself backing up the data. In response to 2) - I guess you're implying that the overhead of cluster/job scheduling won't amortize any benefits from parallelism there? I suppose that could be true, but doesn't seem to be worth optimizing towards that. What I'm asking is whether split and merge are purely an instrument to allow you to choose the granularity of parallelism you want to use? Or is there something else? As I said before, we are considering enabling other ways to do distributed import which would work for the former. It might go something like:; - Create a workspace/initialize configuration+intervals to be imported; - Actually do the import by kicking off (multiple) import(s). User can pick the number of intervals each import is responsible for. User must ensure that no interval gets specified in multiple import processes. P.S: regarding 1000s of small contigs - the current GenomicsDBImport doesn't so so well with large number of contigs (unless you do concatenate the contigs into fewer groups). We hope to have some changes coming soon that will help with that by adding an option for the tool to merge multiple contigs into a single folder in the workspace.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548:743,optimiz,optimizing,743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641037548,1,['optimiz'],['optimizing']
Performance,"We're going to release the new model in 3.7 and have users test-drive that for quite a bit before we move to release 4.0, so we should be able to get some feedback on performance in the wild before we need to make any final decisions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589:167,perform,performance,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2255#issuecomment-258412589,2,['perform'],['performance']
Performance,"We've found that, generally speaking, you do pay a penalty on single-core performance when becoming a Spark tool, but gain the ability to easily scale to multiple cores and get the job done quickly. This is why we've been maintaining both Spark and non-Spark versions of important tools. Whether this will be the case for your tools as well can only be determined by profiling. If you extract the logic of your tool into a separate class, it's usually possible to call that shared code from both the Spark and walker frameworks without much or any code duplication. See `BaseRecalibrator` and `BaseRecalibratorSpark` for an example of this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273524313:74,perform,performance,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273524313,1,['perform'],['performance']
Performance,"We've now run into a secondary issue with GenomicsDB workspaces. In the thread above we talked about problems with WGS import, and reducing batchSize seemed to help. Separately, we created a GenomicsDB workspace with a few hundred exome samples - this import worked fine with default settings. Next, we are using this workspace as input for GenotypeGVCFs. However, the performance is much, much slower than we saw when using similar sized combined gVCFs as input. Do you expect this? Are there ways to improve this? If one thinks about a genomicsDB workspace more like a database than single file, are there defrag/shrink-like tasks that need to be performed on the workspace for efficiency?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669327164:369,perform,performance,369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6688#issuecomment-669327164,2,['perform'],"['performance', 'performed']"
Performance,"Well, you're welcome to use gatk-launch as a launch script if you'd like (and feel free to rename to whatever you like...) A. There are a few reasons we have spark and non-spark versions of the tools. . 1. We wanted to port and validate certain tools as quickly as possible and doing a direct port from gatk3 -> gatk4 was easier than making them sparkified at the same time. 2. There's a tradeoff in using spark where you end up spending more total cpu hours in order to finish a job faster. Ideally this would be 1:1, double the number of cores and you halve the time to finish a job. It never scales perfectly though, there's always some overhead for being parallel. Our production pipelines are extremely sensitive to cost and not very sensitive to runtime, so they prefer we have a version that's optimized to use the least cpu hours even if that means a longer runtime. Other users prefer to be able to finish a job quickly and are willing to pay slightly more to do so, so we also have a spark version. . 3. Some tool are complicated to make work well spark. Spark works best when you can divide the input data into independent shards and then process them separately. This is complicated for things like the AssemblyRegion walker where you need context around each location of interest. We had to do things like add extra overlapping padding and things like that to avoid boundary issues where there are shard divisions. We don't yet fully understand spark performance and it's caveats, we're looking into that actively now. We hope that we'll be able to optimize our tools so that a spark pipeline of several tools in series is faster than running the individual non-spark versions, since it lets us avoid doing things like loading the bam file multiple times from disk. Whether or not we can achieve this is still and open question though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100:801,optimiz,optimized,801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273318100,4,"['load', 'optimiz', 'perform']","['loading', 'optimize', 'optimized', 'performance']"
Performance,"What [this](https://stackoverflow.com/a/45713118) user describes seems to be what is happening (or supposed to happen): GATK extracts the .so file to a tmp directory and then uses `System.load(""/path/to/lib.so"")` to load the library. Is it possible that something inside the GATK package was not cross-compiled / is misconfigured [like in this case](https://github.com/xerial/snappy-java/issues/168#issuecomment-279928072)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-688642099:188,load,load,188,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-688642099,2,['load'],['load']
Performance,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:72,perform,performing,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363,1,['perform'],['performing']
Performance,"Will you have time to have a look to this in the following weeks, @droazen? Because the sliding-window walker is going to take more time, I would like to explore the `AssemblyRegionWalker` for window-traversal of reads instead of the sliding-window variant to see if it is enough for my purpose. But I require to tune the arguments for the shards, and with argument collections is the best way. Thank you in advance. I rebased to the latest master to skip the tests in the cloud and have the proper status for the checks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-281645901:313,tune,tune,313,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2371#issuecomment-281645901,1,['tune'],['tune']
Performance,"With `--genomicsdb-shared-posixfs-optimizations`, the storage system should only require read access. @droazen, will work towards a fix for this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468563326:34,optimiz,optimizations,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468563326,1,['optimiz'],['optimizations']
Performance,"With the exception of the HMM package, all of our R dependencies are available through the conda R or bioconda channels; the HMM package is available only through a user's custom channel. However, the HMM package is only used to generate truth for testing the Java HMM code by @vruano (which is currently unused, but we thought was worth keeping around). I'm sure we could easily rewrite the tests to load the truth from a file. I think we should get rid of the install_R_packages.R script altogether and just roll all of these dependencies into the conda environment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920:401,load,load,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4250#issuecomment-406067920,1,['load'],['load']
Performance,With this new version I'm able to make it fail again; I opened a million channels to read the same file (across 1k threads) and got the error below. Yes I know a million parallel reads on a single file is more than a normal user would issue. ```; shaded.cloud_nio.com.google.api.client.http.HttpRequest execute; WARNING: exception thrown while executing request; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992); 	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403); 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387); 	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559); 	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); 	at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1316); 	at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1291); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:250); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:77); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); 	at shaded.cloud_nio.com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:365); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:135); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:156); 	at shaded.cloud_nio.c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156:554,perform,performInitialHandshake,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156,1,['perform'],['performInitialHandshake']
Performance,"Wow, thanks for the detailed comments so far, @davidbenjamin! But perhaps let's quickly chat before you go any further?. There are a lot of things you commented on---temporary integration tests using local files, lots of code/arguments/etc. intentionally copied verbatim over from VQSR/tranches, and entire tools (the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain)---that are rather in flux or will be scrapped/cleaned up shortly. That said, the comments on the code inherited from VQSR will certainly be useful in this process!. But it might save you some time if we could chat so I can give you a rough orientation and perhaps point out where the vestigial VQSR code remains. I think focusing discussion on the high level design of the tools that are likely to stay would also be most useful at this stage. Feel free to throw something on my calendar!. In the end, I think we will probably just retain the BGMM backend + the versions of the tools in the ""scalable"" package. I left the ""monolithic"" GMMVariantTrain and ScikitLearnVariantTrain tools in this branch so I could do one round of tieout. That tieout came out OK, so I think we'll abandon the monolithic tools, along with all the associated code outside of the scalable package. If it helps, I can go ahead and remove that stuff from this draft PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942:968,scalab,scalable,968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7659#issuecomment-1029393942,2,['scalab'],['scalable']
Performance,YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `83.911% <86.765%> (+2.516%)` | :arrow_up: |; | [...rs/variantutils/SelectVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50c0ludGVncmF0aW9uVGVzdC5qYXZh) | `96.928% <94.702%> (-0.783%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `81.633% <0.000%> (-2.041%)` | :arrow_down: |; | [...ct/CreateSomaticPanelOfNormalsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=p,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:3268,scalab,scalable,3268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,1,['scalab'],['scalable']
Performance,YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `98.214% <Ã¸> (+1.548%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `73.529% <45.161%> (-4.248%)` | :arrow_down: |; | [...stering/BayesianGaussianMixtureModelPosterior.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxQb3N0ZXJpb3IuamF2YQ==) | `58.333% <58.333%> (Ã¸)` | |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `82.667% <66.667%> (+6.417%)` | :arrow_up: |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `68.421% <66.667%> (Ã¸)` | |; | [...calable/modeling/BGMMVariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comme,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:4429,scalab,scalable,4429,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"Yea, I believe the header mismatching has to do with the canonical header lines in the gatk being updated in the middle of this work. htsjdk doesn't discriminate between bad header line count fields so there are a number of sloppy header lines like this. I will update my genomicsDB branch to perform a proper check of the allele specific annotations based on the new version now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-355645211:293,perform,perform,293,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-355645211,1,['perform'],['perform']
Performance,"Yeah, I don't like these new interface methods -- they make `GATKRead` significantly worse. We should cache `isUnmapped`, etc. in the adapter to accomplish the same thing, as @lbergelson suggests. Not that hard, and we can just unconditionally invalidate the cached values (using `Boolean` fields set to null) whenever the read is mutated in any way in order to simplify the logic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282:102,cache,cache,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235102282,2,['cache'],"['cache', 'cached']"
Performance,"Yeah, the goofy name is just a placeholder, as is the separate tool itself. Unless there are any objections (@fleharty @mwalker174?) or I run into any unforeseen snafus or parameter ambiguities along the way, I am going to roll the multisample-segmentation functionality into ModelSegments. We can toggle this functionality by passing multiple `--denoised-copy-ratios` and `--allelic-counts` arguments, e.g.:. ```; gatk ModelSegments ; --normal-allelic-counts normal.allelicCounts.tsv (this is only used for het genotyping); --denoised-copy-ratios normal.denoisedCR.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; ...; --denoised-copy-ratios tumor-N.denoisedCR.tsv; --allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; ...; --allelic-counts tumor-N.allelicCounts.tsv \; -O .; --output-prefix joint-segmentation; ```. This will perform both het genotyping and joint segmentation, but will yield a Picard interval-list `joint-segmentation.interval_list` as its sole output. (Although we could proceed to perform MCMC model inference on each sample in series, we'll stop at segmentation to enforce the scattering of inference across samples, which will be quicker.) We can also allow for copy-ratio-only and allelic-count-only modes. Users can use this joint segmentation in their own downstream tools, but we can also allow ModelSegments to ingest it via in a new `--segments` argument:. ```; gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv (equivalently, we could omit this and adjust minimum-total-allele-count-case, as is done in the WDL); --allelic-counts normal.allelicCounts.tsv; --denoised-copy-ratios normal.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix normal. gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix tumor-1. ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:865,perform,perform,865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"Yes, @magicDGS I have answered a continuous stream of questions on indel realignment in the forum and these questions are ongoing. Here's one I answered just last week: http://gatkforums.broadinstitute.org/gatk/discussion/comment/39359#Comment_39359. So it appears that it is a step that folks continue to use. . I would add that indel realignment is especially important for Mutect1 pipelines, which continues as the standard in somatic calling, pending performance of Mutect2. Even if Mutect2 were to outperform Mutect1 calling, because of the extreme difference in compute between the two, I conjecture folks will continue to use Mutect1. In fact, the only sane way to output a BAM for the entirety of a somatic sample set is via indel realignment. Somatic folks are keen on manual review and indel realignment is the cheapest way to get there. Here's another scenerio for which indel realignment is useful: the case of a low coverage site where multiple haplotypes are present. These cannot be resolved by HaplotypeCaller if the difference in allele depth between reads supporting either haplotype does not pass some threshold. In this case, sites receive `./.` no calls. Indel realignment using a known sites resource can aid in the resolution of haplotypes at sites that correspond to common population variants. One could argue that the cohort-level analysis could aid in resolution of these, but it may be that large cohorts are a luxury that cannot be afforded by many research groups. ### I vote yes please to porting the indel realignment pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307895094:455,perform,performance,455,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3084#issuecomment-307895094,1,['perform'],['performance']
Performance,"Yes, I have also just tested HaplotypeCaller on a CRAM and it seems to run fine locally.; ```; /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch HaplotypeCaller \; -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; -I HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; -O HG00190_cram_HC.vcf \; -L chr17; ```; gives; ```; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar HaplotypeCaller -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa -I HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram -O HG00190_cram_HC.vcf -L chr17; 14:50:16.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; [October 6, 2017 2:50:16 PM EDT] HaplotypeCaller --output HG00190_cram_HC.vcf --intervals chr17 --input HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram --reference /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa --group StandardAnnotation --group StandardHCAnnotation --GVCFGQBands 1 --GVCFGQBands 2 --GVCFGQBands 3 --GVCFGQBands 4 --GVCFGQBands 5 --GVCFGQBands 6 --GVCFGQBands 7 --GVCFGQBands 8 --GVCFGQBands 9 --GVCFGQBands 10 --GVCFGQBands 11 --GVCFGQBands 12 --GVCFGQBands 13 --GVCFGQBands 14 --GVCFGQBands 15 --GVCFGQBands 16 --GVCFGQBands 17 --GVCFGQBands 18 --GVCFGQBands 19 --GVCFGQBands 20 --GVCFGQBands 21 --GVCFGQBands 22 --GVCFGQBands 23 --GVCFGQBands 24 --GVCFGQBands 25 --GVCFGQBands 26 --GVCFGQBands 27 --GVCFGQBands 28 --GVCFGQBands 29 --GVCFGQBands 30 --GVCFGQBands 31 --GVCF",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:993,Load,Loading,993,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['Load'],['Loading']
Performance,"Yes, I ran option 3, 5 times (using the older version not latest master). Of those 5, 3 failed. One of the failures was: `java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All reopens failed`. The other two were both the non-negative error message:. ```; Using GATK jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:gc_log.log -Xms4000m -jar /usr/gitc/gatk4/gatk-package-4.beta.1-local.jar BaseRecalibrator -R /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-960e-17578f6b382c/call-SortAndFixSampleBam/CHMI_CHMI3_WGS2.aligned.duplicate_marked.sorted.bam --useOriginalQualities -O CHIM.recal_data.csv -knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf -knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz -L chr6:1+ --use_jdk_deflater --use_jdk_inflater; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.4ZS1WV; [July 24, 2017 5:48:10 PM UTC] BaseRecalibrator --useOriginalQualities true --knownSites gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf --knownSites /cromwell_root/broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --knownSites /cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz --output CHIM.recal_data.csv --intervals chr6:1+ --input gs://broad-gotc-dev-cromwell-execution/PairedEndSingleSampleWorkflow/66442def-ad3f-4c6c-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:160,concurren,concurrent,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"Yes, I'd like to test out the new tools in the workflow. All of these changes seem major and I'm excited to see how the workflow performs. I'll stop by to chat with you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333530110:129,perform,performs,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333530110,1,['perform'],['performs']
Performance,"Yes, I'm not sure I would expect that to be true. My guess is that we continue to run long after suitable convergence with the current inference parameters (which we have not had a chance to optimize over for runtime). I think we can probably afford to be less conservative. @mbabadi do you have a better handle on this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392154161:191,optimiz,optimize,191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-392154161,1,['optimiz'],['optimize']
Performance,"Yes, that is easy to do. The guidance on how much memory to leave wasnt clear on the docs. Is there a rule of thumb on how much we should leave? We can do whatever makes sense. the command is something like:. ```. /home/exacloud/gscratch/prime-seq/java/java8/bin/java \; -Djava.io.tmpdir=/mnt/scratch/prime-seq/tmp.5E76utMagnGenomicsDB_Append_Merge_2020-11-04_09-17-56-Job1 \; -Xmx104g \; -Xms104g \; -Xss2m \; -jar /home/exacloud/gscratch/prime-seq/bin/GenomeAnalysisTK4.jar GenomicsDBImport \; -V <Repeated 183 times for gVCFs> \ ; --genomicsdb-update-workspace-path /home/exacloud/gscratch/prime-seq/workDir/9a2611e8-0112-1039-8c80-f8f3fc869aa5/Job1.work/WGS_Nov_1300.gdb \; --batch-size 50 \; --consolidate \; --genomicsdb-shared-posixfs-optimizations. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565:742,optimiz,optimizations,742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-724293565,2,['optimiz'],['optimizations']
Performance,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:688,perform,performed,688,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646,2,['perform'],['performed']
Performance,"Yes, the ""no space left on device"" was temporary and has been fixed - I'm creating a new workspace with the --genomicsdb-shared-posixfs-optimizations turned on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-760345071:136,optimiz,optimizations,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-760345071,1,['optimiz'],['optimizations']
Performance,"Yes, this looks very similar to what I'm seeing. The VariantSparkSinkUnitTests run fine on their own. When the whole test suite is run though, an OOM error is thrown when VariantSparkSinkUnitTests run:. java.lang.OutOfMemoryError: GC overhead limit exceeded; at org.gradle.internal.remote.internal.hub.MethodInvocationSerializer$MethodInvocationReader.read(MethodInvocationSerializer.java:120); at org.gradle.internal.remote.internal.hub.MethodInvocationSerializer$MethodInvocationReader.read(MethodInvocationSerializer.java:99); at org.gradle.internal.serialize.kryo.TypeSafeSerializer$1.read(TypeSafeSerializer.java:34); at org.gradle.internal.remote.internal.hub.InterHubMessageSerializer$MessageReader.read(InterHubMessageSerializer.java:66); at org.gradle.internal.remote.internal.hub.InterHubMessageSerializer$MessageReader.read(InterHubMessageSerializer.java:52); at org.gradle.internal.remote.internal.inet.SocketConnection.receive(SocketConnection.java:78); at org.gradle.internal.remote.internal.hub.MessageHub$ConnectionReceive.run(MessageHub.java:250); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 13:37 DEBUG: [kryo] Read object reference 986: reference=GRCh37.3; Test: Test method testWritingToFileURL[0](/Users/cnorman/projects/gatk/src/test/resources/Homo_sapiens_assembly19.dbsnp135.chr1_1M.exome_intervals.vcf, .vcf)(org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSinkUnitTest) produced standard out/err: 13:37 DEBUG: [kryo] Read object reference 203: contig=<ID=20,length=63025520,assembly=b37>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-287857575:1088,concurren,concurrent,1088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2490#issuecomment-287857575,4,['concurren'],['concurrent']
Performance,"You can probably eyeball it from the 4.1.8.1 results above, which provide two points on the respective curves (although note those numbers are not broken down by SNP/indel). Iâ€™ll generate nicer plots once things are finalized, this is just what is spit out by rtg rocplot. There are a few regions of parameter space where precision seems to improve at the expense of a bit of sensitivity. Again, this might not be optimal if we are going to filter afterwards. I guess the main takeaway at this stage is that changing the parameters can result in non-negligible performance changes, probably beyond the level of sample-to-sample variation, which wasn't obvious from the first few runs I posted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712344348:561,perform,performance,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-712344348,1,['perform'],['performance']
Performance,You can start with consolidating all of them at once (with no batch size) as your fragments are more or less the same size and it should perform OK with the new tool. You can even use the default buffer size as it should not be a factor with the new tool. If that does not work you can probably use `number-of-fragments/2` or `number-of-fragments/4` for a batch size.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081075538:137,perform,perform,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1081075538,1,['perform'],['perform']
Performance,Yup. Adding the note here to fill in some cost optimizations for some of the common tasks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352761918:47,optimiz,optimizations,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-352761918,1,['optimiz'],['optimizations']
Performance,"[MKL](https://software.intel.com/en-us/intel-mkl) has highly optimized versions of `log10()` and `pow()`, which are much faster than standard native implementations. @mbabadi would you mind sharing your profiling code? I'd like to make an apples-to-apples comparison of MKL to Apache and Nd4j.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292621448:61,optimiz,optimized,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292621448,1,['optimiz'],['optimized']
Performance,"_FIELD_FORMAT : DECIMAL; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Deflater IntelDeflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Inflater IntelInflater; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Initializing engine; 02:03:27.963 INFO ParallelCopyGCSDirectoryIntoHDFSSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@4769b07b] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@5ef60048].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363:2041,load,loaded,2041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2618#issuecomment-296871363,3,['load'],['loaded']
Performance,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:283,scalab,scalability,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434,1,['scalab'],['scalability']
Performance,"_WGS2.bam --interval_set_rule UNION --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [July 21, 2017 6:20:54 PM UTC] Executing as root@9b75c21b7620 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 21, 2017 6:29:58 PM UTC] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 9.06 minutes.; Runtime.totalMemory()=2088763392; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Blo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:2517,concurren,concurrent,2517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,_WRITE_FOR_TRIBBLE : false; 01:39:03.364 INFO FilterAlignmentArtifacts - Deflater: IntelDeflater; 01:39:03.364 INFO FilterAlignmentArtifacts - Inflater: IntelInflater; 01:39:03.364 INFO FilterAlignmentArtifacts - GCS max retries/reopens: 20; 01:39:03.364 INFO FilterAlignmentArtifacts - Requester pays: disabled; 01:39:03.364 WARN FilterAlignmentArtifacts - . [1m[31m !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: FilterAlignmentArtifacts is an EXPERIMENTAL tool and should not be used for production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![0m. 01:39:03.364 INFO FilterAlignmentArtifacts - Initializing engine; 01:39:07.644 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-ac4624cb-a8fc-49a2-b071-d3a0ae799418/cb1feccb-0a69-42bf-ba5f-fde762934a59/Mutect2/fe3623c8-0eaf-4cd4-9f81-d1fda4073f2e/call-Filter/22.hg38-filtered.vcf; 01:39:08.399 INFO FilterAlignmentArtifacts - Done initializing engine; 01:39:09.523 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 01:39:09.565 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 01:39:09.566 INFO IntelPairHmm - Available threads: 4; 01:39:09.566 INFO IntelPairHmm - Requested threads: 4; 01:39:09.566 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 01:39:09.567 INFO ProgressMeter - Starting traversal; 01:39:09.567 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; munmap_chunk(): invalid pointer; Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx11500m -jar /root/gatk.jar FilterAlignmentArtifacts -R gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fast,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860:4838,Load,Loading,4838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-664539860,1,['Load'],['Loading']
Performance,_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (Ã¸)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (Ã¸)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (Ã¸)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (Ã¸)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `72.222% <72.222%> (Ã¸)` | |; | ... and [20 more](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:5127,scalab,scalable,5127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --executor-memory 25G --driver-memory 5G /home/genomics/Projects/TomWhitePatches/gatk/build/libs/gatk-package-4.alpha.2-230-g19db939-SNAPSHOT-spark.jar BaseRecalibratorSpark -I hdfs://n001:54310/GATK4TEST/LargeBroadData/WGS-G94982-NA12878.bam -knownSites hdfs://n001:54310/GATK4TEST/DBSNP/dbsnp_138.hg19.vcf.gz -R hdfs://n001:54310/GATK4TEST/OldData/human_g1k_v37.2bit -O hdfs://n001:54310/GATK4TEST/LargeOutput/WGS_BQSR --sparkMaster spark://n001:7077; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; Picked up JAVA_TOOL_OPTIONS: -XX:+UseG1GC -XX:ParallelGCThreads=4; java.lang.ClassNotFoundException: org.broadinstitute.hellbender.Main; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:229); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:695); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877:1977,load,loadClass,1977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2620#issuecomment-299259877,4,['load'],['loadClass']
Performance,"`KnownSitesCache` loads a VCF file and turns it into an in-memory `IntervalsSkipList`, which is shared by all tasks in the same VM. The Spark job stage that uses `KnownSitesCache` in BQSR takes 6 minutes to load the file into memory, but then the stage takes another 4 minutes to complete (total 10 minutes, stage name `treeAggregate at BaseRecalibratorSparkFn.java:39`). Another approach is to uses Spark's `--files` option to share the VCF file with all executors, then use the file as a `FeatureDataSource` directly. I did this using a `.vcf.gz` file and the accompanying `.vcf.gz.tbi`. See the code in this branch: https://github.com/broadinstitute/gatk/tree/tw_known_sites_perf_files. This however was slower, the same stage took 39 minutes to complete, compared to 10. This is probably because reading from a file is much slower than using an in-memory lookup to find variants overlapping each read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412896366:18,load,loads,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5103#issuecomment-412896366,2,['load'],"['load', 'loads']"
Performance,```; tarting a Gradle Daemon (subsequent builds will be faster); :compileJava; :processResources; :classes; :shadowJarex; org.gradle.api.GradleException: Could not add file '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/classes/java/main/org/broadinstitute/hellbender/metrics/MultiLevelCollector$Distributor.class' to ZIP '/wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/libs/gatk-package-1.0-SNAPSHOT-local.jar'.; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:83); 	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:105); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:60); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:235); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:255); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$StreamAction.visitFile(ShadowCopyAction.groovy:185); 	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:210); 	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166); 	at com.github.jengelman.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:818,Cache,CachedConstructor,818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,2,['Cache'],['CachedConstructor']
Performance,a:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:213); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3013#issuecomment-308145149:4244,concurren,concurrent,4244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3013#issuecomment-308145149,2,['concurren'],['concurrent']
Performance,"a:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5461,concurren,concurrent,5461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,a:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10466,concurren,concurrent,10466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"ach which did not generate an error:. 1. imported the 10 not-reblocked gvcfs from chr16 into genomicsdb ; 2. GenotypeGVCFs with the same command line as number 3 above. . So the error appears to be related to the reblocking of the gvcfs. ```; gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1069,optimiz,optimizations,1069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['optimiz'],['optimizations']
Performance,"adinstitute/gatk/pull/5099. With that patch, we are now retrying on `UnknownHostException`, but the retries are all failing: . ```; [August 14, 2018 7:09:18 AM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 896.64 minutes.; Runtime.totalMemory()=3966238720; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:1103,concurren,concurrent,1103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"adoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://github.com/broadinstitute/gatk/files/9355026/log-no-parsing-loading.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1306,load,loading,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,8,['load'],['loading']
Performance,"alByDepth' annotations have been disabled; 22:42:22.719 INFO NativeLibraryLoader - Loading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:3527,multi-thread,multi-threaded,3527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['multi-thread'],['multi-threaded']
Performance,amF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2ZpbHRlcnMvVmFyaWFudEZpbHRyYXRpb24uamF2YQ==) | `92.593% <Ã¸> (Ã¸)` | |; | [...der/tools/walkers/variantutils/SelectVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50cy5qYXZh) | `83.911% <86.765%> (+2.516%)` | :arrow_up: |; | [...rs/variantutils/SelectVariantsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9TZWxlY3RWYXJpYW50c0ludGVncmF0aW9uVGVzdC5qYXZh) | `96.928% <94.702%> (-0.783%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <0.000%> (-2.991%)` | :arrow_down: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/8092?src=pr&el=tree&utm_medium=referral&utm_source=githu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874:2821,scalab,scalable,2821,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8092#issuecomment-1374581874,1,['scalab'],['scalable']
Performance,"ampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence` (hmmm in HC `readAlleleLikelihoods.retainEvidence(variantCallingRelevantOverlap::overlaps);` occurs immediately before `contaminationDownsampling`); * `indexOfEvidence` (nothing stands out)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:1460,cache,cached,1460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"andemRepeat`, `VariantOverlapAnnotator`, `GenotypingEngine`,`AlleleFrequencyCalculator`, `AssemblyRegionTrimmer`, `PartiallyDeterminedHaplotypedComputationEngine`, `RampedHaplotypeCallerEngine`, `ReadThreadingAssembler`, `SomaticGenotypingEngine`, `HaplotypeBasedVariantRecaller`, `PartiallyDeterminedHaplotype`: the only real change in these classes is using `Event` instead of `VariantContext` in places where only a lightweight container for locus/ref/alt is required. Some of the diffs are big but trivial. (In `AssemblyRegionTrimmer` I also deleted the unused `nonVariantTargetRegions` method).; - `AlleleAndContext` and `LocationAndAlleles` were deleted as the new `Event` class makes them obsolete.; - `AlleleFiltering` is a big diff but it's just a bunch of replacing `AlleleAndContext` with `Event`.; - `HaplotypeCallerGenotypingEngine`: some trivial renaming and use of `Event` instead of `VariantContext`.; - `PileupDetectionArgumentCollection`: fixed a typo.; - `LeftAlignAndTrimVariants`: just some optimized imports.; - `Event`: this is the new class at the heart of the PR.; - `EventMap`: lighter than it was. Storing a haplotype and other state were replaced by a static method to extract events from a haplotype. Using `Event` instead of `VariantContext` allowed several things to be written more concisely, but no big-picture changes. I didn't like how `buildEventMapForHaplotypes` both returned the set of event positions and filled in the haplotype's events as a side efffect, so I split it into a void method for filling events and another method for extracting start positions.; - `AlleleFilteringUnitTest`, `AssemblyBasedCallerUtilsUnitTest`, `HaplotypeCallerGenotypingEngineUnitTest.java`, `PartiallyDeterminedHaplotypeComputationEngineUnitTest`, `EventMapUnitTest`: everything that was tested before is still tested now. Lots of replacing `VariantContext` with `Event`. Deleted tests for a few unused methods here and there.; - The exact match expected VCF outputs in `Haplot",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574146393:1223,optimiz,optimized,1223,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8332#issuecomment-1574146393,1,['optimiz'],['optimized']
Performance,"apis.com/hellbender-test-logs/build_reports/master_32072.5/tests/test/index.html). Example error:. ```; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: A nack was received from the Python process (most likely caused by a raised exception caused by): nkm received. : Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 26, in start_session_get_args_and_model; return args_and_model_from_semantics(semantics_json, weights_hd5, tensor_type); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 33, in args_and_model_from_semantics; model = set_args_and_get_model_from_semantics(args, semantics_json, weights_hd5); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/models.py"", line 90, in set_args_and_get_model_from_semantics; model = load_model(weights_hd5, custom_objects=get_metric_dict(args.labels)); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/saving.py"", line 419, in load_model; model = _deserialize_model(f, custom_objects, compile); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/saving.py"", line 224, in _deserialize_model; model_config = json.loads(model_config.decode('utf-8')); AttributeError: 'str' object has no attribute 'decode'. 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:222); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.sendSynchronousCommand(StreamingPythonScriptExecutor.java:183); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.initializePythonArgsAndModel(CNNScoreVariants.java:557); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:317); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1052); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-724836660:1509,load,loads,1509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6718#issuecomment-724836660,1,['load'],['loads']
Performance,"ark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:20.229 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantia",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:4711,load,loaded,4711,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"asspaths.; ....... **the spark-shell**; bash-4.2$ spark-shell; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; Failed to created SparkJLineReader: java.io.IOException: Permission denied; Falling back to SimpleReader.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 1.6.0; /_/. Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:1998,load,loader,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['load'],['loader']
Performance,"async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:35:11.509 INFO CountReadsSpark - Start Date/Time: January 9, 2019 1:35:09 PM EST; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.509 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.510 INFO CountReadsSpark - HTSJDK Ver",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:2282,Load,Loading,2282,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,1,['Load'],['Loading']
Performance,"atedException; 15 Feb 2022 15:46:19,123 DEBUG: 		at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:355); 15 Feb 2022 15:46:19,128 DEBUG: 		at htsjdk.samtools.util.BlockCompressedOutputStream.close(BlockCompressedOutputStream.java:333); 15 Feb 2022 15:46:19,150 DEBUG: 		at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:172); 15 Feb 2022 15:46:19,160 DEBUG: 		at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:233); 15 Feb 2022 15:46:19,169 DEBUG: 		at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.closeTool(GenotypeGVCFs.java:297); 15 Feb 2022 15:46:19,180 DEBUG: 		at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); 15 Feb 2022 15:46:19,185 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 15 Feb 2022 15:46:19,190 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 15 Feb 2022 15:46:19,212 DEBUG: 		at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 15 Feb 2022 15:46:19,217 DEBUG: 		at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 15 Feb 2022 15:46:19,223 DEBUG: 		at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 15 Feb 2022 15:46:19,229 DEBUG: 		at org.broadinstitute.hellbender.Main.main(Main.java:289); 15 Feb 2022 15:46:19,235 DEBUG: 	Caused by: java.lang.ClassNotFoundException: htsjdk.samtools.FileTruncatedException; 15 Feb 2022 15:46:19,242 DEBUG: 		at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 15 Feb 2022 15:46:19,249 DEBUG: 		at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 15 Feb 2022 15:46:19,258 DEBUG: 		at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 15 Feb 2022 15:46:19,265 DEBUG: 		at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040915714:2403,load,loadClass,2403,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7675#issuecomment-1040915714,3,['load'],['loadClass']
Performance,"ation true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; [September 16, 2016 4:55:32 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 16:55:32.335 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_INDEX : false; 16:55:32.335 INFO BwaSpark - Defaults.CREATE_MD5 : false; 16:55:32.335 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 16:55:32.335 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 16:55:32.335 INFO BwaSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 16:55:32.335 INFO BwaSpark - Defaults.REFERENCE_FASTA : null; 16:55:32.335 INFO BwaSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:55:32.336 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:32.336 INFO BwaSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 16:55:32.336 INFO BwaSpark - Deflater IntelDeflater; 16:55:32.336 INFO BwaSpark - Initializing engine; 16:55:32.336 INFO BwaSpark - Done initializing engine; 16:55:32.757 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 16:55:34.473 INFO BwaSpark - Shutting down engine; [September 16, 2016 4:55:34 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=499646464. ---. null. ---",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200:2787,load,load,2787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247709200,1,['load'],['load']
Performance,"atk/genomicsdb/genomicsDB.chr16; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -G StandardAnnotation -G AS_StandardAnnotation -V gendb:///restricted/projectnb/kageproj/gatk/genomicsdb/genomicsDB.chr16 -L chr16:105582-211160 --use-new-qual-calculator --only-output-calls-starting-in-intervals TRUE --genomicsdb-shared-posixfs-optimizations TRUE --tmp-dir tmp -O chr16-105582-211160.vcf.gz; 07:46:18.893 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 07:46:18.944 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 25, 2021 7:46:19 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:46:19.128 INFO GenotypeGVCFs - ------------------------------------------------------------; 07:46:19.128 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.0.0; 07:46:19.128 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:46:19.129 INFO GenotypeGVCFs - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.36.2.el7.x86_64 amd64; 07:46:19.129 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 07:46:19.129 INFO GenotypeGVCFs - Start Date/Time: August 25, 2021 7:46:18 AM EDT; 07:46:19.129 INFO GenotypeGVCFs - ---------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278:1317,Load,Loading,1317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-905431278,1,['Load'],['Loading']
Performance,"ator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileRead",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5584,concurren,concurrent,5584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"avadoc.doclet](https://docs.oracle.com/en/java/javase/11/docs/api/jdk.javadoc/jdk/javadoc/doclet/package-summary.html). The javadoc tools in `org.broadinstitute.hellbender.utils.help` may need to be re-written (and it's not clear if it's possible to support Java 8 and Java 11 simultaneously).; * Travis build. Getting this to build and test on Java 11 in addition to the current builds may be fairly involved as the matrix is already quite complicated. (The current PR just changes Java 8 to Java 11 for testing purposes - we'd need a way of getting both to run.). The vast majority of tests are passing on Java 11, the following are failing:; * Missing `TwoBitRecord` (from ADAM); * `ReferenceMultiSparkSourceUnitTest`; * `ImpreciseVariantDetectorUnitTest`; * `SVVCFWriterUnitTest`; * `DiscoverVariantsFromContigAlignmentsSAMSparkIntegrationTest`; * `StructuralVariationDiscoveryPipelineSparkIntegrationTest`; * `SvDiscoverFromLocalAssemblyContigAlignmentsSparkIntegrationTest`; * `java.lang.NoSuchMethodError: java.nio.ByteBuffer.clear()Ljava/nio/ByteBuffer;`; * `SeekableByteChannelPrefetcherTest`; * `GatherVcfsCloudIntegrationTest`; * `Could not serialize lambda`; * `ExampleAssemblyRegionWalkerSparkIntegrationTest`; * `PileupSparkIntegrationTest`; * Native HMM library code caused the tests to crash on my Mac:; ```; Running Test: Test method testLikelihoodsFromHaplotypes[0](org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM@6282d367, true)(org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest); dyld: lazy symbol binding failed: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; dyld: can't resolve symbol __ZN13shacc_pairhmm9calculateERNS_5BatchE in /private/var/folders/cj/wyp4zgw17vj4m9qdmddvmcc80000gn/T/libgkl_pairhmm13775554937319419112.dylib because dependent dylib #1 could not be loaded; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359:2536,load,loaded,2536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-527179359,4,['load'],['loaded']
Performance,"ax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFControlSample/Benchmark/145d88de-5810-47e1-972a-18ff0169fe27/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""92.82975"",; ""NIST evalHCsystemhours"": ""0.17177777777777778"",; ""NIST evalHCwallclockhours"": ""66.4404388888889"",; ""NIST evalHCwallclockmax"": ""3.325327777777778"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-EVALRuntimeTask/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFTestSample/Benchmark/e37c2b01-a62d-4b8c-9fb3-6f86d8377ca7/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CreateHTMLReport/cacheCopy/report.html""; },; ""errors"": null; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:21348,cache,cacheCopy,21348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"b174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1921,load,load,1921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFControlSample/Benchmark/3046acf7-ded7-40c8-9b7a-3826f480418f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8778"",; ""CHM evalindelPrecision"": ""0.8968"",; ""CHM evalsnpF1Score"": ""0.9813"",; ""CHM evalsnpPrecision"": ""0.9774"",; ""CHM evalsnpRecall"": ""0.9852"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFTestSample/Benchmark/2f376005-bdfb-42bd-8736-1e6df978ab80/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:11446,cache,cacheCopy,11446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,"be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be valid for non-default values. I'll highlight this with a comment below. We can restore it if we add code to check whether the assumptions hold, but I'd be curious to see in which cases the optimization makes a big difference. See https://github.com/broadinstitute/gatk/issues/6863#issuecomment-7078703",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:2190,optimiz,optimization,2190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimization']
Performance,"be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$nul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3780,concurren,concurrent,3780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"bender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18/12/21 13:48:33 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 18/12/21 13:48:33 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 18/12/21 13:48:37 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 806177853; 14:54:41.938 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 2:54:41 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 66.18 minutes. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725:11294,load,loaded,11294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-449510725,1,['load'],['loaded']
Performance,bender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); 	... 3 more; Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:2622,concurren,concurrent,2622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"bgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best regards,; > Robert; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6794>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AAKOLMAQSKLJ5N7SHDOJQ7DSEESQFANCNFSM4QZASPYQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:6203,load,loaded,6203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,2,['load'],"['load', 'loaded']"
Performance,"bleByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:9108,concurren,concurrent,9108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,3,['concurren'],['concurrent']
Performance,bmRlci90b29scy93YWxrZXJzL3N2L0NvbGxlY3RTVkV2aWRlbmNlLmphdmE=) | `74.888% <0.000%> (-4.990%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `84.746% <0.000%> (-3.762%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <0.000%> (-3.696%)` | :arrow_down: |; | [.../walkers/haplotypecaller/graphs/InverseAllele.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvSW52ZXJzZUFsbGVsZS5qYXZh) | `59.091% <0.000%> (-3.409%)` | :arrow_down: |; | ... and [104 more](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&u,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473:4765,scalab,scalable,4765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473,1,['scalab'],['scalable']
Performance,browse/SPARK-21133. A sample error from my log:; 17/07/17 14:33:17 ERROR org.apache.spark.util.Utils: Exception encountered; java.lang.NullPointerException; 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167); 	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459); 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619); 	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562); 	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491:1912,concurren,concurrent,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3290#issuecomment-315846491,2,['concurren'],['concurrent']
Performance,"c VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says th",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5555,multi-thread,multi-threaded,5555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['multi-thread'],['multi-threaded']
Performance,"c facing evaluations.; - Some hyperparameter tweaking was necessary to achieve good performance. Hyperparameters changed were contained mostly only to `psi_t` parameter.; - We developed a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The following items are necessary done for automatic evaluation:** ; - Dataset + truth. We need an access to a high quality public cohort with matched whole genomes. These genomes have to have a corresponding high quality truth set generated from split-read/read-pair methods. From that cohort we need to find 50-200 relatively homogeneous samples.; - An established vali",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:1324,optimiz,optimization,1324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,1,['optimiz'],['optimization']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_libs__7473738539612638927.zip; 2019-01-07 11:33:38 INFO Client:54 - Uploading resource file:/tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed/__spark_conf__4147634812449814799.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0153/__spark_conf__.zip; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-07 11:33:38 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-07 11:33:38 INFO Client:54 - Submitting application application_1542127286896_0153 to ResourceManager; 2019-01-07 11:33:38 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0153; 2019-01-07 11:33:38 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0153 and attemptId None; 2019-01-07 11:33:39 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:39 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1546878818531; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0153/; user: farrell; 2019-01-07 11:33:40 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:41 INFO Client:54 - Application report for application_1542127286896_0153 (state: ACCEPTED); 2019-01-07 11:33:42 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:13432,queue,queue,13432,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['queue'],['queue']
Performance,"c/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_libs__7821719163562430010.zip; 2019-01-09 13:35:22 INFO Client:54 - Uploading resource file:/tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0/__spark_conf__4520928824604875683.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0166/__spark_conf__.zip; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls to: farrell; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing view acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - Changing modify acls groups to:; 2019-01-09 13:35:22 INFO SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-09 13:35:22 INFO Client:54 - Submitting application application_1542127286896_0166 to ResourceManager; 2019-01-09 13:35:22 INFO YarnClientImpl:251 - Submitted application application_1542127286896_0166; 2019-01-09 13:35:22 INFO SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1542127286896_0166 and attemptId None; 2019-01-09 13:35:23 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:23 INFO Client:54 -; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1547058922320; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0166/; user: farrell; 2019-01-09 13:35:24 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:25 INFO Client:54 - Application report for application_1542127286896_0166 (state: ACCEPTED); 2019-01-09 13:35:26 INFO Client:54 - Applic",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:13171,queue,queue,13171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['queue'],['queue']
Performance,c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `Ã¸` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `Ã¸` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `Ã¸` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `37.037%` |; | [...alable/modeling/PythonVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvUHl0aG9uVmFyaWFudEFubm90YXRpb25zTW9kZWwuamF2YQ==) | `66.667%` |; | [...e/TrainVariantAnnotationsModelIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_t,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:3954,scalab,scalable,3954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"c_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:46:24 PM CST; 13:46:24.885 INFO BaseRecalibrator - -----------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13146,Load,Loading,13146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"ch can be inspected via the src.zip file...; }; ```. So looking at the code portions of `computeDefaultSUID()` and I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorData",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2222,perform,performing,2222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performing']
Performance,com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.testng.TestClass.<init>(TestClass.java:39); 	at org.testng.TestRunner.initMethods(TestRunner.java:466); 	at org.testng.TestRunner.init(TestRunner.java:345); 	at org.testn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2216,concurren,concurrent,2216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --conf spark.executor.extraJavaOptions=-Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true --deploy-mode client --num-executors 59 --executor-cores 4 --executor-memory 24180M --driver-memory 10G /mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar CountReadsSpark -I hdfs://arlab174:54310/GATK4TEST/BroadData/CEUTrio.HiSeq.WEx.b37.NA12892.bam -O hdfs://arlab174:54310/GATK4TEST/Output/Test_CEU_ReadsCount --sparkMaster yarn; 14:56:20.999 INFO IntelGKLUtils - Trying to load Intel GKL library from:; 	jar:file:/mnt/raid5/frankliu/code/GATK/gatk/build/libs/gatk-package-4.alpha.2-120-g00a40ea-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /tmp/frankliu/libgkl_compression8271576113600851848.so: /tmp/frankliu/libgkl_compression8271576113600851848.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64-bit platform); 	at java.lang.ClassLoader$NativeLibrary.load(Native Method); 	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); 	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); 	at java.lang.Runtime.load0(Runtime.java:809); 	at java.lang.System.load(System.java:1086); 	at com.intel.gkl.IntelGKLUtils.load(IntelGKLUtils.java:133); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:58); 	at com.intel.gkl.compression.IntelDeflater.load(IntelDeflater.java:53); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:16); 	at com.intel.gkl.compression.IntelDeflaterFactory.<init>(IntelDeflaterFactory.java:20); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885:1559,load,load,1559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265854885,1,['load'],['load']
Performance,"cotator_dataSources.v1.6.20190124s/achilles/hg38/achilles_lineage_results.import.txt; > 12:28:28.866 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xrefseq_v90_38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode_xrefseq/hg38/gencode_xrefseq_v90_38.tsv; > 12:28:31.215 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hgnc_download_Nov302017.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/hgnc/hg38/hgnc_download_Nov302017.tsv; > 12:28:31.563 INFO Funcotator - Initializing Funcotator Engine...; > 12:28:31.593 INFO Funcotator - Creating a VCF file for output: file:/home/pkus/mutect_test/filtered_variants/P1.avcf.gz; > 12:28:31.731 INFO ProgressMeter - Starting traversal; > 12:28:31.731 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 12:28:31.969 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; > 12:28:31.975 INFO Funcotator - Shutting down engine; > [July 21, 2020 12:28:31 PM CEST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.26 minutes.; > Runtime.totalMemory()=2200961024; > org.broadinstitute.hellbender.exceptions.GATKException: Unable to query the database for geneName: NCRNA00115; > at org.broadinstitute.hellbender.tools.funcotator.dataSources.cosmic.CosmicFuncotationFactory.createFuncotationsOnVariant(CosmicFuncotationFactory.java:320); > at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:245); > at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211); > at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:173); > at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:16958,cache,cache,16958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,cov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/84fbda69bf9528059777496a415be8eb6db63e61?src=pr&el=desc) will **increase** coverage by `0.012%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3032 +/- ##; ===============================================; + Coverage 79.973% 79.985% +0.012% ; - Complexity 16727 16745 +18 ; ===============================================; Files 1139 1139 ; Lines 60902 60938 +36 ; Branches 9437 9439 +2 ; ===============================================; + Hits 48705 48741 +36 ; + Misses 8401 8398 -3 ; - Partials 3796 3799 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...itute/hellbender/tools/walkers/SplitIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1NwbGl0SW50ZXJ2YWxzLmphdmE=) | `88.235% <Ã¸> (Ã¸)` | `6 <0> (Ã¸)` | :arrow_down: |; | [...itute/hellbender/tools/walkers/mutect/Mutect2.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyLmphdmE=) | `92.593% <0%> (Ã¸)` | `32% <0%> (+16%)` | :arrow_up: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <0%> (Ã¸)` | `4% <0%> (+2%)` | :arrow_up: |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3032?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `73.649% <0%> (+2.027%)` | `34% <0%> (Ã¸)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3032#issuecomment-306375449:1515,Perform,PerformAlleleFractionSegmentation,1515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3032#issuecomment-306375449,1,['Perform'],['PerformAlleleFractionSegmentation']
Performance,"created for the TH evaluation:. Here is the 100% tumor run through ModelSegments, which yields 130 segments:; ![T modeled](https://user-images.githubusercontent.com/11076296/76560309-43d2dd80-6477-11ea-87d2-75d430d220a2.png). Here is the 75% tumor + 25% normal mixture run through ModelSegments, which yields 83 segments:; ![N-25-T-75 modeled](https://user-images.githubusercontent.com/11076296/76558609-fd2fb400-6473-11ea-9bd3-293a66eb3e4e.png). Here is the 25% tumor + 75% normal mixture run through ModelSegments, which yields 50 segments:; ![N-75-T-25 modeled](https://user-images.githubusercontent.com/11076296/76560439-85638880-6477-11ea-8d8d-f0f9a11d70a6.png). We can compare against the new workflow, in which we run SegmentJointSamples to jointly segment on the two mixtures. This yields a joint-sample segmentation with 162 segments, which can be passed as an additional input to individual ModelSegments runs on the two mixtures. It is used as the initial segmentation for both runs, after which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reaso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:1187,perform,performed,1187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,2,['perform'],['performed']
Performance,ctOriginalAlignmentRecordsByNameSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9FeHRyYWN0T3JpZ2luYWxBbGlnbm1lbnRSZWNvcmRzQnlOYW1lU3BhcmsuamF2YQ==) | `87.5% <0%> (-0.735%)` | `12% <0%> (+6%)` | |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `92.529% <0%> (-0.412%)` | `14% <0%> (+8%)` | |; | [...s/spark/sv/evidence/experimental/CalcMetadata.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9leHBlcmltZW50YWwvQ2FsY01ldGFkYXRhLmphdmE=) | `0% <0%> (Ã¸)` | `0% <0%> (Ã¸)` | :arrow_down: |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <0%> (Ã¸)` | `6% <0%> (+3%)` | :arrow_up: |; | [.../broadinstitute/hellbender/tools/exome/Sample.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9TYW1wbGUuamF2YQ==) | `100% <0%> (Ã¸)` | `10% <0%> (+5%)` | :arrow_up: |; | [...v/evidence/experimental/FindSmallIndelRegions.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9leHBlcmltZW50YWwvRmluZFNtYWxsSW5kZWxSZWdpb25zLmphdmE=) | `0% <0%> (Ã¸)` | `0% <0%> (Ã¸)` | :arrow_down: |; | [...e/hellbender/engine/filters/LibraryReadFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/3838/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344512890:2844,Perform,PerformSegmentation,2844,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3838#issuecomment-344512890,1,['Perform'],['PerformSegmentation']
Performance,"ctors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (wa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3231,Load,Loading,3231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['Load'],['Loading']
Performance,"d I notice in our instance `ReadFilter` is a interface, which gets defined later via [ReadFilterLibrary.java](https://github.com/broadinstitute/hellbender/blob/62ef76ba60951c562a0d4c39189aa3f01f27f8d3/src/main/java/org/broadinstitute/hellbender/engine/filters/ReadFilterLibrary.java) or via `new ReadsFilter(readFilter, header)`, in either of these instances the fields would be different, based on this portion of `computeDefaultSUID` when looking at declared fields:. ```; Field[] fields = cl.getDeclaredFields();; MemberSignature[] fieldSigs = new MemberSignature[fields.length];; for (int i = 0; i < fields.length; i++) {; fieldSigs[i] = new MemberSignature(fields[i]);; }; ```. Therefore the `SUID` would be different based on the fields it would have. Please let me know if I misinterpreted something. @jean-philippe-martin I would be happy to help out, but even when I try a small test like this I get an error - I probably am performing something incorrectly. Below are the steps I performed using the codebase from this PR:. ```; $ wget https://github.com/broadinstitute/hellbender/archive/67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ unzip 67b380fbed272bb92ef667ec2128d5720718a5e8.zip; $ cd hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8; $ gradle fatJar; $ java -jar build/libs/hellbender-67b380fbed272bb92ef667ec2128d5720718a5e8-all-version-unknown-SNAPSHOT.jar BaseRecalibratorDataflow -R ./src/test/resources/human_g1k_v37.chr17_1Mb.fasta --knownSites ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -I ./src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_132.b37.excluding_sites_after_129.chr17_69k_70k.vcf -RECAL_TABLE_FILE gatk4.pre.cols.table --sort_by_all_columns true; [June 1, 2015 5:51:02 PM EDT] org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BaseRecalibratorDataflow --knownSites /home/pgrosu/me/gg_hellbender_broad_institute/test/hellbender-67b380fbed272bb92ef667ec2128d5720",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499:2278,perform,performed,2278,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/535#issuecomment-107730499,1,['perform'],['performed']
Performance,"d Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 5 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:4017,concurren,concurrent,4017,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,3,['concurren'],['concurrent']
Performance,"d positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservatively filtered as truth, which will bias us towards high scores and the peaks of the positive distribution. Perhaps we can also experiment with just treating training/truth on an equal footing (I think the d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:1316,tune,tune,1316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,2,['tune'],['tune']
Performance,"d try to do it, as that is a relatively expensive resource to create. For example, some very naive hard filtering (red) of the histogram yields a peak that is easily fit by a negative binomial (green)---even a Poisson fit does not appear to bias the depth estimates, and certainly does not result in incorrect ploidy estimates:. ![masked_fit](https://user-images.githubusercontent.com/11076296/37863641-827a6e8a-2f37-11e8-83d5-cb4af32a898b.png). (Incidentally, it is helpful to plot on a log scale when checking the fit of these distributions.). This strategy also gives us a way to ignore low-level mosaicism or large germline events, which filtering on mappability may not address:. ![mosaic](https://user-images.githubusercontent.com/11076296/37863649-d0ac378c-2f37-11e8-8e98-45e1fa9a3d7a.png). So let's try to encapsulate changes to the ploidy tool. I agree that the histogram creation can be easily done on the Java side, to save on intermediate file writing. We can probably just cap the maximum bin to `k` and pass a samples x contig TSV where each entry is a vector with `k + 1` elements. I agree that there is still a lot of important work to be done in exploring our best practices for coverage collection, and I know that you have been interested in improving them for a while. Ultimately, we may want to consider incorporating mappability or other informative metadata, as we've discussed. However, this will require some non-trivial investment in method/tool development time. Since our preliminary evaluations show that even with the current, naive strategies the tool is performing reasonably well, I am prioritizing cutting a release and improving/automating the evaluations. As we discussed, this will both allow users to start using the tool (which will hopefully result in useful feedback) and establish a baseline for us. This will ultimately provide the necessary foundation for future exploratory work and method development---which always takes more time than we think it will!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040:2153,perform,performing,2153,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375881040,2,['perform'],['performing']
Performance,"d; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenvalues and left-singular vectors) for the specified number of eigensamples, and command line.; - [x] In a future iteration, we could allow an input PoN to be the source of read counts. This would allow iteration on filter parameters without needing output from CombineReadCounts. The code should easily allow for this.; - [x] ReadCountCollection is too memory intensive; minimize use in DenoiseReadCounts when writing results.; - [x] Optimize and clean up HDF5 writing (e.g., transpose skinny matrices, make writing of intervals <s>as a string matrix</s> faster).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:2072,perform,performed,2072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,2,"['Optimiz', 'perform']","['Optimize', 'performed']"
Performance,"data 1). These intervals were used for scattering tasks in the Germline short variant calling and Somatic short variant calling workflows outlined below.; > ; > Germline short variant calling; > ; > HaplotypeCaller was run at the 3,200 pre-defined intervals for each sample and interval VCFs were gathered into GVCFs per sample. The following steps were then performed for the OSCC, TCGA and CSCC cohorts separately. Per sample GVCFs were consolidated with GenomicsDBImport and joint-called with GenotypeGVCFs at the 3,200 pre-defined intervals. Multi-sample interval calls were gathered using GatherVcfs. VariantFiltration and MakeSitesOnlyVcf was used to filter records with excess heterozygosity and retain only the site-level annotations. VariantRecalibrator and ApplyRecal were then applied for SNP and indels separately following GATKâ€™s recommendations to obtain a final, filtered cohort VCF.; > ; > Somatic short variant calling; > ; > Somatic short variant calling was performed for tumour-normal pairs, first by preparing a panel of normals (PoN) for the OSCC, TCGA and CSCC cohorts separately. To create a PoN, Mutect2 was run in tumour only mode for the normal samples in the cohort at the 3,200 pre-defined intervals. GatherVcfs was used to gather interval VCFs into per sample VCFs. GenomicsDBImport and CreateSomaticPON was used to consolidate sample VCFs and create the PoN resource at the 3,200 pre-defined intervals. Interval VCFs were gathered into a single, cohort PoN VCF using GatherVcfs.; > ; > ; > ; > For each tumour-normal pair, Mutect2 was used for calling SNP and indels at the 3,200 pre-defined intervals, applying settings â€“f1r2-tar-gz, --germline-resource with gnomAD v3 as input and â€“panel-of-normals with PoN for the tumour-normal pairâ€™s respective cohort as input. Interval stats files were merged into a single stats file with MergeMutectStats and interval VCFs were gathered with GatherVcfs into a single VCF for each tumour-normal pair, as preparation for further f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472:3312,perform,performed,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-656499472,1,['perform'],['performed']
Performance,db63e61?src=pr&el=desc) will **increase** coverage by `0.169%`.; > The diff coverage is `90%`. ```diff; @@ Coverage Diff @@; ## master #3035 +/- ##; ===============================================; + Coverage 79.973% 80.142% +0.169% ; - Complexity 16727 16787 +60 ; ===============================================; Files 1139 1140 +1 ; Lines 60902 60867 -35 ; Branches 9437 9437 ; ===============================================; + Hits 48705 48780 +75 ; + Misses 8401 8296 -105 ; + Partials 3796 3791 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...coveragemodel/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <Ã¸> (Ã¸)` | `40 <0> (Ã¸)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/Du,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1253,cache,cachemanager,1253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,decov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `Ã¸` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `Ã¸` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `Ã¸` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:2331,scalab,scalable,2331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9NYXRoT2JqZWN0QXNzZXJ0cy5qYXZh) | `31.818% <0%> (Ã¸)` | `5% <0%> (?)` | |; | [...e/hellbender/utils/icg/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ29tcHV0YWJsZU5vZGVGdW5jdGlvbi5qYXZh) | `100% <0%> (Ã¸)` | `4% <0%> (?)` | |; | [...der/utils/linalg/GeneralLinearOperatorNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9saW5hbGcvR2VuZXJhbExpbmVhck9wZXJhdG9yTkRBcnJheS5qYXZh) | `66.667% <0%> (Ã¸)` | `4% <0%> (?)` | |; | [...hellbender/utils/icg/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ29tcHV0YWJsZUdyYXBoU3RydWN0dXJlLmphdmE=) | `100% <0%> (Ã¸)` | `63% <0%> (?)` | |; | [...ender/utils/icg/ImmutableComputableGraphUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvSW1tdXRhYmxlQ29tcHV0YWJsZUdyYXBoVXRpbHMuamF2YQ==) | `81.081% <0%> (Ã¸)` | `1% <0%> (?)` | |; | [...broadinstitute/hellbender/utils/icg/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvQ2FjaGVOb2RlLmphdmE=) | `80.645% <0%> (Ã¸)` | `9% <0%> (?)` | |; | [.../hellbender/utils/nd4j/Nd4jApacheAdapterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9uZDRqL05kNGpBcGFjaGVBZGFwdGVyVXRpbHMuamF2YQ==) | `85.714% <0%> (Ã¸)` | `12% <0%> (?)` | |; | ... and [15 more](https://codecov.io/gh/broadinstitute/gatk/pull/4203/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4203#issuecomment-358765311:3295,Cache,CacheNode,3295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4203#issuecomment-358765311,1,['Cache'],['CacheNode']
Performance,"done with my review. small edits, 1 bug, and a few performance questions. I'd like to see a simple perf run of HC with and without those changes. All those streams in math-heavy tight loops make me concerned a bit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260:51,perform,performance,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979#issuecomment-231096260,1,['perform'],['performance']
Performance,"ds.bam; **********. 11:25:52.673 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/fleharty/resources/picard.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Tue Jul 14 11:25:52 EDT 2020] ValidateSamFile INPUT=concatenated_ACC5611A1_XXXXXX_consensusalign_ds.bam MODE=VERBOSE MAX_OUTPUT=100 IGNORE_WARNINGS=false VALIDATE_INDEX=true INDEX_VALIDATION_STRINGENCY=EXHAUSTIVE IS_BISULFITE_SEQUENCED=false MAX_OPEN_TEMP_FILES=8000 SKIP_MATE_VALIDATION=false VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false; [Tue Jul 14 11:25:52 EDT 2020] Executing as fleharty@wm462-624 on Mac OS X 10.15.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_191-b12; Deflater: Intel; Inflater: Intel; Provider GCS is not available; Picard version: 2.20.4-SNAPSHOT; WARNING	2020-07-14 11:25:52	ValidateSamFile	NM validation cannot be performed without the reference. All other validations will still occur.; ERROR: Record 18321, Read name UMI-ATT-GAA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 26312, Read name UMI-CCT-TTC-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 70755, Read name UMI-CAG-GGA-2, Zero-length read without FZ, CS or CQ tag; ERROR: Record 145082, Read name UMI-AAC-ATG-5, Zero-length read without FZ, CS or CQ tag; ERROR: Record 181500, Read name UMI-ACT-CTT-1, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186837, Read name UMI-CAA-CTC-4, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186862, Read name UMI-CGC-GCC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186904, Read name UMI-AGG-GTC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186919, Read name UMI-CGC-TGC-0, Zero-length read without FZ, CS or CQ tag; ERROR: Record 186947, Read name UMI-TAA-TAG-3, Zero-length read without FZ, CS or CQ tag; ERROR",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132:1895,perform,performed,1895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695#issuecomment-658247132,1,['perform'],['performed']
Performance,"e (no insight so far, just looking up the line from the stack trace):; ```; final Object2IntMap<EVIDENCE> evidenceIndexes = evidenceIndexBySampleIndex(sampleIndex);; final int[] indexesToRemove = evidences.stream().mapToInt(e -> {; final int index = evidenceIndexes.getInt(e);; if (index == MISSING_INDEX) {; throw new IllegalArgumentException(""evidence provided is not in sample"");; }; ```; We get an error when `evidenceIndexBySampleIndex(sampleIndex)` yields a `Map` that for some reason doesn't contain a read that it should. So let's investigate `evidenceIndexBySampleIndex()`. This method returns the `evidenceIndexBySampleIndex.get(sampleIndex)` field if it is not `null` (ie uninitialized); otherwise it fills it and then returns it. The code for filling it seems fine, and it explicitly loops over every sample read, so it's hard to see that the error could come from there. It seems rather that the problem is in returning the cached value whenever it is not `null`. The cached value of `evidenceIndexBySampleIndex.get(sampleIndex)` becomes invalid whenever reads are added or removed. However, you can check all the accesses of `evidenceIndexBySampleIndex` (there are only six) and verify that the class never accounts for this. So, suppose that an `AlleleLikelihoods` object invokes `evidenceIndexBySampleIndex(sampleIndex)` more than once and adds or removes reads between these. The second call returns the cached map from the first call, which is bogus. Even if it doesn't explain this issue, it is a bug. Now let's think about which public methods `evidenceIndexBySampleIndex(sampleIndex)` is called in and where this occurs in HaplotypeCaller:. * `addEvidence` (in HC this happens only in the likelihoods for annotations, downstream of our issue, so this is not the culprit).; * `filterPoorlyModeledEvidence` (this happens after Pair-HMM to the haplotype likelihoods, so not the culprit either); * `contaminationDownsampling`; * `retainEvidence` (hmmm in HC `readAlleleLikelihoods.ret",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336:1020,cache,cached,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6586#issuecomment-625021336,1,['cache'],['cached']
Performance,"e GATK3 traversal. Initially, I did plan on having `AssemblyRegionWalker` extend the former `ReadWindowWalker`, or an adapted version of your `SlidingWindowWalker`, and I did implement it like this at first, but ultimately I collapsed it into a single class for several reasons:; - Inheriting from a more generic traversal type caused usability issues and confusion with respect to the command-line arguments. The `ReadWindow` was the unit of processing for the superclass, but for `AssemblyRegionWalker` it was the unit of I/O and `AssemblyRegion` was the unit of processing, and I couldn't update the docs for `ReadWindowWalker` to clear up the confusion without mentioning `AssemblyRegion`-specific concepts.; - The `ReadShard` / `ReadWindow` was/is **only** there to prove that we can shard the data without introducing calling artifacts, and to provide a unit of parallelism for the upcoming Spark implementation. It's not something we really want to expose to users as a prominent knob, and we may hide it completely in the future once the shard size is tuned for performance.; - Inheriting from a more abstract walker type caused a number of other problems as well: methods that should have been final in the supertype could no longer be made final, with the result that tool implementations could inappropriately override engine initialization/shutdown routines. Also, there were issues with the progress meter, since both the supertype traversal and subtype traversal needed their own progress meter for their different units of processing. Ultimately it was just too awkward and forced, and the read shard is something that we eventually want to make an internal/encapsulated implementation detail anyway. GATK3 made the mistake, I think, of using long, confusing inheritance chains for its walker types, with the result that you got awkward and forced relationships like `RodWalker` inheriting from `LocusWalker`. It's better, I think, to make each traversal as standalone as possible, esp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513:1359,tune,tuned,1359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708#issuecomment-210806513,2,"['perform', 'tune']","['performance', 'tuned']"
Performance,"e forceValidOutput=false filter_reads_with_N_cigar=false filter_mismatching_base_and_quals=false filter_bases_not_stored=false"">; ##GATKCommandLine.SelectVariants=<ID=SelectVariants,Version=3.4-228-g2497091,Date=""Tue Jan 05 13:29:45 EST 2016"",Epoch=1452018585661,CommandLineOptions=""analysis_type=SelectVariants input_file=[] showFullBamList=false read_buffer_size=null phone_home=AWS gatk_key=null tag=NA read_filter=[] disable_read_filter=[] intervals=null excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/humgen/1kg/reference/human_g1k_v37.fasta nonDeterministicRandomSeed=false disableDithering=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 baq=OFF baqGapOpenPenalty=40.0 refactor_NDN_cigar_string=false fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false useOriginalQualities=false defaultBaseQualities=-1 performanceLog=null BQSR=null quantize_quals=0 static_quantized_quals=null round_down_quantized=false disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 globalQScorePrior=-1.0 validation_strictness=SILENT remove_program_records=false keep_program_records=false sample_rename_mapping_file=null unsafe=null disable_auto_index_creation_and_locking_when_reading_rods=false no_cmdline_in_header=false sites_only=false never_trim_vcf_format_field=false bcf=false bam_compression=null simplifyBAM=false disable_bam_indexing=false generate_md5=false num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false variant_index_type=DYNAMIC_SEEK variant_index_parameter=-1 reference_window_stop=0 logging_level=INFO log_to_file=null help=false version=false variant=(RodBinding name=var",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834:4069,perform,performanceLog,4069,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4252#issuecomment-364237834,2,['perform'],['performanceLog']
Performance,"e to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5462,load,loaded,5462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"e to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one to set the maximum number of values allowed per chunk, so that heap usage can be controlled, but the downside is that this translates into a corresponding limit on the number of columns (i.e., intervals). On the other hand, you could theoretically crank this number up to Integer.MAX_VALUE, as long as you set -Xmx high enough... In practice, it's very unlikely that we'll need to go to bins smaller than a read length.); - [ ] <s>Check that CreatePanelOfNormals works correctly on Spark cluster.</s> Implement Randomized SVD, which should give bet",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:2412,perform,performing,2412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['performing']
Performance,"e up 20GB as a combined TSV file and a whopping 63GB as individual TSV files---as well as the eigenvectors, filtering results, etc.). The run can be found in /dsde/working/slee/wgs-pon-test/tieout/no-gc. It completed successfully with **-Xmx32G** (in comparison, CreatePanelOfNormals crashed after 40 minutes with -Xmx128G). The runtime breakdown was as follows:. - ~45 minutes simply from reading of the 90 TSV read-count files in serial. Hopefully #3349 should greatly speed this up. (In comparison, CombineReadCounts reading 10 files in parallel at a time took ~100 minutes to create the aforementioned 20GB combined TSV file, creating 25+GB of temp files along the way.). - ~5 minutes from the preprocessing and filtering steps. We could probably further optimize some of this code in terms of speed and heap usage. (I had to throw in a call to System.gc() to avoid an OOM with -Xmx32G, which I encountered in my first attempt at the run...). - ~5 minutes from performing the SVD of the post-filtering 8643028 x 86 matrix, maintaining 30 eigensamples. I could write a quick implementation of randomized SVD, which I think could bring this down a bit (the scikit-learn implementation takes <2 minutes on a 10M x 100 matrix), but this can probably wait. Clearly making I/O faster and more space efficient is the highest priority. Luckily it's also low hanging fruit. The 8643028 x 30 matrix of eigenvectors takes <2 minutes to read from HDF5 when the WGS PoN is used in DenoiseReadCounts, which gives us a rough idea of how long it should take to read in the original ~11.5M x 90 counts from HDF5. So once #3349 is in, then I think that a **~15 minute single-core WGS PoN could easily be viable**. I believe that a PoN on the order of this size will be all that is required for WGS denoising, if it is not already overkill. To go bigger by more than an order of magnitude, we'll have to go out of core, which will require more substantial changes to the code. But since the real culprit responsible ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503:1167,perform,performing,1167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-317614503,2,['perform'],['performing']
Performance,"e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3312,Load,Loading,3312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398,1,['Load'],['Loading']
Performance,e-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=./ -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Dat,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13016,load,load,13016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,e.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:40,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5626,Cache,CachedMethod,5626,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['Cache'],['CachedMethod']
Performance,"e.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(Se",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7920,concurren,concurrent,7920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFControlSample/Benchmark/8d0e47ca-66f5-42a0-8785-6aa8d2db2663/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM evalindelF1Score"": ""0.8724"",; ""CHM evalindelPrecision"": ""0.8814"",; ""CHM evalsnpF1Score"": ""0.9784"",; ""CHM evalsnpPrecision"": ""0.9706"",; ""CHM evalsnpRecall"": ""0.9863"",; ""CHM evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFTestSample/Benchmark/96c96714-3ac6-4d2b-a79c-57086cda6373/call-CombineSummaries/summary.csv"",; ""EXOME1 controlindelF1Score"": ""0.727"",; ""EXOME1 controlindelPrecision"": ""0.632"",; ""EXOME1 controlsnpF1Score"": ""0.9878"",; ""EXOME1 controlsnpPrecision"": ""0.9815"",; ""EXOME1 controlsnpRecall"": ""0.9941"",; ""EXOME1 controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-EXOME1Sampl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:11445,cache,cacheCopy,11445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,eExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.interna,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5855,concurren,concurrent,5855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,eExecutorsAction.execute(DefaultTaskExecutionGraph.java:343); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4729,concurren,concurrent,4729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,"ed to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old allele-fraction model only models hets, we perform a `GetHetCoverage`-like binomial genotyping step (and output the results) before modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3795,bottleneck,bottleneck,3795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['bottleneck'],['bottleneck']
Performance,"eed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6616,perform,performing,6616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['performing']
Performance,eflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false. METRICS CLASS	picard.sam.DuplicationMetrics; LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES	READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11933661	585768	0.22221	06317338. ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:2459,optimiz,optimized,2459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905,1,['optimiz'],['optimized']
Performance,"eflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:00:13 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=373293056; Tool returned:; 0; ```; The only way I can reproduce it is to delete one of the files so it *really* doesn't exist at the specified location:; ```; (base) /tmp/test a /Users/cnorman/projects/gatk/gatk MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; Using GATK jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar MergeVcfs -I data/calling/my.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz; 16:03:19.691 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/cnorman/projects/gatk/build/libs/gatk-package-4.1.7.0-41-g79586b8-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; [Mon Jun 22 16:03:19 EDT 2020] MergeVcfs --INPUT data/calling/my.vcf.gz --INPUT data/calling/b.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon Jun 22 16:03:19 EDT 2020] Executing as cnorman@WMCEA-78B on Mac OS X 10.13.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0-41-g79586b8-SNAPSHOT; [Mon Jun 22 16:03:19 EDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.00 minutes.; Runtime.totalMemory()=372244480; To get help, see ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321:2563,Load,Loading,2563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647743321,1,['Load'],['Loading']
Performance,"eflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(Assemb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7604,concurren,concurrent,7604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,"ehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpc=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-isl=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-cloog=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-boot-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --with-stage1-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --enable-checking=release --with-tune=generic --disable-multilib; Thread model: posix; gcc version 4.8.5 (GCC); `; Perhaps using the Xcode version would fix things?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:3715,tune,tune,3715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177,1,['tune'],['tune']
Performance,el/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <Ã¸> (Ã¸)` | `40 <0> (Ã¸)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1927,cache,cachemanager,1927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,"elow. ### Log from run with value of `--max-alternate-alleles` left at default value of 6:; ```; on-chinookomes-dna-seq-gatk-variant-calling]--% gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz. Using GATK jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 -O results/vcf_parts/CM031199.1.vcf.gz; 22:17:18.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 10:17:18 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:17:18.863 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:17:18.863 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 22:17:18.863 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:17:18.864 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 22:17:18.864 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 22:17:18.864 INFO GenotypeGVCFs - Start Date/Time",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:1803,Load,Loading,1803,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['Load'],['Loading']
Performance,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:46039,concurren,concurrent,46039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,4,['concurren'],['concurrent']
Performance,"encePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7985,concurren,concurrent,7985,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"ent Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:42:34 02:00:00 1200GB 1200GB 3072GB 768; ```. - Excellent CPU efficiency if running serially (but defeats the purpose of a H.P.C. with Lustre). ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105381052 R ds6924 hm82 genotype 61 00:19:55 10:00:00 1487MB 1487MB 4096MB 1. 09:17:51.114 INFO ProgressMeter - chr10:106687146 1.2 1000 822.3; 09:18:01.308 INFO ProgressMeter - chr10:106710146 1.4 24000 17315.6; 09:18:21.691 INFO ProgressMeter - chr10:106721171 1.7 35000 20281.0; 09:18:31.944 INFO ProgressMeter - chr10:106742172 1.9 56000 29526.0; ```. Intervals take about fifteen minutes each instead of about seven hours if running serially. Outputting results to `$PBS_JOBFS` folder on compute node instead of directly to project folder did not improve performance at all.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:2932,perform,performance,2932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['perform'],['performance']
Performance,ent&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RSZWFkQ291bnRzLmphdmE=) | `85.484% <Ã¸> (Ã¸)` | |; | [...ools/copynumber/CreateReadCountPanelOfNormals.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NyZWF0ZVJlYWRDb3VudFBhbmVsT2ZOb3JtYWxzLmphdmE=) | `89.831% <Ã¸> (Ã¸)` | |; | [...e/hellbender/tools/copynumber/utils/HDF5Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0hERjVVdGlscy5qYXZh) | `79.787% <Ã¸> (Ã¸)` | |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `0.000% <0.000%> (Ã¸)` | |; | [...calable/modeling/BGMMVariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc1Njb3Jlci5qYXZh) | `0.000% <0.000%> (Ã¸)` | |; | [...oadinstitute/hellbender/utils/NaturalLogUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:2695,scalab,scalable,2695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,ent.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at shaded.cloud_nio.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at shaded.cloud_nio.com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:5130); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:494); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.io.EOFException: SSL peer shut down incorrectly; 	at sun.security.ssl.InputRecord.read(InputRecord.java:505); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	... 34 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156:3652,concurren,concurrent,3652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3070#issuecomment-309120156,3,['concurren'],['concurrent']
Performance,ents&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (Ã¸)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (Ã¸)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (Ã¸)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3914,cache,cache,3914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,eption: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:228); at ht,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2393,concurren,concurrent,2393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"er it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be va",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1983,optimiz,optimizer,1983,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizer']
Performance,erationExecutor.java:199); 	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:110); 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:249); 	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:238); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:123); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:79); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:104); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:98); 	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:663); 	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:597); 	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:98); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: /wrkdirs/usr/ports/biology/gatk/work/gatk-4.0.11.0/build/classes/java/main/org/broadinstitute/hellbender/metrics/MultiLevelCollector$Distributor.class (Too many open files); 	at java.io.FileInputStream.open0(Native Method); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:12075,concurren,concurrent,12075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,5,['concurren'],['concurrent']
Performance,"erent results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage model is in for somatic).</s> I've implemented a fast kernel-segmentation method that seems very promising, see below.; - [ ] Investigate performance vs. CGA ReCapSeg pipeline on THCA samples.; - [ ] Investigate concordance with Genome STRiP.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:5096,perform,performed,5096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,4,['perform'],"['performance', 'performed']"
Performance,"error message:. ```; 01:15:27.623 INFO GenotypeGVCFs - Shutting down engine; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),8476.664214527651,Cpu time(s),8391.206707930733; [January 14, 2020 1:15:30 AM BRT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 279.78 minutes.; Runtime.totalMemory()=16865820672; htsjdk.tribble.TribbleException: Invalid block size -122708061; at htsjdk.variant.bcf2.BCF2Decoder.readNextBlock(BCF2Decoder.java:66); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:134); at htsjdk.variant.bcf2.BCF2Codec.decode(BCF2Codec.java:58); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490); at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.eng",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941:1811,load,loadNextNovelFeature,1811,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574113941,1,['load'],['loadNextNovelFeature']
Performance,"est; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3516,concurren,concurrent,3516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,ethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6086#issuecomment-519578293:4067,concurren,concurrent,4067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6086#issuecomment-519578293,5,['concurren'],['concurrent']
Performance,etryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolEx,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5038,concurren,concurrent,5038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these events aren't called in the normal and don't become candidates for tagging, as currently implemented). > And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me. I've already expanded the scope of https://github.com/broadinstitute/gatk/issues/4115 to include t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3501,tune,tuned,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,2,['tune'],['tuned']
Performance,executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4943,concurren,concurrent,4943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"explain what the splitting index is a bit better and then it will make more sense I think. Spark works by splitting files up into similar sized chunks and passing those chunks to different worker machines. ; Bam files are hard to split nicely into chunks. The way they're structured makes it hard to identify where safe boundaries are to split on. If you don't have a splitting index, we have an algorithm to start reading at essentially random locations and look for safe splitting points, but we've had some issues in the past where you can misidentify a split (which results in a crash) or miss good splits. The splitting index is a precomputed list of split points, which works around the problem of having to find the splits again next time. It's only used by spark tools that load bams, so it won't benefit Mutect2 because that's not built on spark. . We should add some documentation about this somewhere... #4235",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965:782,load,load,782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4219#issuecomment-359826965,1,['load'],['load']
Performance,"f variants from a single scaffold to the output file but then exits with `java.lang.ArrayIndexOutOfBoundsException` (see below). I have also tried adding the `-L` flag and an interval list, which performs similarly but outputs variants from a different scaffold. Any idea why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: February 3, 2020 2:59:47 PM CST; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:1137,Load,Loading,1137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,1,['Load'],['Loading']
Performance,"factor of around four, which gzip often does not reach (because it doesn't know ahead of time that DNA has only four letters).; Use reference genome fasta as proxy for nearly no repetition at all. It doesn't compress much beyond 2bit. Tweaking of the Huffmann coding etc. might have influenced the compression level much in this case, by ""giving the compressor a subtle hint about the four letters"".; Paradoxically, Intel might have optimized for average data and thus brought a disadvantage for the four letter nature of DNA (and also the few letters used in quality data encoding compared to text). 3. BQSR:; When I did interleaving compression experiments, I noticed that the BQSR step decreases compressiblity considerably.; In this example I had the same BAM file in different versions that were aligned to hs38DH, hs38, hs37d5 and could compress them to nearly the size of one, by putting similar pieces of the files after one another.; Adding the same BAM with BQSR increased final file size more than several pre-BQSR versions together.; Note: This piece-meal packing might be useful for different BAMs mostly only with many BAMs where similar regions accumulate. 4. Even faster:; In my experience, level 0 (no compression) (with samtools view -u) increases speed even more, if files are on a lz4 encrypted disk (such as with ZFS).; The speed-up of lz4 over even level 1 of any gzip-like compression is substantial.; With data on SSDs or similarly fast storage, that can make a huge difference. Another factor 6 six faster than level 1 on compression and a factor 9 on decompression. The then possible decompression speed of 3GB/s makes it possible to e.g. load a 180GB bam into a RAM disk in 60 seconds on a sufficiently fast SSD array (e.g. as on an aws ec2 i3.8xlarge instance).; Still 600MB/s if the RAM is also lz4 compressed. See image from https://github.com/lz4/lz4 below.; ![grafik](https://user-images.githubusercontent.com/1612006/35339046-d84b8b78-011f-11e8-99ec-a36cde725bb3.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673:3932,load,load,3932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3413#issuecomment-360179673,1,['load'],['load']
Performance,fc221f39ae35a0604d3b3eca?src=pr&el=desc) will **decrease** coverage by `0.57%`.; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #3903 +/- ##; ==============================================; - Coverage 79.487% 78.917% -0.57% ; + Complexity 18094 16760 -1334 ; ==============================================; Files 1187 1103 -84 ; Lines 65403 59950 -5453 ; Branches 9932 9464 -468 ; ==============================================; - Hits 51987 47311 -4676 ; + Misses 9429 8972 -457 ; + Partials 3987 3667 -320; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3903?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...der/utils/linalg/FourierLinearOperatorNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9saW5hbGcvRm91cmllckxpbmVhck9wZXJhdG9yTkRBcnJheS5qYXZh) | `47.619% <Ã¸> (Ã¸)` | `7 <0> (?)` | |; | [...te/hellbender/tools/exome/PerformSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9QZXJmb3JtU2VnbWVudGF0aW9uLmphdmE=) | `100% <Ã¸> (Ã¸)` | `3 <0> (Ã¸)` | :arrow_down: |; | [...itute/hellbender/utils/GATKProtectedMathUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9HQVRLUHJvdGVjdGVkTWF0aFV0aWxzLmphdmE=) | `69.531% <Ã¸> (-13.802%)` | `51 <0> (-8)` | |; | [...stitute/hellbender/utils/icg/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pY2cvRHVwbGljYWJsZU51bWJlci5qYXZh) | `80% <Ã¸> (Ã¸)` | `5 <0> (?)` | |; | [...bender/tools/exome/NormalizeSomaticReadCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/3903/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911:1254,Perform,PerformSegmentation,1254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3903#issuecomment-348528911,1,['Perform'],['PerformSegmentation']
Performance,"file path: file:///home/deepak/software_library/gatk-4.1.7.0/Cosmic.db -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_3/hg38/Cosmic.db; .; .; .; .; .; .; .; . 16:01:43.969 INFO DataSourceUtils - Resolved data source file path: file:///home/deepak/software_library/gatk-4.1.7.0/dnaRepairGenes.20180524T145835.csv -> file:///media/deepak/EXTRA/FUNCOTATOR_DATA/DATA_SOURCES/data_source_8/hg38/dnaRepairGenes.20180524T145835.csv; 16:01:43.979 INFO Funcotator - Initializing Funcotator Engine...; 16:01:43.983 INFO Funcotator - Creating a VCF file for output: file:/home/deepak/software_library/gatk-4.1.7.0/variants.funcotated.vcf; 16:01:44.020 INFO ProgressMeter - Starting traversal; 16:01:44.020 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 16:01:44.068 WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr1:1-10454 due to alternate allele: <NON_REF>; 16:01:44.116 INFO VcfFuncotationFactory - dbSNP 9606_b150 cache hits/total: 0/0; 16:01:44.121 INFO Funcotator - Shutting down engine; [12 May, 2020 4:01:44 PM IST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=2889875456; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:-9 end:10464; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:733); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createReferenceSnippet(FuncotatorUtils.java:1439); at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getBasesInWindowAroundReferenceAllele(FuncotatorUtils.java:1468); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationForSymbolicAltAllele(GencodeFuncotationFactory.java:25",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036:6037,cache,cache,6037,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-664565036,1,['cache'],['cache']
Performance,"flow. We can add a single BAM case workflow or expand the matched-pair workflow to handle this, depending on the discussion at https://github.com/broadinstitute/gatk/issues/3657.; - WES/WGS is toggled by providing an optional target-file input.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] O",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1544,perform,performed,1544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['performed']
Performance,"fter which the usual modelling and smoothing steps are performed. For the 75% tumor + 25% normal mixture, this yields 122 segments (up from 83):; ![N-25-T-75-SJS modeled](https://user-images.githubusercontent.com/11076296/76558618-015bd180-6474-11ea-996a-48d39770149b.png). For the 25% tumor + 75% normal mixture, this yields 105 segments (up from 50):; ![N-75-T-25-SJS modeled](https://user-images.githubusercontent.com/11076296/76560726-34a05f80-6478-11ea-9027-a54726c46b9e.png). One could imagine that smoothing could be disabled (so that all samples retain the common segmentation after modeling) or made more aggressive (so that private events don't get inadvertently introduced into other samples due to noise, perhaps), depending on the use case. It looks like the joint segmentation allows some additional events to be resolved, although I haven't done any rigorous evaluations. We could probably cook up some evaluations using simulated toy data or in silico mixtures, but there's really no reason why this shouldn't work decently well, especially if the kernel-segmentation method works well on a single sample for your data. It would also be interesting to understand at which point changing segmentation parameters on a single sample can no longer yield the same performance as joint segmentation on a fixed number of samples; however, this is probably a function of various S/N ratios, and it might not be easy to characterize this behavior outside of toy data. The segmentation parameter space is big enough to make this unwieldy even for toy data, too. Perhaps we can get some feedback from test users---not only on performance, but also on the structure of the new workflow. It might also be worth gauging whether a new WDL is warranted. Otherwise, we just need to add some unit tests for correctness of the multisample-segmentation backend class, integration tests for plumbing of the new tool, and perhaps address some of the issues mentioned above. Then I'd say this is good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823:2407,perform,performance,2407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-598386823,4,['perform'],['performance']
Performance,"g.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; ^C; ####################### Ctrl-C after 16 hours ##############; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6363,load,loaded,6363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,4,['load'],['loaded']
Performance,"germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; Using GATK jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar Mutect2 -R /home/proj/stage/cancer/reference/GRCh37/genome/human_g1k_v37_decoy.fasta -L /home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed -I consensus/concatenated_ACC5611A5_XXXXXX_consensusalign_ss_r2.bam --germline-resource /home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz -O mutect2/concatenated_ACC5611A5_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 11:47:50.850 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 02, 2020 11:47:51 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:47:51.054 INFO Mutect2 - ------------------------------------------------------------; 11:47:51.055 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 11:47:51.055 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:47:51.055 INFO Mutect2 - Executing as ashwini.jeggari@compute-0-0.local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 11:47:51.055 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:47:51.055 INFO Mutect2 - Start Date/Time: July 2, 2020 11:47:50 AM CEST; 11:47:51.056 INFO Mutect2 - --------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:1345,Load,Loading,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"ges.githubusercontent.com/11076296/29582016-23fbc6a6-8749-11e7-951e-f618e8489a0b.png). ![4](https://user-images.githubusercontent.com/11076296/29582044-3eb20a1e-8749-11e7-84a0-3734bad15e1f.png). ![5](https://user-images.githubusercontent.com/11076296/29582047-410ac490-8749-11e7-8a98-b2098cf1b5ea.png); 4) For each of these cost functions, find (up to) the _C<sub>max</sub>_ most significant local minima. The problem of finding local minima of a noisy function can be solved by using topological persistence (e.g., https://people.mpi-inf.mpg.de/~weinkauf/notes/persistence1d.html and http://www2.iap.fr/users/sousbie/web/html/indexd3dd.html?post/Persistence-and-simplification). A straightforward watershed algorithm can sort all local minima by persistence in linear time after an initial sort of the data.; 5) These sets of local minima from all window sizes together provide the pool of candidate changepoints (some of which may overlap exactly or approximately). We perform backwards selection using the global segmentation cost. That is, we compute the global segmentation cost given all the candidate changepoints, calculate the cost change for removing each of the changepoints individually, remove the changepoint with the minimum cost change, and repeat. This gives the global cost as a function of the number of changepoints _C_.; 6) Add a penalty _a C + b C log(N / C)_ to the global cost and find the minimum to determine the number of changepoints. For the above simulated data, _a = 2_ and _b = 2_ works well, recovering all of the changepoints in the above example with no false positives:; ![6](https://user-images.githubusercontent.com/11076296/29582517-fbd604be-874a-11e7-8ef7-7bd727f65dcb.png). ![7](https://user-images.githubusercontent.com/11076296/29582518-fddf015c-874a-11e7-89e4-87250d2a52ab.png). In contrast, CBS produces two false positives (around the third and seventh of the true changepoints):. ![8](https://user-images.githubusercontent.com/11076296/29582545-18875126-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586:2312,perform,perform,2312,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-324125586,2,['perform'],['perform']
Performance,"gion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost of storing the undownsampled reads for an active region in memory. We'd also have to educate users on exactly what the various downsampling arguments do for active region walkers. [...]. Making the hardcoded per-active-region cap settable from the command line is the easy part -- what seems hard is:; - Determining whether we can avoid storing all undownsampled reads in memory at once without affecting the quality of calls. Currently, as outlined in earlier comments on this ticket, we do a downsampling pass per locus which respects dcov (in Locu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4890,perform,performance,4890,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,"gion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38177,concurren,concurrent,38177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"given that we only have easy access to scores for positive truth---and hence, no false positives, which precludes calculation of precision and F1. I *think* we could pass a VCF for a sample with gold-standard positives and negatives and use the existing code for extracting labels, but this will require a bit of engineering and be more trouble than it's worth. There are other options---see https://ir.cwi.nl/pub/30479, for example. We might want to experiment with the LL score discussed there (see https://www.aaai.org/Papers/ICML/2003/ICML03-060.pdf for the original paper---although note that despite the paper's high citation count, I'm not sure what the canonical name for this metric actually is, but it doesn't appear to be ""LL score""---perhaps someone else knows or has better Google-fu and can figure it out) before moving on to their methods for estimating F1. Doing a literature search for other discussions of optimizing F1 or other metrics in the context of positive-unlabeled learning might be worthwhile, but I think most methods will probably involve some sort of estimation of the base rate in unlabeled data. I think we may have to add some mechanism for holding out a validation set during training if we want to automatically tune thresholds in a rigorous fashion. Shouldn't be too bad---we can just have the training tool randomly mask out a set of the truth and pass the mask to the scoring tool (or maybe just determine the threshold in the training tool, if we are running in positive/negative mode and have access to unlabeled data)---but does add a couple of parameters to the tool interfaces. This also adds additional dependence on the quality of the truth resources. I think an implicit assumption in any use of the truth---even just thresholding/calibrating by sensitivity---is that it is a random sample; however, I'm not sure how true this is in actual use. For example, in malaria, it looks like we may have to resort to using a callset that has been very conservat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241:992,optimiz,optimizing,992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1062931241,2,['optimiz'],['optimizing']
Performance,"gradle builds the dependency cache archive while online, and then builds a package using this archive while offline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584455056:29,cache,cache,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6395#issuecomment-584455056,1,['cache'],['cache']
Performance,graph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/cb2/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103); at org.gradle.api.internal.project.taskf,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716:5945,concurren,concurrent,5945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155#issuecomment-566796716,1,['concurren'],['concurrent']
Performance,graph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336); at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193); at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); Caused by: org.gradle.api.GradleException: Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/usr/bin/gatk/build/tmp/gatkDoc/javadoc.options'; at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:58); at org.gradle.api.tasks.javadoc.internal.JavadocGenerator.execute(JavadocGenerator.java:31); at org.gradle.api.tasks.javadoc.Javadoc.executeExternalJavadoc(Javadoc.java:158); at org.gradle.api.tasks.javadoc.Javadoc.generate(Javadoc.java:146); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103); at org.gradle.api.internal.project.taskfa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:4819,concurren,concurrent,4819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,1,['concurren'],['concurrent']
Performance,"hat's happening. We wouldn't expect gatk4 haplotype caller to be that much slower. . It looks like they're running beta2 which is kind of old as well. Can you ask them what exact version they're using?. Can you ask if they have the log (stdout + stderr) for the gatk4 non-spark run? I can't tell what pairhmm they're actually running with and the logs would help with that. . Can you also find out what sort of hardware they're running on? Specifically, is it an intel machine with support for AVX?. A good setting for` --nativePairHmmThreads` is probably 4-8, you won't see any improvement after that. I also noticed that they're setting -XX:+UseParallelGC -XX:ParallelGCThreads=32 for the gatk3. They would be better off setting it to 2-4 threads. Performance gets worse beyond that typically from what I've seen. They can set the same thing for gatk4 using`--javaOptions ' -XX:+UseParallelGC -XX:ParallelGCThreads=4'`. Their spark configuration looks wrong in a number of ways which is probably a big part of why they're not seeing any improvement. In general you want executors with ~4-8 cores and at least 4g of memory per core. I don't know how much memory their nodes have, and I don't know if they're running with autoscaling turned on, but I suspect they're only allocating 1 executor on 1 node and then it's thrashing memory because it's trying to run 32 threads at once. Spark tuning for haplotype caller is going to be complicated though and I don't know how to do it will yet, we will be revisiting it in the next quarter probably. They're also running withs spark 2.1.0, we currently require spark 2.0.2 which is an unfortunately specific version, we're planning on upgrading to spark 2.2.+ in the next quarter. . You should make it clear to them that the results will not be the same between 3, 4, and 4-spark yet and that 4 is in rapid state of flux and has known performance issues that we're planning on working soon. Even so though, that slowdown they're seeing is bizarrely large.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964:1917,perform,performance,1917,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631#issuecomment-332879964,2,['perform'],['performance']
Performance,"he RDD:. ``` Java; JavaRDD<Variant> rddParallelVariants =; variantsSparkSource.getParallelVariants(output);. System.out.println( rddParallelVariants.first().toString() );; ```. And after re-compiling GATK and running `PrintVCFSpark`, I got the following to print the first element of the RDD:. ``` Bash; $ ./gatk-launch PrintVCFSpark --input test.vcf --output test.vcf. Running:; /home/pgrosu/me/hellbender_broad_institute/gatk/build/install/gatk/bin/gatk PrintVCFSpark --input test.vcf --output test.vcf; [February 14, 2016 7:04:16 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark --output test.vcf --input test.vcf --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false; [February 14, 2016 7:04:16 PM EST] Executing as pgrosu on Linux 2.6.32-358.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_05-b13; Version: Version:4.alpha-86-g154d0a8-SNAPSHOT JdkDeflater; 19:04:16.098 INFO PrintVCFSpark - Initializing engine; 19:04:16.100 INFO PrintVCFSpark - Done initializing engine; 2016-02-14 19:04:17 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2016-02-14 19:04:19 WARN MetricsSystem:71 - Using default name DAGScheduler for source because spark.app.id is not set.; MinimalVariant -- interval(1:737406-737411), snp(false), indel(true); 19:04:24.266 INFO PrintVCFSpark - Shutting down engine; [February 14, 2016 7:04:24 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVCFSpark done. Elapsed time: 0.14 minutes.; Runtime.totalMemory()=90177536; ```. This seems to have the ability to load a VCF as a JavaRDD. Let me know if this is what you were looking for, and sorry if I am assuming this solves the problem. Thanks and hope you're keeping warm :); Paul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857:2182,load,load,2182,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486#issuecomment-184011857,2,['load'],['load']
Performance,"he per-contig coverage histograms (unfiltered bins in blue, bins retained after filtering in red, and negative-binomial fit in green) and a heatmap of per-contig ploidy probabilities. Both the panel (first 20) and case (remaining) samples are shown:. ![prototype-result](https://user-images.githubusercontent.com/11076296/37938642-e9fbd804-312c-11e8-8a6c-02ea4e4fa704.png). Although the prototype model is clearly a good fit to the filtered data, some care in choosing the optimizer and its learning parameters is required to achieve convergence to the correct solution. This is because the problem is inherently multimodal and thus there are many local minima. I found that using AdaMax with a naive strategy of warm restarts (to help kick us out of local minima) worked decently; we can achieve convergence in <10 minutes for 60 samples x 24 contigs x 250 count bins:. ![elbo](https://user-images.githubusercontent.com/11076296/37938658-fc176f12-312c-11e8-89e2-40c68e0f9953.png). I expect that @mbabadi's annealing implementation in the gcnvkernel package will handle the local minima much better. The course of action needed to implement this model should be as follows:. 1) Alter Java code to emit per-contig histograms. Change python code to consume histograms, perform filtering, and fit using the above model (or some variation).; 2) Choose learning parameters appropriate with annealing and check that results are still good.; 3) Update gCNV model to consume the depth emitted by this model properly, if necessary, and rerun evaluations. Other improvements enabled by mappability filtering (as discussed in #4558) or coverage collection can follow this initial model revision. In the meantime, we will continue the first round of evaluations using the old ploidy model, spot checking genotype calls as necessary. This will allow us to tune gCNV parameters (which will hopefully be largely unaffected by any changes to the ploidy model). How does this sound, @ldgauthier @mbabadi @asmirnov239?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271:3289,perform,perform,3289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376278271,4,"['perform', 'tune']","['perform', 'tune']"
Performance,"he; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1869,load,load,1869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,ich seems like it's probably related. My favorite part about that exception is `This is likely because code is not running on Google Compute Engine.`. ```; java.util.concurrent.CompletionException: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:605); 	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); 	at com.in,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:1044,concurren,concurrent,1044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,icsdb.GenomicsDBImport.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.loadHeaderFromVCFUri(GenomicsDBImport.java:252); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.initializeHeaderAndSampleMappings(GenomicsDBImport.java:223); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onStartup(GenomicsDBImport.java:202); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:114); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:562); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:541); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:493); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:451); 	at htsjdk,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:1870,concurren,concurrent,1870,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,ign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (Ã¸)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (Ã¸)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `87.719% <100.000%> (+19.135%)` | :arrow_up: |; | [...titute/hellbender/engine/cache/LocatableCache.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGUuamF2YQ==) | `100.000% <100.000%> (Ã¸)` | |; | ... and [1863 more](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:5125,cache,cache,5125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741,1,['cache'],['cache']
Performance,"iled to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$601(GenomicsDBImport.java:602); 	... 8 more; Caused by: org.broadinstitute.hellbender.exceptions.UserException: Failed to create reader from gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:640); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$600(GenomicsDBImport.java:593); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	... 3 more; Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engine., for input source: gs://fc-c64a0fcd-fb30-4a7e-bdc6-3c09a9286941/f6aeb0ce-044a-4b36-a5f0-d3fda62252a5/ReblockGVCF/66c24439-cfeb-4b85-bc02-aefe693177b6/call-GenotypeGVCF/NWD197223.vcf.gz; 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:97); 	at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:82); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:109); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:638); 	... 5 more; Caused by: com.google.cloud.storage.StorageException: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Goog",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423:3344,concurren,concurrent,3344,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-411503423,1,['concurren'],['concurrent']
Performance,"imation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch over to a BitSet, which seems to let us get away with -Xmx8g instead of -Xmx12g. Calling:; - I've ported over the naive `ReCapSegCaller` wholesale. This can take in the output of `ModelSegments`, so we can take advantage of the improved segmentation as before, but we still don't use the modeled minor-allele fractions when making calls. The method for copy-ratio calling is also extremely naive, with hardcoded bounds for identifying the copy-neutral state.; - [ ] @MartonKN is going to work on an improved caller for his next project. This caller should also make simple calls (not full allelic copy number, but just `0`, `+`, `-`), but should also take advantage of the copy-ratio and minor-allele fraction posteriors estimated by `ModelSegments` to generate quality scores. Plotting:; - Other than the allele-fraction model, the limiting factor was the original plotting code (some plotting runs originally to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:7677,perform,performance,7677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['performance']
Performance,ines 60902 60867 -35 ; Branches 9437 9437 ; ===============================================; + Hits 48705 48780 +75 ; + Misses 8401 8296 -105 ; + Partials 3796 3791 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...coveragemodel/CoverageModelArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL0NvdmVyYWdlTW9kZWxBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `86.592% <Ã¸> (Ã¸)` | `40 <0> (Ã¸)` | :arrow_down: |; | [...gemodel/cachemanager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java],MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:1592,cache,cachemanager,1592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,institute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFuture,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2937,concurren,concurrent,2937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"ion. We will repeat the het-genotyping step, but this is cheap and it's probably better to repeat it to make sure filtering is applied consistently. It would also require more changes to the command line to specify where to output the hets for each sample during multisample segmentation and to skip genotyping in each scatter, if we were to go that route. There are many possible combinations of inputs that need to be tested, but the same is already true of the current ModelSegments. Furthermore, there are slight wrinkles when running in tumor-only mode (i.e., when `--normal-allelic-counts` are not available). Because each sample is genotyped indiviudally, each may yield a different set of hets (in contrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as opposed to the *.modelFinal.seg result, since that undergoes segment smoothing/merging), but will also contain the segment-level p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:3104,perform,perform,3104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"ion.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:8035,Concurren,ConcurrentModificationException,8035,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['Concurren'],['ConcurrentModificationException']
Performance,"ire count file for all samples a determining factor? If we can drastically reduce this cost, then we can dedicate more to increasing resolution, etc. Here is a minimal set of fixes that could enable the querying of intervals for GermlineCNVCaller (and also for DetermineGermlineContigPloidy without too much extra work, since we also subset intervals there) *only in the gCNV WGS pipeline*, without disrupting other interfaces:. 1) Write a Tribble SimpleCountCodec for the `counts.tsv` extension. I've already done this in a branch.; 2) Change GermlineCNVCaller and DetermineGermlineContigPloidy tools to accept paths.; 3) If an index is present for each count path, create a FeatureDataSource, merge the requested -L/-XL intervals, and query to perform the subset. We will also need to stream the SAM header metadata. It should not require much code to extract all this to a temporary IndexedSimpleCountCollection class. (Caveat: for now, this will work with the current gCNV convention of providing bins via -L/-XL. Technically, it will also work with the more conventional use of -L/-XL to denote contiguous regions, but we may have to perform checks that bins are not duplicated in adjacent shards if they overlap both, since querying a FeatureDataSource will return any bins that overlap the interval---rather than only those that are completely contained within it.); 4) Index read-count TSVs in the gCNV WGS pipeline after collection and modify the DetermineGermlineContigPloidy and GermlineCNVCaller tasks to take read-count paths and indices, if necessary. These changes could be confined in the gCNV WGS WDL for now. I think that should do the trick. If this is high priority, I can implement now. In the future, we might be able to promote all Locatable CNV Records to Features and write code to automatically pass the columns/encoders/decoders (currently held in the Collection corresponding to each Record) to a single Tribble codec. This codec should not depend upon the file extension.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082:1291,perform,perform,1291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716#issuecomment-468360082,1,['perform'],['perform']
Performance,"is also includes an addition of `libblas-dev`.; - [x] Update expected results for integration tests, perhaps add any that might be missing. EDIT: These were generated on WSL Ubuntu 20.04.2, we'll see if things pass on 22.04. Note that changing the ARD priors does change the *names* of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back the fix for 2-interval shards in https://github.com/broadinstitute/gatk/pull/8180, which I removed since the required functionality wasn't immediately available in Pytensor. Not sure if this actually broke things though---need to check. (However, I don't actually think this is a very important use case to support...); - [x] Delete/deprecate/etc. CNN tools/tests as appropriate. Note that this has to be done concurrently, since we remove Tensorflow. @droazen perhaps I can take a first stab at this in a subsequent commit to this PR once more of the gCNV dust settles and/or has undergone a preliminary review? EDIT: Disabled integration/WDL tests. We should add some deprecation messages to the too",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:2883,perform,performance,2883,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['perform'],['performance']
Performance,"is run appear to be:. haplotype-to-reference: 2, -8, -19; read-to-haplotype: 1, -4, -3. I wouldn't put much stock in interpreting these parameters or their exact values for now, but it does appear that the match values and the haplotype-to-reference gap-open penalty might be saturating the bounds of the search. Plots of the type suggested by @dalessioluca might be more illuminating. Compare with default performance:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 9.000 4003 4019 494 1036 0.8905 0.7944 0.8397; None 4009 4025 511 1030 0.8873 0.7956 0.8390; ````. That the corresponding curve with a precision/sensitivity endpoint of (0.8873, 0.7956) above isn't at the top of the pack means that we could squeeze out some extra calls by varying the SW parameters. Of course, this doesn't account for negative impact elsewhere. One could imagine writing a loss where this sensitivity is optimized while putting minimum constraints on precision, sensitivity, and/or F1 in the high-confidence, high-complexity regions (the assumption being the truth set is complete in those regions), or some weightings/variations thereof. EDIT: Actually, looks like overall performance in the high-confidence region improves:. ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 12.000 49955 49955 1932 2065 0.9628 0.9603 0.9615; None 49988 49988 1994 2032 0.9616 0.9609 0.9613; ````; vs. defaults:; ````; Threshold True-pos-baseline True-pos-call False-pos False-neg Precision Sensitivity F-measure; ----------------------------------------------------------------------------------------------------; 12.000 49837 49865 2012 2183 0.9612 0.9580 0.9596; None 49870 49898 2077 2150 0.9600 0.9587 0.9594; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692:1501,optimiz,optimized,1501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564#issuecomment-715465692,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,ispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2046,concurren,concurrent,2046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,it/757112ff6cd8c8a81bb43319a0936dc0bf69a9ec?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (757112f) will **increase** coverage by `0.020%`.; > The diff coverage is `85.791%`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #8132 +/- ##; ===============================================; + Coverage 86.642% 86.662% +0.020% ; - Complexity 38963 39097 +134 ; ===============================================; Files 2336 2341 +5 ; Lines 182730 183522 +792 ; Branches 20066 20117 +51 ; ===============================================; + Hits 158321 159043 +722 ; - Misses 17366 17399 +33 ; - Partials 7043 7080 +37 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <Ã¸> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <Ã¸> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:1444,scalab,scalable,1444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"ites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; Using GATK jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/gatktest -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:35:32.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:32.890 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.891 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:35:32.891 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:32.891 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:35:32.891 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:35:32.891 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:35:32 PM CST; 13:35:32.891 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.892 INFO BaseRecalibrator - ------------------------------------------------------------; 13:35:32.892 INFO BaseRecalibrat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:1732,Load,Loading,1732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,"ith downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenvalues and left-singular vectors) for the specified number of eigensamples, and command line.; - [x] In a future iteration, we could allow an input PoN to be the source of read counts. This would allow iteration on filter parameters without needing output from CombineReadCounts. The code should easily allow for this.; - [x] ReadCountCollection is too memory intensive; minimize use in DenoiseReadCounts when writing results.; - [x] Optimize and clean up HDF5 writing ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:1586,Perform,Perform,1586,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['Perform'],['Perform']
Performance,"itializing engine; Created workspace /humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/forkTest; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3426,concurren,concurrent,3426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,itute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `Ã¸` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `Ã¸` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `Ã¸` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `Ã¸` |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:3141,scalab,scalable,3141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"itute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). Next, I tried only ""gencode"" in the data-source folder. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/test/ --ref-version hg38; 23:01:57.151 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:01:57.341 INFO Funcotator - ------------------------------------------------------------; 23:01:57.341 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 23:01:57.341 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:01:57.342 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:01:57.342 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:01:57.342 INFO Funcotator - Start Date/Time: April 27, 2018 11:01:57 PM ICT; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.344 INFO Funcotator - HTSJDK Version: 2.13.2; 23:01:57.344 INFO Funcotator - Picard Version: 2.17.2; 23:01:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:4281,Load,Loading,4281,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157,1,['Load'],['Loading']
Performance,java.lang.Thread.run(Thread.java:745); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:605); at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReadersInParallel(GenomicsDBImport.java:600); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.createSampleToReaderMap(GenomicsDBImport.java:491); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:602); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 3 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$614(GenomicsDBImport.java:602); ... 8 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.re,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:2172,concurren,concurrent,2172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"k-4.0.12.0/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /home/user/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar GenomicsDBImport -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-03_S44_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-04_S45_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-01_S42_L006.filt.srt.nodup.recal.bam.g.vcf.gz -V /mnt/isilon/experiment/18075-01/aligned/variant_calling/18175D-01-02_S43_L006.filt.srt.nodup.recal.bam.g.vcf.gz --genomicsdb-workspace-path /mnt/isilon/experiment/18075-01/aligned/variant_calling/genomics_db -L /mnt/isilon/experiment/resources/final_mm10_exon.bed --interval-padding 500 --reader-threads 5; 11:57:49.202 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/user/gatk-4.0.12.0/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:57:50.896 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.896 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:57:50.896 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:57:50.896 INFO GenomicsDBImport - Executing as user@user-machine on Linux v3.13.0-119-generic amd64; 11:57:50.896 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_191-b12; 11:57:50.896 INFO GenomicsDBImport - Start Date/Time: January 11, 2019 11:57:49 AM EST; 11:57:50.897 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.897 INFO GenomicsDBImport - ------------------------------------------------------------; 11:57:50.897 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 11",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820:1078,Load,Loading,1078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342#issuecomment-453590820,1,['Load'],['Loading']
Performance,"k.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600,spark.executor.cores=2,spark.executor.instances=2 --jar /Users/droazen/src/hellbender/build/libs/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar -- CountReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn; Job [acdae2af-e0ce-4822-87f5-dcd165d85cf4] submitted.; Waiting for job output...; 20:39:42.869 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 20:39:43.053 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/acdae2af-e0ce-4822-87f5-dcd165d85cf4/gatk-package-4.beta.6-54-g0ee99da-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [November 27, 2017 8:39:43 PM UTC] CountReadsSpark --input gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 27, 2017 8:39:43 PM UTC] Executing as root@droazen-test-cluster-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_131-8u131-b11-1~bpo8+1-b11; Version: 4.beta.6-54-g0ee99da-SNAPSHOT; 20:39:43.245 INFO Cou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994:2824,Load,Loading,2824,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855#issuecomment-347320994,1,['Load'],['Loading']
Performance,k/commit/c5bccee40e3b645a71f0f3ffb48a6c7b485e99a8?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) (c5bccee) will **increase** coverage by `0.051%`.; > The diff coverage is `69.697%`. <details><summary>Additional details and impacted files</summary>. ```diff; @@ Coverage Diff @@; ## master #8074 +/- ##; ===============================================; + Coverage 86.593% 86.644% +0.051% ; - Complexity 38899 38963 +64 ; ===============================================; Files 2336 2336 ; Lines 182709 182730 +21 ; Branches 20060 20066 +6 ; ===============================================; + Hits 158213 158325 +112 ; + Misses 17441 17365 -76 ; + Partials 7055 7040 -15 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8074?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:1428,scalab,scalable,1428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"kableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:8880,concurren,concurrent,8880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,3,['concurren'],['concurrent']
Performance,"kableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at shaded.cloud_nio.com.google.common.base.Preconditions.checkArgument(Preconditions.java:146); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:487); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:93); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:49); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	... 7 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:8068,concurren,concurrent,8068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,3,['concurren'],['concurrent']
Performance,ketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.g,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6433,concurren,concurrent,6433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,lDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializing engine; 15:47:42.352 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 15:47:42.423 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 15:47:42.482 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 15:47:42.483 INFO IntelPairHmm - Available threads: 8; 15:47:42.483 INFO IntelPairHmm - Requested threads: 4; 15:47:42.483 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 15:47:42.936 INFO ProgressMeter - Starting traversal; 15:47:42.936 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 15:47:53.565 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:19555 0.2 90 508.0; 15:48:05.962 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:136820 0.4 600 1563.5; 15:48:16.023 INFO ProgressMeter - ENA|LVXK01000001|LVXK01000001.1:360783 0.6 1560 2828.9; 15:48:19.342 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.010346494000000001; 15:48:19.342 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 6.453042841; 15:48:19.347 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 10.39 sec; 15:48:19.348 INFO Mutect2 - Shutting down engine; [28 novembre 2019 15:48:19 CET] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.72 minutes.; Runtime.totalMemory()=3822583808; java.lang.IllegalArgumentException: Ca,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:2717,multi-thread,multi-threaded,2717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['multi-thread'],['multi-threaded']
Performance,"ld code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I've added this, but there could be some minor improvements. Right now, only use one het per copy-ratio interval and throw away those off-target/bin. There are a few percent of targets/bins that have more than one het, and we could potentially rescue the off-target/bin hets as well with some care.; - Old code and models are used for modeling. Since the old all",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3504,perform,perform,3504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['perform']
Performance,lerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	... 1 more; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:14621,concurren,concurrent,14621,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,2,['concurren'],['concurrent']
Performance,"ll overlap the locus post-realignment. The end result is that DP for the HaplotypeCaller represents the undownsampled depth of coverage at the locus in question, subject to the hardcoded cap of 1000 and realignment to the haplotypes. In this particular case, the actual depth at locus 1:14464 is 561 (with no downsampling), and the DP value is 546. The difference is likely due to realignment of reads to the haplotypes by the walker. So, we basically have two options:; 1. Change the documentation for the DP annotation to mention that for ActiveRegion walkers it reflects the undownsampled depth subject to things like realignment to the haplotypes (easiest option, but doesn't fix the underlying craziness); 2. Change the ActiveRegion traversal so that it respects the dcov value (could be hard -- the LIBS downsampling process discards reads on-the-fly from previous loci when moving to a new locus, but an active region involves data for multiple loci. The potential performance win for the HC is huge, though, if we could pull this off). [...]. Alright, next step then is to figure out whether it's even feasible to make the ActiveRegion traversal fully respect dcov. I think Mark, in implementing the current scheme, might have been thinking that maintaining the undownsampled reads in memory is actually less expensive in typical (non-extreme) cases than reconstructing the full set of post-downsampling reads in an active region from multiple AlignmentContexts emitted by LIBS without any duplicates. I'll have to do some performance testing to see whether or not this is the case. Will try to get to this within the next few weeks, but the QC project has immediate priority. [...]. Discussed this with Ryan -- we agreed that the right thing to do is to move the enforcement of the hard cap on the total number of reads that can be in an active region from the HC walker to the engine, and have the size of the cap be controlled by a new argument (not dcov). That way you never pay the cost ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345:4331,perform,performance,4331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/103#issuecomment-78379345,1,['perform'],['performance']
Performance,"llclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFControlSample/Benchmark/e71074a5-27ad-4a8b-a533-cdc111c0374f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9843"",; ""NIST evalindelPrecision"": ""0.9895"",; ""NIST evalsnpF1Score"": ""0.9908"",; ""NIST evalsnpPrecision"": ""0.992"",; ""NIST evalsnpRecall"": ""0.9896"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFTestSample/Benchmark/d39f91bf-295b-4a15-bd0b-2b7c6b43a347/call-CombineSummaries/summary.csv"",; ""ROC_Plots_Reported"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CreateHTMLReport/report.html""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:14467,cache,cacheCopy,14467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,loaded for pull request base (`master@d9fd22f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `12.5%`. ```diff; @@ Coverage Diff @@; ## master #5787 +/- ##; ==========================================; Coverage ? 44.104% ; Complexity ? 19589 ; ==========================================; Files ? 1973 ; Lines ? 147147 ; Branches ? 16215 ; ==========================================; Hits ? 64898 ; Misses ? 77129 ; Partials ? 5120; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5787?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...utils/activityprofile/ActivityProfileUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9hY3Rpdml0eXByb2ZpbGUvQWN0aXZpdHlQcm9maWxlVW5pdFRlc3QuamF2YQ==) | `0.442% <0%> (Ã¸)` | `1 <0> (?)` | |; | [...ils/optimization/PersistenceOptimizerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL29wdGltaXphdGlvbi9QZXJzaXN0ZW5jZU9wdGltaXplclVuaXRUZXN0LmphdmE=) | `2% <0%> (Ã¸)` | `1 <0> (?)` | |; | [...utils/downsampling/DownsamplingMethodUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvRG93bnNhbXBsaW5nTWV0aG9kVW5pdFRlc3QuamF2YQ==) | `3.448% <0%> (Ã¸)` | `1 <0> (?)` | |; | [...yper/StandardCallerArgumentCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5787/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9TdGFuZGFyZENhbGxlckFyZ3VtZW50Q29sbGVjdGlvblVuaXRUZXN0LmphdmE=) | `4.098% <0%> (Ã¸)` | `2 <0> (?)` | |; | [...lbender/utils/mcmc/ParameterizedStateUnitTest.java](https://codecov,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750:1097,optimiz,optimization,1097,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-471963750,2,['optimiz'],['optimization']
Performance,"loped a clustering procedure that is based on coverage profile at the set of targets that are highly variable across different capture kits. ; - We found that filtering on a QS metric on a final callset significantly boosted the specificity while lowering sensitivity insignificantly.; - We developed a hyperparameter optimization framework prototype that could be used in a future for general optimizations of cost/performance parameters for all GATK pipelines.; - We resolved several memory issues that came up during validations. **A few issues were encountered along the way:**; - The sensitivity and specificity on multiallellic (common) sites was significantly lower than on rare events.; - Single target calling sensitivity was lower than 20%.; - Pipeline WDL required optimization in order to handle whole genome data, however these changes were not consolidated in the official WDL. **Currently the ongoing work is focused on the following:**; - Improving sensitivity/specificity of calls on common regions. One solution being tested involves setting a prior for common regions derived from a high quality callset. Second solution is to set a different filtering threshold for common regions.; - Consolidating validation scripts to process gCNV output and outputs of competing tools measure their performances against ground truth.; - Analyzing 1000 Genomes exomes, which could be potentially used for public facing automatic evaluations. **The following items are necessary done for automatic evaluation:** ; - Dataset + truth. We need an access to a high quality public cohort with matched whole genomes. These genomes have to have a corresponding high quality truth set generated from split-read/read-pair methods. From that cohort we need to find 50-200 relatively homogeneous samples.; - An established validation workflow that outputs a set predetermined metrics that are unlikely to change in a future. Such as a sensitivity/specificity stratified by event size and allelic frequency.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502:1854,perform,performances,1854,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4123#issuecomment-532500502,1,['perform'],['performances']
Performance,"ltBaseQualities -1 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --disableToolDefaultReadFilters false; [July 24, 2017 5:48:10 PM UTC] Executing as root@57972df58207 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 24, 2017 6:04:42 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 16.54 minutes.; Runtime.totalMemory()=4191682560; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Bl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:3554,concurren,concurrent,3554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"ltBaseQualities -1 --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --disableToolDefaultReadFilters false; [July 24, 2017 5:49:13 PM UTC] Executing as root@bc900e525fef on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.1; [July 24, 2017 6:18:40 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 29.45 minutes.; Runtime.totalMemory()=4191682560; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:554); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:543); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:512); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:455); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:445); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:194); 	at htsjdk.samtools.util.Blo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:3329,concurren,concurrent,3329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,mF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `78.696% <100.000%> (+61.304%)` | :arrow_up: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `60.563% <100.000%> (+29.577%)` | :arrow_up: |; | [...der/tools/walkers/vqsr/CNNVariantWriteTensors.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFdyaXRlVGVuc29ycy5qYXZh) | `85.714% <100.000%> (+53.571%)` | :arrow_up: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-1.023%)` | :arrow_down: |; | [...titute/hellbender/utils/help/GATKGSONWorkUnit.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9oZWxwL0dBVEtHU09OV29ya1VuaXQuamF2YQ==) | `100.000% <0.000%> (Ã¸)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/8128/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8128#issuecomment-1358686586:2669,scalab,scalable,2669,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8128#issuecomment-1358686586,1,['scalab'],['scalable']
Performance,m_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `Ã¸` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `Ã¸` |; | [...scalable/modeling/BGMMVariantAnnotationsModel.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc01vZGVsLmphdmE=) | `Ã¸` |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zU2NvcmVyLmphdmE=) | `Ã¸` |; | [...able/ExtractVariantAnnotationsIntegrationTest.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:2734,scalab,scalable,2734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"mapped.aligned.duplicates_marked.recalibrated.bam -bqsr S3_2.unmapped.recal_data.csv --add-output-sam-program-record --use-original-qualities`; RecalTables in S3_2.unmapped.recal_data.csv are empty. Here is the screen dump of BaseRecalibrator and ApplyBQSR.; BaseRecalibrator; ```; Using GATK jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:39:34.915 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.915 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.4.1-83-g031c407-SNAPSHOT; 23:39:34.915 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:39:34.915 INFO BaseRecalibrator - Executing as <XXX@XXX> on Linux v3.10.0-957.12.1.el7.x86_64 amd64; 23:39:34.915 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-b08; 23:39:34.916 INFO BaseRecalibrator - Start Date/Time: February 26, 2020 11:39:34 PM EST; 23:39:34.916 INFO BaseRecal",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:1694,Load,Loading,1694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['Load'],['Loading']
Performance,"mjdk.compression_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Åuksza_2022_Nature -jar /data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk_resource/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:36:33 PM CST; 13:36:33.671 INFO BaseRecalibrator - -----------------------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:6838,Load,Loading,6838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['Load'],['Loading']
Performance,mment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <45.455%> (-3.801%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvTGFiZWxlZFZhcmlhbnRBbm5vdGF0aW9uc1dhbGtlci5qYXZh) | `86.822% <46.154%> (+0.208%)` | :arrow_up: |; | [...rs/vqsr/scalable/TrainVariantAnnotationsModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=githu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:2317,scalab,scalable,2317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,"more concretely the private method getReferenceBases(SAMSeqRecord) should be syncronized or avoid it calling directly to the syncronized getReferenceBases(SSR, boolean) and getReferenceBasesByRegions should not update the cache fields.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615:222,cache,cache,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139#issuecomment-1376313615,1,['cache'],['cache']
Performance,mtools.BAMRecordCodec.decode(BAMRecordCodec.java:209); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); 	at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:576); 	at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); 	at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:29); 	at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:15); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractP,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955:5494,load,loadNextRecord,5494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317028955,5,['load'],['loadNextRecord']
Performance,"mutect2 stopped at chromosome 1 . $ java -jar -Xmx12g gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Mutect2 -R Homo_sapiens_assembly19.fasta -I Specimen_SNCR.10_1.bam -tumor Specimen_10_1 -O mutect2/10_1.vcf&; [1] 40657; $ 09:49:03.454 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 09:49:05.899 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.899 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:49:05.900 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 09:49:05.900 INFO Mutect2 - Java runtime: IBM J9 VM v8.0.5.25 - pxa6480sr5fp25-20181030_01(SR5 FP25); 09:49:05.901 INFO Mutect2 - Start Date/Time: March 7, 2019 9:49:03 AM EST; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - ------------------------------------------------------------; 09:49:05.901 INFO Mutect2 - HTSJDK Version: 2.18.2; 09:49:05.901 INFO Mutect2 - Picard Version: 2.18.25; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:49:05.902 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:49:05.902 INFO Mutect2 - Deflater: IntelDeflater; 09:49:05.902 INFO Mutect2 - Inflater: IntelInflater; 09:49:05.902 INFO Mutect2 - GCS max retries/reopens: 20; 09:49:05.902 INFO Mutect2 - Requester pays: disabled; 09:49:05.902 INFO Mutect2 - Initializing engine; 09:49:06.887 INFO Mutect2 - Done initializing engine; 09:49:06.935 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/Tools/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 09:49:06.937 INFO PairHMM - OpenMP multi-threaded A",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844:261,Load,Loading,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-470579844,1,['Load'],['Loading']
Performance,"n ""==""?; if len(args) is 0 or (len(args) is 1 and (args[0] == ""--help"" or args[0] == ""-h"")):; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:80: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if len(args) is 0 or (len(args) is 1 and (args[0] == ""--help"" or args[0] == ""-h"")):; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:117: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if len(args) is 1 and args[0] == ""--list"":; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:308: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if call([""gsutil"", ""-q"", ""stat"", gcsjar]) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:312: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?; if call([""gsutil"", ""cp"", jar, gcsjar]) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:467: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(properties) is 0:; /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/bin/gatk:471: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?; if not len(filesToAdd) is 0:; Using GATK jar /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar Mutect2 -R /omaha-beach/jpollet/MYD88/data/ref/BALBcJ.fasta -I /omaha-beach/jpollet/MYD88/result/valide_3060_R1vsBALBcJ.sorted.md.bam -O /omaha-beach/jpollet/MYD88/result/valide_3060_R1vsBALBcJ.sortedunf.vcf; 15:47:36.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:7651,Load,Loading,7651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['Load'],['Loading']
Performance,"nConcordanceEvaluation.hapMap"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.haplotype_database.txt"",; ""GatkDragenConcordanceEvaluation.include_in_fe_analysis"": ""test_output:FunctionalEquivalenceTest.out_include_in_fe_analysis"",; ""GatkDragenConcordanceEvaluation.refDict"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""GatkDragenConcordanceEvaluation.refIndex"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""GatkDragenConcordanceEvaluation.reference"": ""gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""GatkDragenConcordanceEvaluation.referenceVersion"": ""hg38"",; ""GatkDragenConcordanceEvaluation.replicate_no"": ""test_output:FunctionalEquivalenceTest.out_replicate_no"",; ""GatkDragenConcordanceEvaluation.sample_id"": ""test_output:FunctionalEquivalenceTest.out_sample_id"",; ""GatkDragenConcordanceEvaluation.stratIntervals"": [; ""gs://broad-dsde-methods-ckachulis/benchmarking/stratifiers/LCR_Hg38.interval_list"",; ""gs://broad-dsde-methods-ckachulis/benchmarking/stratifiers/HCR_hg38.bed""; ],; ""GatkDragenConcordanceEvaluation.stratLabels"": [; ""LCR"",; ""HCR""; ],; ""GatkDragenConcordanceEvaluation.truth_vcf"": ""test_output:FunctionalEquivalenceTest.out_truth_vcf"",; ""GatkDragenConcordanceEvaluation.truth_vcf_index"": ""test_output:FunctionalEquivalenceTest.out_truth_vcf_index""; },; ""test_cromwell_job_id"": ""bc61debd-03be-4a13-a7d6-33479e047d08"",; ""eval_cromwell_job_id"": ""2fd3d6b8-6d82-4664-8217-3872d44f89ca"",; ""created_at"": ""2021-01-14T19:14:37.025719"",; ""created_by"": null,; ""finished_at"": ""2021-01-15T02:47:24.222"",; ""results"": {; ""FE plots"": ""gs://dsde-methods-carrot-prod-cromwell/GatkDragenConcordanceEvaluation/2fd3d6b8-6d82-4664-8217-3872d44f89ca/call-MergeFE/cacheCopy/plots.png"",; ""ROC plots"": ""gs://dsde-methods-carrot-prod-cromwell/GatkDragenConcordanceEvaluation/2fd3d6b8-6d82-4664-8217-3872d44f89ca/call-MergeROC/cacheCopy/plots.png""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7039#issuecomment-760609431:7581,cache,cacheCopy,7581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7039#issuecomment-760609431,2,['cache'],['cacheCopy']
Performance,nJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9SYXdHdENvdW50LmphdmE=) | `63.415% <0.000%> (-12.195%)` | :arrow_down: |; | [...er/utils/MergeAnnotatedRegionsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL01lcmdlQW5ub3RhdGVkUmVnaW9uc0ludGVncmF0aW9uVGVzdC5qYXZh) | `93.333% <0.000%> (-6.667%)` | :arrow_down: |; | [...hellbender/tools/walkers/sv/CollectSVEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3N2L0NvbGxlY3RTVkV2aWRlbmNlLmphdmE=) | `74.888% <0.000%> (-4.990%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `68.421% <0.000%> (-3.801%)` | :arrow_down: |; | [...ls/genomicsdb/GenomicsDBImportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `84.746% <0.000%> (-3.762%)` | :arrow_down: |; | [...vqsr/scalable/LabeledVariantAnnotationsWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/8163?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473:3901,scalab,scalable,3901,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8163#issuecomment-1387550473,1,['scalab'],['scalable']
Performance,nager/ComputableGraphStructure.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlR3JhcGhTdHJ1Y3R1cmUuamF2YQ==) | `100% <100%> (+26.994%)` | `63 <62> (+24)` | :arrow_up: |; | [...ragemodel/cachemanager/ComputableNodeFunction.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9Db21wdXRhYmxlTm9kZUZ1bmN0aW9uLmphdmE=) | `100% <100%> (+66.667%)` | `4 <1> (+2)` | :arrow_up: |; | [.../coveragemodel/cachemanager/DuplicableNDArray.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTkRBcnJheS5qYXZh) | `81.818% <100%> (+38.068%)` | `6 <2> (+2)` | :arrow_up: |; | [...s/coveragemodel/cachemanager/DuplicableNumber.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9EdXBsaWNhYmxlTnVtYmVyLmphdmE=) | `80% <100%> (+80%)` | `5 <2> (+5)` | :arrow_up: |; | [...coveragemodel/cachemanager/PrimitiveCacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9QcmltaXRpdmVDYWNoZU5vZGUuamF2YQ==) | `83.333% <71.429%> (+30.702%)` | `10 <7> (+3)` | :arrow_up: |; | [...er/tools/coveragemodel/cachemanager/CacheNode.java](https://codecov.io/gh/broadinstitute/gatk/pull/3035?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2NhY2hlbWFuYWdlci9DYWNoZU5vZGUuamF2YQ==) | `80.645% <76.923%> (+30.645%)` | `9 <8> (+4)` | :arrow_up: |; | [...overagemodel/cachemanager/ComputableCacheNode.java](h,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418:2253,cache,cachemanager,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3035#issuecomment-306412418,1,['cache'],['cachemanager']
Performance,"nc_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 --num-executors 20 --executor-cores 6 --executor-memory 6g /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar CountReadsSpark --input /project/casa/gcad/adsp.cc/cram/A-ADC-AD010072-BL-NCR-11AD44210.hg38.realign.bqsr.cram --reference ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 23:10:10.737 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 23:10:10.965 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/gatk-4.1.0.0/gatk-package-4.1.0.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 23:10:12.679 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.680 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:10:12.680 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:10:12.680 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 23:10:12.681 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 23:10:12.681 INFO CountReadsSpark - Start Date/Time: February 5, 2019 11:10:10 PM EST; 23:10:12.681 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.681 INFO CountReadsSpark - ------------------------------------------------------------; 23:10:12.683 INFO Count",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912:3356,Load,Loading,3356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-460895912,1,['Load'],['Loading']
Performance,"ncher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instan",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5607,load,loaded,5607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,nds 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gvcf-gq-bands 21 --gvcf-gq-bands 22 --gvcf-gq-bands 23 --gvcf-gq-bands 24 --gvcf-gq-bands 25 --gvcf-gq-bands 26 --gvcf-gq-bands 27 --g; vcf-gq-bands 28 --gvcf-gq-bands 29 --gvcf-gq-bands 30 --gvcf-gq-bands 31 --gvcf-gq-bands 32 --gvcf-gq-bands 33 --gvcf-gq-bands 34 --gvcf-gq-bands 35 --gvcf-gq-bands 36 --gvcf-gq-bands 37 -; -gvcf-gq-bands 38 --gvcf-gq-bands 39 --gvcf-gq-bands 40 --gvcf-gq-bands 41 --gvcf-gq-bands 42 --gvcf-gq-bands 43 --gvcf-gq-bands 44 --gvcf-gq-bands 45 --gvcf-gq-bands 46 --gvcf-gq-bands 47; --gvcf-gq-bands 48 --gvcf-gq-bands 49 --gvcf-gq-bands 50 --gvcf-gq-bands 51 --gvcf-gq-bands 52 --gvcf-gq-bands 53 --gvcf-gq-bands 54 --gvcf-gq-bands 55 --gvcf-gq-bands 56 --gvcf-gq-bands ; 57 --gvcf-gq-bands 58 --gvcf-gq-bands 59 --gvcf-gq-bands 60 --gvcf-gq-bands 70 --gvcf-gq-bands 80 --gvcf-gq-bands 90 --gvcf-gq-bands 99 --floor-blocks false --indel-size-to-eliminate-in-re; f-model 10 --disable-optimizations false --dragen-mode false --flow-mode NONE --apply-bqd false --apply-frd false --disable-spanning-event-genotyping false --transform-dragen-mapping-quali; ty false --mapping-quality-threshold-for-genotyping 20 --max-effective-depth-adjustment-for-frd 0 --just-determine-active-regions false --dont-genotype false --do-not-run-physical-phasing ; false --do-not-correct-overlapping-quality false --use-filtered-reads-for-annotations false --use-flow-aligner-for-stepwise-hc-filtering false --adaptive-pruning false --do-not-recover-dan; gling-branches false --recover-dangling-heads false --kmer-size 10 --kmer-size 25 --dont-increase-kmer-sizes-for-cycles false --allow-non-unique-kmers-in-ref false --num-pruning-samples 1 ; --min-dangling-branch-length 4 --recover-all-dangling-branches false --max-num-haplotypes-in-population 128 --min-pruning 2 --adaptive-pruning-initial-error-rate 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:4341,optimiz,optimizations,4341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['optimiz'],['optimizations']
Performance,"ne.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(See",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7693,concurren,concurrent,7693,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"ne.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(See",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6881,concurren,concurrent,6881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,nfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7078,load,load,7078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['load'],['load']
Performance,"ng. commit dd2dd503a92e6fbb5a49be6a88d2e813eb8bf85b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 15:14:08 2023 -0500. update gCNV expected results, generated on WSL Ubuntu 20.04.2. commit 27d76e8f22d61df90eeb337e033ae128ce07ab90; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 14:53:04 2023 -0500. update python env integration tests. commit 348df9192235f7d1ea941d0b31e5c96acc0d6491; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 10:59:23 2023 -0500. disable CNN tests, add deprecation message. commit ed59372b4be226785af1d3fb1b1a39a9ad3b4f6a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 12 09:55:24 2023 -0500. clean up rebase. commit 18e530db26f803ee46a0006843cb36d4ed4194b4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 11:31:46 2023 -0500. postprocess fixed. commit f510c2e9f10d7066c15f1835669d676964b8a4cb; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 10:13:01 2023 -0500. fix deprecated np.int in optimizer. commit 939a032f356f2f8f67b5aae426fc427d1d1ea6c4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:50:57 2023 -0500. remove unnecessary seeding in cohort denoising script. commit cf82ea5c99250f1784f8b1a9279e7dbb8841fa89; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:38:08 2023 -0500. add back setup.py files. commit 8348f546de6b3d32e1f02f6851730226c0dbffc9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:37:09 2023 -0500. update pymc version in init. commit 850d60ef95b6126c05af9cd7c2cb528a306e1224; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:32:42 2023 -0500. added pip editable docs. commit 9c51b311442b0796ab1224213e83290caea0f93f; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:50 2023 -0500. whitespace. commit d9b180385168fdd1ef55cee8a1069fc1f7928f38; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:10 2023 -0500. update setup_gcnvkernel.py and pin pytensor. commit 7ccbd6d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:2086,optimiz,optimizer,2086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['optimiz'],['optimizer']
Performance,ng.IllegalArgumentException: Unknown Java version: 11; 	at net.bytebuddy.ClassFileVersion.ofJavaVersion(ClassFileVersion.java:135); 	at net.bytebuddy.ClassFileVersion$VersionLocator$ForJava9CapableVm.locate(ClassFileVersion.java:357); 	at net.bytebuddy.ClassFileVersion.ofThisVm(ClassFileVersion.java:147); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:301); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection$Dispatcher$CreationAction.run(ClassInjector.java:290); 	at java.base/java.security.AccessController.doPrivileged(Native Method); 	at net.bytebuddy.dynamic.loading.ClassInjector$UsingReflection.<clinit>(ClassInjector.java:70); 	at net.bytebuddy.dynamic.loading.ClassLoadingStrategy$Default$InjectionDispatcher.load(ClassLoadingStrategy.java:184); 	at net.bytebuddy.dynamic.TypeResolutionStrategy$Passive.initialize(TypeResolutionStrategy.java:79); 	at net.bytebuddy.dynamic.DynamicType$Default$Unloaded.load(DynamicType.java:4456); 	at org.mockito.internal.creation.bytebuddy.SubclassBytecodeGenerator.mockClass(SubclassBytecodeGenerator.java:115); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:37); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator$1.call(TypeCachingBytecodeGenerator.java:34); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:138); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:346); 	at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:161); 	at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:355); 	at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:32); 	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMockType(SubclassByteBuddyMockMaker.java:71); 	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMock(Subclass,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836:1152,load,load,1152,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6119#issuecomment-532377836,1,['load'],['load']
Performance,"ngine; [January 6, 2021 4:26:39 PM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=2303197184; ***********************************************************************. A USER ERROR has occurred: Couldn't create GenomicsDBFeatureReader. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4125,optimiz,optimizations,4125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['optimiz'],['optimizations']
Performance,"ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/gof/link.py"", line 325, in raise_with_op; reraise(exc_type, exc_value, exc_trace); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/six.py"", line 692, in reraise; raise value.with_traceback(tb); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/compile/function_module.py"", line 903, in __call__; self.fn() if output_subset is None else\; File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 963, in rval; r = p(n, [x[0] for x in i], o); File ""/ngc/projects/gm/data/resources/envs/conda/ngs_gatk_cnv/4.1.6.0/lib/python3.6/site-packages/theano/scan_module/scan_op.py"", line 952, in p; self, node); File ""scan_perform.pyx"", line 215, in theano.scan_module.scan_perform.perform; NotImplementedError: We didn't implemented yet the case where scan do 0 iteration; Apply node that caused the error: forall_inplace,cpu,scan_fn}(Elemwise{minimum,no_inplace}.0, InplaceDimShuffle{0,2,1}.0, Subtensor{int64:int64:int64}.0, IncSubtensor{InplaceSet;:int64:}.0, Shape_i{0}.0); Toposort index: 95; Inputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(int64, scalar)]; Inputs shapes: [(), (0, 6, 6), (0, 6), (2, 6), ()]; Inputs strides: [(), (288, 8, 48), (48, 8), (48, 8), ()]; Inputs values: [array(0), array([], shape=(0, 6, 6), dtype=float64), array([], shape=(0, 6), dtype=float64), 'not shown', array(6)]; Inputs type_num: [7, 12, 12, 12, 7]; Outputs clients: [[Subtensor{int64:int64:int8}(forall_inplace,cpu,scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})]]. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282:3137,perform,perform,3137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-613371282,1,['perform'],['perform']
Performance,"nning, top indicates is using aboutÂ  240g (after importing the 65 batches).; ```; PID USER Â  Â  Â PR Â NI Â  Â VIRT Â  Â RES Â  Â SHR S Â %CPU %MEM Â TIME+ COMMAND; 21698 farrell Â  20 Â  0Â  Â  Â  443.7g 240.3g Â  1416 S Â 86.7 95.5 Â  7398:14 java; ```. ```; #!/bin/bash -l; #$ -l mem_total=251; #$ -P casa; #$ -pe omp 32; #$ -l h_rt=240:00:00; module load miniconda/4.9.2; module load gatk/[4.2.6.1](http://4.2.6.1/); conda activate /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1](http://4.2.6.1/install/gatk-4.2.6.1). CHR=$1; DB=""genomicsDB.rb.chr$CHR""; rm -rf $DB; # mkdir -p $DB; # mkdir tmp; echo ""Processing chr$CHR""; echo ""NSLOTS: $NSLOTS""; # head sample_map.chr$CHR.reblock.list; head sample_map.chr$CHR; wc Â  sample_map.chr$CHR; gatk --java-options ""-Xmx150g -Xms16g"" \; Â  Â  Â  Â GenomicsDBImport \; Â  Â  Â  Â --sample-name-map sample_map.chr$CHR \; Â  Â  Â  Â --genomicsdb-workspace-path $DB \; Â  Â  Â  Â --genomicsdb-shared-posixfs-optimizations True\; Â  Â  Â  Â --tmp-dir tmp \; Â  Â  Â  Â --L chr$CHR\; Â  Â  Â  Â --batch-size 50 \; Â  Â  Â  Â --bypass-feature-reader\; Â  Â  Â  Â --reader-threads 5\; Â  Â  Â  Â --merge-input-intervals \; Â  Â  Â  Â --overwrite-existing-genomicsdb-workspace\; Â  Â  Â  Â --consolidate. ```; End of log on chr3. ```; 07:19:44.855 INFO Â GenomicsDBImport - Done importing batch 38/65; 08:05:11.651 INFO Â GenomicsDBImport - Done importing batch 39/65; 08:49:12.112 INFO Â GenomicsDBImport - Done importing batch 40/65; 09:32:39.526 INFO Â GenomicsDBImport - Done importing batch 41/65; 10:23:36.849 INFO Â GenomicsDBImport - Done importing batch 42/65; 11:24:50.566 INFO Â GenomicsDBImport - Done importing batch 43/65; 12:17:11.236 INFO Â GenomicsDBImport - Done importing batch 44/65; 13:11:10.869 INFO Â GenomicsDBImport - Done importing batch 45/65; 13:56:22.927 INFO Â GenomicsDBImport - Done importing batch 46/65; 14:45:02.333 INFO Â GenomicsDBImport - Done importing batch 47/65; 15:35:20.713 INFO Â GenomicsDBImport - Done importing batch 48/65; 16:32:30.162 INFO Â GenomicsDBImport - Done importing batch 49/65; 17:1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:1380,optimiz,optimizations,1380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['optimiz'],['optimizations']
Performance,"nomicsDB/GenomicsDB/releases/download/v1.4.3/consolidate_genomicsdb_array) for consolidation. This executable will consolidate a given array in a GenomicsDB workspace, it has been instrumented to output memory stats to help tune the segment size. Note that the executable is for Centos 7, if you find any unresolved shared library dependencies during usage, please let me know and I will work on getting another one to you. For usage from a bash shell:; ```; ~/GenomicsDB: ./consolidate_genomicsdb_array; Usage: consolidate_genomicsdb_array [options]; where options include:; 	 --help, -h Print a usage message summarizing options available and exit; 	 --workspace=<GenomicsDB workspace URI>, -w <GenomicsDB workspace URI>; 		 Specify path to GenomicsDB workspace; 	 --array-name=<Array Name>, -a <Array Name>; 		 Specify name of array that requires consolidation; 	 --segment-size=<Segment Size>, -z <Segment Size>; 		 Optional, default is 10M. Specify a buffer size for consolidation; 	 --shared-posixfs-optimizations, -p; 		 Optional, default is false. If specified, the array folder is not locked for read/write and file handles are kept open until a final close for write; 	 --version Print version and exit; ```. ```; ~/GenomicsDB.: ./consolidate_genomicsdb_array -w /Users/xxx/WGS.gdb/ -a ""1\$1\$249250621"" -z 1048576 -p; 21:09:47.100 info consolidate_genomicsdb_array - pid=30881 tid=30881 Starting consolidation of 1$1$249250621 in ws; Using buffer_size=1048576 for consolidation; 21:9:47 Memory stats(pages) beginning consolidation size=45821 resident=18998 share=1824 text=3530 lib=0 data=16810 dt=0; 21:9:47 Memory stats(pages) after alloc for attribute=END size=45821 resident=19009 share=1835 text=3530 lib=0 data=16810 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=REF size=46788 resident=19743 share=1889 text=3530 lib=0 data=17773 dt=0; 21:9:48 Memory stats(pages) after alloc for attribute=ALT size=47143 resident=20124 share=1889 text=3530 lib=0 data=18129 dt=0; 21:9",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354:1175,optimiz,optimizations,1175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1057680354,1,['optimiz'],['optimizations']
Performance,nomicsDBImport - Done importing batch 53/65; 20:18:42.274 INFO Â GenomicsDBImport - Done importing batch 54/65; 21:01:51.304 INFO Â GenomicsDBImport - Done importing batch 55/65; 21:36:00.458 INFO Â GenomicsDBImport - Done importing batch 56/65; 22:08:38.587 INFO Â GenomicsDBImport - Done importing batch 57/65; 22:40:44.082 INFO Â GenomicsDBImport - Done importing batch 58/65; 23:14:11.202 INFO Â GenomicsDBImport - Done importing batch 59/65; 23:48:23.805 INFO Â GenomicsDBImport - Done importing batch 60/65; 00:20:35.869 INFO Â GenomicsDBImport - Done importing batch 61/65; 00:51:47.408 INFO Â GenomicsDBImport - Done importing batch 62/65; 01:25:23.587 INFO Â GenomicsDBImport - Done importing batch 63/65; 01:59:03.103 INFO Â GenomicsDBImport - Done importing batch 64/65; Using GATK jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) defined in environment variable GATK_LOCAL_JAR; Running:; Â  Â  java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx150g -Xms16g -jar /share/pkg.7/gatk/[4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar](http://4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar) GenomicsDBImport --sample-name-map sample_map.chr3 --genomicsdb-workspace-path genomicsDB.rb.chr3 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --L chr3 --batch-size 50 --bypass-feature-reader --reader-threads 5 --merge-input-intervals --overwrite-existing-genomicsdb-workspace --consolidate; [farrell@scc-hadoop genomicsdb]$ ls genomicsDB.rb.chr3; __tiledb_workspace.tdb Â chr3$1$198295559 Â vcfheader.vcf Â vidmap.json. ```; It never indicates that it imported batch 65/65.Â No error and theÂ Â callset.json is missing which we found in chr4 to chr22. ; Â Â ; ls genomicsDB.rb.chr4. __tiledb_workspace.tdb Â callset.json Â chr4$1$190214555 Â vcfheader.vcf Â vidmap.json,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232:4133,optimiz,optimizations,4133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1246785232,1,['optimiz'],['optimizations']
Performance,"not the issue. Stacktrace in the bottom. The folder permission of the datastore folder is as follows:; `drwx--S---+ 26 vidprijatelj group 4096 Mar 14 15:29 Vid_database`. When changing to 766, the error disappears. ```; Tue Mar 14 15:37:57 CET 2023; Using GATK jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=zzz_tmpdir -Xmx128G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs --reference /data/Scratch/References/ucsc.hg38.fa --variant gendb://Vid_database --output Step05_MultiSampleCalling/Vid.vcf.gz --intervals /data/Scratch/References/hg38_exome_v2.0.2_merged_probes_sorted_validated.annotated.bed --genomicsdb-shared-posixfs-optimizations True --merge-input-intervals; 15:37:59.895 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:38:00.018 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.018 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 15:38:00.018 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:38:00.018 INFO GenotypeGVCFs - Executing as user@server; 15:38:00.018 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-b08; 15:38:00.019 INFO GenotypeGVCFs - Start Date/Time: March 14, 2023 3:37:59 PM CET; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - HTSJDK Version: 3.0.1; 15:38:00.019 INFO GenotypeGVCFs - Picard Version: 2.27.5; 15:38:00.019 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918:1094,Load,Loading,1094,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918,1,['Load'],['Loading']
Performance,"nsferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Tas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38080,concurren,concurrent,38080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"ntutils` package, which is strange because the PR did not modify the javadoc for any class in that package. The integration test runs `com.sun.tools.javadoc.Main.execute` and asserts that the output code is zero, which does not yield a useful error message. In order to produce something more meaningful I hacked the test to output the entire `stdout` and `stderr` as follows:. ```; final StringWriter out = new StringWriter();; final PrintWriter err = new PrintWriter(out);. final int result = com.sun.tools.javadoc.Main.execute(""program"", err, err, err, ""doclet"",docArgList.toArray(new String[] {}));; err.flush(); // probably not needed; String message = out.toString(); // message contains the entire stdout and stderr of the call to execute; Assert.assertEquals(result, 0, message);; ```. The output is about 2000 lines, but a lot of it is clearly innocuous. Removing lines such as; * `2022-08-16T00:09:07.2336106Z [parsing completed 1ms]`; * `2022-08-16T00:09:07.4456202Z [loading ZipFileIndexFileObject[/jars/gatk-package-4.2.6.1-56-gad9a538-SNAPSHOT-test.jar(org/broadinstitute/hellbender/tools/walkers/variantutils/ReblockGVCFIntegrationTest.class)]]`; * `2022-08-16T00:09:07.4459732Z [loading RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalIntervalArgumentCollection.java]]`; * `2022-08-16T00:09:07.4462012Z [parsing started RegularFileObject[src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/OptionalReferenceInputArgumentCollection.java]]`; * 2022-08-16T00:09:07.2322755Z [loading ZipFileObject[/gatk/gatk-package-unspecified-SNAPSHOT-local.jar(htsjdk/samtools/SAMSequenceDictionary.class)]]. brings it down to 353 lines, the majority of which look like . ```2022-08-16T00:09:07.4435974Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:479: error: cannot find symbol; 2022-08-16T00:09:07.4436105Z @VisibleForTesting; ```. Here's that 353-line file:. [log-no-parsing-loading.txt](https://",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488:1090,load,loading,1090,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217231488,2,['load'],['loading']
Performance,"o cumbersome for large matrices. Change CreatePanelOfNormals to take in multiple -I instead.; - [x] Rename NormalizeSomaticReadCounts to DenoiseReadCounts and require integer read counts as input. These will still be backed by a ReadCountCollection until @asmirnov239's changes are in.; - [x] Remove optional outputs (factor-normalized and beta-hats) from DenoiseReadCounts. For now, TN and PTN output will remain in the same format (log2) to maintain compatibility with downstream tools.; - [x] Maximum number of eigensamples K to retain in the PoN is specified; the smaller of this or the number of samples remaining after filtering is used. The number actually used to denoise can be specified in DenoiseReadCounts. If we are going to spend energy computing K eigensamples, there is no reason we shouldn't expose all of them in the PoN, even if we don't want to use all of them for denoising. (Also, the current SVD utility methods do not allow for specification of K < N when performing SVD on an MxN matrix, even though the backend implementations that are called do allow for this; this is terrible. In any case, randomized SVD should be much faster than the currently available implementations, even when K = N).; - [x] Rename CreatePanelOfNormals to CreateReadCountPanelOfNormals; - [x] Refer to ""targets"" as intervals. See #3246.; - [x] Remove QC.; - [x] Refer to proportional coverage as fractional coverage.; - [x] Perform optional GC-bias correction internally if annotated intervals are passed as input.; - [x] Make standardization process for panel and case samples identical. Currently, a sample mean is taken at one point in the PoN standardization process, while a sample median is taken in the case standardization process.; - [x] HDF5 PoN will store version number, all integer read counts, all/panel intervals, all/panel sample paths/names, all annotated intervals (if GC-bias correction was performed), fractional-coverage medians for all intervals, relevant SVD results (eigenva",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687:1140,perform,performing,1140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-313921687,1,['perform'],['performing']
Performance,o.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:94); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:621); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.tribble.util.ParsingUtils.resourceExists(ParsingUtils.java:419); at htsjdk.tribble.AbstractFeatureReader.isTabix(AbstractFeatureReader.java:222); at htsjdk.tribble.AbstractFeatureReader$ComponentMethods.isTabix(AbstractFeatureReader.java:228); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:106); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getReaderFromPath(GenomicsDBImport.java:638); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.lambda$getFeatureReadersInParallel$613(GenomicsDBImport.java:593); at java.util.concurrent.FutureTask.run(FutureTask.java:266); ... 3 more; Caused by: com.google.cloud.storage.StorageException: ComputeEngineCredentials cannot find the metadata server. This is likely because code is not running on Google Compute Engine.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:614); ... 11 more; Caused by: java.io.IOException: ComputeEngineCredentials cannot find the ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420:3744,concurren,concurrent,3744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412904420,1,['concurren'],['concurrent']
Performance,"oading libgkl_utils.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.dylib; 22:42:22.720 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.dylib; 22:42:22.722 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 22:42:22.724 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to 0.0 for reference-model confidence output; 22:42:22.724 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output; 22:42:22.734 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 22:42:22.734 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 22:42:22.734 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; 22:42:22.748 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 22:42:22.748 WARN IntelPairHmm - Ignoring request for 4 threads; not using OpenMP implementation; 22:42:22.748 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 22:42:22.751 WARN GATKVariantContextUtils - Can't determine output variant file format from output file extension ""bam"". Defaulting to VCF.; 22:42:22.776 INFO ProgressMeter - Starting traversal; 22:42:22.777 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000000010f47efd3, pid=96919, tid=0x0000000000002303; #; # JRE version: OpenJDK Runtime Environment (8.0_192-b01) (build 1.8.0_192-b01); # Java VM: OpenJDK 64-Bit ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:3646,Load,Loading,3646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['Load'],['Loading']
Performance,"odec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at htsjdk.tribble.AbstractFeatureCodec.decodeLoc(AbstractFeatureCodec.java:43); at org.broadinstitute.hellbender.utils.codecs.ProgressReportingDelegatingCodec.decodeLoc(ProgressReportingDelegatingCodec.java:46); at htsjdk.tribble.index.IndexFactory$FeatureIterator.readNextFeature(IndexFactory.java:689); at htsjdk.tribble.index.IndexFactory$FeatureIterator.<init>(IndexFactory.java:606); at htsjdk.tribble.index.IndexFactory.createDynamicIndex(IndexFactory.java:446); at org.broadinstitute.hellbender.tools.IndexFeatureFile.createAppropriateIndexInMemory(IndexFeatureFile.java:118); at org.broadinstitute.hellbender.tools.IndexFeatureFile.doWork(IndexFeatureFile.java:75); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Full Traceback (most recent call last):; File ""/home/ychrysostomakis/.local/lib/python3.9/site-packages/snakemake/executors/__init__.py"", line 2578, in run_wrapper; run(; File ""/share/pool/CompGenomVert/phoxy_snp_calling/VC_HIFI/joint_snp_calling.smk"", line 422, in __rule_index_feature_file; File ""/home/ychrysostomakis/.local/lib/python3.9/site-packages/snakemake/shell.py"", line 300, in __new__; raise sp.CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'set -euo pipefail; ; module load gatk/4.1.4.1; gatk IndexFeatureFile -I output/called/final/allsites.filtered.vcf' returned non-zero exit status 3.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:5581,load,load,5581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['load'],['load']
Performance,"of `--max-alternate-alleles` set to 7:; ```; on-chinookomes-dna-seq-gatk-variant-calling]--% gatk --java-options ""-Xmx4g"" GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz. Using GATK jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar GenotypeGVCFs -R resources/genome.fasta -V gendb://results/genomics_db/chromosomes/CM031199.1 --max-alternate-alleles 7 -O results/vcf_parts/CM031199.1.vcf.gz; 21:57:11.346 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/eanderson/Documents/projects/yukon-chinookomes-dna-seq-gatk-variant-calling/.snakemake/conda/cd50d464/share/gatk4-4.2.4.1-0/gatk-package-4.2.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2022 9:57:11 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:57:11.476 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:57:11.477 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.4.1; 21:57:11.477 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:57:11.477 INFO GenotypeGVCFs - Executing as eanderson@node34.cluster on Linux v4.18.0-193.28.1.el8_2.x86_64 amd64; 21:57:11.477 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src; 21:57:11.477 INFO GenotypeGVCFs - Start Date/Time:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059:11121,Load,Loading,11121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7639#issuecomment-1014180059,1,['Load'],['Loading']
Performance,"of the expected files, since the transform is appended to the corresponding variable name. DetermineGermlineContigPloidy and PostprocessGermlineCNVCalls are missing exact-match tests and should probably have some, but I'll leave that to someone else.; - [x] Update other python integration tests.; - [x] Clean up some of the changes to the priors.; - [x] Clean up some TODO comments that I left to track code changes that might result in changed numerics. I'll try to go through and convert these to PR comments in an initial review pass.; - [x] Test over multiple shards on WGS and WES. Probably some scientific tests on ~100 samples in both cohort and case mode would do the trick. We should also double check runtime/memory performance (I noted ~1.5x speedups, but didn't measure carefully; I also want to make sure the changes to posterior sampling didn't introduce any memory issues). @mwalker174 will ping you when a Docker is ready! Might be good to loop in Isaac and/or Jack as well.; - [x] Perhaps add back the fix for 2-interval shards in https://github.com/broadinstitute/gatk/pull/8180, which I removed since the required functionality wasn't immediately available in Pytensor. Not sure if this actually broke things though---need to check. (However, I don't actually think this is a very important use case to support...); - [x] Delete/deprecate/etc. CNN tools/tests as appropriate. Note that this has to be done concurrently, since we remove Tensorflow. @droazen perhaps I can take a first stab at this in a subsequent commit to this PR once more of the gCNV dust settles and/or has undergone a preliminary review? EDIT: Disabled integration/WDL tests. We should add some deprecation messages to the tools---we can note that they should still work in previous environments but will be untested. I might set up a separate PR for deletion, to be done at the appropriate time (but I call dibs on this, can't have @davidbenjamin overtaking my all-time record for number of lines deleted ðŸ˜›).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285:3582,concurren,concurrently,3582,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1847549285,1,['concurren'],['concurrently']
Performance,oken=7RuX7LsQVf&height=150&src=pr)](https://codecov.io/gh/broadinstitute/gatk/pull/5413?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #5413 +/- ##; ============================================; + Coverage 87.02% 87.08% +0.06% ; - Complexity 30454 30511 +57 ; ============================================; Files 1853 1856 +3 ; Lines 140995 141484 +489 ; Branches 15518 15536 +18 ; ============================================; + Hits 122706 123217 +511 ; + Misses 12648 12617 -31 ; - Partials 5641 5650 +9; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5413?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...der/tools/walkers/contamination/PileupSummary.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vUGlsZXVwU3VtbWFyeS5qYXZh) | `90% <Ã¸> (+1.42%)` | `14 <0> (+1)` | :arrow_up: |; | [...dinstitute/hellbender/utils/OptimizationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9PcHRpbWl6YXRpb25VdGlscy5qYXZh) | `42.1% <100%> (+3.21%)` | `4 <1> (+1)` | :arrow_up: |; | [...ination/CalculateContaminationIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vQ2FsY3VsYXRlQ29udGFtaW5hdGlvbkludGVncmF0aW9uVGVzdC5qYXZh) | `100% <100%> (Ã¸)` | `14 <1> (+2)` | :arrow_up: |; | [...ols/walkers/contamination/ContaminationRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/5413/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vQ29udGFtaW5hdGlvblJlY29yZC5qYXZh) | `88.23% <100%> (-2.88%)` | `5 <1> (-1)` | |; | [.../walkers/contamination/CalculateContamination.java](https://codecov.io/gh/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438824631:1478,Optimiz,OptimizationUtils,1478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5413#issuecomment-438824631,1,['Optimiz'],['OptimizationUtils']
Performance,"omicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:44:02.752 INFO GenomicsDBImport - Requester pays: disabled; 15:44:02.752 INFO GenomicsDBImport - Initializing engine; 15:44:07.262 INFO FeatureManager - Using codec BEDCodec to read file file:///home/akansha/vivekruhela/gatk_bundle/hglift_genome1.bed; 15:44:07.274 INFO IntervalArgumentCollection - Processing 2759468497 bp from intervals; 15:44:07.276 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 15:44:07.307 INFO GenomicsDBImport - Done initializing engine; 15:44:07.591 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 15:44:07.592 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/akansha/vivekruhela/pon_db/vidmap.json; 15:44:07.592 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/akansha/vivekruhela/pon_db/callset.json; 15:44:07.592 INFO GenomicsDBImport - Complete VCF Header will be written to /home/akansha/vivekruhela/pon_db/vcfheader.vcf; 15:44:07.592 INFO GenomicsDBImport - Importing to workspace - /home/akansha/vivekruhela/pon_db; 15:44:07.592 WARN GenomicsDBImport - GenomicsDBImport cannot use multiple VCF reader threads for initialization when the number of intervals is greater than 1. Falling back to serial V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:2538,perform,performance,2538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['perform'],['performance']
Performance,omment&utm_campaign=pr+comments&utm_term=broadinstitute) (c9bf941) will **increase** coverage by `0.264%`.; > The diff coverage is `56.604%`. > :exclamation: Current head 2268ee6 differs from pull request most recent head d1dbe69. Consider uploading reports for the commit d1dbe69 to get more accurate results. ```diff; @@ Coverage Diff @@; ## master #8131 +/- ##; ===============================================; + Coverage 86.362% 86.626% +0.264% ; + Complexity 39551 38919 -632 ; ===============================================; Files 2362 2336 -26 ; Lines 186121 182603 -3518 ; Branches 20305 20062 -243 ; ===============================================; - Hits 160738 158181 -2557 ; + Misses 18236 17379 -857 ; + Partials 7147 7043 -104 ; ```. | [Files Changed](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage |; |---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `Ã¸` |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `0.000%` |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://app.codecov.io/gh/broadinstitute/gatk/pull/8131?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153:1560,scalab,scalable,1560,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8131#issuecomment-1352314153,1,['scalab'],['scalable']
Performance,"omparison/1269d993-e13f-4635-a12a-e65fdaa4ed16/call-BenchmarkVCFControlSample/Benchmark/492b823a-1e34-46cd-b842-5f042bb31ee8/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-EXOME1SampleHeadToHead/BenchmarkComparison/1269d993-e13f-4635-a12a-e65fdaa4ed16/call-BenchmarkVCFTestSample/Benchmark/834b6562-65d7-4daf-857a-d9118a6456b7/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-NISTSampleHeadToHead/BenchmarkComparison/338d644e-3327-471e-9d17-1c103fa5e01e/call-BenchmarkVCFControlSample/Benchmark/145d88de-5810-47e1-972a-18ff0169fe27/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""92.82975"",; ""NIST evalHCsystemhours"": ""0.17177777777777778"",; ""NIST evalHCwallclockhours"": ""66.4404388888889"",; ""NIST evalHCwallclockmax"": ""3.325327777777778"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:19681,cache,cacheCopy,19681,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"omparison/3b586c16-feb0-4cdd-8850-8426205cced2/call-BenchmarkVCFControlSample/Benchmark/31dfb54a-9ecc-4af2-9fcd-ea9af745342e/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-EXOME1SampleHeadToHead/BenchmarkComparison/3b586c16-feb0-4cdd-8850-8426205cced2/call-BenchmarkVCFTestSample/Benchmark/7c7e45ee-4fe9-48e6-b8ed-cd4372c9e726/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-NISTSampleHeadToHead/BenchmarkComparison/d1a60d2b-8100-459a-9b05-72a22afccb4a/call-BenchmarkVCFControlSample/Benchmark/9f6d4e85-981d-4607-8ff6-97495034807f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""96.65376666666666"",; ""NIST evalHCsystemhours"": ""0.17881944444444442"",; ""NIST evalHCwallclockhours"": ""68.38394444444445"",; ""NIST evalHCwallclockmax"": ""3.8226138888888888"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:19694,cache,cacheCopy,19694,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"omparison/3ba68beb-5853-4beb-b31c-cbef12825001/call-BenchmarkVCFControlSample/Benchmark/18840f82-6653-4365-8e02-daf8790ea4f0/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-EXOME1SampleHeadToHead/BenchmarkComparison/3ba68beb-5853-4beb-b31c-cbef12825001/call-BenchmarkVCFTestSample/Benchmark/194337cf-f57b-46fa-812c-e6510f51fd8d/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-NISTSampleHeadToHead/BenchmarkComparison/a39481f5-0969-4891-a843-f3c3fd7437d1/call-BenchmarkVCFControlSample/Benchmark/0c99102a-bca1-4426-97c6-5a311ace93c1/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:13467,cache,cacheCopy,13467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"omparison/4803682b-a3c6-46d6-924b-dbc96a877e16/call-BenchmarkVCFControlSample/Benchmark/dd059ca4-251d-4793-bbff-10dd76123882/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-EXOME1SampleHeadToHead/BenchmarkComparison/4803682b-a3c6-46d6-924b-dbc96a877e16/call-BenchmarkVCFTestSample/Benchmark/e3563584-017d-476b-bbca-775128c80272/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFControlSample/Benchmark/6d64f12a-ca50-4ecd-8608-93dc53d241bb/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:13467,cache,cacheCopy,13467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,"omparison/5bf5f11a-64cb-4b50-8d05-b61b7f4c803c/call-BenchmarkVCFControlSample/Benchmark/c64dbce6-4a90-42c0-a84b-59857afb98a5/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-EXOME1SampleHeadToHead/BenchmarkComparison/5bf5f11a-64cb-4b50-8d05-b61b7f4c803c/call-BenchmarkVCFTestSample/Benchmark/d501a36a-a881-4e5c-9499-ef7dea22980f/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-NISTSampleHeadToHead/BenchmarkComparison/4ffa2353-b1bc-4960-a5a4-96291208a7eb/call-BenchmarkVCFControlSample/Benchmark/8cf95ec9-48a7-4e20-a8fe-816dc3e652ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:20354,cache,cacheCopy,20354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"omparison/688ca200-89b9-479b-b701-5fa0b0854778/call-BenchmarkVCFControlSample/Benchmark/59d8f8b1-1323-4e56-a1b1-0b1b2c8f2cc0/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-EXOME1SampleHeadToHead/BenchmarkComparison/688ca200-89b9-479b-b701-5fa0b0854778/call-BenchmarkVCFTestSample/Benchmark/1b8ccc58-1ead-4443-b6a8-64f767abfc70/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-NISTSampleHeadToHead/BenchmarkComparison/d1047505-b7bc-455d-851f-fbed8d81e895/call-BenchmarkVCFControlSample/Benchmark/5388d7b6-6bcd-451d-9a4e-925b386ecd0c/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.03499722222222"",; ""NIST evalHCsystemhours"": ""0.17304166666666665"",; ""NIST evalHCwallclockhours"": ""67.81165555555557"",; ""NIST evalHCwallclockmax"": ""3.691061111111111"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-b",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:19695,cache,cacheCopy,19695,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,"omparison/efb51584-614a-4702-bc80-17a6a388e888/call-BenchmarkVCFControlSample/Benchmark/ea5e6517-663b-4cfb-b264-0dc933da9ae3/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.727"",; ""EXOME1 evalindelPrecision"": ""0.632"",; ""EXOME1 evalsnpF1Score"": ""0.9878"",; ""EXOME1 evalsnpPrecision"": ""0.9815"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-EXOME1SampleHeadToHead/BenchmarkComparison/efb51584-614a-4702-bc80-17a6a388e888/call-BenchmarkVCFTestSample/Benchmark/086dd5e8-74c8-4603-b618-a70d77398545/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-NISTSampleHeadToHead/BenchmarkComparison/ed0dc9e1-2d64-47e4-82e0-811971957020/call-BenchmarkVCFControlSample/Benchmark/8c516721-e955-41d1-907e-fcee92f592d3/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""100.56416111111112"",; ""NIST evalHCsystemhours"": ""0.19999166666666665"",; ""NIST evalHCwallclockhours"": ""74.00048055555555"",; ""NIST evalHCwallclockmax"": ""4.007605555555555"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:20388,cache,cacheCopy,20388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"on appears to have a memory leak or using just requiring too much memory. The -consolidate option was the culprit. So rerunning chr1-3 with just the --bypass-feature-reader option (test2) ran fine without lots of memory being used. Below is the time output from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socke",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:1912,optimiz,optimizations,1912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,['optimiz'],['optimizations']
Performance,on.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.net.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.Buffe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:5930,concurren,concurrent,5930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"onf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - Start Date/Time: April 24, 2018 5:39:18 PM CEST; 17:39:19.220 INFO PathSeqPipelineSpark - --------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2535,Load,Loading,2535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,1,['Load'],['Loading']
Performance,"onfun$reportAllBlocks$3.apply(BlockManager.scala:219); 	at org.apache.spark.storage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5314,concurren,concurrent,5314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"ons (@fleharty @mwalker174?) or I run into any unforeseen snafus or parameter ambiguities along the way, I am going to roll the multisample-segmentation functionality into ModelSegments. We can toggle this functionality by passing multiple `--denoised-copy-ratios` and `--allelic-counts` arguments, e.g.:. ```; gatk ModelSegments ; --normal-allelic-counts normal.allelicCounts.tsv (this is only used for het genotyping); --denoised-copy-ratios normal.denoisedCR.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; ...; --denoised-copy-ratios tumor-N.denoisedCR.tsv; --allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; ...; --allelic-counts tumor-N.allelicCounts.tsv \; -O .; --output-prefix joint-segmentation; ```. This will perform both het genotyping and joint segmentation, but will yield a Picard interval-list `joint-segmentation.interval_list` as its sole output. (Although we could proceed to perform MCMC model inference on each sample in series, we'll stop at segmentation to enforce the scattering of inference across samples, which will be quicker.) We can also allow for copy-ratio-only and allelic-count-only modes. Users can use this joint segmentation in their own downstream tools, but we can also allow ModelSegments to ingest it via in a new `--segments` argument:. ```; gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv (equivalently, we could omit this and adjust minimum-total-allele-count-case, as is done in the WDL); --allelic-counts normal.allelicCounts.tsv; --denoised-copy-ratios normal.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix normal. gatk ModelSegments; --normal-allelic-counts normal.allelicCounts.tsv; --allelic-counts tumor-1.allelicCounts.tsv; --denoised-copy-ratios tumor-1.denoisedCR.tsv; --segments joint-segmentation.interval_list; -O .; --output-prefix tumor-1. ...; ```. Each scatter of ModelSegments will run as before, aside from skipping the segmentation step i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:1040,perform,perform,1040,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['perform']
Performance,"onsolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code optimization that I removed, since it makes assumptions on the SW parameter values that might not be valid for non-default values. I'll highlight this with a comment below. We can restore it if we add code to check whether the assumptions hold, but I'd be curious to see in which cases the optimization makes a big difference. See https://github.com/broadinstitute/gatk/issues/6863#issuecomment-707870344.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:2671,optimiz,optimizing,2671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,3,['optimiz'],"['optimization', 'optimizing']"
Performance,"ontrast to genotyping in matched-normal mode, in which the normal determines the set of hets used in all samples). We will thus have to take the intersection of these hets before performing multisample segmentation. Unfortunately, we will not be able to re-perform this intersection in each scatter, since we will no longer have access to the hets from the other samples. However, we *will* ultimately intersect the hets from each sample with the joint segmentation before modeling, which may be a rough proxy for the intersection of hets from all samples. As always, tumor-only mode may yield suboptimal results in certain scenarios, e.g., high purity CNLOH. I think I'm OK with just documenting these wrinkles, rather than working too hard to iron them out. I think this structure sets us up nicely to accommodate germline tagging/filtering in the near future. We can still pass the Picard interval list containing the joint segmentation to the scatter for the normal, but can instead subsequently pass the *.modelBegin.seg result from the normal to the tumors. This modeled-segment file will have breakpoints identical to those from the joint segmentation (as opposed to the *.modelFinal.seg result, since that undergoes segment smoothing/merging), but will also contain the segment-level posteriors necessary for performing germline filtering. We will just need to add code to toggle on the type of `--segments` input (Picard interval list or modeled segments), add filtering arguments and code, perhaps output an additional seg file showing filter status for the *.modelBegin.seg segments, and modify the segment merging/smoothing code to properly account for filter status (so we don't incorrectly impute across filtered segments, which is currently done by the unsupported code, for whatever reason). I think this should clock in at well under ~5k lines of code, which is the count for the current unsupported code (see https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549:4164,perform,performing,4164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6499#issuecomment-607313549,1,['perform'],['performing']
Performance,"ook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; 11:36:23.022 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:36:25.027 INFO CountReads - ------------------------------------------------------------; 11:36:25.028 INFO CountReads - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:36:25.028 INFO CountReads - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:36:25.029 INFO CountReads - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:36:25.029 INFO CountReads - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:36:25.030 INFO CountReads - Start Date/Time: January 7, 2019 11:36:22 AM EST; 11:36:25.030 INFO CountReads - ------------------------------------------------------------; 11:36:25.031 INFO CountReads - ------------------------------------------------------------; 11:36:25.032 INFO CountReads - HTSJDK Version: 2.18.1; 11:36:25.033 INFO CountReads -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:44263,Load,Loading,44263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['Load'],['Loading']
Performance,"ore modeling. However, instead of assuming the null hypothesis of het (f = 0.5) and accepting a site when we cannot reject the null, we assume the null hypothesis of hom (f = error rate or 1 - error rate) and accept a site when we can reject the null. This entire process is very similar to what @davidbenjamin does in https://github.com/broadinstitute/gatk/pull/3638. We should consider combining this code (along with `AllelicCount`/`PileupSummary`) at some point.; - [x] Added option to use matched normal.; - [ ] Rather than port over the old modeling code, I would rather expand the allele-fraction model to allow for the modeling of hom sites. I wrote up such a model in some notes I sent around a few months back. This model allows for an allelic PoN that uses all sites to learn reference bias, not just hets. Depending on how our python development proceeds, I may try to implement this model using the old `GibbsSampler` code instead.; - [x] In the meantime, we can try to speed up the old allele-fraction model, which is now the main bottleneck. An easy (lazy) strategy might simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density est",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:5690,bottleneck,bottleneck,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['bottleneck'],['bottleneck']
Performance,"org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5915,load,loaded,5915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,ort.getReaderFromVCFUri(GenomicsDBImport.java:437); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.getFeatureReaders(GenomicsDBImport.java:419); 	at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.traverse(GenomicsDBImport.java:344); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:740); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Read timed out; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 28 more; Caused by: com.google.cloud.storage.StorageException: Read timed out; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:2872,concurren,concurrent,2872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,oryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ; at org.broadin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:3114,load,loadNextFeature,3114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['load'],['loadNextFeature']
Performance,"ot supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5507,load,loaded,5507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['loaded']
Performance,"ot sure if that was already covered. @sooheelee - this is going to be an exact port of the indel-realignment pipeline, as it is in the GATK3 code, so that means that I won't modify the interval list format or anything (although I will use the HTSJDK/Picard classes as used on GATK3). Because this will be an experimental/beta feature, I think that I can have a look to the new format after acceptance of the original port. @cmnbroad - I understand that a fully functional tool is a requirement for acceptance, but what I mean is that some specific features might require more work than others. I am only concerned about the `NWaySAMFileWriter`, which is just an specific way of output the data but does not add anything to the real realignment process (actually, I think that I've never heard about anyone around me using it). That is a nice feature, but I don't think that it is a high-priority - I care more about having the algorithm implemented to test if the actual processing of the data works, and add support for some way of output the data in a different PR. In addition, if the people still using indel-realignment does not require the n-way output, then it is pointless to spend time on it. I was also thinking about the mate-fixing algorithm in the tool, because it can be performed afterwards with Picard, which is not constraining by any distance between reads or records in RAM - nevertheless, this is really a drop of functionality that will change results, and that's why I didn't propose that. About the target-creator, known indels are really easy to port because the code is within the tool and is simpler - the only problem might be code coverage if there is no data for known indels. I will propose very soon two PRs with fully functional tools (without the n-way out feature for indel-realignment), and trying to add simple integration tests with the data already available on the repository and running with GATK3.8-1. If that is OK for you, I will proceed with this approach.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115:1619,perform,performed,1619,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371515115,2,['perform'],['performed']
Performance,otator/filtrationRules/FuncotationFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2ZpbHRyYXRpb25SdWxlcy9GdW5jb3RhdGlvbkZpbHRlci5qYXZh) | `100% <100%> (Ã¸)` | `8 <4> (+4)` | :arrow_up: |; | [...r/filtrationRules/FilterFuncotationsExacUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2ZpbHRyYXRpb25SdWxlcy9GaWx0ZXJGdW5jb3RhdGlvbnNFeGFjVXRpbHMuamF2YQ==) | `81.818% <80.952%> (-0.535%)` | `12 <7> (+5)` | |; | [...walkers/genotyper/afcalc/AFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQUZDYWxjdWxhdG9yUHJvdmlkZXIuamF2YQ==) | `22.222% <0%> (-44.444%)` | `2% <0%> (-2%)` | |; | [...notyper/afcalc/ConcurrentAFCalculatorProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9hZmNhbGMvQ29uY3VycmVudEFGQ2FsY3VsYXRvclByb3ZpZGVyLmphdmE=) | `50% <0%> (-33.333%)` | `1% <0%> (-1%)` | |; | [...nder/utils/downsampling/PositionalDownsampler.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9kb3duc2FtcGxpbmcvUG9zaXRpb25hbERvd25zYW1wbGVyLmphdmE=) | `88.462% <0%> (-11.538%)` | `22% <0%> (+1%)` | |; | [...er/engine/spark/datasources/VariantsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvVmFyaWFudHNTcGFya1NpbmsuamF2YQ==) | `78.125% <0%> (-11.53%)` | `8% <0%> (-1%)` | |; | ... and [138 more](https://codecov.io/gh/broadinstitute/gatk/pull/5588/diff,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5588#issuecomment-455358539:3191,Concurren,ConcurrentAFCalculatorProvider,3191,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5588#issuecomment-455358539,1,['Concurren'],['ConcurrentAFCalculatorProvider']
Performance,oud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.executeAttempt(RetryingFutureImpl.java:141); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.access$500(RetryingFutureImpl.java:59); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl$AttemptFutureCallback.onFailure(RetryingFutureImpl.java:177); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures$1.onFailure(ApiFutures.java:52); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$6.run(Futures.java:1764); 	at shaded.cloud_nio.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:456); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures$ImmediateFuture.addListener(Futures.java:153); 	at shaded.cloud_nio.com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:47); 	at shaded.cloud_nio.com.google.api.gax.core.internal.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:53); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1776); 	at shaded.cloud_nio.com.google.common.util.concurrent.Futures.addCallback(Futures.java:1713); 	at shaded.cloud_nio.com.google.api.gax.core.ApiFutures.addCallback(ApiFutures.java:47); 	at shaded.cloud_nio.com.google.api.gax.retrying.RetryingFutureImpl.setAttemptFuture(RetryingFutureImpl.java:107); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:100); 	at com.google.cloud.RetryHelper.runWithRetries(Retry,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180:4453,concurren,concurrent,4453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-300298180,1,['concurren'],['concurrent']
Performance,"ound first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://user-images.githubusercontent.com/11076296/29322762-a679dba6-81ac-11e7-9360-083a4e1da398.png); ![wave-kern-small-waves](https://user-images.githubusercontent.com/11076296/29322801-dad82010-81ac-11e7-8238-e057b0072e1b.png). This local window approach is still linear in time, so runtime is still ~1s for the above (about ~10x faster than CBS). One issue still remains, which is that even this improved approach tends to find directly adjacent possible changepoints around a true changepoint before moving on to another true changepoint. We can probably clean this up with some simple postprocessing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1964,perform,perform,1964,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,2,['perform'],['perform']
Performance,pacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:48); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:70); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:37); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:181); at org.genomicsdb.reader.GenomicsDBFeatureIterator.next(GenomicsDBFeatureIterator.java:49); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextFeature(FeatureIntervalIterator.java:98); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.loadNextNovelFeature(FeatureIntervalIterator.java:74); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:62); at org.broadinstitute.hellbender.engine.FeatureIntervalIterator.next(FeatureIntervalIterator.java:24); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ; at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) ; at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:490) ; at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132); at org.broadinstitute.hellbender.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:3228,load,loadNextNovelFeature,3228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['load'],['loadNextNovelFeature']
Performance,"parison/113b01be-9124-41dd-acc0-5732ef2c7b38/call-BenchmarkVCFControlSample/Benchmark/7222f3cf-155c-423f-bc1e-8194e87ff05f/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.7573"",; ""EXOME1 evalindelPrecision"": ""0.6882"",; ""EXOME1 evalsnpF1Score"": ""0.9896"",; ""EXOME1 evalsnpPrecision"": ""0.9852"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-EXOME1SampleHeadToHead/BenchmarkComparison/113b01be-9124-41dd-acc0-5732ef2c7b38/call-BenchmarkVCFTestSample/Benchmark/e929ad45-5026-4630-8b85-19f6205f068c/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9843"",; ""NIST controlindelPrecision"": ""0.9895"",; ""NIST controlsnpF1Score"": ""0.9908"",; ""NIST controlsnpPrecision"": ""0.992"",; ""NIST controlsnpRecall"": ""0.9896"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-NISTSampleHeadToHead/BenchmarkComparison/103cd89c-b177-4a0b-84fc-9553a1f8161f/call-BenchmarkVCFControlSample/Benchmark/eaf4d582-e197-4e13-8122-5e1ec22591ae/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-ab",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:13472,cache,cacheCopy,13472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"parison/7b11647c-6643-4c47-8e1c-3f07bd97e371/call-BenchmarkVCFControlSample/Benchmark/086348b1-f09c-49b0-b830-587e28eec63d/call-CombineSummaries/summary.csv"",; ""EXOME1 evalindelF1Score"": ""0.7573"",; ""EXOME1 evalindelPrecision"": ""0.6882"",; ""EXOME1 evalsnpF1Score"": ""0.9896"",; ""EXOME1 evalsnpPrecision"": ""0.9852"",; ""EXOME1 evalsnpRecall"": ""0.9941"",; ""EXOME1 evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-EXOME1SampleHeadToHead/BenchmarkComparison/7b11647c-6643-4c47-8e1c-3f07bd97e371/call-BenchmarkVCFTestSample/Benchmark/47d80f67-4375-460f-9ce0-8186eec9fe5b/call-CombineSummaries/summary.csv"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-NISTSampleHeadToHead/BenchmarkComparison/8e62c1c2-cf9c-4530-846e-1e0d6c6d8acf/call-BenchmarkVCFControlSample/Benchmark/e71074a5-27ad-4a8b-a533-cdc111c0374f/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""73.06777222222223"",; ""NIST evalHCsystemhours"": ""0.1622555555555555"",; ""NIST evalHCwallclockhours"": ""46.65241388888888"",; ""NIST evalHCwallclockmax"": ""2.7461055555555554"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:13470,cache,cacheCopy,13470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,"park - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 16:55:20.230 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:5014,load,loaded,5014,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,park.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); at scala.collection.Iterator$class.foreach(Iterator.scala:891); at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1334); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$26.apply(RDD.scala:1190); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$27.apply(RDD.scala:1191); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724:2923,concurren,concurrent,2923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854#issuecomment-808817724,2,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29089,concurren,concurrent,29089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31788,concurren,concurrent,31788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetMana",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25902,concurren,concurrent,25902,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35526,concurren,concurrent,35526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.forea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40722,concurren,concurrent,40722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,patch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94); 	at com.sun.proxy.$Proxy5.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:132); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.base/java.lang.Thread.run(Thread.java:834); ```. Unlikely to be related to this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253:5377,concurren,concurrent,5377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6652#issuecomment-672024253,5,['concurren'],['concurrent']
Performance,pi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:92); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:47); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:125); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:109); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:104); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.available(AppInputStream.java:60); 	at java.io.BufferedInputStream.available(BufferedInputStream.java:410); 	at sun.net.www.MeteredStream.available(MeteredStream.java:170); 	at sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:85); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.close(HttpURLConnection.java:3448); 	at java.io.FilterInputStream.close(FilterInputStream.java:181); 	at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:97); 	at shade,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:6495,concurren,concurrent,6495,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""050d2d6e-4a50-4145-a9da-8a39731ebdd2"",; ""eval_cromwell_job_id"": ""0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8"",; ""created_at"": ""2023-05-04T15:40:52.834692"",; ""created_by"": null,; ""finished_at"": ""2023-05-04T17:03:53.525"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7fe5c8/call-CHMSampleHeadToHead/BenchmarkComparison/a332776f-175a-4595-bdeb-ab62e7f89921/call-BenchmarkVCFControlSample/Benchmark/06cbfab4-17a7-4415-9118-d0ebbe156bfd/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/0e5c32ab-65e6-451f-a04e-6a3f5e7f",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202:17381,cache,cacheCopy,17381,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1535104202,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""07271d7b-729d-4db9-862d-5f992a60a598"",; ""eval_cromwell_job_id"": ""89508d5f-29f1-4534-9fe1-220a80de17c4"",; ""created_at"": ""2022-07-22T17:23:11.546971"",; ""created_by"": null,; ""finished_at"": ""2022-07-23T02:09:23.327"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CHMSampleHeadToHead/BenchmarkComparison/a2a2515a-b32a-44a6-a6d1-9a6d0d2199bb/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de17c4/call-CHMSampleHeadToHead/BenchmarkComparison/a2a2515a-b32a-44a6-a6d1-9a6d0d2199bb/call-BenchmarkVCFControlSample/Benchmark/2c4ad666-e885-4e23-bd5c-d54ca521ffbf/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.99195555555558"",; ""CHM evalHCsystemhours"": ""0.16168333333333337"",; ""CHM evalHCwallclockhours"": ""55.43875833333334"",; ""CHM evalHCwallclockmax"": ""2.913311111111111"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/89508d5f-29f1-4534-9fe1-220a80de1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382:16685,cache,cacheCopy,16685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1193038382,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""410a88f6-62ca-4745-89fd-df6e30aac65b"",; ""eval_cromwell_job_id"": ""bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9"",; ""created_at"": ""2022-03-16T19:53:45.833854"",; ""created_by"": null,; ""finished_at"": ""2022-03-17T00:11:38.702"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb5d9/call-CHMSampleHeadToHead/BenchmarkComparison/79d1a2a4-6b5e-424a-8528-9059bda6db1c/call-BenchmarkVCFControlSample/Benchmark/3046acf7-ded7-40c8-9b7a-3826f480418f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/bf86d5b6-04bd-4344-b4fc-8a1df66bb",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069765064,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""54997ade-421d-439f-acc9-abf50b3f9cb5"",; ""eval_cromwell_job_id"": ""6ea2705f-a3fa-41fc-8d17-a2c55d875eab"",; ""created_at"": ""2022-03-16T19:52:46.276978"",; ""created_by"": null,; ""finished_at"": ""2022-03-17T00:13:17.198"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875eab/call-CHMSampleHeadToHead/BenchmarkComparison/1fb97a8b-caee-4184-8e36-be21e6c43549/call-BenchmarkVCFControlSample/Benchmark/3b068fb2-7140-4c1e-8860-df8df21821ec/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/6ea2705f-a3fa-41fc-8d17-a2c55d875e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069766207,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""5e9a598e-1e80-4622-b153-78e97491a478"",; ""eval_cromwell_job_id"": ""f7eac327-c59c-43f7-a850-21bc3e0ccf52"",; ""created_at"": ""2022-07-12T17:28:58.385152"",; ""created_by"": null,; ""finished_at"": ""2022-07-13T02:47:47.016"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CHMSampleHeadToHead/BenchmarkComparison/cd28fe49-1672-4321-a836-47f76419c1c8/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0ccf52/call-CHMSampleHeadToHead/BenchmarkComparison/cd28fe49-1672-4321-a836-47f76419c1c8/call-BenchmarkVCFControlSample/Benchmark/d5df8455-36cf-4ecb-8dc2-ec35b974c0b7/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.23616944444446"",; ""CHM evalHCsystemhours"": ""0.16188333333333332"",; ""CHM evalHCwallclockhours"": ""55.167422222222214"",; ""CHM evalHCwallclockmax"": ""2.887522222222222"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/f7eac327-c59c-43f7-a850-21bc3e0c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672:16697,cache,cacheCopy,16697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1182703672,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""5f0f8f34-cdc7-46ff-a59d-2368edcdf007"",; ""eval_cromwell_job_id"": ""e6f57e40-2025-46fd-9aa0-d591a3799007"",; ""created_at"": ""2022-03-16T14:20:46.087600"",; ""created_by"": null,; ""finished_at"": ""2022-03-16T17:21:08.639"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-CHMSampleHeadToHead/BenchmarkComparison/f65a7960-7b66-4a5d-a346-bd188a1b3830/call-BenchmarkVCFControlSample/Benchmark/8d0e47ca-66f5-42a0-8785-6aa8d2db2663/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""80.5165222222222"",; ""CHM evalHCsystemhours"": ""0.1713305555555555"",; ""CHM evalHCwallclockhours"": ""53.10978888888891"",; ""CHM evalHCwallclockmax"": ""2.7458416666666667"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a37990",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""9886a710-334a-41eb-a495-6968d322730a"",; ""eval_cromwell_job_id"": ""9bc521dc-3c4c-4274-972c-9d1e4be850d5"",; ""created_at"": ""2023-05-03T15:51:41.295461"",; ""created_by"": null,; ""finished_at"": ""2023-05-04T01:24:02.606"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CHMSampleHeadToHead/BenchmarkComparison/092bfb4f-d978-4964-a8ae-e5a7f7362f7c/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be850d5/call-CHMSampleHeadToHead/BenchmarkComparison/092bfb4f-d978-4964-a8ae-e5a7f7362f7c/call-BenchmarkVCFControlSample/Benchmark/6ab078fb-b668-452c-bbaa-8fb1fd8e25ba/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""84.26158888888888"",; ""CHM evalHCsystemhours"": ""0.19243055555555555"",; ""CHM evalHCwallclockhours"": ""60.242008333333345"",; ""CHM evalHCwallclockmax"": ""3.176513888888889"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/9bc521dc-3c4c-4274-972c-9d1e4be8",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590:17357,cache,cacheCopy,17357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6351#issuecomment-1533946590,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""a8ee297d-9fd6-433f-ac22-14488a09b832"",; ""eval_cromwell_job_id"": ""2a8ce326-baa5-4052-bff9-bd684393ff6c"",; ""created_at"": ""2022-07-25T15:10:00.795227"",; ""created_by"": null,; ""finished_at"": ""2022-07-26T00:11:26.646"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CHMSampleHeadToHead/BenchmarkComparison/a1db35b8-cc7b-4019-bdd0-9f423762542e/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8724"",; ""CHM controlindelPrecision"": ""0.8814"",; ""CHM controlsnpF1Score"": ""0.9784"",; ""CHM controlsnpPrecision"": ""0.9706"",; ""CHM controlsnpRecall"": ""0.9863"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd684393ff6c/call-CHMSampleHeadToHead/BenchmarkComparison/a1db35b8-cc7b-4019-bdd0-9f423762542e/call-BenchmarkVCFControlSample/Benchmark/7195c554-534f-43ef-80c2-77bdafa1827f/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""78.10181666666668"",; ""CHM evalHCsystemhours"": ""0.16157500000000005"",; ""CHM evalHCwallclockhours"": ""55.006172222222226"",; ""CHM evalHCwallclockmax"": ""2.8554194444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/2a8ce326-baa5-4052-bff9-bd68439",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748:16697,cache,cacheCopy,16697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7876#issuecomment-1194801748,1,['cache'],['cacheCopy']
Performance,"piens_assembly38.fasta"",; ""BenchmarkVCFsHeadToHeadOrchestrated.referenceVersion"": ""HG38"",; ""BenchmarkVCFsHeadToHeadOrchestrated.stratIntervals"": [; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/HCR_hg38.bed"",; ""gs://dsp-methods-carrot-data/test_data/haplotypecaller_tests/LCR_Hg38.interval_list""; ],; ""BenchmarkVCFsHeadToHeadOrchestrated.stratLabels"": [; ""HCR"",; ""LCR""; ]; },; ""eval_options"": null,; ""test_cromwell_job_id"": ""d6f96a63-9657-4ff6-9934-fe1ab3cea617"",; ""eval_cromwell_job_id"": ""e372bd14-cd1f-4563-8d8a-abf6b6ca7883"",; ""created_at"": ""2022-03-16T14:19:54.192086"",; ""created_by"": null,; ""finished_at"": ""2022-03-16T17:26:08.529"",; ""results"": {; ""CHM controlHCprocesshours"": ""75.88741944444445"",; ""CHM controlHCsystemhours"": ""0.1663777777777778"",; ""CHM controlHCwallclockhours"": ""52.24009722222222"",; ""CHM controlHCwallclockmax"": ""2.852152777777778"",; ""CHM controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""CHM controlindelF1Score"": ""0.8778"",; ""CHM controlindelPrecision"": ""0.8968"",; ""CHM controlsnpF1Score"": ""0.9813"",; ""CHM controlsnpPrecision"": ""0.9774"",; ""CHM controlsnpRecall"": ""0.9852"",; ""CHM controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7883/call-CHMSampleHeadToHead/BenchmarkComparison/7ff0db7c-0871-4cda-95f3-fa75436cbb21/call-BenchmarkVCFControlSample/Benchmark/16cd1efe-5cea-403e-8e85-aec15e71bd1d/call-CombineSummaries/summary.csv"",; ""CHM evalHCprocesshours"": ""67.35536666666667"",; ""CHM evalHCsystemhours"": ""0.1557166666666667"",; ""CHM evalHCwallclockhours"": ""42.53388888888889"",; ""CHM evalHCwallclockmax"": ""2.7197444444444443"",; ""CHM evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e372bd14-cd1f-4563-8d8a-abf6b6ca7",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494:10462,cache,cacheCopy,10462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7723#issuecomment-1069381494,1,['cache'],['cacheCopy']
Performance,"pled solution. And this workflow is clearly marked as an unsupported prototype anyway (as are the GATK CLIs). I want to emphasize that this whole workflow is not a long-term solution. In other words, I would like to get this in and then focus on a supported solution. Two comments: ; > If these events were indeed not CNLOH, as we discussed, then I don't think we should merge this. Perhaps we should take a step back and answer definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:1512,tune,tuned,1512,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,2,['tune'],['tuned']
Performance,ply$23.apply(RDD.scala:801); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Opti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:6843,concurren,concurrent,6843,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['concurren'],['concurrent']
Performance,possibly relate to this warning:. ```; 17/01/09 21:57:53 INFO com.google.cloud.genomics.dataflow.readers.bam.BAMIO: getReadsFromBAMFile - got input resources; 17/01/09 21:57:54 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input paths to process : 1; 17/01/09 22:01:44 WARN com.google.cloud.hadoop.gcsio.FileSystemBackedDirectoryListCache: Got null fileList for listBaseFile '/hadoop_gcs_connector_metadata_cache/hellbender/test/output/gatk4-spark/recalibrated.bam' even though exists() was true!; 17/01/09 22:01:45 INFO com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage: Populating missing itemInfo on-demand for entry: gs://hellbender/test/output/gatk4-spark/recalibrated.bam; 17/01/09 22:01:45 WARN com.google.cloud.hadoop.gcsio.CacheSupplementedGoogleCloudStorage: Possible stale CacheEntry; failed to fetch item info for: gs://hellbender/test/output/gatk4-spark/recalibrated.bam - removing from cache; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271421976:571,Cache,CacheSupplementedGoogleCloudStorage,571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2306#issuecomment-271421976,4,"['Cache', 'cache']","['CacheEntry', 'CacheSupplementedGoogleCloudStorage', 'cache']"
Performance,"r definitively whether simply blacklisting common germline regions is enough to replicate/obviate most of the postprocessing. Should be straightforward to run an evaluation with and without blacklisting---and hopefully our truth data accurately reflects whether blacklisting is desirable. There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. I am pretty sure that most common germline regions are being blacklisted already. The hotspots addressed in this PR (faux-CNLoH) could be added, but I think we will find new areas and a few of these areas were rather big. I have users that are actively using this from the branch, for reasons other than the faux-CNLoH pruning. Results are improving without an appreciable hit to sensitivity, which we got when using parameters like num_changepoints_penalty_factor. As a compromise, I can always default the CNLoH piece to `false`, since there are other useful changes on this branch. (Users did not have as strong an opinion about the faux-CNLoH pruning, since GISTIC does not use MAF and ABSOLUTE requires a manual review). > simple filtering based on CR-AF as described above could be implemented. If the normal is available, we can make IS_NORMAL calls simply based on the overlap of the ModelSegments posteriors (with corresponding qualities). If not, then some heuristic determination of the normal state from the tumor alone as in Marton's caller could be performed. This would combine the IS_NORMAL calling and filtering steps into one simple tool. The output could be a tagged/filtered ModelSegments .seg file and the corresponding VCF. And this would be a possible ""better solution"" Shall I file an issue for this? This could also allow us to obviate the TagGermline tool, which is fine by me.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874:2597,perform,performed,2597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461258874,2,['perform'],['performed']
Performance,"r$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could not instantiate appender named ""NullAppender"".; log4j:ERROR A ""org.apache.log4j.varia.NullAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@53d8d10a] whereas object of type; log4j:ERROR ""org.apache.log4j.varia.NullAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@18a70f16].; log4j:ERROR Could no",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998:6057,load,loaded,6057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312229998,1,['load'],['loaded']
Performance,"r,Description=""Phasing set (typically the position of the first variant in the set)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --emit-ref-confidence GVCF --output CMC_C_1.g.vcf --input CMC_C_1.sorted.markdup.addRG.bam --reference kxc_hic_final.fast; a --use-posteriors-to-calculate-qual false --dont-use-dragstr-priors false --use-new-qual-calculator true --annotate-with-num-discovered-alleles false --heterozygosity 0.001 --indel-hetero; zygosity 1.25E-4 --heterozygosity-stdev 0.01 --standard-min-confidence-threshold-for-calling 30.0 --max-alternate-alleles 6 --max-genotype-count 1024 --sample-ploidy 2 --num-reference-samp; les-if-no-call 0 --genotype-assignment-method USE_PLS_TO_ASSIGN --contamination-fraction-to-filter 0.0 --output-mode EMIT_VARIANTS_ONLY --all-site-pls false --flow-likelihood-parallel-thre; ads 0 --flow-likelihood-optimized-comp false --flow-use-t0-tag false --flow-probability-threshold 0.003 --flow-remove-non-single-base-pair-indels false --flow-remove-one-zero-probs false -; -flow-quantization-bins 121 --flow-fill-empty-bins-value 0.001 --flow-symmetric-indel-probs false --flow-report-insertion-or-deletion false --flow-disallow-probs-larger-than-call false --f; low-lump-probs false --flow-retain-max-n-probs-base-format false --flow-probability-scaling-factor 10 --flow-order-cycle-length 4 --flow-number-of-uncertain-flows-to-clip 0 --flow-nucleoti; de-of-first-uncertain-flow T --keep-boundary-flows false --gvcf-gq-bands 1 --gvcf-gq-bands 2 --gvcf-gq-bands 3 --gvcf-gq-bands 4 --gvcf-gq-bands 5 --gvcf-gq-bands 6 --gvcf-gq-bands 7 --gvc; f-gq-bands 8 --gvcf-gq-bands 9 --gvcf-gq-bands 10 --gvcf-gq-bands 11 --gvcf-gq-bands 12 --gvcf-gq-bands 13 --gvcf-gq-bands 14 --gvcf-gq-bands 15 --gvcf-gq-bands 16 --gvcf-gq-bands 17 --gvc; f-gq-bands 18 --gvcf-gq-bands 19 --gvcf-gq-bands 20 --gv",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:2445,optimiz,optimized-comp,2445,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['optimiz'],['optimized-comp']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:25072,concurren,concurrent,25072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26807,concurren,concurrent,26807,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 3, start 97885291, span 192458, expected MD5 ef90368731b6e0be845bc82cd92b0c6a; at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28998,concurren,concurrent,28998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31187,concurren,concurrent,31187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24359,concurren,concurrent,24359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26096,concurren,concurrent,26096,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 131325815, span 181534, expected MD5 c240a972d49aa89fb57dae94d1d90d36; a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27832,concurren,concurrent,27832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:30023,concurren,concurrent,30023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34977,concurren,concurrent,34977,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutput",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34727,concurren,concurrent,34727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38193,concurren,concurrent,38193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,['concurren'],['concurrent']
Performance,r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input HG04302.alt_bwamem_GRCh3,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43088,concurren,concurrent,43088,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"r.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar CountReads --input H",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42840,concurren,concurrent,42840,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"r_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel_error=5.000000e-04 --max_advi_iter_first_epoch=1000 --max_advi_iter_subsequent_epochs=1000 --min_training_epochs=20 --max_training_epochs=100 --initial_temperature=2.000000e+00 --num_thermal_advi_iters=5000 --convergence_snr_averaging_window=5000 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=1 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=7.500000e-01 --disable_caller=false --disable_sampler=false --disable_annealing=false --interval_list=/tmp/intervals9016836733228000464.tsv --contig_ploidy_prior_table=/home/n.liorni/snakemake_cnv_gatk/resources/contig_ploidy_priors.tsv --output_model_path=/home/n.liorni/snakemake_cnv_gatk/results/cnv/ploidy/ploidy-model; Stdout: 15:09:46.970 INFO cohort_determine_ploidy_and_depth - THEANO_FLAGS environment variable has been set to: device=cpu,floatX=float64,optimizer=fast_run,compute_test_value=ignore,openmp=true,blas.ldflags=-lmkl_rt,openmp_elemwise_minsize=10; 15:09:47.017 INFO gcnvkernel.structs.metadata - Generating intervals metadata...; 15:09:47.024 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the germline contig ploidy determination model...; 15:09:50.320 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy emission sampler...; 15:09:50.321 INFO gcnvkernel.tasks.task_cohort_ploidy_determination - Instantiating the ploidy caller...; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Global model variables: {'psi_j_log__', 'mean_bias_j_lowerbound__'}; 15:09:50.957 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'psi_s_log__'}; 15:09:50.957 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 15:09:50.958 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 15:10:03.310 INFO gcnvkern",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:6755,optimiz,optimizer,6755,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['optimiz'],['optimizer']
Performance,"rce/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:36:33.528 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.547 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.550 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:36:33.551 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:36:33.669 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.670 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:36:33.670 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:36:33.670 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:36:33.670 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:36:33.671 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:36:33 PM CST; 13:36:33.671 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.671 INFO BaseRecalibrator - ------------------------------------------------------------; 13:36:33.672 INFO BaseRecalibrator - HTSJDK Version: 2.24.1; 13:36:33.672 INFO BaseRecalibrator - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:7056,load,load,7056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,"rce/Homo_sapiens_assembly38.fasta -I /data/xieduo/Immun_genomics/data/Åuksza_2022_Nature/bam/PAAD11N.bam --known-sites /data/xieduo/WES_pipe/pipeline/gatk_resource/dbsnp_146.hg38.vcf.gz --known-sites /data/reference/gatk_resource/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/reference/gatk_resource/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O PAAD11N.recal_data.test.table; 13:46:24.742 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.761 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.764 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/xieduo/WES_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 13:46:24.764 WARN NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory); 13:46:24.884 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.884 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1; 13:46:24.885 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:46:24.885 INFO BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86_64 amd64; 13:46:24.885 INFO BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087; 13:46:24.885 INFO BaseRecalibrator - Start Date/Time: September 22, 2022 at 1:46:24 PM CST; 13:46:24.885 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.885 INFO BaseRecalibrator - ------------------------------------------------------------; 13:46:24.886 INFO BaseRecalibrator - HTSJDK Version: 2.24.1; 13:46:24.886 INFO BaseRecalibrator - Picard",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081:13364,load,load,13364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005#issuecomment-1254561081,1,['load'],['load']
Performance,rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); 17:43:23.161 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/tmp/spark-ecd63991-68be-4879-b481-68e6789a2004/userFiles-b72d4821-5e36-4d36-aa79-aa6263768669/1000G_phase1.indels.hg19.sites.vcf; 20/01/05 17:43:23 INFO NewHadoopRDD: Input split: file:/panfs/roc/groups/6/clinicalmdl/shared/wgs_exome_v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam:167436615680+33554432; 20/01/05 17:43:23 ERROR Executor: Exception in task 4990.0 in stage 0.0 (TID 4990); java.io.FileNotFoundException: /panfs/roc/groups/6/clinicalmdl/shared/v1.0/projects/BT_WGS_Flex_S1/data/exome_dedup_reads.bam (Too many open files); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855:4783,concurren,concurrent,4783,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316#issuecomment-570992855,1,['concurren'],['concurrent']
Performance,"re is the full output:. > (base) [pkus@wn45 mutect_test]$ ~/programs/gatk-4.1.8.0/gatk Funcotator --variant filtered_variants/P1.vcf.gz --reference ~/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path ~/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 12:28:16.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 21, 2020 12:28:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 12:28:16.537 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.538 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 12:28:16.538 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 12:28:16.541 INFO Funcotator - Executing as xxx on Linux v3.10.0-123.20.1.el7.x86_64 amd64; > 12:28:16.541 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 12:28:16.542 INFO Funcotator - Start Date/Time: July 21, 2020 12:28:16 PM CEST; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:1087,Load,Loading,1087,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['Load'],['Loading']
Performance,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3514,concurren,concurrent,3514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230,2,['concurren'],['concurrent']
Performance,redReader.readLine(LongLineBufferedReader.java:298); 	at htsjdk.tribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4815,concurren,concurrent,4815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"rencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7758,concurren,concurrent,7758,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"rencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:113); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:131); 	at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6946,concurren,concurrent,6946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,"ression_level=2 ,spark.kryoserializer.buffer.max=512m,spark.yarn.executor.memoryOverhead=600 --jar gs://hellbender-test-logs/staging/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar -- PrintVariantsSpark --V gs://hellbender/test/resources/large/gvcfs/gatk3.7_30_ga4f720357.24_sample.21.expected.vcf --output gs://hellbender-test-logs/staging/12dc38b0-0b40-49d5-a98e-fe83ca658003.vcf --spark-master yarn; Job [654b5b8e01de4c60bd87d941d4ec8831] submitted.; Waiting for job output...; 19/02/18 16:58:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 16:58:09.526 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 16:58:09.705 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/654b5b8e01de4c60bd87d941d4ec8831/gatk-package-4.1.0.0-24-g18a95c7-SNAPSHOT-spark_3e9078b7e67707952fa12a0c5c4d2b71.jar!/com/intel/gkl/native/libgkl_compression.so; 16:58:10.112 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.113 INFO PrintVariantsSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0-24-g18a95c7-SNAPSHOT; 16:58:10.113 INFO PrintVariantsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:58:10.113 INFO PrintVariantsSpark - Executing as root@gatk-test-2495f43b-04fc-49e7-aa0a-7108cc876246-m on Linux v4.9.0-8-amd64 amd64; 16:58:10.114 INFO PrintVariantsSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_181-8u181-b13-2~deb9u1-b13; 16:58:10.114 INFO PrintVariantsSpark - Start Date/Time: February 18, 2019 4:58:09 PM UTC; 16:58:10.114 INFO PrintVariantsSpark - ------------------------------------------------------------; 16:58:10.114 INFO Pri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:1701,Load,Loading,1701,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['Load'],['Loading']
Performance,rg.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.intern,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9075,Cache,CacheStep,9075,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,ribble.readers.LongLineBufferedReader.readLine(LongLineBufferedReader.java:354); 	at htsjdk.tribble.readers.SynchronousLineReader.readLine(SynchronousLineReader.java:51); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); 	at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); 	at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:89); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TabixFeatureReader.readHeader(TabixFeatureReader.java:100); 	... 12 more; Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 41 more; Caused by: com.google.cloud.storage.StorageException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:186); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:512); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:128); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:125); 	at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931:4880,concurren,concurrent,4880,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-301610931,1,['concurren'],['concurrent']
Performance,"ribute to the project so I'm excited by the prospect!. -Dan. On Fri, Sep 4, 2020, 11:53 AM R-obert <notifications@github.com> wrote:. > Hello,; >; > I'm trying to use GATK4 (4.1.8.1) on an Ubuntu (16.04) machine. The; > machine is a ""PowerLinux"" machine and I'm guessing that the most relevant; > info for the following problem is that it is a ppc64le system. When I use; > HaplotypeCaller, I see the following messages on the screen:; >; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -jar /home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar HaplotypeCaller -R ref.fa -I mybam.bam -O mycalls.vcf.gz -L snps.vcf -ip 100; >; > 16:17:04.377 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.397 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:1632,load,load,1632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,rk.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); 	... 87 more; Caused by: java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); 	at org.apache.spark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:12275,Concurren,ConcurrentModificationException,12275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['Concurren'],['ConcurrentModificationException']
Performance,"roblem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit(s) of work during this build to ensure correctness.; Please consult deprecation warnings for more details. BUILD FAILED in 33s; 5 actionable tasks: 5 executed; ```; which does not seem related to any changes I made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:1950,optimiz,optimizations,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['optimiz'],['optimizations']
Performance,"rogram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:149); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:190); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:107); at org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults(HaplotypeCallerSparkIntegrationTest.java:109); Caused by:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; at java.util.ArrayList.sort(ArrayList.java:1464); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssemblerArgumentCollection.java:37); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerArgumentCollection.createReadThreadingAssembler(AssemblyBasedCallerArgumentCollection.java:36); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.initialize(HaplotypeCallerEngine.java:231); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.<init>(HaplotypeCallerEngine.java:166); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$assemblyFunction$29848511$1(HaplotypeCallerSpark.java:174); at org.apache.spark.api.jav",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:4523,Concurren,ConcurrentModificationException,4523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['Concurren'],['ConcurrentModificationException']
Performance,"rote:. > Hi again,; > I tried installing java8 and switching to this version prior to running; > gatk. It runs and looks to be running the right Java, but spits out roughly; > the same error:; >; > Thoughts?; >; > /cold/drichard/gatk/./gatk --java-options ""-Xmx25g"" SplitNCigarReads; > -R /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam; > --tmp-dir /thing -O thing.bam; > Using GATK jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false; > -Dsamjdk.use_async_io_write_samtools=true; > -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2; > -Xmx25g -jar; > /cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar; > SplitNCigarReads -R; > /cold/drichard/VARIANTS/Homo_sapiens.GRCh38.dna.primary_assembly.fa -I; > subset_TINY_rehead.bam --tmp-dir /thing -O thing.bam; > 15:34:59.974 INFO NativeLibraryLoader - Loading libgkl_compression.so from; > jar:file:/cold/drichard/gatk/build/libs/gatk-package-4.3.0.0-44-g227bbca-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; > 15:35:00.220 INFO SplitNCigarReads -; > ------------------------------------------------------------; > 15:35:00.226 INFO SplitNCigarReads - The Genome Analysis Toolkit (GATK); > v4.3.0.0-44-g227bbca-SNAPSHOT; > 15:35:00.226 INFO SplitNCigarReads - For support and documentation go to; > https://software.broadinstitute.org/gatk/; > 15:35:00.226 INFO SplitNCigarReads - Executing as ***@***.*** on; > Linux v5.19.0-32-generic amd64; > 15:35:00.226 INFO SplitNCigarReads - Java runtime: OpenJDK 64-Bit Server; > VM v1.8.0_362-8u362-ga-0ubuntu1~22.04-b09; > 15:35:00.226 INFO SplitNCigarReads - Start Date/Time: March 2, 2023; > 3:34:59 PM EST; > 15:35:00.226 INFO SplitNCigarReads -; > ------------------------------------------------------------; > 15:35:00.226 INFO SplitNCigarReads -; > --------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344:1196,Load,Loading,1196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8232#issuecomment-1452528344,1,['Load'],['Loading']
Performance,"rray - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$La",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3869,concurren,concurrent,3869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"rvalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seem",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5679,Load,Loading,5679,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['Load'],['Loading']
Performance,"s doc. Note that some of the CNV section is out of date and incorrect. In particular, we have been taking in PCOV as input to CreatePanelOfNormals for some time now, but the doc states that we take integer read counts. This already yields different results by the first filtering step (on intervals by interval median). @LeeTL1220 @davidbenjamin what is the ""official ReCapSeg"" behavior, and do we want to keep the current behavior? In general, I think all of the standardization (i.e., filtering/imputation/truncation/transformation) steps could stand some revisiting. Evaluation:. - [ ] Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - [x] <s>Investigate the effect of keeping duplicates. I am still not sure why we do this, and it may have a more drastic impact on WGS data.</s> Turns out we don't keep duplicates for WGS; see #3367.; - [ ] Check that GC-bias-correction+PCA and PCA-only perform comparably, even at small bin sizes (~300bp). From what I've seen, this is true for larger bin sizes (~3kbp), so explicit GC-bias correction may not be necessary. (That is, even at these (purportedly) large bin sizes, the effect of the read-based GC-bias correction is obvious for those samples where it is important. However, the end result is not very different from PCA-only denoising with no GC-bias correction performed.); - [x] <s>Check that changing CBS alpha parameter sufficiently reduces hypersegmentation.</s> <s>Looks like the hybrid p-value calculation in DNACopy is not accurate enough to handle WGS-size data. (Also, it's relatively slow, taking ~30 minutes on ~10M intervals.) Even if I set alpha to 0, I still get a ridiculous number of segments! So I think it's finally time to scrap CBS. I'll look into other R segmentation packages that might give us a quick drop-in solution, but we may want to roll our own simple algorithm (which we will scrap anyway once the coverage",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:4673,perform,perform,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['perform']
Performance,"s method than our previous probabilistic approaches. Even SNP segmentation will be much cheaper. > What is the name of this approach? ""KernSeg""?. Not sure...I couldn't find an R package, although an R/C implementation is mentioned in the paper. But the python implementation is straightforward and a pure Java implementation should not be so bad. There are some cythonized numpy methods that my python implementation used, but I think equivalent implementations of these methods should be relatively fast in pure Java as well. > What variant of the algorithm did you implement? the paper lists several. I implemented what they call ApproxKSeg. It's an approximate version that combines binary segmentation with the low-rank approximation to the Gaussian kernel. > I haven't read the paper in detail yet, but is it possible to choose a conservatively large number of possible break points and then filter bad break points, possibly based on the rapid decline of the change point probability? i.e. does the algorithm naturally produce change point probabilities?. Yes, you can oversegment and then choose which breakpoints to retain. However, there are no proper changepoint probabilities, only changepoint costs. Adding a penalty term based on the number of changepoints seems to perform relatively well in simple tests, but one could certainly devise other ways to filter changepoints (some of which could yield probabilities, if you are willing to assume a probabilistic model). I think we should just think of this as a fast, heuristic, non-parametric method for finding breakpoints in multidimensional data. > Is it possible to throw in additional change points incrementally, without doing extra work, until a certain criterion is met? (see above). The version I implemented adds changepoints via binary segmentation. The time complexity required to split a segment is linear in the number of points contained in the segment, although some care must be taken in the implementation to ensure this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715:2775,perform,perform,2775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321140715,2,['perform'],['perform']
Performance,"s on 10 of our 2000 samples (only in WES) none of our 600 WGS seems to have the same issue. It is always on some small contig (you can see here range is 544, but all cases are small ranges like this one). Everything is the default mutect2 pipeline and params (e.g. [gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0?prefix=Homo_sapiens_assembly38.fasta&authuser=jkalfon%40broadinstitute.org)) : except the interval file: [gs://ccleparams/region_file_wgs.list](https://console.cloud.google.com/storage/browser/ccleparams?prefix=region_file_wgs.list&authuser=jkalfon%40broadinstitute.org); GATK 4.2.6.1. . Here is the VCF file to annotate `gs://ccleparams/test/CDS-2jucw0.hg38-filtered.vcf.gz`. Here is the stacktrace:. ```; ....; 10:53:39.044 INFO VcfFuncotationFactory - ClinVar_VCF 20180429_hg38 cache hits/total: 0/2145; 10:53:39.249 INFO VcfFuncotationFactory - dbSNP 9606_b151 cache hits/total: 0/1069225; 10:53:39.520 INFO Funcotator - Shutting down engine; [July 12, 2022 10:53:39 AM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 115.46 minutes.; Runtime.totalMemory()=2050490368; java.lang.StringIndexOutOfBoundsException: String index out of range: 544; at java.lang.String.substring(String.java:1963); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForInsertion(ProteinChangeInfo.java:293); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:101); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:399); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:2054); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProteinCoding",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653:1029,cache,cache,1029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1182102653,1,['cache'],['cache']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-1056964608]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.cl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472:7742,concurren,concurrent,7742,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317782472,1,['concurren'],['concurrent']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-134217728]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.clou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881:7516,concurren,concurrent,7516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317549881,1,['concurren'],['concurrent']
Performance,"s$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:838); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:136); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:255); 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:300); 	... 44 more; Caused by: com.google.cloud.storage.StorageException: java.lang.IllegalArgumentException: Position should be non-negative, is %d [-830472192]; 	at com.google.cloud.storage.StorageException.translateAndThrow(StorageException.java:71); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:139); 	at com.google.clou",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564:6704,concurren,concurrent,6704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3316#issuecomment-317442564,1,['concurren'],['concurrent']
Performance,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false 13:39:56.672 INFO HaplotypeCaller - Deflater: IntelDeflater 13:39:56.673 INFO HaplotypeCaller - Inflater: IntelInflater 13:39:56.674 INFO HaplotypeCaller - GCS max retries/reopens: 20 13:39:56.679 INFO HaplotypeCaller - Requester pays: disabled 13:39:56.680 INFO HaplotypeCaller - Initializing engine 13:39:56.968 INFO HaplotypeCaller - Done initializing engine 13:39:56.971 INFO HaplotypeCallerEngine - Tool is in reference confidence mode and the annotation, the following changes will be made to any specified annotations: 'StrandBiasBySample' will be enabled. 'ChromosomeCounts', 'FisherStrand', 'StrandOddsRatio' and 'QualByDepth' annotations have been disabled 13:39:57.000 INFO HaplotypeCallerEngine - Standard Emitting and Calling confidence set to -0.0 for reference-model confidence output 13:39:57.000 INFO HaplotypeCallerEngine - All sites annotated with PLs forced to true for reference-model confidence output 13:39:57.020 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/omics/groups/OE0540/internal/software/jvm/gatk/4.4.0.0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so 13:39:57.026 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported 13:39:57.026 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation! 13:39:57.108 INFO ProgressMeter - Starting traversal 13:39:57.110 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute 13:40:07.119 INFO ProgressMeter - chr19:8969701 0.2 29900 179382.1 13:40:17.116 INFO ProgressMeter - chr19:20264701 0.3 67550 202609.5 13:40:27.115 INFO ProgressMeter - chr19:31874701 0.5 106250 212471.7 13:40:37.116 INFO ProgressMeter - chr19:44792701 0.7 149310 223937.0 13:40:49.251 WARN InbreedingCoeff - InbreedingCoeff will not be calculated at position chr19:55910",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195:13505,Load,Loading,13505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5727#issuecomment-1781017195,1,['Load'],['Loading']
Performance,s.iterators.PushToPullIterator.advanceToNextElement(PushToPullIterator.java:58); 	at org.broadinstitute.hellbender.utils.iterators.PushToPullIterator.<init>(PushToPullIterator.java:37); 	at org.broadinstitute.hellbender.utils.variant.writers.GVCFBlockCombiningIterator.<init>(GVCFBlockCombiningIterator.java:14); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.lambda$writeVariantsSingle$516343c4$1(VariantsSparkSink.java:127); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)`,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994:4379,concurren,concurrent,4379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2554#issuecomment-530773994,2,['concurren'],['concurrent']
Performance,"sal complete. Processed 1 total batches in 2531.4 minutes.; 05:39:42.061 INFO GenomicsDBImport - Import completed!; 05:39:42.061 INFO GenomicsDBImport - Shutting down engine; [January 16, 2021 5:39:42 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 2,531.64 minutes.; Runtime.totalMemory()=9711910912; Tool returned:; true; **Calling Variants Attempt**; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx32g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 21:16:35.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 17, 2021 9:16:35 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 21:16:35.496 INFO GenotypeGVCFs - ------------------------------------------------------------; 21:16:35.497 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 21:16:35.497 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:16:35.497 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 21:16:35.497 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 21:16:35.497 INFO GenotypeGVCFs - Start Date/Time: January 17, 2021 9:16:35 PM CST; 21:16:35.497 INFO GenotypeGVCFs - ------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839:2437,Load,Loading,2437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-761953839,1,['Load'],['Loading']
Performance,"samjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar PrintReadsSpark -I /gatk4/output.bam -O /gatk4/output_3.bam --sparkMaster yarn-client; Warning: Master yarn-client is deprecated since 2.0. Please use master ""yarn"" with specified deploy mode instead.; 18:11:33.604 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:11:33.737 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/Software/gatk/build/libs/gatk-package-4.beta.5-70-gdc3237e-SNAPSHOT-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [October 13, 2017 6:11:33 PM CST] PrintReadsSpark --output /gatk4/output_3.bam --input /gatk4/output.bam --sparkMaster yarn-client --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --interval_merging_rule ALL --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 13, 2017 6:11:33 PM CST] Executing as hdfs@mg on Linux 3.10.0-514.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: 4.beta.5-70-gdc3237e-SNAPSHOT; 18:11:33.870 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 18:11:3",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775:1859,Load,Loading,1859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-336412775,1,['Load'],['Loading']
Performance,sciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:18.125 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/simple_uniprot_Dec012014.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/simple_uniprot/hg38/simple_uniprot_Dec012014.tsv; > 12:28:18.424 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/Familial_Cancer_Genes.no_dupes.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/familial/hg38/Familial_Cancer_Genes.no_dupes.tsv; > 12:28:18.442 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hg38_All_20170710.vcf.gz -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:18.442 INFO DataSourceUtils - Setting lookahead cache for data source: dbSNP : 100000; > 12:28:18.452 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:18.599 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/hg38_All_20170710.vcf.gz -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.018 INFO FeatureManager - Using codec VCFCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dbsnp/hg38/hg38_All_20170710.vcf.gz; > 12:28:19.213 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/CancerGeneCensus_Table_1_full_2012-03-15.txt -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cancer_gene_census/hg38/CancerGeneCensus_Table_1_full_2012-03-15.txt; > 12:28:19.227 INFO DataSourceUtils -,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:11525,cache,cache,11525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"sh Babadi <mehrtash@broadinstitute.org>; Date: Wed Nov 15 01:50:03 2017 -0500. Polished code, ready for review; ; gCNV computational kernel (initial release); ; renaming gammas_s to psi_s to uniformity (sample-specific unexplained variance); ; renamed determine_ploidy_and_depth.py to cohort_determine_ploidy_and_depth.py; finite-temperature forward-backward algorithm; in the ploidy model, replaced alpha_j (NB over-dispersion) with psi_j (unexplained variance) for uniformity. Also, added the possibility of sample-specific unexplained variance in the germline contig ploidy model; ; updated I/O routines and CLIs according to team discussion; ; updated I/O routines and CLIs according to team discussion; ; changed the output layout of the ploidy determination tool; refactored parts of io.py; upped the version to 0.3 as it is not backwards compatible anymore; ; case ploidy determination tool from a given ploidy model; major code cleanup and refactoring of I/O module; refactoring of common CLI script snippets; ; removed all ""targets""; some code cleanup; ; pad flat class bitmask w/ a given padding value in the hybrid q_c_expectation_mode; option to disable annealing and keep the temperature fixed; ; bugfix in finite-temperature forward-backward; further refactoring of model I/O; ; the option to take a previously trained model as starting point in cohort CLI; the option to take previous calls as a starting point in cohort CLI; ; option to save and load adamax moments; ; import/export adamax bias correction tensor; ; refactoring related to fancy opt I/O; added average ploidy column to read depth; updated docs of hybrid inference; ; modeling intervals can span multiple contigs now; ploidy can change; across contigs with no issue; ; save/load adamax state to .npy instead of .tsv for speed; ; part 1 of doc updates; ; part 2 of doc updates; ; part 3 of doc updates; ; part 4 of doc updates; ; bumped version to 0.5; readme; ; update readme; ; last minute stylistic doc updates.; ````",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598:11775,load,load,11775,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3925#issuecomment-354805598,4,['load'],['load']
Performance,"simply be to downsample and scale likelihoods when estimating global parameters. Addresses #2884.; - [x] Even though the simple copy-ratio model is much faster, it still takes ~15-20 minutes for 100 iterations on WGS, so we can downsample here too.; - [x] Integration tests are still needed; again, these might not test for correctness.; - I've added the ability to specify a prior for the minor-allele fraction, which alleviates the problem of residual bias in balanced segments.; - I've reduced the verbosity of the modeled-segments file. I only report posterior mode and 10%, 50%, and 90% deciles. Global parameters have the full deciles output in the .param files, but I removed the mode and highest density credible interval (because of the below item).; - [x] Some residual bias remains in the estimate of the minor-allele fraction posterior mode. This is simply because we are performing kernel density estimation of a bounded quantity. One possibility would be to logit transform to an unbounded support, perform the estimation, then transform back. EDIT: Just removed kernel density estimation for now, partly due to #3599 as well.; - Hmm, actually still a tiny bit of residual bias. This is apparent e.g. in WGS normals. I think focusing on a new allele-fraction model rather than trying to figure out where the old one is failing would be best.; - [x] For small bins (250bp), the copy-ratio model is currently a bit memory intensive, since it stores an outlier indicator boolean for every data point (it gets by with -Xmx12g for 100 iterations at 250bp). There is no easy away around storing this at the GibbsSampler level (although we could make some non-trivial changes to that code, as @davidbenjamin suggested long ago at https://github.com/broadinstitute/gatk-protected/issues/195). However, I got rid of these at the CopyRatioModeller level. If we want to go down in memory, we could move to a BitSet, but I'm not sure what the performance hit will be. EDIT: It was trivial to switch",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:6745,perform,perform,6745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['perform'],['perform']
Performance,"sion: 2.27.1; 05:39:39.306 INFO CNNScoreVariants - Built for Spark Version: 2.4.5; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 05:39:39.307 INFO CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:39:39.307 INFO CNNScoreVariants - Deflater: IntelDeflater; 05:39:39.307 INFO CNNScoreVariants - Inflater: IntelInflater; 05:39:39.307 INFO CNNScoreVariants - GCS max retries/reopens: 20; 05:39:39.307 INFO CNNScoreVariants - Requester pays: disabled; 05:39:39.307 INFO CNNScoreVariants - Initializing engine; 05:39:39.905 INFO FeatureManager - Using codec VCFCodec to read file file:///home/fmbuga/gatk4_gcp_wgs/06_vcf_raw/SRR16299720_dedup_AORRG_recal_raw.vcf; 05:39:40.108 INFO CNNScoreVariants - Done initializing engine; 05:39:40.109 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/fmbuga/.conda/envs/gatk4/share/gatk4-4.2.6.1-1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; 05:39:40.429 INFO CNNScoreVariants - Done scoring variants with CNN.; 05:39:40.429 INFO CNNScoreVariants - Shutting down engine; [October 9, 2022 5:39:40 AM PDT] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=1903165440; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.hasMessage(ProcessControllerAckResult.java:49); 	at org.broadinstitute.hellbender.utils.runtime.ProcessControllerAckResult.getDisplayMessage(ProcessControllerAckResult.java:69); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:235); 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.waitForAck(StreamingPythonScriptExecutor.java:216);",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490:3190,Load,Loading,3190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7811#issuecomment-1274925490,1,['Load'],['Loading']
Performance,"spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at org.seqdoop.hadoop_bam.BAMRecordReader.nextKeyValue(BAMRecordRe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5700,concurren,concurrent,5700,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,spatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.testng.TestNGException:An error occurred while instantiating class org.broadinstitute.hellbender.engine.spark.ReadsPreprocessingPipelineSparkTestData. Check to make sure it can be instantiated; 	at org.testng.internal.InstanceCreator.createInstanceUsingObjectFactory(InstanceCreator.java:134); 	at org.testng.internal.InstanceCreator.createInstance(InstanceCreator.java:79); 	at org.testng.internal.ClassImpl.getDefaultInstance(ClassImpl.java:110); 	at org.testng.internal.ClassImpl.getInstances(ClassImpl.java:195); 	at org.testng.TestClass.getInstances(TestClass.java:102); 	at org.testng.TestClass.initTestClassesAndInstances(TestClass.java:82); 	at org.testng.TestClass.init(TestClass.java:74); 	at org.testng.TestClass.<init>(TestClass.java:39); 	at org.testng.TestRunner.initMethods(Tes,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858:2131,concurren,concurrent,2131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787#issuecomment-472107858,1,['concurren'],['concurrent']
Performance,"st?. ```; 02 Apr 2022 16:34:31,433 DEBUG: 	[April 2, 2022 4:34:31 PM PDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 5,993.06 minutes.; 02 Apr 2022 16:34:31,438 DEBUG: 	Runtime.totalMemory()=178017796096; 02 Apr 2022 16:34:31,443 DEBUG: 	Tool returned:; 02 Apr 2022 16:34:31,448 DEBUG: 	true; 02 Apr 2022 16:34:34,663 INFO : Will consolidate the workspace using consolidate_genomicsdb_array; 02 Apr 2022 16:34:34,723 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb/1$1$223616942; 02 Apr 2022 16:34:34,748 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb --shared-posixfs-optimizations -a 1$1$223616942; 02 Apr 2022 16:34:34,754 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 02 Apr 2022 16:34:35,059 DEBUG: 	16:34:35.059 info consolidate_genomicsdb_array - pid=34848 tid=34848 Starting consolidation of 1$1$223616942 in /home/exacloud/gscratch/prime-seq/workDir/344c6137-8a85-103a-821a-f8f3fc86deba/Job1.work/WGS_v2_713_2ndMerge.gdb; 02 Apr 2022 16:34:36,091 DEBUG: 	Using buffer_size=10485760 for consolidation; 02 Apr 2022 16:34:36,097 DEBUG: 	Number of fragments to consolidate=26; 02 Apr 2022 16:34:36,101 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats beginning consolidation size=483MB resident=379MB share=6MB text=13MB lib=0 data=371MB dt=0; 02 Apr 2022 16:34:36,105 DEBUG: 	Sat Apr 2 16:34:36 2022 Memory stats Start: batch 1/1 size=503MB resident=379MB share=6MB text=13MB lib=0 data=391MB dt=0; 02 Apr 2022 16:38:15,825 DEBUG: 	Sat Apr 2 16:38:15 2022 Memory stats after alloc for attribute=END size=25GB resident=25GB share=7MB text=13MB lib",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975:1171,optimiz,optimizations,1171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1087750975,1,['optimiz'],['optimizations']
Performance,stitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvQkdNTVZhcmlhbnRBbm5vdGF0aW9uc1Njb3Jlci5qYXZh) | `0.000% <0.000%> (Ã¸)` | |; | [...oadinstitute/hellbender/utils/NaturalLogUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9OYXR1cmFsTG9nVXRpbHMuamF2YQ==) | `77.143% <0.000%> (Ã¸)` | |; | [...ls/clustering/BayesianGaussianMixtureModeller.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jbHVzdGVyaW5nL0JheWVzaWFuR2F1c3NpYW5NaXh0dXJlTW9kZWxsZXIuamF2YQ==) | `0.000% <0.000%> (Ã¸)` | |; | [.../tools/walkers/vqsr/scalable/data/VariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9WYXJpYW50VHlwZS5qYXZh) | `60.000% <60.000%> (Ã¸)` | |; | [.../walkers/vqsr/scalable/SystemCommandUtilsTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU3lzdGVtQ29tbWFuZFV0aWxzVGVzdC5qYXZh) | `60.870% <60.870%> (Ã¸)` | |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/7954/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834:4340,scalab,scalable,4340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7954#issuecomment-1191010834,1,['scalab'],['scalable']
Performance,"t how this should be done. Complete coverage here will be difficult and perhaps not worth the effort, but I can probably put in a few tests that make sure changing the hard-coded values in master and doing the same via the exposed parameters in this branch have the same effect on a few existing test cases. However, while I'm doing the last three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1520,optimiz,optimizations,1520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,1,['optimiz'],['optimizations']
Performance,"t three, I wonder if we could run whatever canonical evaluations/optimizations we have to see whether it's worth consolidating some of the parameter sets at this stage? I think there's an argument for having at least two sets (haplotype-to-reference + read-to-haplotype), but I'm not sure how to justify having a separate set for dangling heads/tails. But also not sure which set the latter should be consolidated with---@jamesemery thoughts? Again, let me reiterate that it seems that many of these parameter values were chosen arbitrarily (or, if not, that the procedure for choosing them has been lost). As a start, you can see the results of some optimizations I did on the CHM mix on slide 15 at https://docs.google.com/presentation/d/1zGuquAZWSUQ-wNxp8D6HhGNjIaFcV0_X9WAS4LODbEo/edit?usp=sharing Here, I optimized over haplotype-to-reference + read-to-haplotype SW parameters on various metrics after variant normalization using vcfeval. These optimizations were done using the Bayesian optimization framework I prototyped long ago (see https://github.com/broadinstitute/gatk-evaluation/tree/master/pipeline-optimizer and https://docs.google.com/presentation/d/1t5WOAEOMp0xAzJgpKbP68BUnclNYfIVRrDSL9wl1-3A/edit?usp=sharing); this entailed running parameter scans using a local Cromwell on my desktop. Probably this optimization work could be redone relatively easily using the Neptune framework put together by @dalessioluca, which was still in development at the time I did this work. Happy to share the resources and scripts I used if we go down this route; they are pretty lightweight. See more discussion starting here: https://github.com/broadinstitute/gatk/issues/5564#issuecomment-710107566. Alternatively, we could merge this branch to expose the parameters now and punt on consolidating/optimizing them. I'm not completely convinced we should even do the former unless we are going to follow through on the latter, but happy to defer to others. Finally, note also there is one code opti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471:1819,optimiz,optimizations,1819,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6885#issuecomment-891907471,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"t2 - HTSJDK Version: 3.0.5; > 14 15:07:52.440 INFO Mutect2 - Picard Version: 3.0.0; > 15 15:07:52.440 INFO Mutect2 - Built for Spark Version: 3.3.1; > 16 15:07:52.440 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 17 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > 18 15:07:52.441 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > 19 15:07:52.442 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > 20 15:07:52.442 INFO Mutect2 - Deflater: IntelDeflater; > 21 15:07:52.442 INFO Mutect2 - Inflater: IntelInflater; > 22 15:07:52.442 INFO Mutect2 - GCS max retries/reopens: 20; > 23 15:07:52.443 INFO Mutect2 - Requester pays: disabled; > 24 15:07:52.443 INFO Mutect2 - Initializing engine; > 25 15:07:52.848 INFO FeatureManager - Using codec VCFCodec to read file file://ref_nobackup/af-only-gnomad.hg38.vcf.gz; > 26 15:07:53.126 INFO Mutect2 - Done initializing engine; > 27 15:07:53.196 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 28 15:07:53.201 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/scicore/soft/apps/GATK/4.4.0.0-GCCcore-10.3.0-Java-17/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 29 15:07:53.223 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 30 15:07:53.223 INFO IntelPairHmm - Available threads: 2; > 31 15:07:53.224 INFO IntelPairHmm - Requested threads: 4; > 32 15:07:53.224 WARN IntelPairHmm - Using 2 available threads, but 4 were requested; > 33 15:07:53.224 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; > 34 15:07:53.231 WARN Mutect2 - Note that the Mutect2 reference confidence mode is in BETA -- the likelihoods model and output format are subject to change in subsequent versions.; > 35 15:07:53.314 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632:2897,Load,Loading,2897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1597198632,1,['Load'],['Loading']
Performance,"tSpot(TM) 64-Bit Server VM, Java 1.8.0_91); Type in expressions to have them evaluated.; Type :help for more information.; Spark context available as sc (master = yarn-client, app id = application_1507683879816_0007).; Wed Oct 11 14:25:24 CST 2017 Thread[main,5,main] java.io.FileNotFoundException: derby.log (Permission denied); ----------------------------------------------------------------; Wed Oct 11 14:25:24 CST 2017:; Booting Derby version The Apache Software Foundation - Apache Derby - 10.11.1.1 - (1616546): instance a816c00e-015f-0a1b-f1bd-00002ce33928 ; on database directory /tmp/spark-98953d35-8594-4907-b4a5-0870f1d17b3e/metastore with class loader sun.misc.Launcher$AppClassLoader@5c647e05 ; Loaded from file:/opt/cloudera/parcels/CDH-5.12.1-1.cdh5.12.1.p0.3/jars/derby-10.11.1.1.jar; java.vendor=Oracle Corporation; java.runtime.version=1.8.0_91-b14; user.dir=/opt/Software/gatk; os.name=Linux; os.arch=amd64; os.version=3.10.0-514.el7.x86_64; derby.system.home=null; Database Class Loader started - derby.database.classpath=''; 17/10/11 14:25:33 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1; 17/10/11 14:25:33 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException; SQL context available as sqlContext. **./gradlew bundle**; **[root@com1 gatk]# ./gradlew bundle; when I executed the command â€./gradlew bundleâ€ï¼Œ it appeared the error in the last ï¼Œdid this matterï¼Ÿ**. .......; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.6.5/d50be1723a09be903887099ff2014ea9020333/jackson-databind-2.6.5.jar(com/fasterxml/jackson/databind/annotation/JsonSerialize$Inclusion.class)]]; [loading ZipFileIndexFileObject[/root/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-core/2.5/7ed845de1dfe070d43511fab1784e6c4118398/log4j-core-2.5.jar(org/apac",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240:2341,Load,Loader,2341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-335696240,1,['Load'],['Loader']
Performance,"ta used was pretty small: chr1-2 training (~20k positive training/truth variants, ~50k negative training variants; note also that the threshold for determining negative training was not tuned---a threshold corresponding to a 98% truth sensitivity was arbitrarily chosen), chr3 validation (~50k variants), and chr4-6 test (~150k variants). The LL score is calculated from a validation set held out from the training/truth positives used to train the model, while the F1 score is calculated using ""orthogonal truth"" positives/negatives determined using 3 families of ~30 trios each. However, there's some arbitrariness in how we define the boundary for the latter positives/negatives, and hence some arbitrariness in the F1 score itself. But I'd expect using gold-standard GIAB truth would be more straightforward. Not sure how much we can conclude, but that the validation and test F1s are similar and that the validation LL score isn't *too* far off are encouraging. That said, there is a pretty big drop in recall when optimizing LL. But we should also expect some discrepancy between LL and F1, according to one of the papers linked above. I would hope that with more variants or reliable training/truth (as in your data), things might stabilize or line up better. I'll try running with more malaria data, as well. The following trios x sites heatmap (top plot) for the validation set might better illustrate the arbitrariness in F1 (click to enlarge):. ![image](https://user-images.githubusercontent.com/11076296/158385585-1a0dfe8e-d4b7-4770-aed0-19ad81162c92.png). Here, yellow = het errors (since these are supposed to be clonal malaria samples), red = Mendelian errors, grey = no calls, green = Mendelian consistency, white = reference. The second plot shows the training/truth positives used to train the model and to calculate the LL score in the validation shard. The third plot shows the ""orthogonal truth"" positives/negatives used to calculate F1. So we can see that the difficulty in deri",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431:1307,optimiz,optimizing,1307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7711#issuecomment-1067396431,2,['optimiz'],['optimizing']
Performance,"taProgramGroup.java. And `6. ` from above. ---; ## FilterLongReadAlignmentsSAMSpark. 1. In the one-line summary, I'm not clear on what is meant by ""Filters"". Based on the result file, seems like it collects metrics on each contig alignment.; 2. ; 3. If metrics, then DiagnosticsAndQCProgramGroup.java. And `6. ` from above. ---; ## FindBadGenomicKmersSpark. 1. The term ""copy number"" should be reserved in reference to CNV analyses. So instead, how about:; Identify sequence contexts that occur at high frequency in a reference; 2. Please define a kmer. If only a reference fasta is required (as listed under Inputs) great. But if the tool also depends on a FAI index and DICT dictionary, please do include them. Also, it would be good to provide an example of how such information is used in SV discovery, e.g. ""the resulting file can be given to FindBreakpointEvidenceSpark, which will then ignore such sequence contexts during analysis."" Also would be good to mention that the default kmer size (--k-size 51) is optimized for human if indeed this is the case.; 3. ReferenceProgramGroup.java. And `6. ` from above. ---; ## FindBreakpointEvidenceSpark. 1. Assembles and aligns contigs of genomic breakpoint regions associated with structural variants ; 2. Overview and Notes could use finessing but let's leave this for next year. One thing to include is a reference to FermiLite for those seeking more information. A publication would be best. And `6. ` from above. ---; ## StructuralVariationDiscoveryPipelineSpark. 1. Runs the structural variant discovery workflow on a single sample in Spark ; 2. Fyi we sanction a ""Caveats"" section, which is likely more appropriate for the PE expectation and the fact that low coverage data less than 30x will give suboptimal results. Also, should mention this workflow is meant only for WGS. Or is it the case one case use exome data? Second note on BwaMemIndexImageCreator could be consolidated with the same under Inputs. Same with third note. And `6. ` from",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451:3315,optimiz,optimized,3315,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3948#issuecomment-351467451,2,['optimiz'],['optimized']
Performance,"tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1907,concurren,concurrent,1907,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,1,['concurren'],['concurrent']
Performance,"tch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; Using GATK jar /gatk/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /gatk/gatk-package-4.1.8.1-local.jar SelectVariants -R /scratch/DBC/BCRBIOIN/SHARED/genomes/homo_sapiens/GRCh38/dna/GRCh38.d1.vd1.fa -V /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.orientation_filtered.vcf.gz --exclude-filtered -O /scratch/DBC/BCRBIOIN/SHARED/analysis/######/######/data/wgs/data/mutect2//tumour_samples/ML13_Ab/ML13_Ab.passed.vcf.gz; 10:52:40.838 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 02, 2020 10:52:41 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 10:52:41.263 INFO SelectVariants - ------------------------------------------------------------; 10:52:41.263 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.8.1; 10:52:41.264 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:52:41.264 INFO SelectVariants - Executing as ######@dav002.prv.davros.compute.estate on Linux v3.10.0-327.3.1.el7.x86_64 amd64; 10:52:41.264 INFO SelectVariants - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; 10:52:41.265 INFO SelectVariants - Start Date/Time: September 2, 2020 10:52:40 AM GMT; 10:52:41.265 INFO SelectVariants - --------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328:1938,Load,Loading,1938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6446#issuecomment-685695328,1,['Load'],['Loading']
Performance,"te_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --spark-master yarn; 2019-01-07 11:33:18 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-07 11:33:19 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 11:33:24.377 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 11:33:24.549 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 11:33:26.271 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.272 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 11:33:26.272 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:33:26.272 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 11:33:26.273 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:33:26.273 INFO CountReadsSpark - Start Date/Time: January 7, 2019 11:33:24 AM EST; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.273 INFO CountReadsSpark - ------------------------------------------------------------; 11:33:26.275 INFO CountReadsSpark - HTSJDK Ve",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:2542,Load,Loading,2542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,1,['Load'],['Loading']
Performance,tect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:51.056 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:51.057 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:51.057 INFO Mutect2 - Deflater: IntelDeflater; 11:47:51.057 INFO Mutect2 - Inflater: IntelInflater; 11:47:51.057 INFO Mutect2 - GCS max retries/reopens: 20; 11:47:51.057 INFO Mutect2 - Requester pays: disabled; 11:47:51.057 INFO Mutect2 - Initializing engine; 11:47:51.372 INFO FeatureManager - Using codec VCFCodec to read file file:///home/proj/stage/cancer/reference/GRCh37/variants/dbsnp_grch37_b138.vcf.gz; 11:47:51.457 INFO FeatureManager - Using codec BEDCodec to read file file:///home/proj/stage/cancer/reference/target_capture_bed/production/balsamic/gicfdna_3.1_hg19_design.bed; 11:47:51.465 INFO IntervalArgumentCollection - Processing 74592 bp from intervals; 11:47:51.474 INFO Mutect2 - Done initializing engine; 11:47:51.487 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 11:47:51.489 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 11:47:51.534 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; 11:47:51.534 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 11:47:51.534 INFO IntelPairHmm - Available threads: 16; 11:47:51.534 INFO IntelPairHmm - Requested threads: 4; 11:47:51.534 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 11:47:51.557 INFO ProgressMeter - Starting traversal; 11:47:51.557 INFO ProgressMeter - Current Locus Elapsed Minutes Regions Processed Regions/Minute; 11:47:52.683 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0;,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482:3647,Load,Loading,3647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-652912482,1,['Load'],['Loading']
Performance,"tect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.Refer",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2415,Load,Loading,2415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['Load'],['Loading']
Performance,"tened for clarity in the following commands. ```; bash faa.sh ; Using GATK jar /app/gatk-package-4.1.8.0-local.jar; Running:; /bin/java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /app/gatk-package-4.1.8.0-local.jar FilterAlignmentArtifacts -V /output/sample.FilterMutectCalls.vcf.gz -R /db/hs37d5.fa --bwa-mem-index-image /db/hg38.fa.img -I /output/sample.Mutect2.bam -O sample.somatic_filter.test.vcf.gz --use-jdk-inflater true; 19:11:56.929 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; 19:11:56.943 INFO NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so; 19:11:56.944 INFO SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation; 19:11:57.168 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/app/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 19, 2020 7:11:57 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:11:57.324 INFO FilterAlignmentArtifacts - ------------------------------------------------------------; 19:11:57.324 INFO FilterAlignmentArtifacts - The Genome Analysis Toolkit (GATK) v4.1.8.0; 19:11:57.325 INFO FilterAlignmentArtifacts - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:11:57.325 INFO FilterAlignmentArtifacts - Executing as foo@bar.local on Linux v2.6.32-696.6.3.el6.x86_64 amd64; 19:11:57.325 INFO FilterAlignmentArtifacts - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_261-b12; 19:11:57.325 INFO FilterAlignmentArtifacts - Start Date/Time: July 19, 2020 7:11:57 PM CST; 19:11:57.325 INFO FilterAlignmentArtifacts - -----",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356:1348,Load,Loading,1348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-660645356,1,['Load'],['Loading']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29005,concurren,concurrent,29005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31704,concurren,concurrent,31704,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_00",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25818,concurren,concurrent,25818,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35442,concurren,concurrent,35442,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40638,concurren,concurrent,40638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['concurren'],['concurrent']
Performance,"terFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:05.932 WARN IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater; >; > 16:17:06.503 INFO FeatureManager - Using codec VCFCodec to read file file:///home/robert/test/snps.vcf; >; > 16:17:06.539 INFO IntervalArgumentCollection - Processing 61464 bp from intervals; >; > 16:17:06.551 INFO HaplotypeCaller - Done initializing engine; >; > 16:17:06.573 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; >; > 16:17:06.588 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. F",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:5389,load,load,5389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"the reply!; Certainly. `umask `returns `0022`. As such I reckon that is not the issue. Stacktrace in the bottom. The folder permission of the datastore folder is as follows:; `drwx--S---+ 26 vidprijatelj group 4096 Mar 14 15:29 Vid_database`. When changing to 766, the error disappears. ```; Tue Mar 14 15:37:57 CET 2023; Using GATK jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=zzz_tmpdir -Xmx128G -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar /appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar GenotypeGVCFs --reference /data/Scratch/References/ucsc.hg38.fa --variant gendb://Vid_database --output Step05_MultiSampleCalling/Vid.vcf.gz --intervals /data/Scratch/References/hg38_exome_v2.0.2_merged_probes_sorted_validated.annotated.bed --genomicsdb-shared-posixfs-optimizations True --merge-input-intervals; 15:37:59.895 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/appl/tools/versions/gatk-4.3.0.0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:38:00.018 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.018 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.3.0.0; 15:38:00.018 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:38:00.018 INFO GenotypeGVCFs - Executing as user@server; 15:38:00.018 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_362-b08; 15:38:00.019 INFO GenotypeGVCFs - Start Date/Time: March 14, 2023 3:37:59 PM CET; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:38:00.019 INFO GenotypeGVCFs - HTSJDK Version: 3.0.1; 15:38",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918:1010,optimiz,optimizations,1010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8233#issuecomment-1468228918,1,['optimiz'],['optimizations']
Performance,this happens while running the command ; ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1128909634:761,cache,cacheCopy,761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849#issuecomment-1128909634,1,['cache'],['cacheCopy']
Performance,"tion true --disableAllReadFilters true --fixedChunkSize 100000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [September 17, 2016 12:08:44 PM EDT] Executing as kh3@rgcaahauva08091.rgc.aws.com on Linux 3.13.0-91-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; Version: Version:4.alpha.2-45-ga30af5a-SNAPSHOT; 12:08:44.930 INFO BwaSpark - Defaults.BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.COMPRESSION_LEVEL : 1; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_INDEX : false; 12:08:44.930 INFO BwaSpark - Defaults.CREATE_MD5 : false; 12:08:44.930 INFO BwaSpark - Defaults.CUSTOM_READER_FACTORY : ; 12:08:44.930 INFO BwaSpark - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 12:08:44.930 INFO BwaSpark - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 12:08:44.930 INFO BwaSpark - Defaults.REFERENCE_FASTA : null; 12:08:44.930 INFO BwaSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 12:08:44.930 INFO BwaSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:08:44.930 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:08:44.931 INFO BwaSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:08:44.931 INFO BwaSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; 12:08:44.931 INFO BwaSpark - Deflater IntelDeflater; 12:08:44.931 INFO BwaSpark - Initializing engine; 12:08:44.931 INFO BwaSpark - Done initializing engine; 12:08:45.439 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:08:47.488 INFO BwaSpark - Shutting down engine; [September 17, 2016 12:08:47 PM EDT] org.broadinstitute.hellbender.tools.spark.bwa.BwaSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=499646464. ---. null. ---",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408:2756,load,load,2756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2171#issuecomment-247785408,1,['load'],['load']
Performance,titionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:6927,concurren,concurrent,6927,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['concurren'],['concurrent']
Performance,"tk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.589 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils347167544598047196.so: /tmp/libgkl_utils347167544598047196.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.589 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.589 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; >; > 16:17:06.589 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_utils.so; >; > 16:17:06.590 **WARN** NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils6186849302609329058.so: /tmp/libgkl_utils6186849302609329058.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:06.590 **WARN** IntelPairHmm - Intel GKL Utils not loaded; >; > 16:17:06.591 **WARN** PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!; >; > Since the calculation takes quite long, I checked the WARN messages of the; > output above. Especially the last one about the AVX instruction set where; > it says that a *MUCH* slower implementation will be used. From the few; > WARN messages it seems like the root cause is the failure to load libgkl; > and that again seems to be related to my platform. Does anyone know more; > about this issue or how to work around it?; >; > Best regards,; > Robert; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/is",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:6085,load,load,6085,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['load'],['load']
Performance,"to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3692,concurren,concurrent,3692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572,1,['concurren'],['concurrent']
Performance,"torage.BlockManager$$anonfun$reportAllBlocks$3.apply(BlockManager.scala:217); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at org.apache.spark.storage.BlockManager.reportAllBlocks(BlockManager.scala:217); 	at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5391,concurren,concurrent,5391,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"tprocessGermlineCNVCalls, so that external dictionaries provided via `--sequence-dictionary` do not override those in the count files, and perhaps fail if one is provided for any of the tools (I donâ€™t recall exactly how VCF indexing is triggered by providing one, as seems to be indicated by the tutorial, but hopefully we can disallow external dictionaries while still taking advantage of the relevant engine features for VCF writing). EDIT: Went digging in Slack to try to remind myself of the context of these changes, and found the following PR comment from 1/7 (although it seems to have mysteriously disappeared from GitHub):. > Just so I understand, are we allowing overriding of the sequence dictionary in the shards (and skipping the consistency check) by allowing the parameter --sequence-dictionary to be specified? If so, we might want to document. Otherwise, I'd be inclined to enforce using the sequence dictionary in the shards (and ensuring the consistency check across shards is performed) by changing the null check in getBestAvailableSequenceDictionary to a check that the dictionary has not been set via the command line. EDIT^2: I think I misremembered the details of how #6330 hooked up the sequence dictionary and how getBestAvailableSequenceDictionary in GATKTool works (which probably explains why that comment was deleted...). Now that I actually go back and look, the `--sequence-dictionary` is not hooked up at all, so there is no change to revert in point 4!. Note that after all of this, it will *still* be possible to get into trouble at the gCNV step if you make funky shards (e.g., you could have shard 1 contain intervals from chr1 and chr3, and shard 2 contain intervals from chr2). I don't think it is possible to check for this case early, but you would still fail at PostprocessGermlineCNVCalls as above. Of course, all of these possibilities can be avoided by simply using the WDL, but it will be good to harden checks for those still working at the command line",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249:3068,perform,performed,3068,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-719576249,2,['perform'],['performed']
Performance,"tribble.readers.TabixReader.<init>(TabixReader.java:129); at htsjdk.tribble.TabixFeatureReader.<init>(TabixFeatureReader.java:80); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:117); ... 9 more; ```. If the file is really missing:; ```java; (cerc_prod) [16:48 xxxxxxx@yyyyyy:test a]$ gatk MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; Using GATK jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/erc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 17:06:37.645 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 17:06:37 CDT 2020] MergeVcfs --INPUT data/calling/erc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 5:06:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 17:06:37 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; De",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:5041,Load,Loading,5041,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['Load'],['Loading']
Performance,ue -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar CountReadsSpark --input /project/casa/gcad/test/HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/wgs.hg38/pipelines/hc/cram.test/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz --spark-master yarn; 2019-01-09 13:35:04 WARN SparkConf:66 - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 2019-01-09 13:35:05 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 13:35:09.640 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 13:35:09.799 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 13:35:11.507 INFO CountReadsSpark - ------------------------------------------------------------; 13:35:11.508 INFO CountReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.12.0; 13:35:11.508 INFO CountReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:35:11.508 INFO CountReadsSpark - Executing as farrell@scc-hadoop.bu.edu on Linux v2.6.32-754.6.3.el6.x86_64 amd64; 13:35:11.508 INFO CountReadsSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:1950,load,load,1950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['load'],['load']
Performance,"ue copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be forced to, since `PreprocessIntervals` will output a Picard interval list, and `AnnotateTargets` outputs a target file).; - [x] Integration tests are still needed for `CreateReadCountPanelOfNormals`. These might not test for correctness, but we could possibly compare to old PoNs. Segmentation/modeling:; - Instead of separate tools for copy-ratio segmentation (`PerformSegmentation`) and allele-fraction segmentation/union/modeling (`AllelicCNV`), there is now just a single segmentation/modeling tool (`ModelSegments`).; - Input is denoised copy ratio and/or allelic counts. If only one input is provided, then we only model only the corresponding quantity.; - There is no separate allele-fraction workflow. Unlike the old approach, we do not perform any genotyping or modeling before doing kernel segmentation.; - [x] Old code and classes are used for segment union. We should port or possibly replace this with a simple method that uses kernel segmentation. EDIT: Actually, just tried running a WGS sample and this is still a major bottleneck. EDIT 2: Hmm...actually doesn't seem to be an issue on my desktop (compared to my laptop, on which the run hangs here). Will try to track down the source of the discrepancy. EDIT 3: Added segment union based on single-changepoint detection using kernel segmentation.; - [x] Segment union should be replaced by a proper joint kernel segmentation. EDIT: I'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:3122,Perform,PerformSegmentation,3122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,1,['Perform'],['PerformSegmentation']
Performance,"update. i've been running the standalone consolidate tool, per chromosome. Below is chr 9. As you can see, it seems to take nearly a full day per attribute. Chr 9 is among the smaller contigs. In contrast, chr 1 has been stuck on the first attribute (END) for ~4 days at this point. I'm not sure if this was the right choice, but you will see this included ""--segment-size 32768"", based on the conversation above. ```; 03 Mar 2022 12:51:23,371 INFO : Consolidating contig folder: /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb/9$1$134124166; 03 Mar 2022 12:51:23,389 INFO : 	/home/exacloud/gscratch/prime-seq/bin/consolidate_genomicsdb_array -w /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb --shared-posixfs-optimizations --segment-size 32768 -a 9$1$134124166; 03 Mar 2022 12:51:23,423 DEBUG: using path: /home/exacloud/gscratch/prime-seq/bin:/home/exacloud/gscratch/prime-seq/bin/:/home/exacloud/gscratch/prime-seq/java/current/bin/:/home/exacloud/gscratch/prime-seq/bin/:/usr/local/bin:/usr/bin; 03 Mar 2022 12:51:23,510 DEBUG: 	12:51:23.510 info consolidate_genomicsdb_array - pid=233371 tid=233371 Starting consolidation of 9$1$134124166 in /home/exacloud/gscratch/prime-seq/workDir/0950f572-7565-103a-a738-f8f3fc8675d2/Job9.work/WGS_1852_consolidated.gdb; 03 Mar 2022 12:55:30,641 DEBUG: 	Using buffer_size=32768 for consolidation; 03 Mar 2022 12:55:30,656 DEBUG: 	12:55:30 Memory stats(pages) beginning consolidation size=9350950 resident=9324278 share=1814 text=3530 lib=0 data=9322130 dt=0; 03 Mar 2022 12:55:30,662 DEBUG: 	12:55:30 Memory stats(pages) after alloc for attribute=END size=9350984 resident=9324313 share=1821 text=3530 lib=0 data=9322164 dt=0; 05 Mar 2022 08:43:03,491 DEBUG: 	8:43:3 Memory stats(pages) after alloc for attribute=REF size=109159142 resident=108901310 share=1425 text=3530 lib=0 data=109130249 dt=0; 06 Mar 2022 06:28:14,322 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1060723659:865,optimiz,optimizations,865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674#issuecomment-1060723659,1,['optimiz'],['optimizations']
Performance,"ut.; - For all workflows, we always collect integer read counts; for WGS, these are output as both HDF5 and TSV and the HDF5 is used for subsequent input.; - For the case workflow, we always collect allelic counts at all sites and output as TSV.; - [x] We should output all data files as HDF5 by default and as TSV optionally. EDIT: This is done for `CollectFragmentCounts`.; - [x] We will need to update the workflows when @MartonKN and @asmirnov239 get `PreprocessIntervals` and `CollectReadCounts` merged, respectively. These tools will remove the awkwardness required by `PadTargets` and `CalculateTargetCoverage`/`SparkGenomeReadCounts`. Denoising:; - All parameters are exposed in the PoN creation tool (#3356).; - Without a PoN, standardization and optional GC correction are performed (#3570).; - Other than the minor point about sample mean/median being used inconsistently noted above, the denoising process is essentially exactly the same mathematically as before (""superficial"" differences include the vastly improved memory and I/O optimizations, the ability to adjust number of principal components used, the removal of redundant SVDs, the enforcement of consistent GC-bias correction).; - [ ] That said, I'll carry over this TODO from above: Revisit standardization procedure by checking with simulated data. We should make sure that the centering of the data does not rescale the true copy ratio.; - The only major difference is we no longer make a QC PoN or check for large events. This was performed awkwardly in the old pipeline, so I'd rather not port it over. Eventually we will do all denoising with the gCNV coverage model anyway.; - Pre/tangent-normalization copy ratio are now referred to as standardized/denoised copy ratio.; - [x] Old code is still used for GC-bias correction in `CreateReadCountPanelOfNormals`, and we still use the `AnnotateTargets` tool. We should port this over (possibly as part of `PreprocessIntervals`) at some point (actually, I think we will be for",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828:1806,optimiz,optimizations,1806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-333202828,2,['optimiz'],['optimizations']
Performance,util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: java.util.concurrent.ExecutionException: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.waitForAck(StreamingProcessController.java:228); 	... 26 more; Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Expected message of length 3 but only found 0 bytes; 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.getBytesFromStream(StreamingProcessController.java:261); 	at org.broadinstitute.hellbender.utils.runtime.StreamingProcessController.lambda$waitForAck$0(StreamingProcessController.java:208); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147:3447,concurren,concurrent,3447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397#issuecomment-895854147,1,['concurren'],['concurrent']
Performance,"utput from chr1. The output shows the Maximum resident set size (kbytes): **2630440**. Using GATK jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar defined in environment variable GATK_LOCAL_JAR; ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx200g -Xms16g -jar /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace; Command being timed: ""gatk --java-options -Xmx200g -Xms16g GenomicsDBImport --sample-name-map sample_map.chr1 --genomicsdb-workspace-path genomicsDB.rb.bypass.time.chr1 --genomicsdb-shared-posixfs-optimizations True --tmp-dir tmp --bypass-feature-reader --L chr1 --batch-size 50 --reader-threads 4 --overwrite-existing-genomicsdb-workspace""; User time (seconds): 270716.45; System time (seconds): 1723.34; Percent of CPU this job got: 99%; Elapsed (wall clock) time (h:mm:ss or m:ss): 76:08:24; Average shared text size (kbytes): 0; Average unshared data size (kbytes): 0; Average stack size (kbytes): 0; Average total size (kbytes): 0; Maximum resident set size (kbytes): 2630440; Average resident set size (kbytes): 0; Major (requiring I/O) page faults: 5; Minor (reclaiming a frame) page faults: 206030721; Voluntary context switches: 11129822; Involuntary context switches: 176522; Swaps: 0; File system inputs: 627981312; File system outputs: 466730160; Socket messages sent: 0; Socket messages received: 0; Signals delivered: 0; Page size (bytes): 4096; Exit status: 0. ```. So using the import on reblocked gvcfs using --bypass-feature-reader was the fastest way to import our 3500 gVCFs and minimize memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687:2253,optimiz,optimizations,2253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1252598687,2,['optimiz'],['optimizations']
Performance,"v"",; ""NIST controlHCprocesshours"": ""90.94291388888888"",; ""NIST controlHCsystemhours"": ""0.182125"",; ""NIST controlHCwallclockhours"": ""63.56370277777778"",; ""NIST controlHCwallclockmax"": ""3.701625"",; ""NIST controlMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-CONTROLRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST controlindelF1Score"": ""0.9902"",; ""NIST controlindelPrecision"": ""0.9903"",; ""NIST controlsnpF1Score"": ""0.9899"",; ""NIST controlsnpPrecision"": ""0.9887"",; ""NIST controlsnpRecall"": ""0.9911"",; ""NIST controlsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFControlSample/Benchmark/6d64f12a-ca50-4ecd-8608-93dc53d241bb/call-CombineSummaries/summary.csv"",; ""NIST evalHCprocesshours"": ""95.62183055555556"",; ""NIST evalHCsystemhours"": ""0.18361111111111117"",; ""NIST evalHCwallclockhours"": ""64.22846111111112"",; ""NIST evalHCwallclockmax"": ""3.3683277777777776"",; ""NIST evalMonitoringLogs"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-EVALRuntimeTask/cacheCopy/monitoring.pdf"",; ""NIST evalindelF1Score"": ""0.9902"",; ""NIST evalindelPrecision"": ""0.9903"",; ""NIST evalsnpF1Score"": ""0.9899"",; ""NIST evalsnpPrecision"": ""0.9887"",; ""NIST evalsnpRecall"": ""0.9911"",; ""NIST evalsummary"": ""gs://dsde-methods-carrot-prod-cromwell/BenchmarkVCFsHeadToHeadOrchestrated/e6f57e40-2025-46fd-9aa0-d591a3799007/call-NISTSampleHeadToHead/BenchmarkComparison/ccdb901c-fb8f-49e4-b542-cf42e011a623/call-BenchmarkVCFTestSample/Benchmark/f0709402-e72d-4013-a781-e50d8d46e2c3/call-CombineSummaries/summary.csv""; }; } ; </pre> </details>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815:14465,cache,cacheCopy,14465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7651#issuecomment-1069378815,1,['cache'],['cacheCopy']
Performance,v; > 12:28:19.507 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/chr1_a_bed.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/chr1_a_bed/hg38/chr1_a_bed.tsv; > WARNING 2020-07-21 12:28:19 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; > 12:28:19.512 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/cosmic_fusion.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/cosmic_fusion/hg38/cosmic_fusion.tsv; > 12:28:19.522 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v28.annotation.REORDERED.gtf -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.annotation.REORDERED.gtf; > 12:28:19.522 INFO DataSourceUtils - Setting lookahead cache for data source: Gencode : 100000; > 12:28:19.552 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.annotation.REORDERED.gtf; > 12:28:19.589 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode.v28.pc_transcripts.fa -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/gencode/hg38/gencode.v28.pc_transcripts.fa; > 12:28:27.529 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/dnaRepairGenes.20180524T145835.csv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/dna_repair_genes/hg38/dnaRepairGenes.20180524T145835.csv; > 12:28:27.546 INFO DataSourceUtils - Resolved data source file path: file:///home/pkus/mutect_test/gencode_xhgnc_v90_38.hg38.tsv -> file:///home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:14710,cache,cache,14710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['cache'],['cache']
Performance,"va:132); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182); 	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64); 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12, localhost, executor driver): java.util.ConcurrentModificationException; 	at java.util.ArrayList.sort(ArrayList.java:1464); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.<init>(ReadThreadingAssembler.java:81); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerReadThreadingAssemblerArgumentCollection.makeReadThreadingAssembler(HaplotypeCallerReadThreadingAssembler",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:7438,concurren,concurrent,7438,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['concurren'],['concurrent']
Performance,veInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38); at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:76); a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9242,Cache,CacheStep,9242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,verage 86.642% 86.662% +0.020% ; - Complexity 38963 39097 +134 ; ===============================================; Files 2336 2341 +5 ; Lines 182730 183522 +792 ; Branches 20066 20117 +51 ; ===============================================; + Hits 158321 159043 +722 ; - Misses 17366 17399 +33 ; - Partials 7043 7080 +37 ; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Î” | |; |---|---|---|; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `89.062% <Ã¸> (-3.125%)` | :arrow_down: |; | [.../scalable/data/LabeledVariantAnnotationsDatum.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0dW0uamF2YQ==) | `63.158% <Ã¸> (-5.263%)` | :arrow_down: |; | [...lable/modeling/VariantAnnotationsModelBackend.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvbW9kZWxpbmcvVmFyaWFudEFubm90YXRpb25zTW9kZWxCYWNrZW5kLmphdmE=) | `100.000% <Ã¸> (Ã¸)` | |; | [...sr/scalable/modeling/VariantAnnotationsScorer.java](https://codecov.io/gh/broadinstitute/gatk/pull/8132?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_conten,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333:1853,scalab,scalable,1853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8132#issuecomment-1370265333,1,['scalab'],['scalable']
Performance,"when trying to build GATK fully I get this error:; ```; > Task :gatkDoc FAILED; Execution optimizations have been disabled for task ':gatkDoc' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/classes/java/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/home/jeremie/GATK/build/resources/main'. Reason: Task ':gatkDoc' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':gatkDoc'.; > Javadoc generation failed. Generated Javadoc options file (useful for troubleshooting): '/home/jeremie/GATK/build/tmp/gatkDoc/javadoc.options'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.gradle.org/7.3.2/userguide/command_line_interface.html#sec:command_line_warnings. Execution optimizations have been disabled for 1 invalid unit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500:90,optimiz,optimizations,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7936#issuecomment-1202544500,1,['optimiz'],['optimizations']
Performance,"which includes combining read-count files, which takes ~30s of I/O) and generated a 520MB PoN, and DenoiseReadCounts took ~30s (~10s of which was composing/writing results, as we are still forced to generate two ReadCountCollections). Resulting PTN and TN copy ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 wr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1555,perform,perform,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['perform']
Performance,xc3Ivc2NhbGFibGUvVHJhaW5WYXJpYW50QW5ub3RhdGlvbnNNb2RlbC5qYXZh) | `77.778% <66.667%> (-2.991%)` | :arrow_down: |; | [...lkers/vqsr/scalable/ExtractVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvRXh0cmFjdFZhcmlhbnRBbm5vdGF0aW9ucy5qYXZh) | `92.188% <100.000%> (+1.116%)` | :arrow_up: |; | [...walkers/vqsr/scalable/ScoreVariantAnnotations.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvU2NvcmVWYXJpYW50QW5ub3RhdGlvbnMuamF2YQ==) | `76.250% <100.000%> (+0.149%)` | :arrow_up: |; | [...r/scalable/data/LabeledVariantAnnotationsData.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3Ivc2NhbGFibGUvZGF0YS9MYWJlbGVkVmFyaWFudEFubm90YXRpb25zRGF0YS5qYXZh) | `75.510% <100.000%> (+1.283%)` | :arrow_up: |; | [.../hellbender/utils/genotyper/AlleleLikelihoods.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nZW5vdHlwZXIvQWxsZWxlTGlrZWxpaG9vZHMuamF2YQ==) | `84.201% <0.000%> (+0.186%)` | :arrow_up: |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/8074/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comme,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323:3603,scalab,scalable,3603,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8074#issuecomment-1294055323,1,['scalab'],['scalable']
Performance,xecution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38); at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24); at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96); at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89); at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54); at org.gradle.internal.execution.steps.S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:9105,Cache,CacheStep,9105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Cache'],['CacheStep']
Performance,"xecutor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); 	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 17/10/18 17:35:58 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/10/18 17:35:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]; java.lang.OutOfMemoryError: Java heap space; 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:208); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); 	at org.seqdoop.hadoop_bam.BAMRecordReader.nextKeyValue(BAMRecordReader.java:225); 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.sc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749:5785,concurren,concurrent,5785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686#issuecomment-337554749,1,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:07 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 3, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:07 WARN TaskSetManager:66 - Lost task 3.0 in stage 0.0 (TID 2, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:24988,concurren,concurrent,24988,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:08 INFO TaskSetManager:54 - Starting task 3.1 in stage 0.0 (TID 4, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:08 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 3) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2, partition 9, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:26723,concurren,concurrent,26723,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:09 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:09 INFO TaskSetManager:54 - Lost task 3.1 in stage 0.0 (TID 4) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 4924320, span 190238, expected MD5 8a9ef2f91a78ffdc56561ece832e9f5d) [duplicate 1]; 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 7, scc-q12.scc.bu.edu, executor 2, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 WARN TaskSetManager:66 - Lost task 9.0 in stage 0.0 (TID 5, scc-q12.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:28914,concurren,concurrent,28914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-07 11:34:10 INFO TaskSetManager:54 - Starting task 3.2 in stage 0.0 (TID 8, scc-q21.scc.bu.edu, executor 1, partition 3, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:10 INFO TaskSetManager:54 - Lost task 1.2 in stage 0.0 (TID 6) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 2]; 2019-01-07 11:34:11 INFO TaskSetManager:54 - Starting task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-07 11:34:11 INFO TaskSetManager:54 - Lost task 3.2 in stage 0.0 (TID 8) on scc-q21.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:31103,concurren,concurrent,31103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:49 INFO TaskSetManager:54 - Starting task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:49 WARN TaskSetManager:66 - Lost task 2.0 in stage 0.0 (TID 1, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 160972515, span 170618, expected MD5 0cc5e1f5ec5c1b06d5a4bd5fff11b77f; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterato",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:24275,concurren,concurrent,24275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:50 INFO TaskSetManager:54 - Starting task 1.1 in stage 0.0 (TID 4, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:50 WARN TaskSetManager:66 - Lost task 4.0 in stage 0.0 (TID 2, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:26012,concurren,concurrent,26012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:51 INFO TaskSetManager:54 - Starting task 4.1 in stage 0.0 (TID 5, scc-q20.scc.bu.edu, executor 2, partition 4, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:51 INFO TaskSetManager:54 - Lost task 1.1 in stage 0.0 (TID 4) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae) [duplicate 1]; 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 2.1 in stage 0.0 (TID 6, scc-q01.scc.bu.edu, executor 1, partition 2, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 WARN TaskSetManager:66 - Lost task 7.0 in stage 0.0 (TID 3, scc-q01.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:27748,concurren,concurrent,27748,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-01-09 13:35:52 INFO TaskSetManager:54 - Starting task 1.2 in stage 0.0 (TID 7, scc-q20.scc.bu.edu, executor 2, partition 1, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:52 INFO TaskSetManager:54 - Lost task 4.1 in stage 0.0 (TID 5) on scc-q20.scc.bu.edu, executor 2: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequence id 1, start 93925364, span 266689, expected MD5 54babf05a23e9e88a8738dfbb20a1683) [duplicate 1]; 2019-01-09 13:35:53 INFO TaskSetManager:54 - Starting task 7.1 in stage 0.0 (TID 8, scc-q01.scc.bu.edu, executor 1, partition 7, NODE_LOCAL, 7992 bytes); 2019-01-09 13:35:53 INFO TaskSetManager:54 - Lost task 2.1 in stage 0.0 (TID 6) on scc-q01.scc.bu.edu, executor 1: htsjdk.samtools.cram.CRAMException (Reference sequence MD5 mismatch for slice: sequenc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:29939,concurren,concurrent,29939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-07 11:34:12 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 9.419880 s; 2019-01-07 11:34:12 INFO AbstractConnector:318 - Stopped Spark@f1d88ea{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-07 11:34:12 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-07 11:34:12 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:34893,concurren,concurrent,34893,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-01-09 13:35:56 INFO DAGScheduler:54 - Job 0 failed: count at CountReadsSpark.java:80, took 12.691336 s; 2019-01-09 13:35:56 INFO AbstractConnector:318 - Stopped Spark@22fda322{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-01-09 13:35:56 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-01-09 13:35:56 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:34643,concurren,concurrent,34643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Opti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:38109,concurren,concurrent,38109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,4,['concurren'],['concurrent']
Performance,xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-1ac79f09-1a36-4668-92d9-0739775f98ed; 2019-01-07 11:34:12 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-ed279998-3783-4f41-8fe5-f44a4fac3ee4; ```. CountReads runs fine..... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg/gatk/4.0.12.0/ins,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:43004,concurren,concurrent,43004,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['concurren'],['concurrent']
Performance,"xt(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-69cc5c72-eff6-4259-8b3b-12fa6f8c42b0; 2019-01-09 13:35:56 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-0bd07e00-4f6d-43bd-b9d2-b1999376c72b; ```. Just to verify, the non-spark version still runs fine with the compressed fasta.... ```; gatk CountReads --input HG04302.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram --reference GRCh38_full_analysis_set_plus_decoy_hla.fa.gz; Using GATK jar /share/pkg/gatk/4.0.12.0/install/bin/gatk-package-4.0.12.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /shar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:42756,concurren,concurrent,42756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['concurren'],['concurrent']
Performance,"xtensive (growing with the number of points in a segment), and 2) binary segmentation is a global, greedy algorithm. These both cause long events to be preferred over short events, and thus the first changepoints found (and retained after applying the penalty) may not include those for small, obvious events. For example, see performance on this simulated data, which includes events of size 10, 20, 30, and 40 within 100,000 points at S/N ratio 3:1 in addition to sine waves of various frequency at S/N ratio 1:2 (to roughly simulate GC waves). Changepoints arising from the sine waves will be found first, since these give rise to longer segments:. ![wave-kern-no-local](https://user-images.githubusercontent.com/11076296/29322673-4dd9a1ac-81ac-11e7-94f5-5c5494e44ac5.png). CBS similarly finds many false positive breakpoints:. ![wave-cbs](https://user-images.githubusercontent.com/11076296/29322677-5576e4ba-81ac-11e7-888b-07ed5bff27e3.png). However, when we tune down the sine waves to 1:10, ApproxKernSeg still gets tripped up, but CBS looks better:. ![wave-kern-no-local-small-waves](https://user-images.githubusercontent.com/11076296/29322732-815df58c-81ac-11e7-8305-6e1798616336.png); ![wave-cbs-small-waves](https://user-images.githubusercontent.com/11076296/29322737-836b78fe-81ac-11e7-93be-753a40011203.png). To improve ApproxKernSeg, we can 1) make the cost function intensive, by simply dividing by the number of points in a segment, and 2) add to the cost function a local term, given by the cost of making each point a changepoint within a local window of a determined size. This local term was inspired by methods such as SaRa (http://c2s2.yale.edu/software/sara/). The reasoning is that with events at higher S/N ratio, we typically don't need to perform a global test to see whether any given point is a suitable changepoint; using the data locally surrounding the point typically suffices. With these modifications, ApproxKernSeg can handle both scenarios:; ![wave-kern](https://us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045:1162,tune,tune,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-322502045,2,['tune'],['tune']
Performance,"y ratios were identical down to 1E-16 levels. Differences are only due to removing the unnecessary pseudoinverse computation. Results after filtering and before SVD are identical, despite the code being rewritten from scratch to be more memory efficient (e.g., filtering is performed in place)---phew!. If we stored read counts as HDF5 instead of as plain text, this would make things much faster. Perhaps it would be best to make TSV an optional output of the new coverage collection tool. As a bonus, it would then only take a little bit more code to allow previous PoNs to be provided via -I as an additional source of read counts. Remaining TODO's:. - [x] Allow DenoiseReadCounts to be run without a PoN. This will just perform standardization and optional GC correction. This gives us the ability to run the case sample pipeline without a PoN, which will give users more options and might be good enough if the data is not too noisy.; - [x] Actually, I'm not sure why we take perform SVD on the intervals x samples matrix and take the left-singular vectors. <s>Typically, SVD is performed on the samples x intervals matrix and the right-singular vectors are taken, which saves some extra computation that is necessary to calculate the left-singular vectors.</s> (EDIT: Actually, looks like Spark's SVD is faster on tall and skinny matrices, which might be due to the fact that the underlying implementation calls Fortran code. I still think that representing samples as row vectors has some benefits, so I've changed things to reflect this; I now just take a transpose before performing the SVD, so that we still operate on the same intervals x samples matrix.) This will also save us some transposing, which we do anyway to make HDF5 writes faster.; - [x] Change HDF5 matrix writing to allow matrices with NxM > MAX_INT, which can be done naively by chunking and writing to multiple HDF5 subdirectories. This will allow for smaller bin sizes. (EDIT: I implemented this in a way that allows one ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351:1812,perform,perform,1812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-316894351,2,['perform'],['perform']
Performance,y.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1.execute(ShadowCopyAction.groovy:78); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction$1$execute.call(Unknown Source); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.withResource(ShadowCopyAction.groovy:109); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:151); 	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:102); 	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:56); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:194); 	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:214); 	at com.github.jengelman.gradle.plugins.shadow.tasks.ShadowCopyAction.execute(ShadowCopyAction.groovy:75); 	at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:53); 	at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42); 	at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyAction,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445:5606,Cache,CachedMethod,5606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5499#issuecomment-446253445,1,['Cache'],['CachedMethod']
Performance,"your recommendation is still to copy the workspace prior to merging/appending to it, then the distributed processing still means copying the original, and to my thinking copying each contig's folder into a new workspace, vs. copying each contig into the same workspace is basically the same overhead. We also tend to keep the long-lived copy on our warm storage, with processing happening on our cluster's lustre filesystem. . 2) Again, i dont think it's necessarily right to assume every job will operate on the same set of intervals. We generally would use the same pattern, but there are legitimate cases in which different intervals/job would better match the cluster's availability. If we're appending a limited number of samples and our cluster is busy, we might want to scatter using more intervals/job since each job would finish fairly quickly and the practical reality is fewer total jobs would complete quicker. if we are performing an operation that requires a lot of time/job (like creating a new workspace or appending a lot of samples), we might do one job/contig. It's also worth pointing out that macaque has 1000s of small unplaced contigs, and therefore we almost never do a simple 1:1 job:contig scheme.; ; 3) When I was originally thinking about how to scatter/gather the creation of a combined gVCF, the overhead of re-merging was huge. There was zero point in taking the per-contig gVCFs and concat/bgzipping a new one, just to split it again. When I started down this road, my idea was to make a folder holding each gVCF, and a top-level JSON file to map contig->filepath, so code could intelligently work with these. The latter essentially describes the structure of a GenomicsDB workspace. Unlike concatenating gVCFS, the overhead of moving directories around is practically zero. Sure, I could make a folder of GenomicsDB workspaces, but if I'm already moving them, what's the point in not merging? . I could understand that is the workspace lived on a shared filesystem and",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049:1038,perform,performing,1038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-640881049,2,['perform'],['performing']
Performance,"zer.read(DefaultArraySerializers.java:396); 	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:307); 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); 	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:362); 	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63); 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992); 	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; 	at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); 	at htsjdk.variant.variantcontext.LazyGenotypesContext.invalidateSampleOrdering(LazyGenotypesContext.java:205); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:353); 	at htsjdk.variant.variantcontext.GenotypesContext.add(GenotypesContext.java:46); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:7760,concurren,concurrent,7760,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['concurren'],['concurrent']
Performance,Ã¸> (Ã¸)` | `7 <0> (Ã¸)` | :arrow_down: |; | [...ools/coveragemodel/germline/GermlineCNVCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3ZlcmFnZW1vZGVsL2dlcm1saW5lL0dlcm1saW5lQ05WQ2FsbGVyLmphdmE=) | `73.196% <Ã¸> (Ã¸)` | `13 <0> (Ã¸)` | :arrow_down: |; | [...exome/sexgenotyper/TargetCoverageSexGenotyper.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZXhnZW5vdHlwZXIvVGFyZ2V0Q292ZXJhZ2VTZXhHZW5vdHlwZXIuamF2YQ==) | `84% <Ã¸> (Ã¸)` | `5 <0> (Ã¸)` | :arrow_down: |; | [...der/tools/spark/sv/AlignAssembledContigsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9BbGlnbkFzc2VtYmxlZENvbnRpZ3NTcGFyay5qYXZh) | `100% <Ã¸> (Ã¸)` | `12 <0> (Ã¸)` | :arrow_down: |; | [...egmentation/PerformAlleleFractionSegmentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9zZWdtZW50YXRpb24vUGVyZm9ybUFsbGVsZUZyYWN0aW9uU2VnbWVudGF0aW9uLmphdmE=) | `88.889% <Ã¸> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |; | [...ools/walkers/contamination/GetPileupSummaries.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2NvbnRhbWluYXRpb24vR2V0UGlsZXVwU3VtbWFyaWVzLmphdmE=) | `83.333% <Ã¸> (Ã¸)` | `12 <0> (Ã¸)` | :arrow_down: |; | [...tools/spark/sv/RunSGAViaProcessBuilderOnSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/3027?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9SdW5TR0FWaWFQcm9jZXNzQnVpbGRlck9uU3BhcmsuamF2YQ==) | `0% <Ã¸> (Ã¸)` | `0 <0> (Ã¸)` | :arrow_down: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3027#issuecomment-306340681:2804,Perform,PerformAlleleFractionSegmentation,2804,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3027#issuecomment-306340681,1,['Perform'],['PerformAlleleFractionSegmentation']
Safety," NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression3825249225068031371.so: /tmp/libgkl_compression3825249225068031371.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > 16:17:04.402 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/robert/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; >; > 16:17:04.407 **WARN** NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (/tmp/libgkl_compression7506152962158874866.so: /tmp/libgkl_compression7506152962158874866.so: cannot open shared object file: No such file or directory (Possible cause: can't load AMD 64-bit .so on a Power PC 64 LE-bit platform)); >; > Sep 04, 2020 4:17:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; >; > INFO: Failed to detect whether we are running on Google Compute Engine.; >; > 16:17:05.842 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; >; > 16:17:05.843 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; >; > 16:17:05.843 INFO HaplotypeCaller - Executing as robert@powerlinux on Linux v4.4.0-184-generic ppc64le; >; > 16:17:05.843 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_252-8u252-b09-1~16.04-b09; >; > 16:17:05.843 INFO HaplotypeCaller - Start Date/Time: September 4, 2020 4:17:04 PM UTC; >; > 16:17:05.843 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.843 INFO HaplotypeCaller - ------------------------------------------------------------; >; > 16:17:05.844 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; >; > 16:17:05.844 INFO HaplotypeCaller - ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600:2607,detect,detect,2607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6794#issuecomment-687344600,1,['detect'],['detect']
Safety," a dangling head which often causes problem and in this case causes the variant to sometimes not be correctly assembled. This is where the dangling head gets separated from garbage in the 20 threshold graph: ![Screen Shot 2022-01-25 at 4 13 52 PM](https://user-images.githubusercontent.com/16102845/151060728-5a0d4d95-2eb4-4777-a0e9-34b07b2e6196.png). And here is that spot in the 60 threshold graph:; ![Screen Shot 2022-01-25 at 4 16 43 PM](https://user-images.githubusercontent.com/16102845/151061165-fb803312-59b8-48c4-b196-b0e97d2e00ea.png). And here it is in the 1 threshold ; <img width=""303"" alt=""Screen Shot 2022-01-25 at 4 22 17 PM"" src=""https://user-images.githubusercontent.com/16102845/151061909-25d41a3d-39c2-461e-8fd3-938e859ef3d7.png"">; graph:. This seems to have caused the two thresholds to assemble different haplotypes after dangling end recovery (since all of these are dangling ends because the assembly engine can't do anything else because there is not enough padding provided) and it just so happens this failed assembly misses the correct haplotype in that 20 threshold graph and we end up throwing away most of the reads as incongruent with assembly as a result which is why the depth drops out so low at that site. This is a pretty rare edge case and I happened to be able to recover the 20 mq threshold variant with reasonable correct coverage by playing with the `--min-pruning 4` argument. In general though this issue might or might not have existed if the bam snippet provided (and especially the calling interval you provided of chr7:145945238-145945238) were not centered on one single point since assembly works best and is most likely to succeed when it has a few hundred bases of padding around the variant in question (typically for a SNP we end up with at least 100 bases of active window plus another 300 bases of padding on either side for assembly) which cuts down on the risk of assembly failures like this one. I'm curious if you observed this behavior on ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139:1118,recover,recovery,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7124#issuecomment-1021629139,1,['recover'],['recovery']
Safety, at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41555,abort,abortStage,41555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['abortStage']
Safety," batch api for it? Multi layer docker builds are pretty standard from what I understand. . It sounds like your suggestions are talking about 2 slightly different issues to me. 1. Too many layers:. We typically have squashed the GATK docker images, but we recently switched to building our release images with google cloud build. Since squash is *STILL* an experimental feature in docker we've had trouble getting it to work there. Since the size reduction was pretty minimal from squashing we figured it would be ok to not prioritize it. It's definitely possible for us to consolidate various layers in the build. Or manually squash the images. We can take a look for our next release. Wide workflows on azure are something we need to support. 2. Docker size reduction:; I've spend a lot of time looking at this in the past. Our docker image is huge, but it's mostly due to the massive size of our python and R dependencies. I've done a bunch of work reducing temporary files in independent layers and using multiple stages to reduce the size. There's not much low hanging fruit left there. Similarly, moving to alpine is tricky an has limited benefit. GATK packages a number of C libraries which do not work out of the box on alpine due to the different C runtime. (At least that was the case the last time I investigated it a few years ago. ) I suspect there's a way to port things so they work on it, but it's not something we can do now. It also wouldn't be much of a help, the base image is completely dwarfed by piles of python and R dependencies which are very difficult to safely trim. Anyway, that's the state of things. We've considered a java only image for a while which would be much smaller than the current one. (although still fat by most docker standards...). We've never released one publicly because it seemed like it might cause confusion, but it's a reasonable possibility. . If you have any secret methods to reduce the size of python or R installations we're happy to take PRs!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427:1793,safe,safely,1793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8684#issuecomment-1934859427,1,['safe'],['safely']
Safety," can hopefully rely on per-bin bias modeling to at least partially account for mappability in gCNV calling (and we certainly wouldn't want to filter out a significant fraction of the genome, in any case). Do we agree?. To answer your first question, the criterion for choosing the peak is quite hacky at the moment, but I found that filtering low-count bins to first check for the presence of a high peak and then falling back to the peak at zero works perfectly fine in practice. . We can certainly try to do something smarter, since, as you say, bin filtering may be desirable---even if we implement mappability filtering---to remove large germline events (it's true that the ""example"" I showed above is indeed from the PAR-like region on X, as you point out, but this is roughly how a large arm-level event would appear even after mappability filtering.) Although the model I fit above, which is simply a sparse mixture of NBs with regularly-spaced means (modulo some sample-specific and contig-specific jitter), could conceivably capture such events as well, we want to avoid models where a single NB might try to capture two or more peaks. Also, just to clarify, the weird mosaic examples are the bottom two plots out of the four above---you can see the shifted (non-X, in one of the examples) single peaks. However, it's interesting that the PARs are still showing up in XY---I'm pretty sure I used the blacklist you provided, although I will double check. Did that only include the ""official"" PARs, or also the additional ones you found?. In any case, are we comfortable calling in those regions (here I'm talking about gCNV, not ploidy)? As I show above, I don't think we need mappability to nail the baseline ploidy. Can we then rely on the per-bin bias to account for these regions in gCNV (pinning them back to the correct CN) without mappability filtering? And with mappability filtering, how substantial is the hit to coverage in these regions? Should we blacklist them for the time bein",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639:1327,avoid,avoid,1327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4558#issuecomment-375923639,2,['avoid'],['avoid']
Safety," first step to correctly identify the issue. So it seems a bit premature to even prototype a method, much less merge it. I think this PR, as is, muddies the waters quite a bit. For example, it introduces a new Record class that denotes this type of ""CNLOH"" with a `C`. If we want to merge this, I suggest that we first correctly identify the issue. If these events are not mosaic CNLOH, then we should clean up all mention of CNLOH in this code. Either way, can we quantify the level of improvement gained by filtering such events in a reproducible evaluation? If so, let's bring that into gatk-evaluation. Finally, there are many more options available to change the segmentation and/or resolution than the single one you mentioned. If the users you are working with can clearly specify their analysis goals in terms of resolution, then it might be possible to sidestep the problem entirely without adding more unsupported code. This would also buy us more time to put in a principled solution, without the risk of unsupported code getting entrenched in their workflows. > There are definitely events that get missed without the germline tagging, so this is an improvement over blacklisting alone. And while I have seen erroneous germline tagging (i.e. false calling a segment germline), it was only ever due to really noisy data (e.g. a bad PoN) or a poorly tuned segment caller. This is encouraging. This means that a straightforward approach to germline filtering, such as simply identifying overlapping posteriors as mentioned above, should work well. Prototyping this approach shouldn't take long at all, especially when the matched normal is guaranteed to be available, as it is in this workflow (tumor-only would require some work to identify the normal state, as mentioned previously). I'd rather just roll that, evaluate it, and merge it instead. Key here is that we sidestep the deficiencies of the current CR-only caller, which also shares the blame for this ""CNLOH"" issue (since these ev",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199:3149,risk,risk,3149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-461431199,2,['risk'],['risk']
Safety," in the generated VCF records; 13:00:15.145 info NativeGenomicsDB - pid=144146 tid=144147 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 13:00:17.976 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/prime-seq/production/Shared/@files/.referenceLibraries/128/tracks/NCBI_Mmul_10.softmask.bed; 13:00:28.734 INFO IntervalArgumentCollection - Initial include intervals span 3961776 loci; exclude intervals span 1586664325 loci; 13:00:28.738 INFO IntervalArgumentCollection - Excluding 2060069 loci from original intervals (52.00% reduction); 13:00:28.738 INFO IntervalArgumentCollection - Processing 1901707 bp from intervals; 13:00:28.816 INFO SelectVariants - Done initializing engine; 13:00:28.816 WARN SelectVariants - ***************************************************************************************************************************; 13:00:28.816 WARN SelectVariants - * Detected unsorted genotype fields on input. *; 13:00:28.816 WARN SelectVariants - * SelectVariants will sort the genotypes on output which could result in slow traversal as it involves genotype parsing. *; 13:00:28.816 WARN SelectVariants - ***************************************************************************************************************************; 13:00:28.941 INFO ProgressMeter - Starting traversal; 13:00:28.941 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.21497791400000002,Cpu time(s),0.113811361; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.9714307110000004,Cpu time(s),0.8294423339999996; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.018746290999999998,Cpu time(s),0.018747005; GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.04312575600000001,Cpu time(s),0.04312843799999999; GENOMICSDB_TIMER,",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1209854842:1869,Detect,Detected,1869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1209854842,1,['Detect'],['Detected']
Safety," require the Conda environment, but the tool itself will not. But I think this is probably preferable to writing test code to compare HDF5s, minimal though that might be, since the schema might change in the future.; - [x] Tool-level docs. Minor TODOs:. - [x] Parameter-level docs. Could perhaps expand on the `resources` parameter once the required labels are settled.; - [x] Parameter validation.; - [x] Clean up docs for parent walker.; - [x] Decide on required labels. I think ""training"" and ""calibration"" (rather than the legacy ""training"" and ""truth"") might be good candidates. EDIT: Switched ""truth"" to ""calibration"" throughout the codebase.; - [x] Validate privileged labels (snp, training, calibration) in parent walker.; ; Future work:. - [ ] Clean up unlabeled outputs. This includes 1) sorting the corresponding HDF5, and 2) outputting a corresponding sites-only VCF. Unlike the labeled sites, which are written individually to VCF as we traverse them, unlabeled sites are placed into a reservoir of fixed size for subsampling purposes. Thus, we cannot write them to VCF as with labeled sites; furthermore, after traversal, the unlabeled sites are not ordered within the reservoir. Ultimately, the lack of this VCF means that extracted, unlabeled sites cannot be tagged as such by the scoring tool in the final VCF.; - [ ] Consider downsampling of labeled data. This is not done because 1) of the complications just mentioned, 2) we assume that labeled data is precious and that one-time extraction of it will always be relatively cheap, especially compared to training (and that training implementations can always downsample, if needed), and 3) using -L functionality to subset genomic regions is perhaps a cleaner strategy for doing so.; - [x] I think we can probably clean up treatment of allele-specific annotations by automatically detecting whether an annotation is an array type. This would obviate the need for the parameter to turn on allele-specific mode. EDIT: Added in #8131.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948059:4634,detect,detecting,4634,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7724#issuecomment-1067948059,1,['detect'],['detecting']
Safety," that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:1486,safe,safelinks,1486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safelinks']
Safety," the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a TSV. However, we may run into trouble if we hit a case sample with very high depth. So some sort of sparse representation of the histogram might indeed be desirable, but I think it should be an exact representation of the full histogram. This would require us to sync up code to emit and consume the representation in both Java and python, so I'd like to avoid it if possible---I think I'd prefer just emitting the ragged matrix, in that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:2356,avoid,avoid,2356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,2,['avoid'],['avoid']
Safety," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1747,avoid,avoid,1747,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349,1,['avoid'],['avoid']
Safety," used is with absolute path as following:; ```; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compres; sion_level=2 -Xmx4g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar VariantRecalibrator \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.filtered.SiteOnly.vcf \; --resource hapmap,known=false,training=true,truth=true,prior=15:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_hapmap_3.3.hg38.vcf.gz \; --resource omni,known=false,training=true,truth=false,prior=12:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_omni2.5.hg38.vcf.gz \; --resource 1000G,known=false,training=true,truth=false,prior=10:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_1000G_phase1.snps.high_confidence.hg38.vcf.gz \; --resource dbsnp,known=true,training=false,truth=false,prior=2:/dsgmnt/db/region/ftpGATK/resources_broad_hg38_v0_Homo_sapiens_assembly38.dbsnp138.vcf \; -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \; -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 \; -mode SNP --max-gaussians 6 \; -O /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.recal \; --output-model /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.model \; --tranches-file /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.tranches \; --rscript-file /dsguser/xhong/llfs_workdir/refinement/VQSR/gatk4100v2/c1joint_c1.snp.plots.R; ```. Somehow the current path **/dsgmnt/seq4_llfs/work/xhong/refinement/VQSR/script/** was added in front of **hapmap**, which should not be there.; It would be very helpful if anyone can instruct me how to avoid this problem in GATK4.1.0.0 . Best,; Xin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400:2380,avoid,avoid,2380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2199#issuecomment-484197400,1,['avoid'],['avoid']
Safety," we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/theano-users/eJ2vl2PUTk4. Last night, I have already tried to reset base_compiledir for theano, through two ways: (1) creating a ~/.theanorc file just like you suggested (2) modifying the file ~/.bashrc for my login node, by adding a line: export THEANO_FLAGS=""base_compiledir=/scratch/gatk-user1/z-Temp/z-Temp-Theano-$chr"". However, the truth is that, in our cluster, when I submit the 25 jobs (for each chromosomes), they are assigned to different computer nodes randomly. It means that I have to set THEANO environment variable for each corresponding random computer nodes respectively, which is quite difficult for me, as the nodes are random assigned. So, now I'm going to add lines like below to the ~/.theanorc in my login node, to see what will happen. Maybe It will work.; #######; [global]; config.compile.timeout = 100000 ; ######. However, I'm really appreciate it if some one in your team can help to add a function to specify a temporary directory for the theano directory, which can be bound to the corresponding node shared by other GATK threads. Thank you and Best regards.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:2816,timeout,timeout,2816,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['timeout'],['timeout']
Safety," why this is happening or what I can do to overcome this problem? I have run `GenomicsDBImport` and `GenotypeGVCFs` successfully in the past (same version, same computer) on a different dataset, so I'm not sure what about this data is causing the problem. Any guidance is much appreciated!. Thanks,; Jessie. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /nfs/data1/jsalt/3RAD/colinus_virginianus_13May2017_V3Fw6_newchrom.fasta -V gendb://odont_cyr_8_snp_db -O odont_cyr_8_snp_db.vcf; 14:59:47.866 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/jsalt/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 03, 2020 2:59:59 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:59:59.674 INFO GenotypeGVCFs - ------------------------------------------------------------; 14:59:59.675 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 14:59:59.675 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:00:09.686 INFO GenotypeGVCFs - Executing as jsalt@mustard on Linux v3.10.0-957.1.3.el7.x86_64 amd64; 15:00:09.686 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:00:09.687 INFO GenotypeGVCFs - Start Date/Time: February 3, 2020 2:59:47 PM CST; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.687 INFO GenotypeGVCFs - ------------------------------------------------------------; 15:00:09.688 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 15:00:09.688 INFO GenotypeGVCFs - Picard Version: 2.19.0; 15:00:09.689 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640:1411,detect,detect,1411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-581619640,2,['detect'],['detect']
Safety,"!!!!!!!!!!!!!!!![0m. 20:12:42.725 INFO FilterAlignmentArtifacts - Initializing engine; 20:12:48.403 INFO FeatureManager - Using codec VCFCodec to read file gs://fc-secure-024a1aae-a4f9-4025-aa93-f759f93a8203/50383670-4607-4e59-9bfc-4db970980f0e/Mutect2/773a91ea-25be-4d49-b97c-16527076250c/call-Filter/cacheCopy/TN-20-36-filtered.vcf; 20:12:50.117 INFO FilterAlignmentArtifacts - Done initializing engine; 20:12:51.042 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.9.0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; 20:12:51.099 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; 20:12:51.100 INFO IntelPairHmm - Available threads: 14; 20:12:51.100 INFO IntelPairHmm - Requested threads: 4; 20:12:51.100 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation; 20:12:51.100 INFO ProgressMeter - Starting traversal; 20:12:51.100 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 20:20:25.766 INFO ProgressMeter - chr3:104142090 7.6 1000 132.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007efc9818177e, pid=24, tid=0x00007f13b3c76700; #; # JRE version: OpenJDK Runtime Environment (8.0_242-b08) (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08); # Java VM: OpenJDK 64-Bit Server VM (25.242-b08 mixed mode linux-amd64 ); # Problematic frame:; # C [libgkl_smithwaterman1809483713436863458.so+0x177e] smithWatermanBackTrack(dnaSeqPair*, int, int, int, int, int*, int)+0x60e; #; # Core dump written. Default location: /cromwell_root/core or core.24; #; # An error report file with more information is saved as:; # /cromwell_root/hs_err_pid24.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098:1650,detect,detected,1650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-781673098,1,['detect'],['detected']
Safety,""":3101046070; },; {; ""name"":""GL000192.1"",; ""length"":547496,; ""tiledb_column_offset"":3101257243; },; {; ""name"":""NC_007605"",; ""length"":171823,; ""tiledb_column_offset"":3101804739; },; {; ""name"":""hs37d5"",; ""length"":35477943,; ""tiledb_column_offset"":3101976562; }; ]; }. And the header generated with GenomicsDBImport is:. ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GVCFBlock0-1=minGQ=0(inclusive),maxGQ=1(exclusive); ##GVCFBlock1-2=minGQ=1(inclusive),maxGQ=2(exclusive); ##GVCFBlock10-11=minGQ=10(inclusive),maxGQ=11(exclusive); ##GVCFBlock11-12=minGQ=11(inclusive),maxGQ=12(exclusive); ##GVCFBlock12-13=minGQ=12(inclusive),maxGQ=13(exclusive); ##GVCFBlock13-14=minGQ=13(inclusive),maxGQ=14(exclu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582:11810,detect,detect,11810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4514#issuecomment-372215582,1,['detect'],['detect']
Safety,"#### Guidelines for converting arguments to kebab case. We're not following an external spec doc, so here some guidelines to follow instead. Keep in mind that the main thing we're going for here is readability and consistency across tools, not absolute purity, so feel free to raise discussion on any cases where you feel the guidelines should be relaxed. Some things are more negotiable than others. . 1. Use all lower-case (yes, even for file formats).; 2. Use only dash (`-`) as separator, no underscores (because lots of newbies struggle to differentiate the two, and underscores take more effort to type than dashes).; 3. Separate words rather than smushing them together, eg use `--do-this-thing` rather than `--dothisthing` (this is really important for readability, especially for non-native English speakers).; 4. Avoid cryptic abbreviations and acronyms; eg use `--do-this-thing` rather than `--dtt`; 5. If you end up with `--really-long-argument-names-that-take-up-half-a-line`, please reach out and ask for a consult; maybe we can find a more succinct way of expressing what you need.; 6. If you run into any situation not covered above, please bring it up in this thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346190915:823,Avoid,Avoid,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346190915,1,['Avoid'],['Avoid']
Safety,"#### Hiding / deprecating tools and their docs. @samuelklee To add to @sooheelee's answer, if there are any tools that you definitely want gone and already have a replacement for, I would encourage you to kill them off (ie delete from the code) before the 4.0 launch. While we're still in beta we can remove anything at the drop of a hat. Once 4.0 is out, we'll have a deprecation policy (exact details TBD) that will allow us to prune unwanted tools over time, but it will be less trivial. And as Soo Hee said, everything that's in the current code release MUST be documented. We used to hide tools/docs in the past and it caused us more headaches than not. . That being said, as part of that TBD deprecation policy it will probably make sense to make a ""Deprecated"" program group where tools go to die. If there are tools you plan to kill but don't want to do it before 4.0 is released for whatever reason, you could put them there. Documentation standards can be less stringent for tools in that bucket. To be clear I think the deprecation group name should be generic, ie not named to match any particular use case or functionality. That will help us avoid seeing deprecation buckets proliferate for each variant class/ use case. Does that sound like a reasonable compromise?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138:1155,avoid,avoid,1155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-346189138,2,['avoid'],['avoid']
Safety,) will **not change** coverage.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #2809 +/- ##; ===========================================; Coverage 79.973% 79.973% ; Complexity 16726 16726 ; ===========================================; Files 1139 1139 ; Lines 60894 60894 ; Branches 9436 9436 ; ===========================================; Hits 48699 48699 ; Misses 8399 8399 ; Partials 3796 3796; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [...ools/archive/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NhbGN1bGF0ZVB1bGxkb3duUGhhc2VQb3N0ZXJpb3JzLmphdmE=) | `85.366% <Ã¸> (Ã¸)` | `8 <0> (?)` | |; | [...ellbender/tools/archive/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NvdmVyYWdlRHJvcG91dFJlc3VsdC5qYXZh) | `92.593% <Ã¸> (Ã¸)` | `17 <0> (?)` | |; | [...lbender/tools/archive/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0NvdmVyYWdlRHJvcG91dERldGVjdG9yLmphdmE=) | `91.803% <Ã¸> (Ã¸)` | `20 <0> (?)` | |; | [...ellbender/tools/archive/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0RldGVjdENvdmVyYWdlRHJvcG91dC5qYXZh) | `84% <Ã¸> (Ã¸)` | `4 <0> (?)` | |; | [...lbender/tools/archive/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/2809?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9hcmNoaXZlL0RlY29tcG9zZVNpbmd1bGFyVmFsdWVzLmphdmE=) | `89.474% <Ã¸> (Ã¸)` | `5 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-306007372:1756,Detect,DetectCoverageDropout,1756,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-306007372,1,['Detect'],['DetectCoverageDropout']
Safety,"* For `PipelineOptions`, my understanding is that it is used for Google genomics API, and we seem to use it very infrequently in GATK (and never in SV), so it is safe to use null whenever engine level or other utility functions API needs it; * For the `END` and `START` annotation, there is NO`START` in VCF spec, but `POS`, so I don't know where the `start` comes from. And yes, I agree that BND records don't have a `start` either, it is merely a novel adjacency between two genomic locations, none of which is a start or end.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325030125:162,safe,safe,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3476#issuecomment-325030125,1,['safe'],['safe']
Safety,"* _Large number of open file handles_: this was an issue in TileDB which got fixed as part of the restructuring that @nalinigans did for supporting HDFS/S3/GCS (#5017). I was too lazy to fix this again. If it's going to take some time for PR #5017 to be merged, I can submit a separate fix for this. This would fix any crashes/termination issues.; * _Performance of a single import process with a large number of intervals_; * Restating the obvious, but this is a single process (and by default, a single thread) with many intervals to import. As you increase the number of samples, this will become a performance pain point.; * More important than the number of intervals is the amount of data imported per interval. Each interval import involves opening the VCF files (loading index structures while creating FeatureReader objects), writing to TileDB/GenomicsDB. and closing the VCF file handles (destroying FeatureReader objects). If the amount of data written for each interval is sufficiently large, the cost of opening/closing the VCF files (creating/destroying FeatureReaders) is small relative to the total time taken.; * In the test cases I and Chris were trying, the amount of data written per interval was small (or 0 in many cases). The time taken in opening/closing the VCF files (and loading/destroying the index) dominates the total time.; * For a single import process (single thread), creating a large interval is better (or no worse) than passing several small intervals. TileDB/GenomicsDB has 0 overhead for regions with no data (for example, WES gVCFs). Having larger intervals will likely avoid issues described above. Hence, an advisory message will be beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757:1610,avoid,avoid,1610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5066#issuecomment-410576757,1,['avoid'],['avoid']
Safety,"**Update:**. Here's how the coverage looks like using `CollectReadCounts` (w/ and w/o MQ > 30 filter) vs. `CollectFragmentCounts`. The lines are offset by +10 and +20 for better visibility. Summary: marked improvement in all cases, however, the error modes are different. `CollectFragmentCounts` tends to underestimate the size of SV regions and uniformly leads to coverage depletion near the breakpoints, `CollectReadCounts` estimates the size of SV regions better, however, coverage near the breakpoints tend to be less predictable (sometimes depletion, sometimes accumulation). Still, IGV seems to do the best job. Any improvement over `CollectReadCounts` requires using supplementary alignment information (e.g. weight sharing among supplementary alignments; this will likely fix the coverage asymmetry of translocation breakpoints), read clipping information, and mismatches. The latter two require a base-level coverage collection strategy (like IGV and `CollectTargetBaseCallCoverage`). _Unbalanced translocation:_. ![unbtr-1](https://user-images.githubusercontent.com/15305869/37840319-1aba29ce-2e93-11e8-9d41-b9eafe450b6d.png). ![unbtr-2](https://user-images.githubusercontent.com/15305869/37840320-1bfff9a8-2e93-11e8-9842-39824f9fad64.png). ![unbtr-3](https://user-images.githubusercontent.com/15305869/37840321-1d82fa00-2e93-11e8-88ec-d7c40876594e.png). _Balanced translocation:_. ![baltr-1](https://user-images.githubusercontent.com/15305869/37840331-26a0962e-2e93-11e8-8dcf-0e69c8e45146.png). _Inversion:_. ![inv-1](https://user-images.githubusercontent.com/15305869/37840347-2f0d2f8e-2e93-11e8-8d36-64367951e7f2.png). ![inv-2](https://user-images.githubusercontent.com/15305869/37840350-306dedbe-2e93-11e8-9837-53369f5fb1f0.png). _Deletion:_. ![del-1](https://user-images.githubusercontent.com/15305869/37840366-3a32c0ea-2e93-11e8-99dc-d949985616d9.png). _Tandem Duplication:_. ![dup-1](https://user-images.githubusercontent.com/15305869/37840373-42052542-2e93-11e8-8891-fa9f79cc9f70.png",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375720743:522,predict,predictable,522,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4551#issuecomment-375720743,1,['predict'],['predictable']
Safety,"*Complete log**: . ```; Using GATK jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx8G -jar /software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar DetermineGermlineContigPloidy -L results/cnv/targets.preprocessed.interval_list -I results/cnv/hdf5/MGM20-0848_S4.hdf5 -I results/cnv/hdf5/MGM20-0872_S2.hdf5 -I results/cnv/hdf5/MGM20-1121_S4.hdf5 -I results/cnv/hdf5/MGM20-1543_S10.hdf5 --contig-ploidy-priors resources/contig_ploidy_priors.tsv --output-prefix ploidy -imr OVERLAPPING_ONLY -O results/cnv/ploidy; 15:09:27.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/software/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 18, 2021 3:09:27 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:09:27.686 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.686 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.2.0.0; 15:09:27.687 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:09:27.687 INFO DetermineGermlineContigPloidy - Executing as n.liorni@hpc001 on Linux v3.10.0-1127.el7.x86_64 amd64; 15:09:27.687 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_302-b08; 15:09:27.687 INFO DetermineGermlineContigPloidy - Start Date/Time: 18 ottobre 2021 15.09.27 CEST; 15:09:27.688 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.688 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 15:09:27.689 INFO DetermineGermlineContigPloidy - HTSJDK",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905:1575,detect,detect,1575,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444#issuecomment-945753905,1,['detect'],['detect']
Safety,"-29 15:14:32.662 INFO 12904 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8282 (http); 2020-05-29 15:14:32.675 INFO 12904 --- [ main] o.a.coyote.http11.Http11NioProtocol : Initializing ProtocolHandler [""http-nio-8282""]; 2020-05-29 15:14:32.676 INFO 12904 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]; 2020-05-29 15:14:32.677 INFO 12904 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.35]; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext; 2020-05-29 15:14:32.802 INFO 12904 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1944 ms; 2020-05-29 15:14:32.899 INFO 12904 --- [ main] com.luz.push.utils.GcmUtils : start init gcm server; 2020-05-29 15:14:33.029 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:12706,detect,detect,12706,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6965,detect,detection-pro,6965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-edit-distance-read-badness-threshold', 'detection-pro']"
Safety,-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-waterman-haplotype-to-reference-mismatch-penalty -150 --smith-waterman-haplotype-to-reference-ga; p-open-penalty -260 --smith-waterman-haplotype-to-reference-gap-extend-penalty -11 --smith-waterman-read-to-haplotype-match-value 10 --smith-waterman-read-to-haplotype-mismatch-penalty -1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:7077,detect,detection-chimeric-read-badness,7077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,2,['detect'],"['detection-chimeric-read-badness', 'detection-template-mean-badness-threshold']"
Safety,"...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_asyn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:998,detect,detected,998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088,1,['detect'],['detected']
Safety,.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:93); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:237); at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33); at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58); at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48); at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33); at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39); at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73); at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54); at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35); at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45); at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31); at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:208); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70); at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45); at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49); at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43); at org.gradle.internal.execution.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973:8517,Timeout,TimeoutStep,8517,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6466#issuecomment-590387973,2,['Timeout'],['TimeoutStep']
Safety,".Main.main(Main.java:292); ```. ## Cases when the error does not occur; * If I rename `test a` folder in `test-a` as previously said.; * If I copy my current `test a` in the `/tmp/` directory (`/tmp/test a/`). This may suggest that the path length plays a role.; * If I renamed the VCF files (first VCF becomes `a.vcf.gz`, second `b.vcf.gz`) (`gatk MergeVcfs -I data/calling/a.vcf.gz -I data/calling/b.vcf.gz -O out.vcf.gz`).; * If I rename the first VCF file with as many `a` character as characters found in the original filename. (aaaaaaaaaaaaaaaaaa.vcf.gz).; * If I rename the first VCF by replacing all alphabetical character with a (aaaa_aaaa2.aa_a7_1.vcf.gz); * If I introduce random `_` in the file name (aaaa_aaa_aaaa_aaaa.vcf.gz).; * If I rename the first VCF file by removing the first character (`cerc_prod2.SM_V7_1.vcf.gz` -> `erc_prod2.SM_V7_1.vcf.gz`); * If I rename the first VCF file by introducing a letter at the beginning (`cerc_prod2.SM_V7_1.vcf.gz` -> `ccerc_prod2.SM_V7_1.vcf.gz`). It really seems that the combination of the path lengh, white space and particular filename triggers this. I cannot get my head around this. I don't think this is coming from the content of the VCF as it works well in some cases. Let me know if you need me to make other tests. Fred. ----. ## Update. I investigated a little further after thinking about the tests I did. Because modifying the VCF filename did not trigger the issue and because of the presence of `tabix` related modules in the traces, I decided to see if removing `tbi` file will avoid having the error message. And it did!. After recreating the `tbi` file (`tabix data/calling/cerc_prod2.SM_V7_1.vcf.gz`), the error message appeared again. So it does not seem related to malformed index file. However, index file seems part of the problem. After renaming `test a` folder in `test-a` with the old or new index file, I did not get any error (as usual). Here is my tabix version in case:; ```bash; $ tabix -h. Version: 1.10.2; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:8607,avoid,avoid,8607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['avoid'],['avoid']
Safety,".local on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; INFO GenotypeGVCFs - Start Date/Time: January 14, 2020 1:53:14 PM BRT; INFO GenotypeGVCFs - ------------------------------------------------------------; INFO GenotypeGVCFs - ------------------------------------------------------------; INFO GenotypeGVCFs - HTSJDK Version: 2.21.0; INFO GenotypeGVCFs - Picard Version: 2.21.2; INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; INFO GenotypeGVCFs - Deflater: IntelDeflater; INFO GenotypeGVCFs - Inflater: IntelInflater; INFO GenotypeGVCFs - GCS max retries/reopens: 20; INFO GenotypeGVCFs - Requester pays: disabled; INFO GenotypeGVCFs - Initializing engine; ```. Run starts, a few variants are detected. Then It gets stuck in a region for about 30 minutes, and prints an out-of-memory error:. ```; INFO ProgressMeter - chrom2:4323711 0.3 2000 7859.6; INFO ProgressMeter - chrom2:4325583 0.6 3000 4753.5; INFO ProgressMeter - chrom2:4327262 0.8 4000 5010.5; INFO ProgressMeter - chrom2:4333146 1.1 7000 6493.1; INFO GenotypeGVCFs - Shutting down engine; ```. ```; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3332); at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuilder.append(StringBuilder.java:136); at htsjdk.tribble.util.ParsingUtils.split(ParsingUtils.java:266); at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:375); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:328); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688:1720,detect,detected,1720,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275#issuecomment-574329688,1,['detect'],['detected']
Safety,".pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:1613,abort,abortStage,1613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['abortStage']
Safety,"0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:49.551 INFO GenotypeGVCFs - Picard Version: 2.19.0; 23:15:49.552 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSIO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:1118,detect,detect,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['detect'],['detect']
Safety,"01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM models, as well. @LeeTL1220 @mbabadi @davidbenjamin I'd be interested to hear your thoughts, if you have any.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:1263,recover,recovered,1263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['recover'],['recovered']
Safety,"019-01-07 11:34:12 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-07 11:34:12 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-07 11:34:12 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-07 11:34:12 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-07 11:34:12 INFO BlockManager:54 - BlockManager stopped; 2019-01-07 11:34:12 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-07 11:34:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-07 11:34:12 INFO SparkContext:54 - Successfully stopped SparkContext; 11:34:12.605 INFO CountReadsSpark - Shutting down engine; [January 7, 2019 11:34:12 AM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.80 minutes.; Runtime.totalMemory()=1003487232; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 9, scc-q21.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonfu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969:36682,abort,aborted,36682,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-451999969,2,['abort'],['aborted']
Safety,"08); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 18 more; 19/02/18 16:58:29 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@45c90a05{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 16:58:29.970 INFO PrintVariantsSpark - Shutting down engine; [February 18, 2019 4:58:29 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.34 minutes.; Runtime.totalMemory()=1106771968; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException; Serialization trace:; genotypes (htsjdk.variant.variantcontext.VariantContext); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765:9549,abort,abortStage,9549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840#issuecomment-464825765,1,['abort'],['abortStage']
Safety,"1. @nalinigans It's a very reasonable question. It's true, the --avoid-nio flag is technically redundant. You can recreate it with a combination of other flags. I added it because ; a) I didn't realize that was the when I started adding it. ; b) The combination of flags was kind of complicated so it was helpful to have something that gave you clear instructions about what you needed to enable. I think we could merge them, although I think there is one sanity check we do even when -bypass-feature-reader is turned on, that we need to turn off. I basically added ""something that works for Megan's project right now."" . 2. Yes, the various cases were getting complicated and I had a bug when -V was enabled so I just disabled it as an option. It would make sense to add -V support for azure files. I just didn't do it because I was in a rush and I figured it was better to disable it than to have it potentially be wrong. . 3. Yeah, that's the error I saw. It's definitely better than nothing. It would be great if it could be propagated back up to the java layer as a Java exception though. It currently ends the program with SIGABORT I think which doesn't play that nicely with various reporting and retry mechanisms. No super high priority, but nice if you have the cycles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020:65,avoid,avoid-nio,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8632#issuecomment-1865021020,6,"['avoid', 'redund', 'sanity check']","['avoid-nio', 'redundant', 'sanity check']"
Safety,"124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > Using GATK jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar; > Running:; > java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar Funcotator --variant filtered_variants/P1.vcf.gz --reference /home/pkus/resources/hg38_for_bwa/hs38DH.fa --ref-version hg38 --data-sources-path /home/pkus/resources/gatk/funcotator/funcotator_dataSources.v1.6.20190124s --output filtered_variants/P1.avcf.gz --output-file-format VCF; > 12:28:16.251 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/pkus/programs/gatk-4.1.8.0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; > Jul 21, 2020 12:28:16 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; > INFO: Failed to detect whether we are running on Google Compute Engine.; > 12:28:16.537 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.538 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.8.0; > 12:28:16.538 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; > 12:28:16.541 INFO Funcotator - Executing as xxx on Linux v3.10.0-123.20.1.el7.x86_64 amd64; > 12:28:16.541 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_251-b08; > 12:28:16.542 INFO Funcotator - Start Date/Time: July 21, 2020 12:28:16 PM CEST; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.542 INFO Funcotator - ------------------------------------------------------------; > 12:28:16.542 INFO Funcotator - HTSJDK Version: 2.22.0; > 12:28:16.543 INFO Funcotator - Picard Version: 2.22.8; > 12:28:16.543 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; > 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975:1374,detect,detect,1374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6708#issuecomment-661776975,1,['detect'],['detect']
Safety,"2019-01-09 13:35:56 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-01-09 13:35:56 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-01-09 13:35:56 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-01-09 13:35:56 INFO MemoryStore:54 - MemoryStore cleared; 2019-01-09 13:35:56 INFO BlockManager:54 - BlockManager stopped; 2019-01-09 13:35:56 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-01-09 13:35:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-01-09 13:35:56 INFO SparkContext:54 - Successfully stopped SparkContext; 13:35:56.383 INFO CountReadsSpark - Shutting down engine; [January 9, 2019 1:35:56 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.78 minutes.; Runtime.totalMemory()=1009254400; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 11, scc-q20.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 87545719, span 186383, expected MD5 492a29f6d7d6fcaf8dde06834861e7ae; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1833); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162); at org.apache.spark.rdd.RDD$$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616:36433,abort,aborted,36433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547#issuecomment-452814616,2,['abort'],['aborted']
Safety,"22-01-18; OpenJDK Runtime Environment (build 17.0.2+8-86); OpenJDK 64-Bit Server VM (build 17.0.2+8-86, mixed mode, sharing); ```. Because it's a shared cluster, we aren't able to run Docker directly. But I attempted converting it in to a Singularity container and it didn't crash in the same way, but the job did end up failing. Logs are as follows -. For the ""bare metal"" known-crashing conditions (AMD-based machine), the final lines of the output are:; ```; 22:47:45.999 INFO ProgressMeter - Scaffold_1:21181812 551.0 125350 227.5; 22:47:56.192 INFO ProgressMeter - Scaffold_1:21203869 551.1 125450 227.6; 22:48:06.937 INFO ProgressMeter - Scaffold_1:21251889 551.3 125650 227.9; 22:48:18.177 INFO ProgressMeter - Scaffold_1:21271601 551.5 125750 228.0; 22:48:29.896 INFO ProgressMeter - Scaffold_1:21281660 551.7 125810 228.0; 22:48:40.223 INFO ProgressMeter - Scaffold_1:21284898 551.9 125830 228.0; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f889b5be310, pid=1422929, tid=1422930; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf310] __memset_avx2_unaligned_erms+0x60; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.1422929); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid1422929.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. When running on singularity (AMD-based machine):; ```; 07:07:35.120 INFO Progress",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680:1095,detect,detected,1095,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2386154680,1,['detect'],['detected']
Safety,"3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar MergeVcfs -I data/calling/cerc_prod2.SM_V7_1.vcf.gz -I data/calling/cerc_prod2.SM_V7_ZW.vcf.gz -O out.vcf.gz; 16:48:58.710 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/master/xxxxxxx/local/pckg/python/miniconda3/envs/cerc_prod/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Mon Jun 22 16:48:58 CDT 2020] MergeVcfs --INPUT data/calling/cerc_prod2.SM_V7_1.vcf.gz --INPUT data/calling/cerc_prod2.SM_V7_ZW.vcf.gz --OUTPUT out.vcf.gz --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; Jun 22, 2020 4:48:58 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; [Mon Jun 22 16:48:58 CDT 2020] Executing as xxxxxxx@yyyyyy on Linux 3.10.0-693.11.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_152-release-1056-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.7.0; [Mon Jun 22 16:48:58 CDT 2020] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1211105280; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to create BasicFeatureReader using feature file , for input source: file:///data/infectious/schistosome/tmp/test%20a/data/calling/cerc_prod2.SM_V7_1.vcf.gz; at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:124); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:148); at htsjdk.variant.vcf.V",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241:1984,detect,detect,1984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6664#issuecomment-647808241,1,['detect'],['detect']
Safety,"4 [addAssemblyQNames -> getKmerIntervals] mapPartitionsToPair, then reduceByKey, then mapPartitions; * Job 5 [getAssemblyQNames] mapPartitions twice and a collect; * Job 6 [generateFastqs] mapPartitions and combineByKey, then write FASTQ to files. A few observations:; * Jobs 0,1,3 are simple map jobs - very quick 1-2 mins each.; * Job 2 is a simple MR, with a tiny shuffle to sum by key (<1MB of shuffle data); * Job 4 takes a bit longer longer (3min), and shuffles ~3GB. This is a lot faster than when I ran it before with less memory, when it took 9 min. Is it creating a lot of garbage? If you wanted to speed things up you might look at what this is doing on a local machine and see if there are any opportunities to improve CPU efficiency.; * Job 5 takes ~8 mins, and has no shuffle. CPU intensive processing again?; * Job 6 take a little over 3 mins, shuffling ~3GB. Overall, it looks like itâ€™s performing pretty well. There is very little data being shuffled relative to the size of the input (~6GB to 133GB input), so itâ€™s not worth looking into optimizing the data structures there. The input data is being read multiple times, so it _might_ be worth seeing if it can be cached by Spark to avoid reading from disk over and over again. This is only worth it if you have sufficient memory available across the cluster to hold the input (which will be bigger than the on-disk size) _plus_ enough memory for the processing, which as we saw is quite memory hungry anyway. There might be some CPU efficiencies to pursue, especially if some code paths are creating a lot of objects that need garbage collecting (as Jobs 4 and 5 seem to be). Jobs 4 and 5 seem to have some skew (judging from the task time distribution in the UI). You might investigate this by logging the amount of data that each task processes (or rather than logging, generating another output that is some description of the task data - or use a Spark accumulator), and then seeing if there's some way to make it more uniform.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884:2599,avoid,avoid,2599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2458#issuecomment-292171884,2,['avoid'],['avoid']
Safety,"4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar <XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar BaseRecalibrator -R Homo_sapiens_assembly38.fasta -I S3_2.unmapped.split.bam --use-original-qualities -O S3_2.unmapped.recal_data.csv -known-sites Homo_sapiens_assembly38.dbsnp138.vcf -known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz; 23:39:34.668 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:<XXX>/gatk-4.1.4.1-83-g031c407-SNAPSHOT/gatk-package-4.1.4.1-83-g031c407-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 26, 2020 11:39:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:39:34.915 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.915 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.4.1-83-g031c407-SNAPSHOT; 23:39:34.915 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:39:34.915 INFO BaseRecalibrator - Executing as <XXX@XXX> on Linux v3.10.0-957.12.1.el7.x86_64 amd64; 23:39:34.915 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_242-b08; 23:39:34.916 INFO BaseRecalibrator - Start Date/Time: February 26, 2020 11:39:34 PM EST; 23:39:34.916 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.916 INFO BaseRecalibrator - ------------------------------------------------------------; 23:39:34.916 INFO BaseRecalibrator - HTSJDK Version: 2.21.2; 23:39:34.916 INFO BaseRecalibrator - Picard Version: 2.21.9; 23:39:34.916 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237:2005,detect,detect,2005,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6242#issuecomment-592005237,1,['detect'],['detect']
Safety,"72d292e2b7acc692670e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 13:57:16 2023 -0500. revert backport of MeanField. commit 67fb373687d32ec7e0fa329d4f3864e2cccabc53; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:45:11 2023 -0500. just used sample_node, finally deterministic?!. commit 754c424566cb9c191b9775b02d464488d7f68f68; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 07:22:51 2023 -0500. port stable logsumexp, though it doesn't seem to make a difference in ploidy. commit 95c944d792642ba5ec8dceaaff67d3a35cb3eab0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 23:51:01 2023 -0500. working with Mixture, still nondeterministic. commit 426375ac6473999ba249e775f2aa7d622534d510; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 22:32:44 2023 -0500. stochastic node cleanup. commit dc66f3f6a07dc82ba4258b3c0a65d990355da8d7; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 21:38:52 2023 -0500. safe log in log ploidy priors. commit 9712fa169a08edcf1a7a56622708e610b862631e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Tue Dec 5 21:35:05 2023 -0500. still debugging stochastic node. commit a79762c616f8758e0fd073b9e8e5d1ad30c1d5d0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 16:48:05 2023 -0500. blas in base, conda with libmamba solver. commit b4f5301c28a03689fca0b95ef652a51dd991686d; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 12:45:53 2023 -0500. freeze conda and set libmamba in base. commit f57a13a08fc3d049f0271e9aa94639ecb87b50f2; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Nov 9 07:29:30 2023 -0500. libmamba 23.9.0. commit 45058f27aae9c9240a167f126e32a6bddd3353ff; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Nov 8 23:33:51 2023 -0500. conda 23.9.0. commit d95494d282dd42ea3377de6c2d3554a3a5db65e4; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Mon Oct 30 17:04:34 2023 -0400. minor fixes to get ploidy working. commit c6a21f33f1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:5062,safe,safe,5062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['safe'],['safe']
Safety,"9] malloc+0x169`: . ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; [dalegre@login4601 fdone]$ head -n 20 hs_err_pid1182729.log; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520:1230,detect,detected,1230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520,1,['detect'],['detected']
Safety,9f29bba22fa8bb0512350bf93?src=pr&el=desc) will **increase** coverage by `0.078%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #3043 +/- ##; ===============================================; + Coverage 79.902% 79.979% +0.077% ; - Complexity 16668 16733 +65 ; ===============================================; Files 1134 1139 +5 ; Lines 60702 60917 +215 ; Branches 9423 9438 +15 ; ===============================================; + Hits 48502 48721 +219 ; + Misses 8412 8400 -12 ; - Partials 3788 3796 +8; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [.../tools/exome/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVQdWxsZG93blBoYXNlUG9zdGVyaW9ycy5qYXZh) | `85.366% <0%> (Ã¸)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (Ã¸)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (Ã¸)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (Ã¸)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:1227,detect,detectcoveragedropout,1227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,1,['detect'],['detectcoveragedropout']
Safety,":08 2023 -0500. add back setup.py files. commit 8348f546de6b3d32e1f02f6851730226c0dbffc9; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:37:09 2023 -0500. update pymc version in init. commit 850d60ef95b6126c05af9cd7c2cb528a306e1224; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:32:42 2023 -0500. added pip editable docs. commit 9c51b311442b0796ab1224213e83290caea0f93f; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:50 2023 -0500. whitespace. commit d9b180385168fdd1ef55cee8a1069fc1f7928f38; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:24:10 2023 -0500. update setup_gcnvkernel.py and pin pytensor. commit 7ccbd6da3d4afd1c987a66f6874bc4918495f943; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 09:20:49 2023 -0500. update conda in base and python packages. commit 693a1f9de10ee9950000abb83ef598cde82e026b; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 08:55:53 2023 -0500. clean up Mixture, sample seeding, safelog. commit 2b211d7ed7875c798020ceaeed865523f25c7096; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Fri Dec 8 00:28:54 2023 -0500. fixed all determinism, need to clean up seeds. commit 799228dd5fafa2bf5d57e65e9aab256cdfc4698a; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 22:22:18 2023 -0500. more dCR, Mixture. commit 124073d9af37c19573f90270c3cb0f4ae5ba4dd0; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 19:39:55 2023 -0500. fix dCR sampling?. commit f6871c9f12dbd52d84e78bba4f03d276ba1efb72; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 15:34:26 2023 -0500. logsumexp cleanup. commit 0c6cba790d8c3566a1a872d292e2b7acc692670e; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Thu Dec 7 13:57:16 2023 -0500. revert backport of MeanField. commit 67fb373687d32ec7e0fa329d4f3864e2cccabc53; Author: Samuel Lee <lee.samuel.k@gmail.com>; Date: Wed Dec 6 10:45:11 2023 -0500. just used sample_node, finally deterministic?!. commit 754",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322:3412,safe,safelog,3412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561#issuecomment-1854434322,1,['safe'],['safelog']
Safety,"; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths (counting only informative reads out of the total reads) for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=AF,Number=A,Type=Float,Description=""Allele fractions for alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=F1R2,Number=R,Type=Integer,Description=""Count of reads in F1R2 pair orientation supporting each allele"">; ##FORMAT=<ID=F2R1,Number=R,Type=Integer,Description=""Count of reads in F2R1 pair orientation supporting each allele"">; ##FORMAT=<ID=GP,Number=G,Type=Float,Description=""Phred-scaled posterior probabilities for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=ICNT,Number=2,Type=Integer,Description=""Counts of INDEL informative reads based on the reference confidence model"">; ##FORMAT=<ID=MB,Number=4,Type=Integer,Description=""Per-sample component statistics to detect mate bias"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PRI,Number=G,Type=Float,Description=""Phred-scaled prior probabilities for genotypes"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##FORMAT=<ID=SPL,Number=.,Type=Integer,Descrip",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397:1423,detect,detect,1423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1112612397,2,['detect'],['detect']
Safety,============; Files 1134 1139 +5 ; Lines 60702 60917 +215 ; Branches 9423 9438 +15 ; ===============================================; + Hits 48502 48721 +219 ; + Misses 8412 8400 -12 ; - Partials 3788 3796 +8; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree) | Coverage Î” | Complexity Î” | |; |---|---|---|---|; | [.../tools/exome/CalculatePulldownPhasePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9DYWxjdWxhdGVQdWxsZG93blBoYXNlUG9zdGVyaW9ycy5qYXZh) | `85.366% <0%> (Ã¸)` | `8% <0%> (?)` | |; | [...detectcoveragedropout/CoverageDropoutDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0RGV0ZWN0b3IuamF2YQ==) | `91.803% <0%> (Ã¸)` | `20% <0%> (?)` | |; | [...e/detectcoveragedropout/DetectCoverageDropout.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvRGV0ZWN0Q292ZXJhZ2VEcm9wb3V0LmphdmE=) | `84% <0%> (Ã¸)` | `4% <0%> (?)` | |; | [...ellbender/tools/exome/DecomposeSingularValues.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9EZWNvbXBvc2VTaW5ndWxhclZhbHVlcy5qYXZh) | `89.474% <0%> (Ã¸)` | `5% <0%> (?)` | |; | [...e/detectcoveragedropout/CoverageDropoutResult.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leG9tZS9kZXRlY3Rjb3ZlcmFnZWRyb3BvdXQvQ292ZXJhZ2VEcm9wb3V0UmVzdWx0LmphdmE=) | `92.593% <0%> (Ã¸)` | `17% <0%> (?)` | |; | [.../broadinstitute/hellbender/utils/tsv/DataLine.java](https://codecov.io/gh/broadinstitute/gatk/pull/3043?src=pr&e,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614:1548,detect,detectcoveragedropout,1548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043#issuecomment-306585614,2,"['Detect', 'detect']","['DetectCoverageDropout', 'detectcoveragedropout']"
Safety,"> . @rsasch I don't believe that CreateFilteringFiles.java is used anywhere else except in GVS. As for JointVcfFiltering.wdl, I really tried to avoid changing it, but the changes I had to make were for memory and disk. At this point the changes are in our branch (ah_var_store) and when we merge with main, we can deal with the PR then. But, I also know there's an updated version of this WDL that we are supposed to update to - that will presumably happen before we merge with main.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8206#issuecomment-1444234025:144,avoid,avoid,144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8206#issuecomment-1444234025,1,['avoid'],['avoid']
Safety,"> > > LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis; > > ; > > ; > > Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings.; > ; > Yes, that's also what I thought as well. That's why it seemed a little odd that this PR included a change that _lowered_ the default disk size on the VMs if they weren't specified from 500 to 200. I always thought going too low was the worry, and that disk size in general isn't much of a concern. But this PR appears to lower the default disk size by 60%, unless I am reading that completely wrong. The logs showed a max of 7% disk space, so it seemed fair to reduce it somewhat. But your comment reminded me that I also meant to adjust the `effective_extract_memory_gib` calculation, so I will do that now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349192759:272,risk,risks,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349192759,1,['risk'],['risks']
Safety,"> > LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis; > ; > Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings. Yes, that's also what I thought as well. That's why it seemed a little odd that this PR included a change that _lowered_ the default disk size on the VMs if they weren't specified from 500 to 200. I always thought going too low was the worry, and that disk size in general isn't much of a concern. But this PR appears to lower the default disk size by 60%, unless I am reading that completely wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349179555:260,risk,risks,260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349179555,1,['risk'],['risks']
Safety,> @Bowen1992 Could you please try running with the latest GATK release (`4.2.6.1`) and reporting whether the issue persists?. Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ22æ—¥ ä¸‹åˆ05æ—¶49åˆ†50ç§’; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097:680,Redund,Redundant,680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135302097,1,['Redund'],['Redundant']
Safety,"> @Siadjeu Don't worry about the ""Failed to detect"" message. It indicates some internal state in one of the google libraries but not an error we need to worry about. Generally you shouldn't worry about INFO messages if everything else is going fine and they don't say something particular about what you're doing. A WARNING or ERROR message would indicate a problem. This should maybe be downgraded to be a DEBUG level message or something but it's in a third library and convincing them to change it might be a hassle. Although there are results, but the size of the results is wrong, the results are too small.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-1598239834:44,detect,detect,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-1598239834,1,['detect'],['detect']
Safety,"> Can you check the headers of your gvcf inputs to see if any of them has this old tag?. #####this is the tag for gatk4.4; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele not already represented at this location by REF and ALT"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another; will always be hetero; zygous and is not intended to describe called alleles"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing gr; oup"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=PS,Number=1,Type=Integer,Description=""Phasing set (typically the position of the first variant in the set)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ##GATKCommandLine=<ID=HaplotypeCaller,CommandLine=""HaplotypeCaller --emit-ref-confidence GVCF --output CMC_C_1.g.vcf --input CMC_C_1.sorted.markdup.addRG.bam --reference kxc_hic_final.fast; a --use-posteriors-to-calculate-qual false --dont-use-dragstr-priors false --use-new-qual-calculator true --annotate-with-num-discovered-alleles false",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:1638,detect,detect,1638,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detect']
Safety,"> Hi Kevin, our team would like to get this merged into `ah_var_store` soon per VS-1254. I'm aware of only a handful of outstanding issues:; > ; > * The failing PGEN tests. I'm happy to help here in any way I can though right now I don't have a sense of what could be causing this beyond the platform differences you suggested.; > ; > * The `10` vs `10.0` change we discussed recently to avoid division by zero.; > ; > * We'll want to merge / rebase from `ah_var_store` and then build a new GATK Docker image which would be entered into `GetToolVersions` in `GvsUtils.wdl`. I'm happy to take on building this image once the merge / rebase is ready. Hi Miguel, sorry about the delay. I'm working on the failing tests issue this afternoon. I know what the issue is, so I just have to implement a fix, which I think should be fairly simple. Once I have that ready and have confirmed the tests aren't failing anymore, I'll do the rebase and then let you know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723:388,avoid,avoid,388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708#issuecomment-2004517723,2,['avoid'],['avoid']
Safety,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:232,avoid,avoid,232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135,1,['avoid'],['avoid']
Safety,"> I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes.; > ; > Also, this account seems to be a bot and I can't access its listed home pageâ€¦; > ; > I can audit the class at some point. https://codesafe.qianxin.com/#/home",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783:59,safe,safety,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-894740783,1,['safe'],['safety']
Safety,"> I have no objection to these changes, especially since this is just bringing us back to where we were in genomicsDB in the last release. We should spawn a ticket to track reintroducing these improvements and perhaps we should also add a macos test to our travis array so we can catch this kind of issue in the future? I think there is a macOS VM availible on travis that we could rerun some of the integration tests on. Yes, travis has macOS VM. It is very slow, so would recommend only sanity checks on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6204#issuecomment-539574124:489,sanity check,sanity checks,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6204#issuecomment-539574124,1,['sanity check'],['sanity checks']
Safety,> Is there any drawback to them being so high? Maybe just a small additional cost risk?. If the import fails non-transiently (BQ becomes unavailable or perhaps a bug is introduced during development) the import could be retried many times. But even in this scenario the VM wouldn't be up very long before exiting non-0 so I don't think this is a significant risk.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190750331:82,risk,risk,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7953#issuecomment-1190750331,2,['risk'],['risk']
Safety,"> LGTM. Seems superficially odd that we ultimately want to _reduce_ the disk size for pgen on larger callsets, but I trust the results of your analysis. Disk size is less of a concern because (at least with GCP historically) going too low on disk size risks much slower I/O without commiserate savings.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349160369:252,risk,risks,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8979#issuecomment-2349160369,1,['risk'],['risks']
Safety,"> OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by; > ; > `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`; > ; > This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:; > ; > ```; > [global]; > base_compiledir = PATH/TO/COMPILEDIR_#; > ```; > ; > Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?; > ; > This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort.; > ; > However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?; > ; > @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?. Dear samuelklee,. Thank you very much for you reply. I also found this problem last night. It seems that the problem is originally from Theano and Pymc3, rather than GATK 4.0. Some similar problems have been reported just like (1) https://github.com/pymc-devs/pymc3/issues/1463 (2) https://stackoverflow.com/questions/52270853/how-to-get-rid-of-theano-gof-compilelock and (3) https://groups.google.com/forum/#!topic/t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073:820,avoid,avoid,820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548557073,1,['avoid'],['avoid']
Safety,"> Overall the refactoring looks good and makes senseâ€¦ but I'm not seeing how this fixes the problem of eating exceptions we saw during a recent run. Can you explain what was happening before, and how the new code addresses it?. Sure! This code (besides refactoring so that it was only in one place) aims to fix two issues:; 1. if query results in an error, it gets run three more times and then, because of `while len(retry_delay) > 0`, it doesnâ€™t run again and the `raise err` line never gets executed, so no error is ever raised; 2. if the query fails for a reason that has no chance of being fixed by a retry (eg. 404), it will still run three more times. I probably missed some errors that should be ""retry-able"" (maybe `Aborted `? `BadGateway`? `Cancelled `? [full list here](https://googleapis.dev/python/google-api-core/latest/exceptions.html)), but I still think it makes sense to not treat all errors the same.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576:725,Abort,Aborted,725,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7480#issuecomment-930239576,1,['Abort'],['Aborted']
Safety,"> The error comes from two annotations: InbreedingCoeff and ExcessHet. One solution is to add ""-AX ExcessHet -AX InbreedingCoeff"". It doesnt exactly solve the problem, but it avoids hitting the problem code. Awesome! It is useful. Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238890115:175,avoid,avoids,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238890115,1,['avoid'],['avoids']
Safety,"> Why not break up these two things and push the annotation close to where the breakpoint-detected variants are created (ie. in discoverSimpleVariants or something), and then call the imprecise variant detector after that?. I wanted to do that but since the original code was brought in (and tested) that way with the imprecise variant logic, I refrained from doing that. ; Now done in commit 9fa39908fff6f382d899bc66f1760dbcfd22e540.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357543055:90,detect,detected,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357543055,2,['detect'],"['detected', 'detector']"
Safety,"> Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if she had any suggestions to avoid threading the argument for genotypes all the way through the engine. Not sure we can avoid threading the argument for genotypes, but would using GenomicsDBOptions instead work?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357:114,avoid,avoid,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357,2,['avoid'],['avoid']
Safety,> could look into forking gatk. This seems problematic. How can we avoid this?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7142#issuecomment-839995728:67,avoid,avoid,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7142#issuecomment-839995728,1,['avoid'],['avoid']
Safety,"> why would we want to commit this?. I don't think we do, I was just looking for sanity checks that what I ran was correct.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8868#issuecomment-2165892753:81,sanity check,sanity checks,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8868#issuecomment-2165892753,1,['sanity check'],['sanity checks']
Safety,"@AJDCiarla . Hello. So, to preface my answers it's also important to note that I am having these issues on Firefox. Now that I tried it in Chromium the Sign in link works. `When did you start having trouble signing into your GATK account?`; The issue is actually not with my account. I cannot even get to the Sign-in screen that you show.; `What is the username/email address associated with your GATK Forum account?`; As I mentioned before, the issue is not account specific. In fact I don't really remember if I still have one. My plan was to try my email and if it was already in the system to recover it.; `Could you please walk me through more of what you are seeing/doing when trying to log into your existing GATK account?`; When I open the [forum page](https://gatk.broadinstitute.org/hc/en-us/community/topics) and click New Post button or if I just click the ""Sign in"" button in the top panel. ![image](https://user-images.githubusercontent.com/22867431/204882347-257314af-1421-4e74-87e2-dffe760480d1.png). I don't get redirected to the Sign in page, but to the main page of GATK; ; ![image](https://user-images.githubusercontent.com/22867431/204882697-43bd0d15-0dd8-479b-af69-78951bb7d56c.png). The URL that I see in the browser does have some extra info:. https://gatk.broadinstitute.org/hc/en-us/signin?return_to=https%3A%2F%2Fgatk.broadinstitute.org%2Fhc%2Fen-us%2Fcommunity%2Fposts%2Fnew . But it still doesn't change where I end up.; The same is happening eevn if I run Firefox with `--safe-mode` to see if it's due to any of my extensions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332595323:597,recover,recover,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332595323,2,"['recover', 'safe']","['recover', 'safe-mode']"
Safety,@AJDCiarla The user should try re-running `GenotypeGVCFs` with `--max-genotype-count` set to a value greater than 1024. This should prevent the PLs from getting dropped and avoid the downstream error. The user may also need to increase `--max-alternate-alleles` as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1187744382:173,avoid,avoid,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1187744382,1,['avoid'],['avoid']
Safety,"@AlijahArcher We will need to clarify exactly what you intend by skipping the assembly. Here are some possibilities, with my initial thoughts:. * ""skip assembly"" = ""trust the alignments completely and use them directly for variant calling"": if your aligner is good this is reasonable though not ideal. If this is what you want you might as well use samtools for variant calling.; * ""skip assembly"" = ""every unique pattern of variants seen in your reads defines a haplotype"": the problem is that every sequencing error generates a new haplotype, so you need some way to cull bad haplotypes. Also, reads might only cover part of a haplotype so you need a way to sew them together.; * ""skip assembly"" = ""find all variants in your read alignments and let every combination thereof define a haplotype"": if I recall correctly this is FreeBayes. I would call this a quick-and-dirty assembly rather than skipping assembly entirely.; * ""skip assembly"" = ""avoid haplotypes altogether and genotype variants directly"": as you are aware, this is not possible within HC and M2. Hopefully that focuses the conversation somewhat on the two main questions: how do you generate haplotypes, and how do you refine the set of haplotypes to a few good candidates? What did you have in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-770995444:946,avoid,avoid,946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-770995444,1,['avoid'],['avoid']
Safety,"@AxVE Thanks for this PR. We really appreciate your interest and work on resolving this issue! It might take a little bit for me to get to reviewing it properly, we're currently preparing for our release and we're a bit swamped with various issues. I'm worried about changing the `userClassPathFirst` property. We added that a long time ago because it fixed some issues we were running into at the time. It's completely possible that we no longer have the same issue and it's a harmful remnant from a previous time, but I'm afraid that changing it might have unanticipated consequences in our own spark environment. Unfortunately we don't have good automated tests that would necessarily identify any issue. @cwhelan Would you be able to test your pipeline with - ""spark.driver.userClassPathFirst"" : ""false"" and see if you run into any issues? . I'm also a bit confused about why the change to the arguments is necessary. Clearly in your environment it is, but it goes against my understanding of how we set the arguments to spark submit, so I want to properly understand why the existing --deploy-mode arguments aren't working for you before adding an additional hardcoded argument to the launch script. (As I'm sure you've seen, the launch script is a pretty crufty and brittle piece of code that was really meant to be replaced with a more robust solution by now, so any additional complexity in would be great to avoid...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351430567:1417,avoid,avoid,1417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-351430567,1,['avoid'],['avoid']
Safety,"@AxVE Thanks for this! Sorry for the long wait. We'll have to monitor to see if changing it introduces some obscure issues for us that we haven't been able to figure out yet, but it seems like it's probably safe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-358123108:207,safe,safe,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3946#issuecomment-358123108,1,['safe'],['safe']
Safety,"@DanishIntizar Hello! Thank you for this pr. This is great to see an official plugin from amazon available. I appreciate that you took the time to make it an optional include. I think if we're going to include it we might as well just add it as one of our normal dependencies though. Assuming there aren't any dependency conflicts it **should** (always a risky statement) be independent from everything else. . Thanks also for identifying the different issues you mentioned. It's expected that it won't work with most picard tools as you discovered, but we're actively in the process of updating more of them too support Paths instead of Files so that will slowly improve. The second issue is more worrisome. We regularly use an equivalent provider with google to read reference files through the exact same code, so I suspect there is either some sort of mismatched assumptions in the way they are handling things. Maybe something strange with the Path.resolve methods or the like. (Or in in the much worse potential case a bug in their look ahead caching.). I'd like to look into that before we'd merge this. Ideally we would have tests for this. Are there any public AWS paths we could read from without any secret authentication?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721:355,risk,risky,355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8672#issuecomment-1930094721,1,['risk'],['risky']
Safety,"@DarioS Are you running contamination checks on this sample first? Did that not fail your QC?. It's difficult for us to provide a failure QC metric like this because there are so many different cancer types, each with their own behaviors. The fact that variants are all near their limit of detection is not necessarily a failure of M2 or even your sample. Instead it sounds like maybe you need a tool that would take a VCF as input and produce summary statistics on the VCF that you could then decide on QC metrics for. Does that sound like it would solve your problem?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-648951268:290,detect,detection,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6674#issuecomment-648951268,1,['detect'],['detection']
Safety,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:417,unsafe,unsafe,417,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562,2,"['safe', 'unsafe']","['safe', 'unsafe']"
Safety,"@DuyDN This is a known issue in BQSR -- see https://github.com/broadinstitute/gatk/issues/6242. Sorry for the inconvenience! We hope to be able to develop a fix within the next several months. The fact that you ran into this error indicates that there may not actually be any usable reads in that particular read group -- they were likely all filtered out by one of the BQSR filters, which filter out malformed, low mapping quality, unmapped, and secondary alignments. You could likely avoid the error by filtering out that read group using the `ReadGroupBlackListReadFilter` in GATK while running ApplyBQSR (`--read-filter ReadGroupBlackListReadFilter`)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490:486,avoid,avoid,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7549#issuecomment-963494490,2,['avoid'],['avoid']
Safety,"@EdwardDixon Sure, here's my suggested repair process:. 1. If you haven't already, add an ""upstream"" remote to your git clone via `git remote add upstream git@github.com:broadinstitute/gatk.git` (or `https://github.com/broadinstitute/gatk.git` if you don't have ssh authentication set up with github). 2. `git fetch upstream`. 3. Copy the files you actually intended to change in this PR into a temp directory somewhere. 4. Create a new temporary branch off of `upstream/master`: `git checkout -b avxcheck_repaired upstream/master`. 5. Copy the files you saved in step 3 back into their original locations in the working tree. 6. `git commit -a`. 7. Examine the diff against upstream/master via `git diff upstream/master HEAD`. Verify that the diff is what you expect. 8. Run `git rev-parse HEAD` and save the commit ID it outputs. 9. Switch back to the broken version of the branch: `git checkout avxcheck`. 10. Run `git reset --hard commit_id_from_step_8`. This will force the branch to point to the repaired commit we created in step 6. 11. Run `git push -f origin avxcheck:avxcheck` to force-push the repaired version of the branch into your fork. Then check that it looks ok on github. For avoiding this sort of thing in the future, here's a few tips:. * Never run `git merge` or `git pull`. Always update your branch with changes from the latest gatk master branch via the command: `git fetch upstream && git rebase -i upstream/master`, followed by `git push -f` to push the rebased branch into your fork. * If you've never run `git rebase` before, read a tutorial on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495:1195,avoid,avoiding,1195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-437415495,1,['avoid'],['avoiding']
Safety,"@LeeTL1220 A few minor remaining comments. Do what you will. How much of a performance impact does the change have? You said it slows it down, is it significant? It might be faster if you make it a long instead of an atomic long which should be safe it it's single threaded and you don't use parallel streams anywhere.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330:245,safe,safe,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3305#issuecomment-316490330,1,['safe'],['safe']
Safety,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:1098,detect,detect,1098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363,1,['detect'],['detect']
Safety,"@LeeTL1220 OK, tweaked the message a bit. I think I'm OK with this going in for the next point release. This is the sort of thing for which it will be nice to have the automatic validations, as a sanity check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979:196,sanity check,sanity check,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4292#issuecomment-363828979,1,['sanity check'],['sanity check']
Safety,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:167,avoid,avoid,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150,1,['avoid'],['avoid']
Safety,@PPWEST522 I suspect that you may not be limiting allelic-count collection to a set of common variant sites (provided as input to `-L` when running CollectAllelicCounts). See discussion in the tutorial at https://gatk.broadinstitute.org/hc/en-us/articles/360035890011--How-to-part-II-Sensitively-detect-copy-ratio-alterations-and-allelic-segments. Feel free to reopen and provide more details if this is not the case!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7633#issuecomment-1009563051:296,detect,detect-copy-ratio-alterations-and-allelic-segments,296,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7633#issuecomment-1009563051,1,['detect'],['detect-copy-ratio-alterations-and-allelic-segments']
Safety,"@SHuang-Broad Two other potential options:; 1. Potentially modify bwa's xassert so that it instead of calling abort() directly, it has a function pointer that by default points to abort() but our JNI code instead points it to a new method which will throw a java exception instead. ; 2. Patch BWA to do something similar during the jbwa build process. (gross...). Is 1 feasible? It would be good if we could globally avoid exits and redirect them to java exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243228784:110,abort,abort,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243228784,3,"['abort', 'avoid']","['abort', 'avoid']"
Safety,"@SHuang-Broad We'd like to have all of those abort conditions throw exceptions instead of crashing though. Seems like if we could intercept the abort call it would help us in many situations, not just the 1 bad index.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243321053:45,abort,abort,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243321053,2,['abort'],['abort']
Safety,"@Siadjeu Don't worry about the ""Failed to detect"" message. It indicates some internal state in one of the google libraries but not an error we need to worry about. Generally you shouldn't worry about INFO messages if everything else is going fine and they don't say something particular about what you're doing. A WARNING or ERROR message would indicate a problem. This should maybe be downgraded to be a DEBUG level message or something but it's in a third library and convincing them to change it might be a hassle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708443156:42,detect,detect,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708443156,1,['detect'],['detect']
Safety,@TedBrookings I implemented these suggestions of yours:. - replace CLUSTER_NAME with SANITIZED_BAM in the results directory; - allow output directory to be overridden by setting SV_OUTPUT_DIR; - rename `runWholePipeline` to `run-whole-pipeline`. I also found and fixed two other bugs:. - added a `set -f` to the create cluster script to avoid having bash expand the wildcard glob for that step (this caused a problem if a file matching the pattern was present locally); - changed how the copy results script got cluster info so that it parses the result header (this fixes a problem when using preemptible workers because the number of columns in the results of the `gcloud dataproc clusters list` command had a different number of columns. Can you give these changes a quick re-review?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666:337,avoid,avoid,337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666,1,['avoid'],['avoid']
Safety,"@Unip0rn Thank you for the pr. I'm not sure I understand what you're trying to do here though. currently when I run `./gatk --list` it prints the list of gatk tools. ex:; ```; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Base Calling: Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters; CheckIlluminaDirectory (Picard) Asserts the validity for specified Illumina basecalling data.; CollectIlluminaBasecallingMetrics (Picard) Collects Illumina Basecalling metrics for a sequencing run.; CollectIlluminaLaneMetrics (Picard) Collects Illumina lane metrics for the given BaseCalling analysis directory.; ExtractIlluminaBarcodes (Picard) Tool determines the barcode for each read in an Illumina lane.; IlluminaBasecallsToFastq (Picard) Generate FASTQ file(s) from Illumina basecall read data. ...; ```. With this change it instead prints the gatk launcher help, which is not the intended result. ; ```; Usage template for all tools (uses --spark-runner LOCAL when used with a Spark tool); gatk AnyTool toolArgs. Usage template for Spark tools (will NOT work on non-Spark tools); gatk SparkTool toolArgs [ -- --spark-runner <LOCAL | SPARK | GCS> sparkArgs ]. Getting help; gatk --list Print the list of available tools. gatk Tool --help Print help on a particular tool. Configuration File Specification; --gatk-config-file PATH/TO/GATK/PROPERTIES/FILE. gatk forwards commands to GATK and adds some sugar for submitting spark jobs. --spark-runner <target> controls how spark tools are run; valid targets are:; LOCAL: run using the in-memory spark runner; SPARK: run using spark-submit on an existing cluster; --spark-master must be specified; --spark-submit-command may be specified to control the Spark submit command; arguments to spark-submit may optionally be specified after --; GCS: run using Google cloud dataproc; commands after the --",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030:401,detect,detect,401,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5541#issuecomment-449068030,1,['detect'],['detect']
Safety,"@adaykin The difference between the chr1, chr2, etc. convention and the 1, 2, etc. convention is more than just a difference in naming. Different versions of the human reference use different naming schemes. For example the b37 reference uses 1, 2, etc., while the hg38 reference uses chr1, chr2, etc. For this reason, it is not safe to simply translate the contig names on-the-fly. You need to do a proper liftover from one reference to another using a tool such as `LiftoverVcf`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876:329,safe,safe,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7538#issuecomment-963507876,2,['safe'],['safe']
Safety,"@akiezun I think there's a case to be made for having the ability to separate closure of resources from generation of final output on success, even if it's not currently needed -- I'd be ok with keeping both methods provided we can settle on the right names to avoid confusion, and provided we update the docs to make it clear when each method should be overridden. @lbergelson's suggestion of `onTraversalSuccess()` and `cleanup()` seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504:261,avoid,avoid,261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1743#issuecomment-212629504,1,['avoid'],['avoid']
Safety,"@akiezun Instead of adding these overloads would we see the same speedup if we cached the result of isUnmapped and isPaired in the adapter? That would have the downside of complicating the adapter but it might avoid adding these strange methods to the interface. . If caching seems like a bad alternative, I think maybe these methods should have names that make it clear that they're some sort of performance hack and you should generally prefer the standard ones. 'getContigUnsafe` for instance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307:210,avoid,avoid,210,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032#issuecomment-235101307,1,['avoid'],['avoid']
Safety,"@asmirnov239 I think that we probably want all shard sizes to be as close to equal as possible, so that we avoid any possible issues arising from different model capacities across shards. Actually, is this even an issue at the GermlineCNVCaller step? Maybe we just need to worry about it during ploidy/postprocessing?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617321553:107,avoid,avoid,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6559#issuecomment-617321553,1,['avoid'],['avoid']
Safety,"@asmirnov239 I've borrowed the CopyNumberTestUtils class from #7889 into which you moved the method for detecting deltas in the doubles. I'm going to merge this PR once tests pass, so just be aware of this when rebasing your branch if you make any further changes. We might consider adding a simple test of the test method itself. I'll let you do it in your branch, or we can file an issue and tackle it after everything is merged.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972:104,detect,detecting,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1165717972,2,['detect'],['detecting']
Safety,"@bbimber File locking doesn't work on all the filesystems GenomicsDB supports (hdfs/cloud, for instance) and is a pain on others (NFS, for instance -- painful enough that we've sometimes had to recommend users disable the existing file locking). For that reason, I wouldn't want users to depend on file locking for correctness. Unfortunately, I think the cleanest approach is for the user to ensure correctness by avoiding the read/write conflicts themselves.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617448100:414,avoid,avoiding,414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6558#issuecomment-617448100,1,['avoid'],['avoiding']
Safety,"@bbimber Yes, the timeouts are not unusual. I restarted the (1 of the 8) jobs in the build matrix that failed on your PR. It will probably be fine, but I'll keep an eye on it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384694242:18,timeout,timeouts,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4495#issuecomment-384694242,1,['timeout'],['timeouts']
Safety,"@bhanugandham @fleharty this issue touches upon our discussion of https://gatkforums.broadinstitute.org/gatk/discussion/24335/loh-detection-using-gatk4s-somatic-cnv-workflow. We might consider just a simple modification of the genotyping step (e.g., keeping all ROHs longer than a hard threshold) to start, which would probably cover the most common use cases with minimal effort. Can use 100% HCC1143 in tumor-only mode as an initial test, but it would be good to collect other examples.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700:130,detect,detection-using-,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3915#issuecomment-531833700,2,['detect'],['detection-using-']
Safety,"@chapmanb ; _What we can do in bcbio is avoid adding any of these until after running the joint calling so they all get on the final joint VCF rather than the gVCFs._. I think that would be the right way to go (irrespective of the inconsistencies in the headers). My understanding as a layman (I'm not a bioinformatician) is that many of these annotation fields are (population/group) statistics. By storing these annotations in the individual gVCFs and then importing them into GenomicsDB, the size of the storage and the time to import are increased significantly (since the statistics are being stored in every sample for every position with a record in the gVCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407503807:40,avoid,avoid,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407503807,1,['avoid'],['avoid']
Safety,"@cmnbroad : thanks for the reply. yes, obviously tests would need to be added/updated. there's no question it needs robust testing. . my main concern is that VariantEval is a fairly sprawling tool with all sorts of add-ons. The majority of the untouched code taken verbatim from GATK3 isnt going to pass muster based on the bar of our last PR without a lot of petty revision (and maybe some useful updates). There are certainly some code improvements one could make across VariantEval, but I'm just not that keen on combing through the whole thing if it can be avoided. . how about this: while tests need to be updated (as discussed above, they work on the GATK3 data, which isnt checked in), the code in this PR is functional. Would you be willing to review a couple classes, maybe VariantEval itself and a few ancillary classes to see what scope of work we're talking about?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413642544:561,avoid,avoided,561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-413642544,1,['avoid'],['avoided']
Safety,"@cmnbroad @ldgauthier Out of curiosity, we may be able to avoid the complexity entirely if we just dropped the founderID argument from the tools. founderIDs didn't exist in gatk3, it looks like it was added to GATK4 at some point when a tool needed to produce a pedigree annotation before we had support for parsing ped files. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-470283695:58,avoid,avoid,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5663#issuecomment-470283695,1,['avoid'],['avoid']
Safety,"@cmnbroad How about we detect the common case of filters composed using only AND, and use simplified output in that case, and revert back to the complex output when filters are composed in more complex ways? That would resolve the problem in practice, since (as far as I know) all of the filters we actually use are composed using only AND.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562:23,detect,detect,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3520#issuecomment-367072562,2,['detect'],['detect']
Safety,"@cmnbroad I did a pass with a little cleanup. I think this is ready for review, but has a couple things I left the might help review:. 1) As noted above, the primary purpose here is to migrate to MultiVariantWalkerGroupedOnStart, and remove the redundant re-querying of comp alleles. This seems to work, but has the effect of altering behavior in some cases, described more above. In VariantEvalUtils.java I left some debugging code that illustrates the behavior difference that will occur. . 2) It is a fair question as to whether changing the behavior of what is or isnt considered an overlap is appropriate. For now I'm making changes as though it is, since it's sort of a fringe case and this is a beta tool, but it should be discussed. 3) There are ~6 tests with altered expectations, due to that change in detecting overlaps. I just checked in their updated expectations, since it helps illustrate how the iteration change would impact results",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-732478578:245,redund,redundant,245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973#issuecomment-732478578,2,"['detect', 'redund']","['detecting', 'redundant']"
Safety,"@cmnbroad I see. The ""CI"" variable does seem brittle, especially since I'm not strictly sure where it is set. I think a somewhat safer place would be to add some global test flag to the docker image would be to add it to the run_unit_tests.sh script. That way we know it is getting triggered exactly before we run the tests in just the docker environment. Is there some way of detecting what conda environment is active outside of conda.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354:129,safe,safer,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819#issuecomment-474871354,2,"['detect', 'safe']","['detecting', 'safer']"
Safety,"@cmnbroad I understand that I could have retained a bunch of single-use text files, but it seemed like the more permutations one adds, the less it makes sense to have a separate, very redundant, static text file to check each scenario. There's a ton of VariantContext-related tests that parse the output VCF to test some feature as opposed to checking in a bunch of VCF text files.... While I'll grant the 4th test case I added (where we pass chr 2) isnt especially compelling over just testing chr 1, one could argue more breadth is a good thing here. if you want clarity, pulling that VariantEval report parsing code into a method called extractUniqueContigsFromEvalReport(), or simply adding a comment line, supports this goal. Anyway, I'm checking in slightly clarified version of this now, simply to get tests running. If you respond to the above, maybe we go with that. In the interest of time, I'll stage and check in the version which restores the text files and goes that route.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741:184,redund,redundant,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7238#issuecomment-831459741,2,['redund'],['redundant']
Safety,"@cmnbroad OK, thanks. I need to do more investigation, but my initial thoughts/investigation were two-fold:. 1) Our main use-case if our VariantQC tool, which makes several instances of VariantEval in kind of a hacky way and calls apply() on them. Therefore refactoring a VariantEvalEngine out of VariantEval has value on this front, even if tricky. 2) From what I could tell, the worst perf is the iteration pattern. Using something like MultiVariantWalkerGroupedOnStart would be a big savings and avoid re-querying the component VCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033:499,avoid,avoid,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5439#issuecomment-720617033,1,['avoid'],['avoid']
Safety,"@cmnbroad OK, that's what I was afraid of. Has your group given thought to how barclay might attempt to de-convolute ""identical"" arguments defined across diverse plugins like this? . Anyway, I can try to follow the pattern of pedigree. I was trying to avoid special-casing these arguments, but I am already making my own PluginDescriptor anyway. My use-case is effectively to have a VariantAnnotator that supports more annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118:252,avoid,avoid,252,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7213#issuecomment-823493118,1,['avoid'],['avoid']
Safety,"@cmnbroad Thanks for the reply. I will look through that code to see if I turn up where this is happening. Is there a way to override/skip this repair? I understand as the header is coming from SVABA the VCF header is out of spec for GATK. It is one of the reasons I am using the tool to fix the dictionary of the VCF. It would be helpful to avoid the repair of other header fields in such cases. In the meantime, I will add a step to re-repair that particular header line so that downstream python scripts don't throw errors for the type mismatch. If there is no current way to avoid the header repair, and because it is a spec issue within GATK, I can close the issue after your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8629#issuecomment-1861509061:342,avoid,avoid,342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629#issuecomment-1861509061,2,['avoid'],['avoid']
Safety,"@cmnbroad one of the tests failed; however, it seems to just be a timeout:. https://api.travis-ci.org/v3/job/457478297/log.txt. are you able to restart them? again, I believe this addresses all concerns listed above except for the iteration (which will be addressed in the engine in a new PR), and the naming of CompRod and EvalRod classes. I'm fine changing these and associated outputs; however, I would appreciate suggestions on the best new names. CompInput, CompFeatureInput, CompSource, CompTrack, or something like that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440333816:66,timeout,timeout,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5043#issuecomment-440333816,1,['timeout'],['timeout']
Safety,"@cmnbroad ðŸ‘ Rebase and merge when ready. I think people expect to be able to write files with any extension they feel like, and the risks are pretty minimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-243827012:132,risk,risks,132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2046#issuecomment-243827012,1,['risk'],['risks']
Safety,"@cmnbroad, now this is prepared. I'm not using commons-io but the all `FASTA_EXTENSIONS` for detecting what to modify in the name.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-267036006:93,detect,detecting,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2243#issuecomment-267036006,1,['detect'],['detecting']
Safety,"@cwhelan . Thanks for the review!; I've incorporated most of your review suggestions, with the fowling exception because I need to think about what need to be done to make less review rounds. > This logic does more than detect variants, though.. it also annotates existing variants with the imprecise evidence. I'm also a little hesitant to move this all into its own separate class -- we really should be moving towards a model where we look at all three sources of evidence (breakpoint assemblies, imprecise evidence clusters, and coverage) simultaneously for eg @mwalker174 's work, and splitting handling of imprecise evidence into its own class seems like a step in the wrong direction. I agree. That's what I'm thinking about for complex inversions as well. So what about the following in this particular PR:. 1. move `StructuralVariationDiscoveryPipelineSpark.makeEvidenceLinkTree()` into `ImpreciseVariantDetector`;; 2. drop `ImpreciseVariantDetector.detectImpreciseVariantsAndAddReadAnnotations()` considering it really only delegates to `processEvidenceTargetLinks()`; 3. rename `ImpreciseVariantDetector` as `EvidenceTargetLinkHandler`; 4. reduce the work of `DiscoverVariantsFromContigAlignmentsSAMSpark.discoverVariantsAndWriteVCF()` into detecting only simple variants based on assemblies and name it `discoverSimpleVariants()`; 5. let `StructuralVariationDiscoveryPipelineSpark` call into `EvidenceTargetLinkHandler.processEvidenceTargetLinks()` to get back VariantContexts, then write VCF . `processEvidenceTargetLinks()` really does two things at the moment: annotation on breakpoints and call imprecise deletions; preferably, we should go the all-evidence-at-the-same-time approach and decouple the two but I am trying to not mess with it right now. If you agree, I'll implement it in a separate commit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426:220,detect,detect,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357345426,6,['detect'],"['detect', 'detectImpreciseVariantsAndAddReadAnnotations', 'detecting']"
Safety,"@cwhelan I was actually debating with myself about whether to include the initialization script here, as it was living in the bucket referred to in the creation script.; So we could do this:; always store the initialization script locally with the creation script instead of referring to a script living remotely, and makes that a required argument. The good: this makes it easier to track changes; The bad: initialization script must be removed from the bucket to avoid tracking possible different versions. A non-technical issue: we are ""delivering"" SGA in the initialization script, if that comes in to this repo, legal might have a problem with it. On the other hand, if the initialization script lives in a place only we can access, we are ""installing SGA for our own use"", which is not a problem with the GPL license.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289:465,avoid,avoid,465,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289,1,['avoid'],['avoid']
Safety,"@davidbenjamin & @ldgauthier Sorry for commenting on a closed/merged PR but I wasn't sure where else to take the discussion. If there's a more appropriate place please redirect me!. First off, this is very cool and I'm so glad to see this making it's way into HC/M2! It's super helpful for functional annotation/clinical interpretation. Thanks for working on this!. I had two thoughts which maybe belong as separate issues, but I figured I'd raise them here first and see what you thought:. 1. It would actually be useful to be able to combine this behavior with GVCF mode in some cases. I understand all the caveats about merging and joint-genotyping when this has been done, but there are use cases for single-sample calling where both GCVF and MNP mode combined would be useful. E.g. in a clinical setting it's very useful to have the GVCF with the reference blocks, and also call MNPs as MNPs. There would be no merging in this case. Any chance this could be allowed, perhaps with a warning or requirement that `--unsafe` be on?; 2. IIRC RBP would also phase combinations of `indel-SNP` and `indel-indel` in addition to `SNP-SNP`. I'm curious how hard it would be to apply the same grouping logic across indels as well? I tried to read the code in the PR, but honestly I don't think I understand the ramifications of including indels sufficiently well. Would there be any conceptual objections or road-blocks to doing this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602:1018,unsafe,unsafe,1018,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602,1,['unsafe'],['unsafe']
Safety,"@davidbenjamin @jonn-smith I have pushed the latest version of the code. Most of the changes since this was last shown are adding checks to about every level of the code for infinite loops (several places in the dangling end recovery code, and several places the new BestHaplotypeFinder). Additionally tests have been updated to capture these cases as well as several of the changes to functionality (no longer forcing reference start kmer to have a junction tree, limiting the cases where we actually attempt to follow an unsupported reference path when constructing a haplotype, reintroducing reference path weight to the calculation, etc...).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6034#issuecomment-520991019:225,recover,recovery,225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034#issuecomment-520991019,1,['recover'],['recovery']
Safety,@davidbenjamin Can you comment on this? Is this argument actually redundant?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1074250007:66,redund,redundant,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1074250007,1,['redund'],['redundant']
Safety,@davidbenjamin Could you take a look at this? @TedBrookings thinks it might be as simple as changing the check to allow 0 length reads when initializing the pairHmm. Neither of us are sure that that's a great solution though. . It seems like if you have no read bases you can't do any useful calculation. Should there be an earlier check in mutect that avoids assembling a region if there aren't any reads with bases?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131:353,avoid,avoids,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5543#issuecomment-465193131,2,['avoid'],['avoids']
Safety,@davidbenjamin I think that this issue will be addressed by the AFCalculator refactoring one way or another (e.g. by lifting up the max-alt-allele restrictions or simply avoid adding the NON-REF allele before calling the AFCalculator).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394:170,avoid,avoid,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858#issuecomment-221770394,1,['avoid'],['avoid']
Safety,"@davidbenjamin I tried and this time its a different error. ; ```; 14:55:53.232 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/shollizeck/clustering.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 09, 2020 2:55:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:55:53.432 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.433 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.4.1-6-g6bb31a7-SNAPSHOT; 14:55:53.433 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:55:53.433 INFO FilterMutectCalls - Executing as shollizeck@stpr-res-compute02.unix.petermac.org.au on Linux v3.10.0-1062.4.3.el7.x86_64 amd64; 14:55:53.433 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_232-b09; 14:55:53.434 INFO FilterMutectCalls - Start Date/Time: 9 January 2020 2:55:53 PM; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - ------------------------------------------------------------; 14:55:53.434 INFO FilterMutectCalls - HTSJDK Version: 2.21.0; 14:55:53.435 INFO FilterMutectCalls - Picard Version: 2.21.2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:55:53.435 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:55:53.435 INFO FilterMutectCalls - Deflater: IntelDeflater; 14:55:53.435 INFO FilterMutectCalls - Inflater: IntelInflater; 14:55:53.435 INFO FilterMutectCalls - GCS max retries/reopens: 20; 14:55:53.435 INFO FilterMutectCalls - Requester pays: disabled; 14:55:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341:357,detect,detect,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6202#issuecomment-572799341,2,['detect'],['detect']
Safety,"@davidbenjamin Thanks for the workaround in the 4.1.9.0 release!. I tested the updated `CreateSomaticPanelOfNormals` with `genomicsDBs` computed in 4.1.7.0 as above and it seems that the workaround recovers a lot of multiallelic variants that were already missing in 4.1.7.0. Using the record and variant counts in 4.1.7.0 as 100% reference, I'm getting 57% more records (all multiallelic) or 142% more variants. No sites from 4.1.7.0 are missing in 4.1.9.0. As a side note, all of the new records have `FRACTION=1` and most (90%) have `BETA=1,1;FRACTION=1`. Among shared records, all multiallelic sites also have `FRACTION=1` and almost always different beta parameter estimates compared to 4.1.7.0. As expected, biallelic sites are unchanged. As far as I understand, these annotations are irrelevant in deciding whether a site should be output or not, so this is not a concern.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-707740253:198,recover,recovers,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744#issuecomment-707740253,1,['recover'],['recovers']
Safety,"@davidbenjamin there is one test that failed. is this possible an intermittent /timeout problem? i dont have permission to restart them, but i dont see an actual test failure in it: https://travis-ci.com/broadinstitute/gatk/jobs/283688682",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582418410:80,timeout,timeout,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6406#issuecomment-582418410,1,['timeout'],['timeout']
Safety,"@droazen @jamesemery It does seem like it's by design, and as James found the behavior is to multiply the evidence by the length of the soft clip, which seems weird. We avoid this in Mutect2 and treat it like an indel. I would vote for doing the same and just eliminating the `HIGH_QUALITY_SOFT_CLIP` category.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470612215:169,avoid,avoid,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5767#issuecomment-470612215,1,['avoid'],['avoid']
Safety,"@droazen @ldgauthier I'm assuming the travis failure is not caused by this? looking at some of the travis logs its complaining about ""Timeout waiting for network availability"" and the previous travis CI in master failed too...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519716794:134,Timeout,Timeout,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519716794,1,['Timeout'],['Timeout']
Safety,@droazen Adam did profiling that showed that overlaps detector was strictly better. We also had some suspicion that there was some bug lurking in the skip list implementation because of weird non-deterministic results of something that relied on it.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-358031235:54,detect,detector,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4154#issuecomment-358031235,1,['detect'],['detector']
Safety,"@droazen For some reason, the fix in `CommandLineProgram` from #2190 stopped working in this branch (although I couldn't reproduce locally), which is why I tried making the `CommandLineProgram` field in `FeatureManager` transient again to see if it fixed the issue. Which it did. I've now avoided the issue entirely by removing the need for the `CommandLineProgram` field, by passing the instance to the method that needs it. This also addresses the point that @cmnbroad made about serializing the whole tool. The tests pass with this change. This probably needs testing on Google Cloud. @jean-philippe-martin have you run this successfully with the new NIO library (0.5.1)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-258810989:289,avoid,avoided,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220#issuecomment-258810989,1,['avoid'],['avoided']
Safety,"@droazen I believe @DonFreed's new code can be inserted before my new code with no change to either. His code deals with the non-hard-clipped part of the read, and all my code does is add the hard clips to the cigar. I think it's safe to add naively.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3494#issuecomment-418471397:230,safe,safe,230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3494#issuecomment-418471397,1,['safe'],['safe']
Safety,"@droazen I implemented the pr skipping on push builds if there's a pr branch. It seems to work. It has to spin up a vm to do the check, but that takes about a minute instead of many, and it avoids running tests and downloading lfs. The good things is that if it fails for some reason it should just continue on with the build, so flakiness in the github api or network connectivitiy will just result in some extra builds completing rather than extra failures. . <img width=""1054"" alt=""screen shot 2018-09-05 at 11 19 14 am"" src=""https://user-images.githubusercontent.com/4700332/45103843-c5b5ae00-b0fe-11e8-9934-0025af9836ee.png"">. I think I should add a github token though so we don't get api throttling. Should I just add one from my own account?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5156#issuecomment-418773830:190,avoid,avoids,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5156#issuecomment-418773830,1,['avoid'],['avoids']
Safety,@droazen I just looked and it seems that the only other big one I added was `--disable-artificial-haplotype-recovery` and that one is very esoteric indeed and doesn't need to be exposed I don't think.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410:108,recover,recovery,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6737#issuecomment-668197410,1,['recover'],['recovery']
Safety,"@droazen I posted the complete command line I used (the version is above). I posted a test.sam that reproducibly fails on my machine (OSX). And below is the log from my machine:. ```; 22:42:22.298 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/nhomer/miniconda3/envs/bfx/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.dylib; Aug 01, 2020 10:42:22 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.8.1; 22:42:22.412 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:42:22.412 INFO HaplotypeCaller - Executing as nhomer@ip-192-168-7-102.ec2.internal on Mac OS X v10.14.6 x86_64; 22:42:22.412 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 22:42:22.412 INFO HaplotypeCaller - Start Date/Time: August 1, 2020 10:42:22 PM MST; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.412 INFO HaplotypeCaller - ------------------------------------------------------------; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Version: 2.23.0; 22:42:22.413 INFO HaplotypeCaller - Picard Version: 2.22.8; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:42:22.413 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:42:22.413 INFO HaplotypeCaller - Deflater: IntelDeflater; 22:42:22.413 INFO HaplotypeCaller - Inflater: IntelInflater; 22:42:22.413 INFO HaplotypeCaller ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737:536,detect,detect,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667631737,1,['detect'],['detect']
Safety,"@droazen I ran the latest version but the message about google is still there!. 14:08:05.607 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1; .9.0-GCCcore-8.3.0-Java-8/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 14, 2020 2:08:06 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241:434,detect,detect,434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-708360241,1,['detect'],['detect']
Safety,"@droazen I'm not sure this is an improvement. We want the fundamental unit of spark tool to be the transform, not the cli wrapper around it. If we do this then we're pushing more of the contract of the transform outside of itself, i.e. see the newly duplicated bqsr code. I think that it was a deliberate decision to lift all reads into the initial rdd and then filter them in the transforms to what was needed by that transform. This is paying some performance cost in multi-stage pipelines which will potentially apply the same filters over and over again, but it simplifies the code because the filters can be baked into the transform and the pipeline writer doesn't have to think about them. It would be nice if we had a mechanism for adding metadata to an RDD so we can say ""this is a sorted RDD filtered with X,Y,and Z filters"", so we could intelligently avoid re-filtering.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856:861,avoid,avoid,861,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1159#issuecomment-158168856,1,['avoid'],['avoid']
Safety,"@droazen It's not a hard fix but it's bad that we don't have any way of detecting it... The way we set system properties is very gross and error prone. We set them in 2 place in build.gradle AND in gatk-launch, and we have to be careful to duplicate the changes in build.gradle in gatk-protected (which I don't think we ever end up doing...)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267121107:72,detect,detecting,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2316#issuecomment-267121107,1,['detect'],['detecting']
Safety,"@droazen Off the top of my head, we. * cache `log10(n)` and `log10(n!)` up to some large value.; * have a fast version of `log10SumLog10(double a, double a)` that works as follows: we want to compute `log10(10^a + 10^b)`. WLOG `a < b`, so this comes out to `a + log10(1 + 10^(a - b))`. I believe we cache the values of `log10(1 + 10^(x)` over a finely-spaced grid and round `a-b` to the nearest cached `x`. . There might not be anything else. There's a lot of stuff to keep calculations in log space for numerical stability but those don't avoid `log10()` and `Math.pow()`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322:540,avoid,avoid,540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292584322,1,['avoid'],['avoid']
Safety,"@droazen Responded to comments, note that i added a further escape condition where getMatchingPriors is avoided altogether if the VCpriors list is empty. Remember that this method is in a performance sensitive part of the code so every little bit of speed counts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381:104,avoid,avoided,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616#issuecomment-461597381,1,['avoid'],['avoided']
Safety,"@droazen Right - so the reason the pseudo code is formulated as something to detect deletions is because Laura had mentioned in the original ticket (and in the meeting we had last week) that she thought we could handle this case like it was a deletion. Quoting from her ticket:. > I still want to reject the 1/2 trans phased MNPs because they will complicate joint calling, but for a single sample with a single multinucleotide alt allele, it should behave nearly the same way as a deletion. So, yes the second part of the OR expression (we dropped length > 1, since that will be the case if number of mismatching bases > 1) detects MNP. And if the MNPs Laura describes above should be treated as deletions this may be an easy way to do so.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603334267:77,detect,detect,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603334267,2,['detect'],"['detect', 'detects']"
Safety,@droazen That is correct. It was a necessary step to avoid having to make a class equality check on the likelihoods object itself. @davidbenjamin I am open to suggestions if you have an idea of how better to encapsulate the separation between these two likelihood objects.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633:53,avoid,avoid,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633,1,['avoid'],['avoid']
Safety,@droazen are you sure thats not transient ? The first (connection timeout) failures that happened don't appear in previous builds that succeeded.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3179#issuecomment-311408683:66,timeout,timeout,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3179#issuecomment-311408683,1,['timeout'],['timeout']
Safety,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:220,redund,redundant,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666,1,['redund'],['redundant']
Safety,"@droazen great! I faced this problem because I am using a self deployed Docker Swarm and so I was using a GATK version of few weeks ago. Sorry for opening an useful issue. For completeness, I used a VM with double of resources and it took 36,92 minutes, as predictable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369994289:257,predict,predictable,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4479#issuecomment-369994289,1,['predict'],['predictable']
Safety,"@droazen here are the error messages with gatk4.1.8.1 and gatk4.1.4.1:. 15:01:44.424 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.4.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 3:01:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine. 14:28:22.786 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/cm/shared/unil/software/8.3/GATK/4.1.8.1-GCCcore-8.3.0-Java-8/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 09, 2020 2:28:23 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229:424,detect,detect,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875#issuecomment-707085229,2,['detect'],['detect']
Safety,@droazen production informed me that recalibration has passed (and that now they have a different downstream problem...as predicted!),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-641575854:122,predict,predicted,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6625#issuecomment-641575854,1,['predict'],['predicted']
Safety,@droazen sorry for a late response. I agree moving to java 17 would help. I do see that GATK itself is using the newer version of log4j but then its the transitive dependencies for the libraries used that bring in the older version of log4j. . this creates situations that the final compiled jar has both version of the log4j and this could create problems. . Gatk being a very useful tool gets integrated in multiple other tools and pipelines so in a way affecting the security posture of where its being used. The risk might be low being a standalone cli tool but its a very hard conversation with info security :) . May I ask for a ballpark ETA for the new version? Appreciate the work thats gone into this tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264:516,risk,risk,516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1448897264,1,['risk'],['risk']
Safety,"@droazen this behavior hasn't changed in the most recent GenomicsDB release. . Short recap: this happens because bcf codec doesn't support the 64 bit values that GenomicsDB is returning. Running with `--genomicsdb-use-vcf-codec` will resolve it. From our discussions in the office hours, I thought we had decided to change the behavior in htsjdk so that it doesn't try to decode the type if it doesn't recognize it. (maybe I should have filed https://github.com/broadinstitute/gatk/issues/6548 in htsjdk instead? I thought I was told to do in GATK, but its been long enough that I can't remember). Another possibility is to make `--genomicsdb-use-vcf-codec` the default - though I recall you had some potential performance concerns about that. Lastly, we could change GenomicsDB to throw out a warning if a 64 bit value is needed and we're using bcf codec. Of course, this would still require the user to (re)run with `--genomicsdb-use-vcf-codec` to avoid hitting the NPE (or whatever other failure would be hit if the NPE was changed to something a bit more meaningful).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430:950,avoid,avoid,950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6667#issuecomment-646167430,1,['avoid'],['avoid']
Safety,"@droazen, @lbergelson I have the following argument to the tool:. ```java; @Argument(fullName = ""read-tags"", doc = ""read tag names to recover""); public List<String> readTags = DEFAULT_READ_TAGS;; ```. On the command line I want to say. ```; java -jar gatk.jar â€¦; ...; --read-tags RX; ```. and want readTags to be a singleton list. But in my test it's not parsing the arguments correctly---what am I doing wrong here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7739#issuecomment-1081090913:134,recover,recover,134,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7739#issuecomment-1081090913,1,['recover'],['recover']
Safety,"@droazen, how do you interpret these results? I see one failure is a timeout, the other I'm not sure about but neither look really related to this code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-458336902:69,timeout,timeout,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-458336902,1,['timeout'],['timeout']
Safety,@drozen htsjdk PR is [here](https://github.com/samtools/htsjdk/pull/968/). I'm running gatk tests locally as a sanity check.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3448#issuecomment-322799380:111,sanity check,sanity check,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3448#issuecomment-322799380,1,['sanity check'],['sanity check']
Safety,"@erniebrau, the mesage and log file corresponds to the latest master (GKL 0.5.8); for the GKL 0.5.3 the error message is the following:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011dc557f4, pid=20586, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression1417468606951982528.dylib+0x17f4] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid20586.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. And the log file: [hs_err_pid20586.log.txt](https://github.com/broadinstitute/gatk/files/1264191/hs_err_pid20586.log.txt). Let me know if you need more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-326031152:170,detect,detected,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3532#issuecomment-326031152,1,['detect'],['detected']
Safety,"@fnothaft Thanks for this pr. I fixed the problem that was causing compilation to fail, but now we're getting real errors. ex:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1419.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1419.0 (TID 3897, localhost): java.lang.IllegalArgumentException: requirement failed: Failed when trying to create region 21 10006438 10006545 on null strand.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356012620:170,abort,aborted,170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4044#issuecomment-356012620,1,['abort'],['aborted']
Safety,"@frank-y-liu Something has gone slightly wrong in this branch git-wise -- it looks like you've duplicated some commits from master, and the merge commits in your history imply that your git workflow needs some tweaking. In general, you want to always `rebase` rather than `merge` or `pull` (and avoid mixing the two, which can cause problems), since `rebase` produces a much cleaner history. Since you're working in a fork, you should create an ""upstream"" remote if you haven't already:. `git remote add upstream https://github.com/broadinstitute/gatk.git`. Then when you're working in a branch to which you've made one or more commits, and you want to update your branch with the latest changes from our master, do this:. `git fetch upstream`; `git rebase -i upstream/master`. This will bring up a screen allowing you to ""squash"" (combine) your work into a single commit that is suitable for merging into our master branch. If you do this with the current version of your branch, and select ""squash"" for all but the top commit, I believe you'll succeed in repairing your git history. . Note that after each `rebase`, when you want to push your changes to github you'll need to do a `git push -f` instead of a simple `git push`, since `rebase` changes your commit history. Try it out and let me know how it goes!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210:295,avoid,avoid,295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776#issuecomment-215474210,1,['avoid'],['avoid']
Safety,"@gspowley Can you comment on whether this would be appropriate for a possible vectorized implementation in the GKL?. @davidbenjamin @vruano Can you comment on some of the existing approaches GATK takes to try to avoid expensive calls to `Math.log10()`, etc.? I think we rely (or used to rely) extensively on caching, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292557225:212,avoid,avoid,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2577#issuecomment-292557225,1,['avoid'],['avoid']
Safety,"@gspowley Yes, defining the env. var. avoided crash. It just says unable to load GKL. thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530:38,avoid,avoided,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-265880530,1,['avoid'],['avoided']
Safety,"@gudeqing I think you are referrring to the calls to `GetPileupSummaries`, where we have both `-L` and `-V` arguments with the same variable. This is actually not redundant, though I admit it is clumsy. This is a consequence of `GetPileupSummaries` being written as a GATK `LocusWalker`, which is necessary for optimal performance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085:163,redund,redundant,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7731#issuecomment-1154211085,1,['redund'],['redundant']
Safety,"@heliac2000 Thanks for the trace. I suspect that once the timeout initially occurs, the GATK process terminates, but the Python process is still trying to write back to it and gets the ""broken pipe"" return code. We already have plan to make the IPC/timeout more robust. In the meantime it would be interesting to see the last 50-100 lines of the journal file if it contains contains anything other than the repeated `Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, '')` lines (which are expected).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005:58,timeout,timeout,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005,2,['timeout'],['timeout']
Safety,"@hliang I see that many of the tasks are failing and it looks like one of the executors crashed. To find the cause, you can check the error logs of these tasks through the web UI. I suspect increasing executor memory will fix the problem. Heartbeat timeouts usually occur when an executor JVM runs out of memory or requests more memory than the node will allow.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313197140:249,timeout,timeouts,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-313197140,1,['timeout'],['timeouts']
Safety,"@ilyasoifer Looks like this user got tricked by some of the flow based annotations that don't work on their data. I would like to cut down on the risk that this happens for users. If we had more foresight I would advocate renaming all of the flow specific annotations to something like ""flowbased_#####"". How destructive would this be for your pipelines? . We have some appropriate checks in GATK for the flow-ness of the bam that give warnings more broadly about flow-based mode but we don't currently have any safeguards in the annotations. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073082102:146,risk,risk,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8788#issuecomment-2073082102,2,"['risk', 'safe']","['risk', 'safeguards']"
Safety,"@jamesemery - I think that the rebase is done. I'd like to have this in as soon as it can be, to avoid the extra-work of rebasing due to new tests or refactoring of them.... Thank you in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541:97,avoid,avoid,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-338990541,1,['avoid'],['avoid']
Safety,"@jamesemery - we should get this merge as soon as possible to avoid conflicts that pop up in every round of comments. Once this is in, I can go to the open PRs to point out the conflicts and the new structure (e.g., change the new tests to extend `GATKBaseTest`). I added a new commit addressing the issues and I will rebase to resolve conflicts again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214:62,avoid,avoid,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3475#issuecomment-340724214,1,['avoid'],['avoid']
Safety,"@jamesemery @droazen I've updated this branch to ensure all read and write paths to shared state in `GenotypeLikelihoodCalculators` is synchronized. I then wrote a little [test](https://github.com/broadinstitute/gatk/commit/3bb178746b1dd286f55ba77e6939e2104ced98d0) using `AlleleSubsettingUtils` to access `GenotypeLikelihoodCalculators` 10^6 times to see the effect of adding synchronization. R session (times are in millis):; ```; > without_sync = c(10166, 10049, 10306, 10059, 10165); > with_sync = c(10700, 10384, 9923, 10097, 10190); > t.test(without_sync, with_sync, paired=TRUE). 	Paired t-test. data: without_sync and with_sync; t = -0.70447, df = 4, p-value = 0.52; alternative hypothesis: true difference in means is not equal to 0; 95 percent confidence interval:; -542.5421 322.9421; sample estimates:; mean of the differences ; -109.8 ; ```. The p-value is not less than 0.05, so we can't reject the null hypothesis (that the mean times are the same). So adding synchronization doesn't seem to make any difference in this test. BTW, I noticed that `GenotypeLikelihoods` has synchronization, so there is some precedent for thread-safety using this means.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479:1142,safe,safety,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-426338479,1,['safe'],['safety']
Safety,"@jamesemery Back to you, at long last. I adopted your suggestion of a proper search that doesn't revisit already-seen vertices and came up with a better way of seeding the ""good"" subgraph that is safe from your STR concern. As far as code is concerned it's a total rewrite â€” you can pretend the first PR commit doesn't exist. The new criterion for seeding the search is chains with good log odds on both ends and which are incident on a vertex with multiple good out-edges or multiple good in-edges. The rationale is that the adjacency of two bad edges may have good log odds (Suppose a bad edge comes in and two bad edges come out. One is a new error on top of the original error and one is the continuation of the original error) but two have two outgoing edges with good log odds requires an actual real variant. On our M2 validations this essentially no effect on sensitivity and a mild reduction in false positives. I will leave it to you (or to me when I don't have to work like a vampire) to investigate how well it interacts with junction trees. As a first step I wrote a basic unit test for the basic pathology of the old method.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441:196,safe,safe,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520#issuecomment-624265441,1,['safe'],['safe']
Safety,"@jamesemery I agree - all access (read and write) to `GenotypeLikelihoodCalculators` instance variables needs to be synchronized to make it safe. I think it would be sufficient to make `getInstance()` and `calculateGenotypeCountUsingTables()` synchronized. @droazen, are you concerned about performance for the Spark case? For the walker version, presumably the access is single-threaded, and hence [uncontended, which is very cheap](https://books.google.co.uk/books?id=mzgFCAAAQBAJ&pg=PA230&lpg=PA230&dq=java+uncontended+synchronization+goetz&source=bl&ots=7W4J807faW&sig=YALE1qdWoAUELPqLRhIedz-bZ20&hl=en&sa=X&ved=2ahUKEwj4jJeko8zdAhXVFsAKHazkBrcQ6AEwB3oECAIQAQ#v=onepage&q=java%20uncontended%20synchronization%20goetz&f=false). Another option would be to maintain a separate instance of `GenotypeLikelihoodCalculators` per genotyping engine. The size of the table is ploidy * alleles, so not too large?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586:140,safe,safe,140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-423546586,1,['safe'],['safe']
Safety,"@jamesemery I will gladly review. If I understand the code change it seems like there was already basically the right logic to avoid this _but_ the code was neglecting to put the force calling alleles in a representation consistent with the output VCF. And the fix is simply to compute `forcedAlleles = AssemblyBasedCallerUtils.getAllelesConsistentWithGivenAlleles(givenAlleles, vc)` a bit upstream of where we were doing so previously. If I've got that right, this PR gets my :thumbsup:.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841:127,avoid,avoid,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7740#issuecomment-1081342841,2,['avoid'],['avoid']
Safety,"@jean-philippe-martin Can you comment on this error with your thoughts? Despite now doing a channel reopen on `UnknownHostException` in our fork of the NIO library, all reopens are failing, which implies that this error can't be recovered from via a simple retry. Could there be something wrong in our authentication setup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931:229,recover,recovered,229,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412906931,2,['recover'],['recovered']
Safety,"@jean-philippe-martin I like your counter proposal in general for testing path integration. I think writing to GCS over NIO is an important enough feature that we should have at least 1 test in gatk that actually writes to a real GCS bucket in case there's ever an issue specifically with GCS (authentication issues are one potential problem I can imagine). . It seems like we should be able to design in a way that avoids collisions. What does `Files.createTempFile()` do with gcs? My guess is that it probably doesn't do the right thing, but maybe we could fix it so it would? Or use some sort of scheme with random UUID's like the methods in BucketUtils that we have already.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140:416,avoid,avoids,416,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332235140,1,['avoid'],['avoids']
Safety,"@jean-philippe-martin Sorry, I originally typed ""you can't compare `Iterator<CRAMRecord>` with `Iterator<SAMRecord>`"" , but I didn't quote it, so it displays as ""you can't compare Iterator with Iterator"". Anyway, it looks like you're not doing that. Thanks for adding the CRAM tests. Rather than adding separate data providers and methods for them though, can you just change the existing providers and methods to have an output extension and a reference (null is OK), and then thread those through the test code. I made a branch of your branch with a commit [here](https://github.com/broadinstitute/gatk/commit/5e52fca813e57065713852d12f80a7599fcbc3ce) to make sure that would work - it eliminates a lot of redundant code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332381114:708,redund,redundant,708,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332381114,1,['redund'],['redundant']
Safety,"@jean-philippe-martin Thanks for adding the additional test, but by ""integration test"", I meant something that exercises an actual tool (which is why I mentioned SelectVariants) with a non-default provider, not another unit test that uses GCS. I suggested SelectVariants since I thought it would be easy:. > all the previous comments have been addressed with the exception of adding a SelectVariants integration test. It should be pretty easy to clone an existing case and change it use a non-default nio provider. I think this last test is redundant with the one you already added. My apologies if that was confusing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455668135:541,redund,redundant,541,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5378#issuecomment-455668135,1,['redund'],['redundant']
Safety,"@jean-philippe-martin We've found that with the current NIO retry settings, we're still exhausting all retries and failing ~1-2% of the time on large runs. This PR increases the number of retries from 3 to 20, which should trigger waits of several minutes rather than several seconds in the later retries. It also increases the timeout settings in `BucketUtils.setGenerousTimeouts()` by quite a bit, also with the goal of allowing waits in minutes rather than seconds when necessary. We'll try running with this and see what happens, but in the meantime please review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307404453:328,timeout,timeout,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3072#issuecomment-307404453,1,['timeout'],['timeout']
Safety,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:422,avoid,avoid,422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757,1,['avoid'],['avoid']
Safety,"@jjfarrell . After talking with @cmnbroad this afternoon, we'd like to ask you to perform an experiment to limit the scope where hunt down the issue. Is it possible for you to run `PrintReadsSpark` on the same cluster? That is, something similar to . ```bash; gatk --java-options ""-Djava.io.tmpdir=tmp"" \; PrintReadsSpark \; -R $REF \; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.cram \; -- \; --spark-runner SPARK \; --spark-master yarn \; --deploy-mode client \; --executor-memory 85G \; --driver-memory 30g \; --num-executors 40 \; --executor-cores 4 \; --conf spark.yarn.submit.waitAppCompletion=false \; --name ""$SAMPLE"" \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208:733,timeout,timeout,733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942#issuecomment-495357208,1,['timeout'],['timeout']
Safety,@jjfarrell Huh. I expected that sort of annoying delay from splitting on a cloud system but not on a hadoop one. Does running with splitting index avoid the delay?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371292714:147,avoid,avoid,147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506#issuecomment-371292714,1,['avoid'],['avoid']
Safety,"@jkobject @jnktsj @lydiarck We have a prospective fix for this issue that at least avoids the crash: https://github.com/broadinstitute/gatk/pull/7513. It should be part of the next GATK release, or you can try it out yourself if you're comfortable building the GATK from source.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-948791097:83,avoid,avoids,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-948791097,1,['avoid'],['avoids']
Safety,"@jkobject This problem has to do with indels and predicted protein change sequences. I'm starting a refactor of how the predicted protein changes get created. When that's complete, this issue will be fixed. In the meantime, can you post the stack trace and share the example workspace you mention in #6289 ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133:49,predict,predicted,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1181815133,2,['predict'],['predicted']
Safety,"@jonn-smith @LeeTL1220 The CNV test timeout was a temporary issue, but its been fixed. I'm pretty sure if you rebase on current master, it will go away.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4063#issuecomment-355795000:36,timeout,timeout,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4063#issuecomment-355795000,1,['timeout'],['timeout']
Safety,"@jonn-smith Does that happen in reality? That sounds like it would be a bug in the transcript data. If you detect that the transcript has no exons ""NA""?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5187#issuecomment-430779052:107,detect,detect,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5187#issuecomment-430779052,1,['detect'],['detect']
Safety,"@jonn-smith When you get a chance, would you mind quickly reviewing this `LocatableXsvFuncotationFactory` class for thread safety? The issue identified above could be legit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891237272:123,safe,safety,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891237272,1,['safe'],['safety']
Safety,"@kachulis In the past, htsjdk had bugs that resulted in bad index files. Those checks are an attempt to try to detect and reject such files, since they can result in subtle downstream problems. The 1 and -1 allowances are a compromise for common cases that are out-of-spec, but legitimate. So it's a compromise between being too defensive and too aggressive. A value like -2147483647 winds up getting rejected.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1095568749:111,detect,detect,111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7755#issuecomment-1095568749,1,['detect'],['detect']
Safety,"@kgururaj @ldgauthier I'd propose that we add defensive code to detect this, as @kgururaj proposed above. Maybe throw with a message identifying the name of the field and the issue ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-408081250:64,detect,detect,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-408081250,1,['detect'],['detect']
Safety,@ksw9 can you run the vcf validator tool suggested by @komalsrathi in #5045 [here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407476343)? The issue is primarily caused by mismatch in the field description in the VCF header and the data lines. @droazen can you comment on the [sanity check that I suggested here](https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407501684)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882:297,sanity check,sanity check,297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5113#issuecomment-413282882,1,['sanity check'],['sanity check']
Safety,"@kuangtianhui It looks like the bam index on `/home/wangh/kth/Mydata/NAM/NAM_7/Bam/7-99.sorted.markdup.bam` is older than the bam itself. This may indicate that the index is out-of-date with respect to the bam. I recommend that you reindex the bam using `samtools index`. The warning about the missing sequence dictionary is not a big deal, and you can safely process the VCF without a sequence dictionary in the header, provided you've manually confirmed that the VCF uses the same reference as your BAMs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7228#issuecomment-831444745:353,safe,safely,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7228#issuecomment-831444745,1,['safe'],['safely']
Safety,"@kvinter1 In future, it's a good idea to wait for tests to pass before merging, otherwise you risk the potential penalty of having to buy the team beer if test fail once it's in master. Doc changes are pretty low risk, but you never know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465:94,risk,risk,94,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465,2,['risk'],['risk']
Safety,"@lbergelson , 1 is not impossible, but it could turn out to be a bigger-than-expected project, because the bwa code, as I understand it, is accumulated through the years. For this specify error message we saw, the abort call is actually made by some low level code in bwa that once modified could throw away many other parts as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243288136:214,abort,abort,214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243288136,1,['abort'],['abort']
Safety,"@lbergelson @droazen @kgururaj ; 1. I was playing around with the test codes in GATK and did not push GenomicsDB tests in this PR. Will push it in the next commit.; 2. This is at the top of our discussion list for next week. GenomicsDB interfaces use these JSON files today which contain input configuration, list of samples, mapping between sample IDs and TileDB row indexes and stream ids for the input VCFs. If this tool takes the list of VCFs and intervals as input, we'd have to recreate JSON files internally and pass it to GenomicsDB. I wanted to avoid this for now as we are thinking about overhauling the input methodology completely in GenomicsDB with protocol buffers, but this is going to take a while. Also, we need to decide what's the best way to maintain the callset mappings.; 3. Will let you know asap. -Kushal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579:554,avoid,avoid,554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2389#issuecomment-277320579,1,['avoid'],['avoid']
Safety,@lbergelson @vruano @LeeTL1220 Would appreciate a quick review when you guys get a chance. (Want to avoid people wasting time on kebab-case updates to or conflicts with this code.),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3935#issuecomment-350770988:100,avoid,avoid,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3935#issuecomment-350770988,1,['avoid'],['avoid']
Safety,"@lbergelson I agree that your result is undesirable. In fact, it's hard to imagine when you'd want it to work that way. I think folks need to be able to specify:; 1. ""starts within"" the interval so that scatter/gather can work and not generate redundant variants, since a variant can only start in one of a set of non-overlapping intervals; 2. ""overlaps with"" so that when calling a subset of the genome (e.g. for capture experiments) we can output all variants that involve our regions of interest ; 3. ""contained within"" because I suspect there are times you want (e.g. in SelectVariants) to be able to find only variants wholly contained in the region you are interested in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-573089728:244,redund,redundant,244,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-573089728,1,['redund'],['redundant']
Safety,@lbergelson I'd prefer a targeted patch to the retry code in our fork as the lowest-risk option for now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448:84,risk,risk,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448,1,['risk'],['risk']
Safety,"@lbergelson If you query for output after you've terminated the process, the query will fail immediately because the Futures will have been completed with a CancellationException when the pipes were broken by the termination. But I think even that might be subject to a race condition. Previously we were dependent on stdout/stderr for synchronization and error detection, but with the ack fifo and the python exception handler installed, we really aren't anymore. We do need to fix https://github.com/broadinstitute/gatk/issues/5100, and have a better logging integration strategy, but in general I think we should seek to eliminate all use of stdout/stderr except for advisory purposes. On a separate tangent, what I'd really like to do is unify the two PythonExecutors into a single one. All of these features I'm adding like profiling, version checking, logging integration etc., will have to be done in both of them otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698:362,detect,detection,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5097#issuecomment-413575698,1,['detect'],['detection']
Safety,@lbergelson Not sure we should merge this until we solve the intermittent timeout issue in the docker tests.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-305921296:74,timeout,timeout,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804#issuecomment-305921296,1,['timeout'],['timeout']
Safety,"@lbergelson counter-proposal: since writing to a temp location in GCS would risk collisions if multiple people run the test, how about writing to JimFS instead? It's a RAM filesystem so each test machine gets its own, and it still requires the code to use the Path objects correctly since any conversion to File would fail. As a bonus, we do not incur Cloud charges and the test is much faster, so we can keep it as a unit test instead of an integration test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332078512:76,risk,risk,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-332078512,1,['risk'],['risk']
Safety,@lbergelson please take a look. I will use it in a future PR. Just trying to avoid large PRs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334855268:77,avoid,avoid,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3676#issuecomment-334855268,1,['avoid'],['avoid']
Safety,"@lbergelson you beat me because I was stuck trying to actually run a Picard tool in the integration test. (For future reference, that needs a workaround because the test running adds the ERROR level logging to all command lines and Barclay can't parse that for Picard tools for some reason.). The big reason I was using this instead of IntervalListTools is because the Picard version creates a terrible output file structure that I was having trouble capturing with a simple glob in WDL. I agree that the functionality here is largely redundant, but it was helping me get my workflow working faster at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196:535,redund,redundant,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435894196,2,['redund'],['redundant']
Safety,"@ldgauthier - Any thoughts on whether removing toString() from VariantAnnotation is safe - see comments above ? If not, we plan to remove it since there doesn't appear to be any code in GATK that relies on it (at least, no tests fail).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-775429828:84,safe,safe,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-775429828,1,['safe'],['safe']
Safety,"@ldgauthier ; hey Laura...; A new update regarding this ""topic""... ; As I said before (https://github.com/broadinstitute/gatk/pull/7725), when we were working with the ReblockGVCF from the snapshot you sent to us (https://console.cloud.google.com/gcr/images/broad-dsde-methods/US/gatk_subset_dragen_allele_frac@sha256:f5e93bda2278f1c999bd9def027c6851eeb098736b47a93469c524863b46c21f/details) and the JointGenotype pipeline (no Gnarly) using GATK 4.2.5, we could complete the analysis. As we received the instruction to use GATK lastest (4.2.6.1) we tried to run the entire pipeline using the latest one (since ReblockGVCF til JointGenotype)...Unfortunately, it seems that the latest ReblockGVCF hasn't the needed changes to work completely with the JoinGenotype Pipeline (without Gnarly), because we received the error below, once again. I think maybe we'll need to run Snapshop reblock with the newest GATK for the other steps. Please, let me know if you think it's safe to use this approach (snapshot Reblock - 4.2.3~, plus GATK 4.2.6.1 entire JG pipeline). Maybe this info can help regarding this open issue.. ```; 18:19:49.888 INFO GenomicsDBImport - Importing batch 1 with 50 samples; 18:19:53.652 erro NativeGenomicsDB - pid=15219 tid=15235 conflicting field description in the vid JSON and the VCF header of file: 20210421-006_stream; terminate called after throwing an instance of 'VCFAdapterException'; what():VCFAdapterException : Conflicting field length descriptors and/or field lengths in the vid JSON and VCF header for field LOD; Using GATK jar /gatk/gatk-package-4.2.6.1-local.jar; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1108926001:967,safe,safe,967,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7797#issuecomment-1108926001,1,['safe'],['safe']
Safety,@ldgauthier @davidbenjamin please take a look. . Don't allow the number of lines changed intimidate you... (most are in test resource files). The first commit contains the actual main code changes. . The second and third commits update the test resources (where most of the changed lines come from) and test code. . The very last commit changes the default radius to 2... I was planning to set it to 0 since it is more parsimonious (less complex configuration) but it may well affect sensitivity and certainly changes the PL/QUAL values so I guess set the value two the current 2 (for PLs) is a safer and more conservative approach until we evaluate what is the optimal value for this parameter. . Perhaps @davidbenjamin would like to have a different default for Mutec. This is last minute change and may break some of the integration test so bear with me if that is the case. However I think you can start reviewing the code at this point.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042:595,safe,safer,595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6055#issuecomment-516992042,1,['safe'],['safer']
Safety,"@ldgauthier @lucidtronix I'm updating this with a proposed list of CNNScoreVariants issues I think need to be resolved before we can remove the `@Beta` tag (actually is currently marked `@Experimental`). Let me know what you think:. - https://github.com/broadinstitute/gatk/issues/4538 (Python factoring/PEP-8/code review); - factor python args handling (minimally factor out the inference args); - there is only one 2D test, which I think has no reads overlapping any of the variants; - we should add a test that specifies one or more intervals; - the tool currently adds standard VQSR header lines via addVQSRStandardHeaderLines, which is unnecssary; - integrate read downsampling; - determine/handle the failure mode when the user supplies a mix (of mismatched) 1D/2D arch and weights inputs. Other (not necessarily blockers):; - establish all defaults (weights/arch/etc) in Java code; - default arch is 1D - should this change to 2D ?; - see if we can remove the artificially small inference/batch sizes (1) used in the tests. I think we added these due to timeouts which should no longer be an issue.; - remove the `newExpectations` code paths in integration tests",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4540#issuecomment-429074231:1061,timeout,timeouts,1061,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4540#issuecomment-429074231,1,['timeout'],['timeouts']
Safety,"@ldgauthier @yfarjoun We have an update on this! We've identified the bug:. * When `AbstractFeatureReader.getFeatureReader()` tries to open a `.vcf.gz` that doesn't have an index, it returns a `TribbleIndexedFeatureReader` instead of a `TabixFeatureReader`, because `methods.isTabix()` returns false when an index is not present.; * `TribbleIndexedFeatureReader`, in turn, opens a Java vanilla `GZIPInputStream`, instead of the `BlockCompressedInputStream` that gets opened when you create a `TabixFeatureReader`.; * `GZIPInputStream`, in turn, has a *confirmed bug* filed against it in Oracle's bug tracker (see https://bugs.java.com/bugdatabase/view_bug.do?bug_id=7036144#), that it inappropriately relies on the `available()` method to detect end-of-file, which is never safe to do given the contract of `available()`; * As the final piece in the ghastly puzzle, implementations of `SeekableStream` in htsjdk do not implement `available()` at all, instead using the default implementation which always returns 0. As a result of this combination of bugs in Java's `GZIPInputStream` itself and bugs in htsjdk's `SeekableStream` classes, end-of-file can be detected prematurely when within 26 bytes of the end of a block, due to the following code in `GZIPInputStream.readTrailer()`:. ```; if (this.in.available() > 0 || n > 26) {; ....; }; return true; // EOF; ```. Where `n` is the number of bytes left to inflate in the current block. The solution is to replace all usages of the bugged `GZIPInputStream` with `BlockCompressedInputStream` in tribble in htsjdk (at least, for points in the code where the input is known to be block-gzipped rather than regular gzipped). For due diligence we should also implement `available()` correctly for all implementations of `SeekableStream` in htsjdk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461:739,detect,detect,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4224#issuecomment-360282461,3,"['detect', 'safe']","['detect', 'detected', 'safe']"
Safety,"@ldgauthier I'm about to submit a bug fix PR. In the line you found the `.intersect(region)` should be `.intersect(region.getPaddedSpan())`. The intersection is to avoid a bug where the requested trimmed padded region is bigger than the original padded region, but `intersect(region)` causes it to lie within the original unpadded region, which is unnecessary and probably harmful to sensitivity (the trimming cigar didn't hurt sensitivity, but I wonder if this mistake may have offset a net benefit that it should have created). I replicated @jemunro's error in the branch, fixed it (of course), and wrote an equivalent regression test that fails before and passes after the PR. The rest is just the usual annoying updating of integration test files, which as of right now I'm in the middle of.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6495#issuecomment-599885755:164,avoid,avoid,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6495#issuecomment-599885755,1,['avoid'],['avoid']
Safety,@ldgauthier Should we patch `GenotypeGVCFs` to detect reblocked input and throw a `UserException` with an explanatory message?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-906617099:47,detect,detect,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7437#issuecomment-906617099,1,['detect'],['detect']
Safety,"@ldgauthier Thanks for looking into this! Below is the info requested. Let me know if you need anything else. . I am not so sure that it is such a rare case. I have attached a DP histogram of all the GQ=0 for chr19. There are many more than I expected to see. . For the one SNP rs429358, GQ=0 occurred in 1% of the samples. This happens to be one of the 2 SNPs that are used to determine the APOE genotypes for Alzheimers Disease risk. These are all APOE 33. . Sample 1 gVCF record:; `chr19 44908684 . T <NON_REF> . . END=44908688 GT:DP:GQ:MIN_DP:PL 0/0:44:0:41:0,0,1097`. Sample 1: vcf output via gvcf: ; `GT:AD:DP:GQ:PL 0/0:41,0:41:0:0,0,1097`. Sample 1: vcf output without gvcf; `GT:AD:DP:GQ:PL 0/0:37,0:37:99:0,111,1236`. Here is the IGV screenshot.; ![sample1](https://user-images.githubusercontent.com/1960717/49054868-bad15d80-f1c3-11e8-8059-d26e2beb21a2.png). This is a histogram of the DP for each GQ=0 genotype on chr19 for 5000 samples. ![gq0_dp](https://user-images.githubusercontent.com/1960717/49055161-a3df3b00-f1c4-11e8-9acb-904667f78d38.png)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441887865:430,risk,risk,430,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445#issuecomment-441887865,1,['risk'],['risk']
Safety,@ldgauthier Thanks for the comments! I made the changes you requested and deleted the redundant test. Let me know if you have any more edits in mind.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-415436791:86,redund,redundant,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5129#issuecomment-415436791,1,['redund'],['redundant']
Safety,"@ldgauthier They are use-at-your-own-risk, though in general we try to help users with problems. However, in this case the report is so old, and `ReadsPipelineSpark` has changed so much since then, that it's unclear whether this is still an issue with the current version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5481#issuecomment-592082423:37,risk,risk,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5481#issuecomment-592082423,1,['risk'],['risk']
Safety,@ldgauthier thank you so much for your detailed answer!. I have also prepared a ~20k shard file here:. https://github.com/EvanTheB/joint_call_shards. I just avoided known gene regions. Maybe this is pointless but feels a bit safer than splitting any-old-ware. Do you have any information about how the hg38 20k shards file was created?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486906387:157,avoid,avoided,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486906387,2,"['avoid', 'safe']","['avoided', 'safer']"
Safety,"@lucidtronix Specifically it would be useful to try this with various transfer/inference batch sizes now that the timeouts are gone. I tried with the original defaults (256/128), and got no timeouts, but the larger values seemed to result in longer runtimes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436:114,timeout,timeouts,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436,2,['timeout'],['timeouts']
Safety,"@lucidtronix Whats the motivation for wanting to keep it ? I think we really want to avoid it if at all possible, but if there is a reason its necessary let us know and we can discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4465#issuecomment-369244401:85,avoid,avoid,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4465#issuecomment-369244401,1,['avoid'],['avoid']
Safety,"@lucidtronix https://github.com/broadinstitute/gatk/pull/4218 is merged now so you should be able to rebase this on master. It looks like when you squashed you left in some of the timeout changes, so you'll have to resolve the resulting conflicts in favor of master.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4097#issuecomment-360524534:180,timeout,timeout,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4097#issuecomment-360524534,1,['timeout'],['timeout']
Safety,@lucidtronix looks like I got an intermittent 10-minute timeout failure in the CNN WDL test.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5307#issuecomment-430040420:56,timeout,timeout,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5307#issuecomment-430040420,1,['timeout'],['timeout']
Safety,"@magicDGS I think the context for this issue is the Pathogen sequence detection tools, which are Spark tools. `CountingReadFilter` isn't inherently reducible, and shouldn't be used in a Spark tool. Also, I can't say love the idea of using a filter as a control flow mechanism; but I don't have a better idea (maybe a separate tool as @mwalker174 mentioned above ?). One other note. As things currently stand, wrapping a WellFormedReadFilter in a counting filter (which we currently do for walkers) wouldn't give summary counts at the granular level, because WellFormedReadFilter is composed from non-CountingRead filters. Wrapping it yields a single, outer level summary/count. It would be easy enough to create one though that provided detailed summary/counts though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323738847:70,detect,detection,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323738847,1,['detect'],['detection']
Safety,"@magicDGS I'd strongly prefer not to introduce a read filter descriptor hierarchy if we can avoid it, as it will be tricky to get right, and add complexity. We definitely need to be able to extend the package list used by the descriptor to find plugins, but as you point out we'll be able to use the configuration mechanism for that. For before/after-analysis filters, I expect that we'll just add that directly to the existing plugin once we resolve https://github.com/broadinstitute/gatk/pull/2085 (which I hope to get to this week). I think the rest of the cases can be addressed by overriding makeReadFilter and providing custom behavior of filter merging. If this turns out to be something truly common, we could consider allowing the tool to inject an argument collection into the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2353#issuecomment-274970451,1,['avoid'],['avoid']
Safety,"@magicDGS That sounds perfectly fine to use #3614 . Regardless, I think you are safe to merge unless that quick test is worrisome.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332609705:80,safe,safe,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2321#issuecomment-332609705,1,['safe'],['safe']
Safety,"@magigDGS There are 4 separate issues in here, and I have slightly different feelings about each of them. Some comments:. - Removing the RNA string seems fine.; - I deliberately left the GATK test program group in because even though there is a Picard one, its harmless, and easier for people to find.; - I have reservations about exposing and sharing the super category maps. The Picard docgen process is pretty much unused and unmaintained at this point. The `getSuperCategoryMap` mthod really shouldn't be public, and it may even be removed in the near future. The GATK supercategory map ""truth"" should be defined by GATK.; - I'm reluctant to make the GATK `getSuperCategoryMap` public, because I don't think it can have any kind of useful contract. Its tied to the GATK doc templates and doc process, which we need to be able to change freely. It seems much safer for ReadTools to define it's own set categories (the overlap would probably pretty minimal I think - maybe just ReadFilters, right ?).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360832234:862,safe,safer,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4247#issuecomment-360832234,1,['safe'],['safer']
Safety,"@marchoeppner `MarkDuplicatesGATK` was removed because it had fallen out-of-date with respect to the version in Picard, and as an unmaintained tool was in our view not safe for use, and was causing confusion for our users. The loss of CRAM support is an unfortunate side effect of its removal. We've been doing a lot of work on our parallel version of `MarkDuplicates`, however, which is called `MarkDuplicatesSpark`. This version is fully up-to-date with respect to the Picard version, can run much faster than the Picard version when multiple cores or multiple machines are available, and will fully support CRAM in the future. CRAM support in that tool will come as a side effect of our migration to the new Disq library (https://github.com/disq-bio/disq), which is scheduled to happen within the next few months. In the meantime, I'd suggest continuing to request the Picard community to add CRAM support to their version. It's likely not a lot of work, and may simply require passing the reference through to the reader class, which could be a ~1 line change!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567:168,safe,safe,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5218#issuecomment-424882567,2,['safe'],['safe']
Safety,"@maxim-h Thank you for this follow-up information. Please note that if you have any pop-up blockers enabled or are browsing in ""incognito"" or ""safe mode,"" you may encounter these problems. Please ensure you have disabled any blockers and are trying to sign in through a regular browser. . I hope this helps! Please let me know if this leads you to success. Anthony",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332748705:143,safe,safe,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115#issuecomment-1332748705,1,['safe'],['safe']
Safety,"@mbabadi Not sure - we'll need to discuss how to do handle that. It looks like the docker image already has g++ installed, which is a start. Having said that, the docker image is getting pretty huge rapidly... BTW, do you know if there is way to programmatically query theano to determine whether it will compile the graph, vs running python/numpy ? I know it prints out a message, but it would be nice if we could detect that from theano directly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-349733375:415,detect,detect,415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3912#issuecomment-349733375,1,['detect'],['detect']
Safety,"@mcovarr @koncheto-broad Had a talk today with @KevinCLydon about this PR, and he made a convincing case that the pgen project is more likely to require additional stuff from the GVS branch than from gatk/master. To avoid a nightmare scenario of constantly having to request additional transplants from the GVS branch, I'm on board with the idea of having Kevin try to work off of the GVS branch for the pgen project for now, provided that we can get timely rebases of `ah_var_store` onto master if we need them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634847183:216,avoid,avoid,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8355#issuecomment-1634847183,1,['avoid'],['avoid']
Safety,"@mcovarr Huh. I'm surprised that didn't fail the docker build. I would have expected the command failing to abort it, but I guess that's not the case. . This step is not really necessary. If those files aren't checked out at build time gradle will do it for you, so that's why it didn't seem to cause any problems.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7806#issuecomment-1109042816:108,abort,abort,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7806#issuecomment-1109042816,1,['abort'],['abort']
Safety,"@mepowers to clarify, this is not a â€œlong readâ€, rather a short read with a long indel. For cases like this, Iâ€™d expect if the intel HMM fails, GATK should fall back to the Java one automatically. Or can you detect whatâ€™s â€œtoo longâ€ and use the Java version?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674247944:208,detect,detect,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674247944,1,['detect'],['detect']
Safety,"@mlathara As suggested, I removed all MNPs from normal vcf files and changed the bed file format and it worked. But it is not going beyond chromosome 1. Here are the stack trace:. 15:44:02.495 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/akansha/vivekruhela/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 16, 2021 3:44:02 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:44:02.750 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.750 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.9.0; 15:44:02.750 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:44:02.750 INFO GenomicsDBImport - Executing as akansha@sbilab on Linux v4.4.0-169-generic amd64; 15:44:02.751 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~16.04-b01; 15:44:02.751 INFO GenomicsDBImport - Start Date/Time: January 16, 2021 3:44:02 PM IST; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - ------------------------------------------------------------; 15:44:02.751 INFO GenomicsDBImport - HTSJDK Version: 2.23.0; 15:44:02.751 INFO GenomicsDBImport - Picard Version: 2.23.3; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:44:02.752 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:44:02.752 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:44:02.752 INFO GenomicsDBImport - Inflater: IntelInflater; 15:44:02.752 INFO GenomicsDBImport ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811:508,detect,detect,508,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7037#issuecomment-761558811,1,['detect'],['detect']
Safety,"@mlathara Our primary use case involves calling variants on a constantly growing large dataset of WGS and WXS data. As you probably realize, the CombineGVCFs/GenomicsDBImport step is incredibly time consuming, and scatter/gather is pretty much essential to make these operations work in any halfway reasonable period of time. I have off-hand heard people from the broad mention large WXS datasets, and keep in mind we're working mostly w/ WGS. Regarding processing: our main downstream use right now is GenotypeGVCFs, and yes we expect to run that scatter/gather as well. I agree that in principle we could maintain these data as a folder of workspaces. In fact that was my original plan before I realized the GenomicsDB workspace already is essentially a folder of per-contig folders. The reason I like the solution of copying around the folders is b/c our end product is in an official file format that tools understand how to use. . A related point, before we decided to try GenomicsDB, my plan was to create a scheme (""file format"") that would allow our code to better operate on a folder of per-contig CombinedGVCF file. I would probably have written out a top-level JSON file that served the same purpose as the JSON files in a GenomicsDB workspace. As noted above, GenomicsDB is essentially already doing this for me. To the question about usage and support: perhaps that ways to think about this would be interval-based split and merge tools for GenomicsDB workspaces? This would obscure the internal structure of the workspace from the user (even if they basically just to folder copying). The split tool should be really simple and not have many caveats. The merge tool could have a lot of limits on what kind of workspaces can or cannot be merged. Perhaps it could do sanity checking on the JSON files to make sure they're compatible, and then copy the folders into this new merged workspace?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868:1779,sanity check,sanity checking,1779,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-635338868,2,['sanity check'],['sanity checking']
Safety,"@mlathara Travis is totally borked. The timeout may or may not have to do with the fact that everyone is rerunning tests since they're failing sporadically. This looks good to me, so you can merge once tests pass. Also, if you send @droazen some notes on the new functionality to add to the release notes that would be a big help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519952186:40,timeout,timeout,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970#issuecomment-519952186,1,['timeout'],['timeout']
Safety,"@mlathara We were actually trying to craft a boolean expression to detect MNPs, not deletions. The one we came up with was: ref and alt are the same length, the length is > 1, and the number of mismatching bases is > 1. The last condition (mismatching bases > 1) is necessary to rule out padded SNPs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603299769:67,detect,detect,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6500#issuecomment-603299769,1,['detect'],['detect']
Safety,"@mlathara thanks, but one additional question. what occurs is --consolidate runs on a folder that contained additional, redundant/identical arrays? I ask b/c some of our jobs finished already on folders that I am fairly certain contained redundant arrays, but they completed w/o error. note: i think the second bullet in your answer above addresses this, saying they will be consolidated, but I would like to be absolutely certain.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722731714:120,redund,redundant,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722731714,2,['redund'],['redundant']
Safety,"@mwalker174 @asmirnov239 decided not to tackle this yet. There are a few options: 1) warn and/or filter out such contigs in PreprocessIntervals (not a fan of this), 2) filter in FilterIntervals, 3) warn/fail/filter a little earlier at DetermineGermlineContigPloidy (and also GermlineCNVCaller to be safe), 4) warn/filter at PostprocessGermlineCNVCalls, 5) fix the theano code to treat such contigs specially (haven't looked closely at it, but probably has something to do with patching the foward-backward code to handle such cases). Probably option 5 is the right answer, but only if the inferences for such single-interval contigs are at all meaningful. Otherwise I'm inclined to do option 2 and add some warnings/exceptions downstream. Might be other options as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-550352252:299,safe,safe,299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852#issuecomment-550352252,1,['safe'],['safe']
Safety,"@nalinigans OK, thanks. I'll try to get docker going w/ intellij. to the original question/bug: is there anything inherent about GenotypeGVCFs with GenomicsDB as the source vs. a gVCF as the source that one would expect to change how GATK determines the reference allele? I have not actually run this yet, but if I'm correct on the problem this probably should repo it (an addition to GenotypeGVCFsIntegrationTest):. ```. @Test(timeOut = 1000000); public void testGenotypeGVCFsWithGenomicsDbAndForceOutput() throws IOException {; final File input = CEUTRIO_20_21_GATK3_4_G_VCF;; SimpleInterval interval = new SimpleInterval(""20"", 1, 11_000_000);; final File tempGenomicsDB = GenomicsDBTestUtils.createTempGenomicsDB(input, interval);; final String genomicsDBUri = GenomicsDBTestUtils.makeGenomicsDBUri(tempGenomicsDB);. File expected = getTestFile(""CEUTrio.20.gatk3.7_30_ga4f720357.expected.vcf"");; String reference = b37_reference_20_21;. runGenotypeGVCFSAndAssertSomething(genomicsDBUri, expected, Arrays.asList(""--"" + GenotypeGVCFs.FORCE_OUTPUT_INTERVALS_NAME, ""20:10-20""), GenotypeGVCFsIntegrationTest::assertVariantsContextsHaveNonAmgibuousRefs, reference);; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754120858:428,timeOut,timeOut,428,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7005#issuecomment-754120858,1,['timeOut'],['timeOut']
Safety,"@nalinigans the set of jobs was ultimately able to work. While our issues with duplication and bad fragments are most likely our fault, any future improvements to GenomicsDbImport to detect and/or fix these would be useful. Thanks for all your help on this front.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-733064796:183,detect,detect,183,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-733064796,1,['detect'],['detect']
Safety,"@nh13 As a first step I'd suggest filing a ticket against the GKL (https://github.com/Intel-HLS/GKL) so that Intel engineers can have a look (but leave this GATK ticket open so that we can track it here as well). . This will be difficult to debug without a test case that Intel can run to reproduce the issue on their end. Does the crash only occur with this one particular sample, or have you seen it on more than one sample? If you could get to the point where you can reproduce it on a shareable bam snippet, that would obviously maximize the chances of this getting fixed. Intel is currently (at our request) doing a pass on the GKL with `valgrind` to find and fix memory safety issues (https://github.com/Intel-HLS/GKL/issues/107), so we expect the next GKL release to fix a bunch of ""use after free""-type errors. Maybe they'll get lucky and fix this one as well. Timeline for the release is within the next ~2-3 months. . After that we've asked them to test the GKL with long reads data, which is also known to trigger crashes like this (https://github.com/Intel-HLS/GKL/issues/105). If the problem in your case is that you've exceeded some hardcoded length limitation, the tests on long reads data might reveal the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667189113:676,safe,safety,676,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-667189113,1,['safe'],['safety']
Safety,"@nh13 Thank you for clarifying. As of today GKL does not auto-detect when there's a read that's ""too long,"" ie a read length we haven't validated with GKL. We should be able to build that into our pending release. I agree we should also make sure that if the GKL pairHMM fails, the JAVA version is called instead. @Kmannth @droazen let's discuss this in our next sync.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782:62,detect,detect,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6733#issuecomment-674250782,1,['detect'],['detect']
Safety,@nh13 Thank you for your question. I agree that it is particularly painful to lower the mapping quality currently since there are two seperate arguments `--mapping-quality-threshold` and `--minimum-mapping-quality`. In order to lower the threshold they must BOTH be set otherwise nothing will change and low MQ reads will not be handled. This is necessary due to the order in which Barclay parses fields and the way this interacts with the read filte. We either need flags to be able to fill multiple fields in different places or to use reflection to extract the mapping quality threshold to be used for the assembly engine based on the read filters used after the fact which runs into issues if the user has disabled the mapping quality filter. These both still fail to capture the case where you want to explicitly use a different threshold for active region detection and for calling. . Would it be helpful if the new argument were renamed to something like `--mapping-quality-threshold-for-genotyping`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7034#issuecomment-758852673:862,detect,detection,862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7034#issuecomment-758852673,1,['detect'],['detection']
Safety,"@owensgl Sorry you're running into problems. We typically speed up the process by running multiple GenotypeGVCF processes in parallel, subsetting by genomic intervals. GenotypeGVCFs isn't really multicore, you'll probably be best off giving each process 1 or 2 cores. (You'll see better performance with 2 since java has parallel garbage collection, but it might be more cost effective to run twice as many slower processes...) If you do that you'll want to run with `--only-output-calls-starting-in-intervals` enabled in order to avoid problems on the edges of intervals. . Things tend to bog down with many highly multi-allelic sites. If you have a population with very high diversity you may be hitting lots of sites like that. I'm not sure why it's as slow as you say it is though. It should be faster than 800bp / 30 minutes even with old qual. If you could provide a subset of your data we might be able to profile and see if there's some pathological case we're not handling well. I believe new-qual handles multi-allelic sites more efficiently which I suspect is why it's going faster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994:531,avoid,avoid,531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161#issuecomment-358054994,1,['avoid'],['avoid']
Safety,"@pieterlukasse Yeah - I'm planning on updating some of the Funcotator core to be more permissive for input data types and to fix a few long-standing bugs, but have been unable to do so because of other high-priority tasks (as @lbergelson said). The output formats are pretty well-established, so I don't think there's any risk in writing an additional parser. However if you simply want to view the outputs, you can render the annotations in `MAF` format and that will produce a `MAF` (TSV) file that is much more easily viewed / parsed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376:322,risk,risk,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1379018376,2,['risk'],['risk']
Safety,"@ronlevine I know this a port from gatk3, but I think theres a bit of refactoring that can be done. It seems like it's more complicated than it needs to be. Could you take a look and see? In particular I'm not sure why things get converted to a bitset, it looks like you should just be able to derive the indecies directly and avoid creating a bitset. If I'm missing some detail and it can't be simplified let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049:327,avoid,avoid,327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852#issuecomment-242102049,1,['avoid'],['avoid']
Safety,"@ronlevine just pointed out this is redundant w/ another issue (see above). for that issue you guys requested I write a unit test, which I did this morning. Feel free to apply that or not. It's attached to that thread as a patch (sorry, dont have a good local enlistment right now). It's nothing special, but tests are rarely a bad thing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3252#issuecomment-314527518:36,redund,redundant,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3252#issuecomment-314527518,1,['redund'],['redundant']
Safety,"@rsasch my thought was to keep it around for now, but in the future we could add a step that removes it once we know the samples are safely loaded. It could also be that we get rid of the ""is_loaded' flag in sample_info instead since this data is more detailedâ€¦",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451:133,safe,safely,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7573#issuecomment-983938451,1,['safe'],['safely']
Safety,"@samuelklee DRAGEN STRE model doesn't actually make any alterations to the smith waterman parameters or how they work, it just works by adjusting the indel gap penalties that are used for the PairHMM. At one point we were concerned about SW parameters being different with dragen but as it turns out the biggest visible effect of the SW parameters on the output (the alignment we perform after haplotypes discovery) is irrelevant since they don't realign their reads internally. We kept the default gatk alignment behavior and thus the SW parameters that are used (for dangling head recovery which I believe are the old arguments) still match. As far as unifying the parameters I suspect it could be done though one wonders if there aren't risks where the different contexts in which we use the parameters will not perform as well with a unified set. Speculation on my part though. I agree with David that we should be cautious about making changes that will affect the HaplotypeCaller before November. . I support including an argument in any case (possibly multiple) to include the SW parameters. I would actually advocate we read these files in as tables of parameters where you simply point to on the command line to configure new parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993:583,recover,recovery,583,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6863#issuecomment-705611993,4,"['recover', 'risk']","['recovery', 'risks']"
Safety,"@samuelklee I think it's right for what we're doing. We mount the test data as `/testdata` and then create a symlink from src/test/resources to /testdata to provide it to the test files. It seems to work.; ; I'm not clear what they get more of the other way around. More tests? Are they using our create docker script? Or our travis file? Or something else? I think we might just be able to just directly mount test data to src/test/resources and avoid the symlink, but I probably had a reason when I set it up that way... I think this is a non-issue unless they can provide more information.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156:447,avoid,avoid,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3730#issuecomment-339439156,2,['avoid'],['avoid']
Safety,"@shuaiwang2 Hi, we don't currently support indexes that long. We use a bai index for bams and tabix for vcf which only support up to 512 M. You need to use a CSI index for references that large but we don't support writing those. (Reading them is weird, I think we can read BAM csi indexes but not VCF ones). . It might be possible to work around this issue by setting `--create-output-variant-index false`, although downstream gatk tools would need an index if you're sharding them. Otherwise I recommend splitting your chromosomes into two separate parts and calling on the split chromosomes. Splitting along a long region of N's should be a safe way to avoid missing any useful calls. (The telemere might be a good spot unless you have a T2T reference.). . We should probably improve that error message to make it clear what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609:644,safe,safe,644,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8192#issuecomment-1422828609,4,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"@sooheelee By the way, can we avoid using ""ACNV""? Let's just call this tool ModelSegments to distinguish it from the old GATK CNV -> GATK ACNV pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158:30,avoid,avoid,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158,1,['avoid'],['avoid']
Safety,"@sooheelee Do we have a known case in GATK4? I thought we had some code to avoid it but when I recently looked I couldn't find it. So perhaps it was a figment of my imagination, in which case it's certainly an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-350088492:75,avoid,avoid,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-350088492,1,['avoid'],['avoid']
Safety,"@sooheelee I can't speak for CNV, but there isn't any general reason to prefer Picard interval lists in GATK. There was previously an issue with parsing interval queries that used contig names that contained "":"", but thats fixed now. The only time we prefer a Picard list is the theoretical case were you use a query interval against a sequence dictionary that contains contigs that make that query ambiguous (hg38 is not one of those). GATK will detect and reject such a query and suggest using a Picard interval file to disambiguate it. @magicDGS I'm not sure how/if writing tests against existing files in the repository will be useful. I want to restate that we don't want to take ports of these tools if they're marked `@Experimental `or `@Beta` because they haven't been validated, or don't have good test coverage. We need to find a way need to have valid tests so they'll be production ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161:447,detect,detect,447,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-371528161,1,['detect'],['detect']
Safety,"@sooheelee I don't see, at least not immediately, how artifacts would be more of a risk than they are already. If anything I could see this making it a bit harder for false positives to get by.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889:83,risk,risk,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889,1,['risk'],['risk']
Safety,"@sooheelee I have a suggestion regarding categories. Can we change ""Contamination"" to ""Metagenomics"" and perhaps move the ""CalculateContamination"" tool to the ""Diagnostics and Quality Control"" category? . IMO, contamination has a connotation of introducing foreign matter unintentionally. Strictly speaking, PathSeq is not just for detecting sample contaminants but also endogenous organisms in various biological sample types (like stool or saliva). I think users with metagenomic data might overlook this if they are labeled as being for ""contamination.""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349089820:332,detect,detecting,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3853#issuecomment-349089820,1,['detect'],['detecting']
Safety,"@sooheelee I think this was answered above by @ldgauthier and @kgururaj. The warnings are safe to ignore for best-practices workflows, as @kgururaj mentioned above in https://github.com/broadinstitute/gatk/issues/2689#issuecomment-370295035. And as @ldgauthier explained in https://github.com/broadinstitute/gatk/issues/2689#issuecomment-371500366, the annotations in question either get recalculated in `GenotypeGVCFs` or are obsolete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-431168544:90,safe,safe,90,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2689#issuecomment-431168544,1,['safe'],['safe']
Safety,"@sooheelee That's a pitfall we haven't solved yet. I think we'll eventually infer the value locally from the AC of nearby variants in the germline resource. Until then, I would use the smaller value i.e. the one for coding regions. The only possible harm would be a very few rare germline events that aren't in gnomAD, whereas if you set it too high you risk filtering true somatic events.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4366#issuecomment-363851445:354,risk,risk,354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4366#issuecomment-363851445,1,['risk'],['risk']
Safety,"@sooheelee We plan to have internal pilots running by then but haven't discussed releasing to the public in an beta or full release yet. We will have some unsupported WDL pipelines and light documentation available for external users, and we can provide benchmarking stats courtesy of our collaborators. It's probably safer to tag the tool as 'alpha' or 'beta' for the January release, though. We should be ready for full release sometime in the first half of next year, I expect.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769#issuecomment-341538906:318,safe,safer,318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769#issuecomment-341538906,1,['safe'],['safer']
Safety,@takutosato The one test failure occurs because when there is low normal coverage germline risk might get triggered too often. I might need to adjust the default value but it won't affect the code otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595:91,risk,risk,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595,1,['risk'],['risk']
Safety,"@tedsharpe Thanks for checking. In general I've seen the CPB tends to help a lot when reading through long contiguous stretches of BAM file and less when doing anything on smaller or fragmented data. I'm surprised it didn't make any difference here, but seems like it doesn't so that's fine. I've seen catastrophic interactions between insufficiently buffered index inputs with it disabled where it ended up performing an http request for every byte, but hopefully that's avoided just by using a buffered reader for it. . I have a plan to someday enable the more intelligent -L aware prefetcher that will use the list of actual positions of interest to buffer more intelligently, but that's not happening on any specific schedule.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320:472,avoid,avoided,472,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1358245320,1,['avoid'],['avoided']
Safety,@tomwhite I ran your branch manually on gcs and get a new error which I believe is a GCS NIO bug that we discussed in https://github.com/samtools/htsjdk/pull/724. . ```; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDD,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337:362,abort,abortStage,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-285797337,3,['abort'],['abortStage']
Safety,"@tomwhite If you can address the one issue I had with the classes, it could also be accomplished by sterner commenting on the relevant methods and classes just so long as we are avoiding confusion somehow, then I think we can try to get this in quickly for Wednesdays release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-426012345:178,avoid,avoiding,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5127#issuecomment-426012345,1,['avoid'],['avoiding']
Safety,"@tomwhite To clarify, I think that the caller of `ensureCapacity()`, namely `GenotypeLikelihoodCalculators.calculateGenotypeCountUsingTables()`, also needs to be synchronized in order to avoid some unlikely but still-possible races. Given this, I think that we should consider whether `ThreadLocal` might be a better option here. It's not 100% clear to me whether a `ThreadLocal` `get()` call is cheaper than a synchronized method call, but some casual googling suggests that it might be. If we're going to end up entering a synchronized method on every single call to `GenotypeLikelihoodCalculators.getInstance()`, we might want to do some research into whether `ThreadLocal` + no synchronization would be faster, since I believe that this is a performance-sensitive section of code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244:187,avoid,avoid,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422171244,2,['avoid'],['avoid']
Safety,"@tomwhite We just sat down and had a look at this class. @droazen was suggesting that this might still be unsafe, and that the outer layer where we compute the genotype likelihood should either be synchronized as well or the whole thing should be wrapped in a thread local object",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422154723:106,unsafe,unsafe,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5071#issuecomment-422154723,1,['unsafe'],['unsafe']
Safety,"@vdauwera I didn't even know that README existed. It's horribly out of date in a lot of ways. @LeeTL1220 Before I spend time fixing it, is there any way I could drastically shrink this file, like to 3 lines or so, or delete it entirely? I mean, most of it is redundant if you just read the WDL, and I really don't want to add maintenance of this README on top of Terra and the two official M2 wdls we still have to drag around.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484355457:259,redund,redundant,259,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484355457,1,['redund'],['redundant']
Safety,@vdauwera That sounds like a pretty risky change to be making right when we're trying to tie-out our version of HaplotypeCaller. Perhaps this would be better done incrementally.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267110437:36,risk,risky,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2314#issuecomment-267110437,1,['risk'],['risky']
Safety,"@vdauwera Well, the full truth is that the M2 WDL tests are experiencing intermittent failures at the moment due to out-of-disk-space issues on travis. You can safely merge this if only the M2 WDL tests failed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311502730:160,safe,safely,160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3167#issuecomment-311502730,1,['safe'],['safely']
Safety,"@vruano The AF calculator is not where the alleles are removed. It calculates an honest probability that each exists. For example, if we have two alts and PLs are `(999,0,999,0,999,999)` -- that is, it's definitely a het, and we have no clue which alt allele it is, then it correctly says that the posterior probability of each alt allele is 0.5. Now, this probability of 0.5 is quite low for a real variant (below the default confidence threshold). `calculateOutputAlleleSubset` sees that and evaluates each allele independently, thus dropping both, but that's a consequence of relying on marginal probabilities. which this example suggests we ought not to do. The logic in `Mutect2` is different and avoids this issue. It does model comparison to see if we can remove alt alleles one at a time, starting with the alt allele with the worst marginal likelihood. Thus, it drops one alt allele, because there is only a tiny likelihood cost since the other alt allele can explain the data, but does not drop a second alt allele since then half the reads wouldn't fit. This is exactly what Yossi proposed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6364#issuecomment-576775674:702,avoid,avoids,702,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6364#issuecomment-576775674,1,['avoid'],['avoids']
Safety,"@vruano The HGDP crams also trigger this error. . ftp:/Â­/Â­ftp.Â­1000genomes.Â­ebi.Â­ac.Â­uk/Â­vol1/Â­ftp/Â­data_collections/Â­HGDP/Â­data/Â­Brahui/Â­HGDP00001/Â­alignment/Â­HGDP00001.Â­alt_bwamem_GRCh38DH.Â­20181023.Â­Brahui.Â­cram. ```; 14:32:34.745 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 17, 2021 2:32:34 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:32:34.877 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.877 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:32:34.877 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:32:34.877 INFO CalibrateDragstrModel - Executing as farrell@scc-gh3.scc.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 14:32:34.877 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_172-b11; 14:32:34.877 INFO CalibrateDragstrModel - Start Date/Time: April 17, 2021 2:32:34 PM EDT; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - ------------------------------------------------------------; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Version: 2.24.0; 14:32:34.878 INFO CalibrateDragstrModel - Picard Version: 2.25.0; 14:32:34.878 INFO CalibrateDragstrModel - Built for Spark Version: 2.4.5; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:32:34.878 INFO CalibrateDragstrModel - HTSJDK Defaults.USE_ASYNC_IO",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394:548,detect,detect,548,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182#issuecomment-821876394,1,['detect'],['detect']
Safety,"@xysj1989 I would think that if you use the python `gatk` launch script, prefaced immediately by `THEANORC` as above, you should be able to tie each GermlineCNVCaller run to a separate compilation directory even if you donâ€™t have control over which nodes you are running on. Increasing the timeout means that different runs will not be able to compile models at the same time, which will add some overhead; however, I think setting separate directories avoids this. In any case, I will try to issue a PR allowing you to directly set the directory or use a temporary one soon. Thanks again for raising the issue!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548587855:290,timeout,timeout,290,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548587855,2,"['avoid', 'timeout']","['avoids', 'timeout']"
Safety,A UserException when you intersect two interval files and get nothing seems like the behavior we want in almost every case. I'm hesitant to add a global override to avoid it. It does seem like working around this in wdl would be a big pain though.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540746640:165,avoid,avoid,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540746640,1,['avoid'],['avoid']
Safety,A command line that triggered the crash in our testing: ; ```; gatk HaplotypeCaller -R /cromwell_root/gcp-public-data--broad-references/hg38/v0/dragen_reference/Homo_sapiens_assembly38_masked.fasta -I gs://broad-dsde-methods-dragen/reprocessed_data_v3.7.5_masked/CH1_CHM13_WGS1/CH1_CHM13_WGS1.bam -L /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/109scattered.interval_list -O CH1_CHM13_WGS1.vcf.gz --pileup-detection --pileup-detection-absolute-alt-depth 0 --pileup-detection-bad-read-tolerance 0.4 --pileup-detection-enable-indel-pileup-calling --pileup-detection-active-region-phred-threshold 3.0 --use-pdhmm -contamination 0.0 -G StandardAnnotation -G StandardHCAnnotation --dragen-mode --disable-spanning-event-genotyping --dragstr-params-path /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-DragstrAutoCalibration/CH1_CHM13_WGS1.bam.dragstr -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90; ```. `--dragstr-params-path /cromwell_root/fc-971fd540-210c-4e5a-87ce-d3f8c91c7557/submissions/0586864c-d263-4797-89f1-b517e487ad2a/VariantCalling/b177671c-e96b-433e-824a-f9d641184e75/call-DragstrAutoCalibration/CH1_CHM13_WGS1.bam.dragstr` is likely optional. `-L` is optional and any interval will likely work to reproduce the error.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8712#issuecomment-2034726852:580,detect,detection,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8712#issuecomment-2034726852,5,['detect'],"['detection', 'detection-absolute-alt-depth', 'detection-active-region-phred-threshold', 'detection-bad-read-tolerance', 'detection-enable-indel-pileup-calling']"
Safety,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:51,detect,detect,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497,1,['detect'],['detect']
Safety,"A quick look at the code results in the following finding:. The error message : `""SA-BWT inconsistency: seq_len is not the same.""` is generated in function `bwt_restore_sa()` defined in `bwt.c`, and resulted in an `abort()` call.; `bwt_restore_sa()` itself was called in `bwa_idx_load_bwt()` defined in `bwa.c`, when trying to restore the suffix array from a file with extension "".sa"". This function call is issued when bwa mem (in its main) tries to load the reference information. This happens before input are read. Because of the `abort()` call, letting the `BwaMem` class handle the error is difficult, so I would propose two possible solutions:; 1. changing the behavior of `BwaIndex` class, so that it eagerly loads the reference, and throws an `Java.lang.IOException` when this error is encountered. Of course this must lead to code duplication, i.e. copying a large part of index loading code from bwa itself and walk around`abort()`ing.; 2. ask @lh3 to change the behavior so that it will not `abort()` any more, but return a null pointer. But the null pointer return error covers so many error cases that this might not be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189:215,abort,abort,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-243181189,4,['abort'],['abort']
Safety,"A reminder: we can also make the PoN optional. One thought: if users want to run ModelSegments for germline, what should they do? Presumably they could run the pair WDL in ""tumor-only"" mode and just use the normal as input. So maybe we should change all instances of ""tumor"" to ""case"" and ""normal"" to ""matched_normal""? Or is this still too confusing? I'd like to avoid having separate case and pair WDLs if possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362693835:363,avoid,avoid,363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3983#issuecomment-362693835,1,['avoid'],['avoid']
Safety,"A single site changed by a small amount in CNN score which was expected. (We weren't 100% sure we were testing any sites where downsampling would kick in, so this is good from a test coverage perspective. I think this is safe to include. I'm updating the test file and will push once tests pass locally.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458765209:221,safe,safe,221,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5622#issuecomment-458765209,1,['safe'],['safe']
Safety,"AM per task. 768 tasks (16 nodes) in total.; ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:18:25 02:00:00 1064GB 1064GB 3072GB 768; ```; - Jobs eventually finish if not running out of allocated time.; - Takes a long time to begin processing the first set of variants.; ```; 13:51:37.925 INFO GenotypeGVCFs - ------------------------------------------------------------; 13:51:39.736 INFO GenotypeGVCFs - Done initializing engine; 13:51:39.923 INFO ProgressMeter - Starting traversal; 13:51:39.923 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:23:57.323 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chr17:18363145 the annotation AS_RAW_MQ=64800.000|50400.000|0.000 was not a numerical value and was ignored; 14:23:57.346 WARN ReferenceConfidenceVariantContextMerger - Reducible annotation 'AS_RAW_MQ' detected, add -G Standard -G AS_Standard to the command to annotate in the final VC with this annotation.; 14:23:58.180 INFO ProgressMeter - chr17:18363854 32.3 1000 31.0; 14:24:13.258 INFO ProgressMeter - chr17:18376854 32.6 14000 430.0; 14:24:58.358 INFO ProgressMeter - chr17:18382854 33.3 20000 600.5; 14:32:49.287 INFO ProgressMeter - chr17:18393855 41.2 31000 753.2; 14:33:39.240 INFO ProgressMeter - chr17:18405856 42.0 43000 1024.1; 14:33:49.493 INFO ProgressMeter - chr17:18411856 42.2 49000 1162.3; 14:34:17.285 INFO ProgressMeter - chr17:18425856 42.6 63000 1478.1; ```. CPU utilisation does not improve after the variants begin processing after half an hour preparing traversal. ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open&run; 105581211 R ds6924 hm82 genotype 4 00:42:34 02:00:00 1200GB 1200GB 3072GB 768; ```. - Excellent CPU efficiency if running serially (but defeats the purpose of a H.P.C. with Lustre). ```; %CPU WallTime Time Lim RSS mem memlim cpus; normal-exe = open",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089:1355,detect,detected,1355,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8637#issuecomment-1879551089,1,['detect'],['detected']
Safety,"According to this paper https://www.nature.com/articles/s41467-018-03590-5. it is: ""Each resulting qualified captured library with the SureSelect Human; All Exon kit (Aglient) was then loaded on *BGISEQ-5000 *sequencing; platforms, and we performed high-throughput sequencing for each captured; library. High-quality reads were aligned to the human reference genome; (GRCh37) using the Burrows-Wheeler Aligner (BWA v0.7.15) software. All; genomic variations, including single-nucleotide polymorphisms and InDels; were detected by *HaplotypeCaller of GATK *(v3.0.0).; "". On Wed, Dec 12, 2018 at 3:18 PM Louis Bergelson <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Do you know if BGI's sequencing; > is compatible with our tools without any special treatment?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446729153>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0mujY7YzxUJ-6IPU8B7jPiZWQuzMks5u4WR3gaJpZM4ZQNxZ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514:518,detect,detected,518,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5517#issuecomment-446769514,1,['detect'],['detected']
Safety,"Additional feedback from the user for the mutect2 workflow. > ""Of note, it is really difficult and not really 'user-friendly' to have to predict disc space and runtime for Funcotator, which seem to depend (based on calculations you copied above from other Functotator workflows) on outputs of Mutect2 (eg vcf sizes), when here Mutect and Funcotator and bundled together. So I cannot see output of Mutect to predict values for Funcotator - especially not when I get to run this over hundreds of samples. It is also pricey to have jobs failing because of this. It would be much better to have these variables encoded, so that the algorithm uses Mutect outputs to predict memory etc. that it will need to run Funcotator downstream. If this is really how things work (and this is my current understanding), I really do not know how to estimate this for many samples without 'trial and error' that is both costly and it will take extremely long time....""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230:137,predict,predict,137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6680#issuecomment-651354230,6,['predict'],['predict']
Safety,"After extensive discussion, Chris and I are leaning towards taking out PID and PGT, since they're now redundant with the spec-compliant PS and and phased GT (and less accurate in some cases). @davidbenjamin given that the phasing code is shared between HC and M2, do you think this would be a serious problem for Mutect2 users?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6432#issuecomment-705706545:102,redund,redundant,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6432#issuecomment-705706545,1,['redund'],['redundant']
Safety,"Agreed that it would be nice if we abstract away the manner in which import tasks get distributed across multiple nodes. At this point, we're not considering adding a tool that supports that. Sure, we could probably look at Spark or something, but it ; a) is a heavier lift ; b) would probably involve assumptions about user infrastructure. I'm reluctant to support generic split/merge because then we'd be tied to this notion of a meta-workspace (or workspace of workspaces), and how to cleanly track/manipulate those. I'd like to avoid that potential can of worms. Of course, users are free to work with it themselves. . The steps 1-5 you outline match roughly what we have in mind, though there would be a step 0 for initializing the workspace. We haven't decided yet if each independent import job would specify the intervals, or have all the intervals to be imported specified in the initialize phase and then the independent import jobs select intervals by specifying a range of indices/ranks corresponding to those intervals. . Doing the latter would piggyback a bit on how incremental import is currently done. It would also better support the multiple contigs in a single folder mode I mentioned in the last comment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641525195:532,avoid,avoid,532,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6620#issuecomment-641525195,1,['avoid'],['avoid']
Safety,"Ah, so the `ml.dmlc` dependency is the real xgboost library and the tool for predicting from trained models? Got it, thanks for the clarification.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189379178:77,predict,predicting,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189379178,1,['predict'],['predicting']
Safety,"Ah, yes, that's kind of confusing actually... ; The **shadow** jar includes a copy of spark and all of it's dependencies, so if you want to run spark tools locally you can use the shadow jar. If you want to use an existing spark cluster, which may have slightly different versions of spark/spark's dependencies then you need to use the **spark** jar. The spark jar doesn't include it's own copy of spark and expects that the spark cluster will provide the necessary dependencies. This avoids conflicts between different dependency versions. . You don't need to use `gatk-launch` ever, but it can make it easier if you want to potentially run your code in different environments. It knows about 3 different potential ways to invoke spark, 1) running in local mode with --sparkMaster local, 2) running on a cluster using spark-submit and 3) running on a google dataproc cluster using gcloud. Gatk-launch knows which environment needs which jar and will prompt you to create one if you don't have it. . gatk-launch also applies some default arguments when running on spark, you may have to supply them yourself if you're not using it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273307091:485,avoid,avoids,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273307091,1,['avoid'],['avoids']
Safety,"Aha, one interesting wrinkle that came when trying to remove `getReferenceFile()` entirely in favor of `getReferencePath()` : It is valid to have references of the form `gg://foobar` (referencing the Google Genomics Reference API). You can apparently put that string in a `File` just fine, but a `Path` will try to find the matching provider, and will fail since there is no NIO provider for ""gg"" (it doesn't behave like a filesystem). The work around is to use getReferenceFileName(). The risk is that some places that allowed a relative path (relying on the call to getAbsolutePath) may now require an absolute path.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349825595:490,risk,risk,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3921#issuecomment-349825595,1,['risk'],['risk']
Safety,"All comments addressed. Since tests on travis are broken at the moment, we're unfortunately forced to merge this without travis passing, however since only documentation is touched this should be safe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310772135:196,safe,safe,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158#issuecomment-310772135,1,['safe'],['safe']
Safety,"Also - you asked about workarounds. The attached is not pretty, but it is a minimal way to let tools like VariantEval provide their old behavior. It essentially adds a method in FeatureInput to check whether the name matches the default value it would use. Upstream code can call FeautreInput.isUserSuppliedName() and act accordingly. Rather than use the scheme I do here with string matching, we could make it more explicit in ParsedArgument and actually pass a boolean it we detected one in the parsed argument value. Not necessarily advocating this as a production solution, just as one way to make it work; [FeatureInputWorkaround.patch.txt](https://github.com/broadinstitute/gatk/files/1745946/FeatureInputWorkaround.patch.txt); .",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4426#issuecomment-367498695:477,detect,detected,477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4426#issuecomment-367498695,1,['detect'],['detected']
Safety,"Also intermittent segfaults in the Java 11 unit tests:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f946d77f0f2, pid=7513, tid=7540; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.7513); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid7513.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546:89,detect,detected,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-602077546,1,['detect'],['detected']
Safety,Also some tests for collection classes. Refactoring may avoid code duplication here.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910:56,avoid,avoid,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3916#issuecomment-352080910,1,['avoid'],['avoid']
Safety,"Also, the current process risks people dropping a bunch of extraneous files into the image (such as irrelevant build artifacts) that slow down the build process and make the image itself larger.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2700#issuecomment-300582980:26,risk,risks,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2700#issuecomment-300582980,1,['risk'],['risks']
Safety,"An update/unwanted solve:; Caveat: I'm sorry I didn't provide more information about this issue beforehand, in fact, I create the vcf file by calling:. ```; gatk SelectVariants \; -R REF.fasta \; -V allsites.allsamples.vcf \; -O sample.vcf \; --remove-unused-alternates \; --exclude-filtered \; --sample-name ""sample"" \; -select 'vc.getGenotype(""sample"").hasAD() && vc.isVariant() && (vc.getGenotype(""sample"").getAD().1 > 2.0)'; ```. (some of the select checks might be redundant but I wanted to be on the safe side); **The error doesn't happen when I remove ""--remove-unused-alternates""**; This is very unfortunate as I need that to force the sites to be biallelic and to filter my variants and I will have to find a workaround. Does it happen because it's looking for an alternate that's no longer there? Could it be a dictionary mismatch/confusion?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904202463:470,redund,redundant,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904202463,2,"['redund', 'safe']","['redundant', 'safe']"
Safety,"And @hanalangoallen and @mlaylwar what ref and alt alleles are you seeing when the error happens? Are they also cases with redundant equal bases at the end of each allele that, if properly trimmed, would yield a SNP?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-690829403:123,redund,redundant,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6473#issuecomment-690829403,1,['redund'],['redundant']
Safety,"Apparently related, just running IndexFeatureFile on my machine results in several stack traces:; ```; Sep 21, 2017 4:10:53 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Host is down (connect failed); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138); 	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032); 	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966); 	at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); 	at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredentials.java:176); 	at shaded.cloud_nio.com.google.auth.oauth2.DefaultCredentialsProvider.tryGetComputeCredentials(DefaultCredentialsProvider.java:270); 	at shaded.cloud_nio.com.google.auth.oauth2",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:235,detect,detect,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['detect'],['detect']
Safety,ApplicationDefault(GoogleCredentials.java:86); 	at com.google.cloud.ServiceOptions.defaultCredentials(ServiceOptions.java:277); 	at com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:252); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:82); 	at com.google.cloud.storage.StorageOptions.<init>(StorageOptions.java:30); 	at com.google.cloud.storage.StorageOptions$Builder.build(StorageOptions.java:77); 	at org.broadinstitute.hellbender.utils.gcs.BucketUtils.setGlobalNIODefaultOptions(BucketUtils.java:361); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); ```; and ; ```; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Host is down (connect failed); 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); 	at sun.net.www.http.HttpClient.New(HttpClient.java:339); 	at sun.net.www.http.HttpClient.New(HttpClient.java:357); 	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202); 	at sun,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235:3408,detect,detect,3408,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591#issuecomment-331269235,1,['detect'],['detect']
Safety,"Are the errors below part of this, when starting BwaSpark with spark-submit?; I activated ""--disable-sequence-dictionary-validation true"", but that doesn't help. It is very unclear, why a BAM is not recognized as a BAM file. I have tried all kinds of ways to make sure that it is a BAM and not a SAM file.; The documentation for BwaSpark also says ""BAM/SAM/CRAM file containing reads"", so if SAM files are really not possible, that should probably be changed.; ...; Even on verbosity DEBUG, the comments are not at all helpful to get at the problem.; E.g. ""Cannot retrieve file pointers within SAM text files.""; Is that a general statement about SAM files? Or does it only say, that in this specific SAM file (which is actually a BAM file), file pointers cannot be found?; What pointers are meant exactly?; How could this be fixed?. ```; ""SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.""; Which URL?; Which stream?; Why would this happen? What could be the error?; The SAM/BAM distinction seems very unclear. It would be more helpful, if some specific missing aspect (e.g. not queryname sorted) would be clearly declared as the culprit.; ...; 00:29 DEBUG: [kryo] Write: SAMFileHeader{VN=1.5, SO=queryname}; ...; WARNING	2018-01-16 02:11:25	SamReaderFactory	Unable to detect file format from input URL or stream, assuming SAM format.; ...; java.lang.UnsupportedOperationException: Cannot retrieve file pointers within SAM text files.; 	at htsjdk.samtools.SAMTextReader.getFilePointerSpanningReads(SAMTextReader.java:185); ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062:866,detect,detect,866,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4131#issuecomment-357838062,4,['detect'],['detect']
Safety,"Are there any updates on this feature? The use of soft-clipping is not only confusing, but can negatively affect the performance of other tools that use this sort of information. Ignoring soft-clipped reads altogether, if possible at all, is not a good solution. We are forced to use GATK3 because the output of the GATK4 version does not work well with others tools we need for the detection of certain variants in RNA-seq.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589:383,detect,detection,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7356#issuecomment-1846884589,1,['detect'],['detection']
Safety,"As a sanity check, HaplotypeCaller run in GVCF mode over a bam subsetted to only chr15 on my laptop:; master: ; ```; real	4m28.600s; user	5m48.484s; sys	0m4.510s; ```; this branch: ; ```; real	3m58.043s; user	5m15.625s; sys	0m4.012s; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5469#issuecomment-443322836:5,sanity check,sanity check,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5469#issuecomment-443322836,1,['sanity check'],['sanity check']
Safety,"As determined by @davidadamsphd , the `Copyable` interface idea won't work:. The recommendation from the Dataflow team was to make a narrow API and do the copying part of the API. I started down this route, and I think it might be doable for things like the walker interface. The idea is to make a Copyable interface and have our interfaces extend that. . However, we have unsafe code already in the engine. I tried to make this SafeDoFn approach, however it became clear quickly that we'd have a combinatorial explosion of classes because we don't just have `DoFn<GATKRead,POut>`, but also `<Iterable<GATKRead>,POut>`, and many others. So, this approach will not work for the engine. I then tried to make a general purpose solution (using coders to write to bytes and then recreate a new class). This doesn't work for a few reasons, most critical is that the coder registry isn't Serializable, so that can't be passed down deep enough to get this to work. While working on this, I chatted with someone on the Dataflow team who is working on the verification on the direct runner. He has a PR out and likely going to get it approved soon. So, for the engine, we could always test using the direct runner and know for sure there are not issues (once we can use his code). However, there are two downsides:. 1) We will need to wait for a cut of the SDK (which looking at their previous clip is likely ~ two weeks away). . 2) I don't know if we want the direct runner test as our general purpose solution. Can we expect Comp Bios to always test with the direct runner first? Will they write anything more complex than functions that use the Walker interface?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661:373,unsafe,unsafe,373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/702#issuecomment-127403661,1,['unsafe'],['unsafe']
Safety,"As we discussed, it's possible that these are simply common germline CNVs that are being median-normalized out in CR by the PoN. Let's investigate the sample-median-normalized counts in some of the questionable regions, along with the per-bin medians in the PoN. I do not think a gCNV run is necessary (it will probably be a bit expensive, anyway). More generally, I think a better approach to germline tagging would be to avoid the caller entirely. Let's take the ModelSegments output for a normal, and then tag ModelSegments segments in the tumor that sufficiently overlap any normal segment in CR-AF-genomic space (where we have some freedom to define the overlap criteria). Essentially, let's just try to highlight differences between the tumor and normal in CR-AF space. This would rescue events in the normal that may be further amplified or deleted in the tumor. Subsequently, simple filtering of these events would be less misleading than imputation. I do not think such tagging should be implemented in Java, if we can avoid it. Rather, a relatively simple python script that runs through each tumor segment and checks for overlaps would suffice. This script could output a tagged/filtered ModelSegments result, as well as do the conversion step for downstream tools. This also obviates the need for the Java code for combining segment breakpoints and additional CNV collection classes in the current post-processing tools. What do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810:423,avoid,avoid,423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5450#issuecomment-442911810,4,['avoid'],['avoid']
Safety,"At the end of the stdout/stderr it reports 101 warnings and 1 error, so I think it's safe to say that these complaints about the `@VisibleForTesting` annotation are irrelevant. If we further exclude:. * lines saying that a class was imported that follow a complaint about the class; * lines with only a timestamp and a `^` symbol. we obtain the following:. ```; 2022-08-16T00:09:07.2545204Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2547467Z src/main/java/org/broadinstitute/hellbender/cmdline/CommandLineProgram.java:4: error: package com.google.common.base does not exist; 2022-08-16T00:09:07.2647018Z src/main/java/org/broadinstitute/hellbender/engine/FeatureInput.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2671678Z src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/PosteriorProbabilitiesUtils.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2726493Z src/main/java/org/broadinstitute/hellbender/engine/FeatureContext.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2743559Z src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2775681Z src/main/java/org/broadinstitute/hellbender/engine/filters/CountingVariantFilter.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2833952Z src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2841948Z src/main/java/org/broadinstitute/hellbender/cmdline/argumentcollections/IntervalArgumentCollection.java:3: error: package com.google.common.annotations does not exist; 2022-08-16T00:09:07.2856913Z src/main/java/org/broadinstitute/he",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480:85,safe,safe,85,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7991#issuecomment-1217242480,1,['safe'],['safe']
Safety,"Augh, looks like we'll have to actually rebase this onto samtools/htsjdk#796 to get it to pass tests, due to a change in codec detection.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-277355740:127,detect,detection,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2344#issuecomment-277355740,1,['detect'],['detection']
Safety,"Awesome @lbergelson! Now the tests are passing here, so I will probably rebase all of my PRs soon to avoid the annoying ""red cross of death"". Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-281515727:101,avoid,avoid,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2401#issuecomment-281515727,1,['avoid'],['avoid']
Safety,"BTW, if any can provide the output from an interactive python session that tries (and fails) to use Intel-TF on non-AVX hardware it might help us figure out how to detect that case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428293864:164,detect,detect,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-428293864,1,['detect'],['detect']
Safety,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:36,detect,detected,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417,1,['detect'],['detected']
Safety,"CKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; [ccastane9@andersserver-01 GenomicsDB]$ bash *_genotype.3.sh; Using GATK jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Xmx16g -jar /data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar GenotypeGVCFs --genomicsdb-shared-posixfs-optimizations --reference /data1/EquCab/_ECA30/Equus_caballus.EquCab3.0.dna_sm.toplevel.fa/ -V gendb://ECA3_GenomicsDB_260/3 -O ECA3_GenomicsDB_260.3.g.vcf.gz; 16:27:53.573 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data1/_software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 06, 2021 4:27:54 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:27:54.132 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.133 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.8.1; 16:27:54.133 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:54.143 INFO GenotypeGVCFs - Executing as ccastane9@andersserver-01.cvm.tamu.edu on Linux v3.10.0-1127.19.1.el7.x86_64 amd64; 16:27:54.143 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01; 16:27:54.144 INFO GenotypeGVCFs - Start Date/Time: January 6, 2021 4:27:53 PM CST; 16:27:54.144 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.144 INFO GenotypeGVCFs - ------------------------------------------------------------; 16:27:54.145 INFO GenotypeGVCFs - HTSJDK Version: 2.23.0; 16:27:54.145 INFO GenotypeGVCFs - Picard Version: 2.22.8; 16:27:54.145 INFO GenotypeGVCFs - HTSJD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402:4604,detect,detect,4604,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7012#issuecomment-755760402,1,['detect'],['detect']
Safety,"CRAM w/o NIO is also ~3 cents per sample (it was marginally more expensive than CRAM w/ NIO, but within the noise). CRAM w/o NIO w/ SSD is ~5 cents. So I'd say CRAM w/ or w/o NIO is fine. Strictly speaking, we can't directly compare the BAM and CRAM costs, since they were done on different sets of TCGA samples. But both are well under the goal of ~15 cents per sample, so I think it's safe to say that we can turn our attention to optimizing inference costs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453:387,safe,safe,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715#issuecomment-467612453,1,['safe'],['safe']
Safety,Can I replace dictionaries in *hdf5 files ? Will this avoid reruning GermlineCNVCaller step? ; It's just that this step took quite a long time at my machine. About 42 hours for each shard.....; This is why it seems to me that reporting the error in the early stages would be more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720056200:54,avoid,avoid,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-720056200,1,['avoid'],['avoid']
Safety,"Can you have a look to this proposal, @droazen? I really need to have this in before the release of GATK4 to be able to update my dependency for the release one. Otherwise, I will need a version bump or a hacked CLP class (which I prefer to avoid). Thank you very much in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-353034378:241,avoid,avoid,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-353034378,1,['avoid'],['avoid']
Safety,Closed via https://github.com/broadinstitute/gatk/pull/4757 where the timeouts were removed altogether.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4221#issuecomment-397080074:70,timeout,timeouts,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4221#issuecomment-397080074,1,['timeout'],['timeouts']
Safety,Context.finishBeanFactoryInitialization(AbstractApplicationContext.java:878); 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). 2020-05-29 15:14:33.032 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:18046,detect,detect,18046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,Context.finishBeanFactoryInitialization(AbstractApplicationContext.java:878); 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550); 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758); 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750); 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237); 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226); 	at com.luz.push.PushApplication.main(PushApplication.java:10). 2020-05-29 15:14:33.035 WARN 12904 --- [ main] c.g.a.oauth2.ComputeEngineCredentials : Failed to detect whether we are running on Google Compute Engine. java.net.SocketException: Network is unreachable: connect; 	at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method); 	at java.net.DualStackPlainSocketImpl.socketConnect(DualStackPlainSocketImpl.java:85); 	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); 	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); 	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); 	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172); 	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); 	at java.net.Socket.connect(Socket.java:589); 	at sun.net.NetworkClient.doConnect(NetworkClient.java:175); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432); 	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527); 	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211); 	at sun.net.www.http.HttpClient.New(HttpClient.java:308); 	at,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233:23386,detect,detect,23386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-635805233,1,['detect'],['detect']
Safety,"Couldn't open hdf5 files.; ![Screenshot_2020-10-29_17-20-17](https://user-images.githubusercontent.com/29140765/97586406-459fe000-1a0b-11eb-86cf-d70a28c55637.png); Running without the optional --sequence-dictionary argument also causes an error.; `17:00:00.556 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/lmbs02/bio/biosoft/gatk/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 29, 2020 5:00:00 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:00:00.683 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - The Genome Analysis Toolkit (GATK) v4.1.9.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Executing as lmbs02@Lmbs01 on Linux v5.4.0-48-generic amd64; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-8u265-b01-0ubuntu2~18.04-b01; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Start Date/Time: October 29, 2020 5:00:00 PM MSK; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - ------------------------------------------------------------; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Version: 2.23.0; 17:00:00.684 INFO PostprocessGermlineCNVCalls - Picard Version: 2.23.3; 17:00:00.684 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:00:00.685 INFO PostprocessGermlineCNVCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:00:00.685 INFO PostprocessGermlineCNV",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427:580,detect,detect,580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6924#issuecomment-718787427,1,['detect'],['detect']
Safety,"Current status of this: The tool can physically run on 11k samples, but with a 1-5% failure rate, depending on the combination of arguments used. The failures are almost all due to https://github.com/broadinstitute/gatk/issues/2685 (see the stack trace in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308541727 for a representative example error). . One possibility is that we are being throttled in a way that GATK itself can't recover from. GATK is retrying in the face of these SSL errors 20 times, with increasing wait times between each attempt, and still running out of retries. See @jean-philippe-martin 's latest hypothesis in https://github.com/broadinstitute/gatk/issues/2685#issuecomment-308586876. @kcibul @Horneth take note.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736:448,recover,recover,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-308755736,1,['recover'],['recover']
Safety,DD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:7250,abort,abortStage,7250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['abortStage']
Safety,DD.scala:310); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:123); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:10792,abort,abortStage,10792,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['abortStage']
Safety,"David and Lee;; Thanks for the thoughts and heads up on ploidy. I normally only set this for mitochondrial and chrX/Y but it's not a big deal to have diploid calls throughout. Avoiding representing minor variants in the ploidy field would be helpful for downstream processing. Having multiple alleles on a single line per allele is within spec (1.6.1 in the 4.3 spec: ""It is permitted to have multiple records with the same POS."") and a lot of downstream tools deal with them this way. bcftools and vt normalization produces these and I know GEMINI does this to correctly handle multi-alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330549263:176,Avoid,Avoiding,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3564#issuecomment-330549263,1,['Avoid'],['Avoiding']
Safety,"David, thanks for the CWL suggestion. As far as I know most CWL runners don't attempt to edit or mount the internal container `/etc/passwd` unfortunately. They do try to match with the external user outside of Docker to avoid file permission issues. Does Cromwell deal with this problem? We're actively looking to make more use of Cromwell for CWL runs. If we could make that happen that would resolve a lot of issues and I could leave my workaround for other non-conforming callers. Tom, that is a great suggestion and I thought would work as well but we do this (https://github.com/bcbio/bcbio-nextgen/blob/bd03e259877d410045468046a949f6b9724605c5/bcbio/broad/__init__.py#L152) and Spark/Hadoop still wants to look up the user in `/etc/passwd` even if missing. If there is a way to skip that being present I'm happy to tweak that as well. Thanks so much for all this discussion and suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018:220,avoid,avoid,220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018,1,['avoid'],['avoid']
Safety,"Does the problem go away if you use an output path with the 'hdfs://' scheme? E.g. _hdfs://namenode:8020/user/yaron/output.bam_ (where _namenode_ is the hostname of the namenode). There are two libraries being used internally for accessing the filesystem - the Hadoop filesystem API, and the NIO API - and they have slightly different behaviour if no scheme is provided. So to avoid problems it's best to give full paths with URI schemes for all input and output paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242:377,avoid,avoid,377,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066#issuecomment-407791242,1,['avoid'],['avoid']
Safety,"Even if I'll refactore it to be void, the part that I changed is need it: there is no other part which handle the `CommandLineException` and if the `customCommandLineValidation()` throws the exception no error is printed in the terminal. I guess that returning a `String[]` in Picard is done to output several errors in the command line to avoid the user to re-run with another bug not reported. Nevertheless, I prefer you approach. I'm changing now the code in this PR to add what you suggested. Thanks again for make my development smoother, @lbergelson!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377:340,avoid,avoid,340,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2226#issuecomment-255847377,1,['avoid'],['avoid']
Safety,FGQBands 22 --GVCFGQBands 23 --GVCFGQBands 24 --GVCFGQBands 25 --GVCFGQBands 26 --GVCFGQBands 27 --GVCFGQBands 28 --GVCFGQBands 29 --GVCFGQBands 30 --GVCFGQBands 31 --GVCFGQBands 32 --GVCFGQBands 33 --GVCFGQBands 34 --GVCFGQBands 35 --GVCFGQBands 36 --GVCFGQBands 37 --GVCFGQBands 38 --GVCFGQBands 39 --GVCFGQBands 40 --GVCFGQBands 41 --GVCFGQBands 42 --GVCFGQBands 43 --GVCFGQBands 44 --GVCFGQBands 45 --GVCFGQBands 46 --GVCFGQBands 47 --GVCFGQBands 48 --GVCFGQBands 49 --GVCFGQBands 50 --GVCFGQBands 51 --GVCFGQBands 52 --GVCFGQBands 53 --GVCFGQBands 54 --GVCFGQBands 55 --GVCFGQBands 56 --GVCFGQBands 57 --GVCFGQBands 58 --GVCFGQBands 59 --GVCFGQBands 60 --GVCFGQBands 70 --GVCFGQBands 80 --GVCFGQBands 90 --GVCFGQBands 99 --indelSizeToEliminateInRefModel 10 --useAllelesTrigger false --dontTrimActiveRegions false --maxDiscARExtension 25 --maxGGAARExtension 300 --paddingAroundIndels 150 --paddingAroundSNPs 20 --kmerSize 10 --kmerSize 25 --dontIncreaseKmerSizesForCycles false --allowNonUniqueKmersInRef false --numPruningSamples 1 --recoverDanglingHeads false --doNotRecoverDanglingBranches false --minDanglingBranchLength 4 --consensus false --maxNumHaplotypesInPopulation 128 --errorCorrectKmers false --minPruning 2 --debugGraphTransformations false --kmerLengthForReadErrorCorrection 25 --minObservationsForKmerToBeSolid 20 --likelihoodCalculationEngine PairHMM --base_quality_score_threshold 18 --gcpHMM 10 --pair_hmm_implementation FASTEST_AVAILABLE --pcr_indel_model CONSERVATIVE --phredScaledGlobalReadMismappingRate 45 --nativePairHmmThreads 4 --useDoublePrecision false --debug false --useFilteredReadsForAnnotations false --emitRefConfidence NONE --bamWriterType CALLED_HAPLOTYPES --disableOptimizations false --justDetermineActiveRegions false --dontGenotype false --dontUseSoftClippedBases false --captureAssemblyFailureBAM false --errorCorrectReads false --doNotRunPhysicalPhasing false --min_base_quality_score 10 --useNewAFCalculator false --annotateNDA false --heterozygosity 0.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678:2869,recover,recoverDanglingHeads,2869,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-334840678,1,['recover'],['recoverDanglingHeads']
Safety,"Failing tests were from a debug bamout to my Desktop, which of course fails on Travis. Fixed that. Just need to write a unit test with `recoverAll = true`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5693#issuecomment-466143854:136,recover,recoverAll,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693#issuecomment-466143854,1,['recover'],['recoverAll']
Safety,"First-pass review complete -- back to @tomwhite. Many of my suggestions center around pushing arguments and functionality up into `GATKSparkTool` as much as possible, even if they're not applicable to every tool, as we ideally want to spare tool authors from having to manually manage these low-level Spark parameters when they don't want/need to, and we also want to enforce consistency across tools and avoid duplicated boilerplate code. At the same time, there should be clear mechanisms for tools to override the defaults when they have to (eg., overridable methods in `GATKSparkTool`), as I'm not sure whether tools like BQSR are going to be happy with the new 128 MB default input split size.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907:405,avoid,avoid,405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432#issuecomment-172100907,1,['avoid'],['avoid']
Safety,"For a quick analysis, I made a serialized versions of DBSNP (13572728 variants from `dbsnp_135.b37.excluding_sites_after_129.vcf`), size of VCF on disk 2175071049 bytes (2.0G). (all false postive probs are predicted, it'd be easy to measure it too). Map keys are contig names. ```; Map of String->BloomFilter with 0.001 false positive prob = 27320529 bytes (26M); Map of String->BloomFilter with 0.0004 false positive prob = 30943001 bytes (30M); Map of String->BloomFilter with 0.0001 false positive prob = 36423625 bytes (35M); Map of String->BloomFilter with 0.00004 false positive prob = 40046089 bytes (38M); Map of String->BloomFilter with 0.00001 false positive prob = 45526681 bytes (43M); Map of String->BloomFilter with 0.000001 false positive prob = 54629745 bytes (52M); Map of String->int[] of positions = 60790452 bytes (58M); List<GATKVariant> made just like the one in spark BQSR = 366463957 bytes (349M); ```. Variants from dbSNP cover 0.004 of the genome (15195436 bases of 3101804739) so if we want reasonable precision (number of false positives over all reported hits), say 0.9 precision (of 10 hits only 1 can be false) we need (1-0.9) x 0.004 false positive prob = 0.0004. For 0.99 precision (of 100 hits only 1 can be false) we need (1-0.99) x 0.004 false postive prob = 0.00004. These are approximations of course. Given these numbers, I conclude that, for now, exploring BloomFilters does not seem to make sense (too little saving and too many complications with using a probabilistic data structure - eg we'd need to use it too for the walker BQSR). It does make sense however to explore alternatives to the list of GATKVariants because it's very big when serialized (maybe Kryo does a better job but it's still a big object). A simple alternative like sorted int[] may be sufficient and has attractive properties (trivial to implement and understand, O(log) lookups, 0% false positives, small size when serialized).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133:206,predict,predicted,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407#issuecomment-203727133,1,['predict'],['predicted']
Safety,"For reporting the number of reads that fails each of the filters, the composed filter could be changed by a `CountingReadFilter`; using the `getSummaryLine()` method will provide the number of reads failing each of the components. Developers could have in their tools a field with the `WellFormedReadFilter` and call a new method for reporting the summary, to log a warning/debug line. For exploding depending on the tool, maybe an advance/hidden argument can be added to the filter (something like `--failOnMalformed`) to throw an exception if true; developers might add a default filter with this value equals to true if they want to enforce by default this behaviour. I think that this a simpler idea for allow the developer to choose, and give some flexibility to the user to change the behaviour as its own risk (they can disable all filters anyway, which is also risky).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699:812,risk,risk,812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3454#issuecomment-323698699,4,['risk'],"['risk', 'risky']"
Safety,Friendly ping @fleharty! I will really appreaciatte if this can go into before #2185 to avoid the maintenance of the deprecated `PerReadAlleleLikelihoodMap` just for me. Thanks a lot!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-255859144:88,avoid,avoid,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2154#issuecomment-255859144,1,['avoid'],['avoid']
Safety,GATK forum docs mentioning hdfview are:; https://gatk.broadinstitute.org/hc/en-us/articles/360035531712-HDF5-format; https://gatk.broadinstitute.org/hc/en-us/articles/360035889651-Are-there-any-Broad-specific-instructions-for-using-GATK-; https://gatk.broadinstitute.org/hc/en-us/articles/360035531152; https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments; and maybe a few more -- the forum search is terrible. Maybe @gbrandt6 can take care of those?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6927#issuecomment-729948755:393,detect,detect-copy-ratio-alterations-and-allelic-segments,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6927#issuecomment-729948755,1,['detect'],['detect-copy-ratio-alterations-and-allelic-segments']
Safety,George -- thanks much for debugging and identifying the underlying problem. I can confirm that we're able to avoid the error by removing `-XX:+UseSerialGC` and moving back to parallel GC. We'd initially introduced the serial GC usage to avoid problems when running multiple HaplotypeCaller commands simultaneously on a single machine but by letting the Spark implementation take care of parallelizing we should no longer need to worry about that. Thanks again for the workaround and the tip on using `spark.local.dir`. Much appreciated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-333837665:109,avoid,avoid,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3605#issuecomment-333837665,2,['avoid'],['avoid']
Safety,"Going to risk it and take the ""I like this"" as approval :smirk:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418943215:9,risk,risk,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5157#issuecomment-418943215,1,['risk'],['risk']
Safety,"Got another one in the same branch at https://travis-ci.com/github/broadinstitute/gatk/jobs/300319727, this time in HaplotypeCallerSparkIntegrationTest.testNonStrictVCFModeIsConsistentWithPastResults. @lbergelson this branch already updates the base image to 18.04, but I haven't yet made any updates to .travis.yml as you do in https://github.com/broadinstitute/gatk/tree/lb_update_docker_ubuntu. Think that could be causing these issues?. I'll try to debug a bit once I sort out the python stuff. ```; org.broadinstitute.hellbender.tools.HaplotypeCallerSparkIntegrationTest > testNonStrictVCFModeIsConsistentWithPastResults[0](/gatkCloneMountPoint/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam, /gatkCloneMountPoint/src/test/resources/large/human_g1k_v37.20.21.fasta) FAILED; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690:834,abort,aborted,834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513#issuecomment-601702690,1,['abort'],['aborted']
Safety,"Great, thanks for adding this @cmnbroad! Let me manually check that there weren't any numerical changes in the gCNV WDL tests. @asmirnov239 and Jack might also want to test on some small, real data. Probably overkill, but just to be safe... We'll get back to you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6494#issuecomment-597333928:233,safe,safe,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6494#issuecomment-597333928,1,['safe'],['safe']
Safety,"HI @lbergelson - ; I'm working on a bug/warning in the variant calling workflow where it's complaining about not finding a logger:. `21:04:59.525 INFO ProgressMeter - Starting traversal; 21:04:59.526 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (io.grpc.netty.shaded.io.netty.util.internal.logging.InternalLoggerFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 21:05:10.018 INFO ProgressMeter - chr1:4642050 0.2 205000 1172992.6`. I found that if I had gatk's build.gradle NOT exclude the log4j.properties file I get rid of that warning, so I'm trying to understand the issue [here](https://github.com/broadinstitute/gatk/blob/33bda5e08b6a09b40a729ee525d2e3083e0ecdf8/build.gradle#L441): (where you found log4j.properties clashed with log4j2.xml) . James Emery is on the git blame for that, but he thinks that's because of the refactoring he did. Thanks in advance - I'm not sure if there's something else I should be doing with the xml version of that file to avoid this warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722:1157,avoid,avoid,1157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7778#issuecomment-1098029722,1,['avoid'],['avoid']
Safety,"Hang on, it looks like this work might have been redundant with changes in https://github.com/broadinstitute/gatk/pull/3917/files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-349428395:49,redund,redundant,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3911#issuecomment-349428395,1,['redund'],['redundant']
Safety,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:63,timeout,timeout,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155,1,['timeout'],['timeout']
Safety,Hello @bbimber thank you for the response. I would recommend using the read filters (in your case `-rf MappingQualityReadFilter --minimum-mapping-quality ##` to achieve the same functionality as the `-mmq` argument from GATK3. When porting over the tool we tried to push as much functionality from obscure arguments into the existing filtering framework as possible and `-mmq` was one of the ones that was redundant as it was a simple filter placed on the reads before counting them which the existing filtering code was able to handle. I will add some lines to the documentation clarifying this for users in the future.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928:406,redund,redundant,406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6617#issuecomment-634752928,2,['redund'],['redundant']
Safety,Hello @cmnbroad @ldgauthier - just following up here. It seems like we have passing tests and general approval of this change. I believe the only question is whether removing toString() from VariantAnnotation is safe. As noted above it doesnt seem to be needed.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-780808041:212,safe,safe,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7041#issuecomment-780808041,1,['safe'],['safe']
Safety,"Hello @nalinigans,. As part of gatk-sv pipeline we are using GATK : v4.1.8.1 which doesn't have bypass-feature-reader option. Also, we didnâ€™t capture strace for the run with just ""--genomicsdb-shared-posixfs-optimizations"" so wont be able to share the FUTEX process counts. So after using v4.2.4.1 we get below results. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" the process took 118 mins.; ""FUTEX_WAIT_PRIVATE, 0, NULL"" : 1266. 	- Using ""--genomicsdb-shared-posixfs-optimizations"" & ""--bypass-feature-reader"" and ; TILEDB_UPLOAD_BUFFER_SIZE=5242880 as env variable the process took 113 mins.; 	""FUTEX_WAIT_PRIVATE, 0, NULL"" : 3. 	- Even using 10 MB as buffer size resulted in same execution time of 113 mins.; 	- Using a buffer size bigger i.e. 50 MBs caused the process to run slower so we aborted it. Please let us know if we can improve it further.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845:830,abort,aborted,830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646#issuecomment-1040947845,1,['abort'],['aborted']
Safety,"Hello, Could you tell me the exact source websites of funcotator_dataSources.v1.7.20200521g? I did not find it in your Google Cloud (genomics-public-data).; Besides, I used this code to download`./gatk-4.1.9.0/gatk FuncotatorDataSourceDownloader --germline --validate-integrity --extract-after-download; `, but the error appeared as following:`Nov 18, 2023 1:15:05 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:15:05.202 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - The Genome Analysis Toolkit (GATK) v4.1.9.0; 13:15:05.203 INFO FuncotatorDataSourceDownloader - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Executing as yaoxq@mu01 on Linux v3.10.0-693.el7.x86_64 amd64; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_151-b12; 13:15:05.203 INFO FuncotatorDataSourceDownloader - Start Date/Time: November 18, 2023 1:15:04 PM CST; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.203 INFO FuncotatorDataSourceDownloader - ------------------------------------------------------------; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Version: 2.23.0; 13:15:05.204 INFO FuncotatorDataSourceDownloader - Picard Version: 2.23.3; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 13:15:05.204 INFO FuncotatorDataSourceDownloader - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:15:05.204 I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417:473,detect,detect,473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8275#issuecomment-1817434417,1,['detect'],['detect']
Safety,"Here the error log ; Using GATK jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar IndexFeatureFile -I output/called/final/allsites.filtered.vcf; 00:57:08.257 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/scientific_bin/gatk/4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Sep 11, 2023 12:57:08 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 00:57:08.789 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.789 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.4.1; 00:57:08.789 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 00:57:08.790 INFO IndexFeatureFile - Executing as [ychrysostomakis@compute-0-3.local](mailto:ychrysostomakis@compute-0-3.local) on Linux v3.10.0-1160.53.1.el7.x86_64 amd64; 00:57:08.790 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_231-b11; 00:57:08.790 INFO IndexFeatureFile - Start Date/Time: 11. September 2023 00:57:07 MESZ; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - ------------------------------------------------------------; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Version: 2.21.0; 00:57:08.790 INFO IndexFeatureFile - Picard Version: 2.21.2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:57:08.790 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316:732,detect,detect,732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8372#issuecomment-1733069316,1,['detect'],['detect']
Safety,"Here's a stack trace of the area I think the two minute wait may be occurring. The below example fails-fast and prints out stack trace when there is no internet. I suspect that the slow-and-quiet alternative occurs when the connection to [google](https://github.com/googleapis/google-cloud-java/blob/v0.72.0/google-cloud-clients/google-cloud-core/src/main/java/com/google/cloud/ServiceOptions.java#L450) is blocked vs. completely unavailable. ```java; Dec 02, 2018 7:50:25 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused (Connection refused); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843:584,detect,detect,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-443830843,1,['detect'],['detect']
Safety,"Here's a suggested set of things to look at as part of this ticket:. -See if we can avoid fetching all headers on startup by passing in the needed info via alternate args (https://github.com/broadinstitute/gatk/issues/2639). -Do profiling to find an appropriate value for the --batchSize argument,; once it's merged (https://github.com/broadinstitute/gatk/issues/2641). -Shrink NIO buffers (--cloudPrefetchBuffer and --cloudIndexPrefetchBuffer) down to the smallest values that still produce acceptable performance (https://github.com/broadinstitute/gatk/issues/2640). Thibault of red team aka @Horneth has agreed to take this on.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616:84,avoid,avoid,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2633#issuecomment-298338616,1,['avoid'],['avoid']
Safety,"Here's what the code will be, I think:; ```; if (refSeq.length == altSeq.length && IntStream.range(0, refSeq.length).filter(n -> refSeq[n] != altSeq[n]).count() < 3) {; return same shortcut as before; }; ```. So 5 was actually an overestimate, unless we avoid the stream.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5459#issuecomment-442897874:254,avoid,avoid,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5459#issuecomment-442897874,1,['avoid'],['avoid']
Safety,"Hey @jean-philippe-martin, this looks good. I've made some very small changes to avoid the back-and-forth of a review, and rebased the branch onto latest master. . The main change I made was in `IOUtils.getPath()`. It now traps the `IOException` and throws a `UserException` instead. This has the benefit of not requiring client code to put `throws IOException` (or catch the `IOException`) everywhere, and is more consistent with the rest of the codebase, which typically traps checked exceptions as early as possible and wraps them in unchecked exceptions (either `UserException` or `GATKException`, depending on whether it's likely to be the user's fault or not). Also made a small change in `NioBam` to make it use a logger instead of `System.out.println()` for debug output.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014:81,avoid,avoid,81,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2224#issuecomment-259798014,1,['avoid'],['avoid']
Safety,"Hi . I have a few questions about using Mutect2 versus HaplotypeCaller to call mitochondrial (or chloroplast) variants (I'm working on plants). 1) In article 11127, it says; ""The tool can run on unmatched tumors but this produces high rates of false positives. Technically speaking, somatic variants are both (i) different from the control sample and (ii) different from the reference."". In my case, there isn't a ""normal"" sample to compare the ""tumour"" (i.e. mitochondria) to, just a reference. Are my results likely to be prone to false positives then? Or is it that the case for mitochondria is different because we are not truly looking for somatic variants but rather variants in general that may not be 100% ""pure"" (because of heteroplasty?). Is the latter point the justification for Mutect vs HaplotypeCaller in the first place?. 2) I am interested in both variant and invariant sites (relative to the reference). Ultimately, I want to be able to go base for base and make a call (ref, alt, or ""N""; where ""N"" is used where I have no confidence in the base call at that site). The goal is to have a full haplotype sequence for the mitochondria/chloroplast of each sample. . In HaplotypeCaller, I read that I could use --emit-ref-confidence BP_RESOLUTION to get the confidence of a site being homozygous reference. Does --out-mode EMIT_ALL_CONFIDENT_SITES give similar information?. 3) In this thread, it's said that the --min-pruning argument is very important. I'm very new to this and don't fully understand what this parameter is doing. Is the advice to set this parameter = ceiling(average coverage/500) general? Or specific to the project mentioned above?. 4) I don't why we don't have to set the sample ploidy to 1? Is this only for applications where we want to detect heteroplasmy? If we are after the majority haplotype, does it make sense to set this parameter?. Many thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-432443695:1776,detect,detect,1776,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193#issuecomment-432443695,1,['detect'],['detect']
Safety,"Hi @OgnjenMilicevic ,. Is this the latest GATK version? Highly multi-allelic sites are known to cause a slowdown in GenotypeGVCFs, but this was dramatically improved with the `-newQual` option, made default in 4.1.5.0. 110 whole genomes should be manageable, but excluding the low complexity regions could avoid some of the highly multi-allelic variants.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6896#issuecomment-710147958:306,avoid,avoid,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6896#issuecomment-710147958,1,['avoid'],['avoid']
Safety,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:1049,avoid,avoid,1049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973,1,['avoid'],['avoid']
Safety,"Hi @bbimber . So regarding this first question:. > the guts of a GenomicsDB workspace has one folder per contig anyway. Is there a reasonable way to merge multiple workspaces together?. You can try to pull together each of the contig folders under a single workspace. You just need a single copy of the callset.json, vidmap.json, vcfheader.vcf, and __tiledb_workspace.tdb in the merged workspace. As @ldgauthier suggests, it falls under the ""probably works but unsupported"" umbrella. Most of the obvious pitfalls can be avoided by ensuring the same set of VCF files get used by the `GenomicsDBImport` call for each of the multiple workspaces (so the only difference in each command should be the interval list). I can't quite be sure, but the last part of your comment/question might be asking if some tool or option supports reading/merging from multiple genomicsdb workspaces. No such tool currently exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6557#issuecomment-630573640:520,avoid,avoided,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6557#issuecomment-630573640,1,['avoid'],['avoided']
Safety,"Hi @bbimber, our next release is imminent -- we're just waiting on 1-2 final PRs to be merged. I can safely say it will go out this week.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7322#issuecomment-865082820:101,safe,safely,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7322#issuecomment-865082820,1,['safe'],['safely']
Safety,"Hi @hh1985 . Memory tuning is pretty tricky and can depend on a lot of things. How is your cluster configured? ; Are you using YARN? Are you running in client or cluster mode? . I'm assuming you're running with YARN. Mesos should also work but I don't have any experience configuring it. . BQSR should run safely with 4g of memory per core. (It should really work with much less I think, but 4 should definitely be sufficient.) There are a few different parameters that can help you adjust the memory ratios.; A good tuning might be something like; ; ```; --num-executors 5 ; --executor-cores 8 ; --executor-memory 32g ; ```. if you're not running with gatk-launch you'll need to set; ```; --conf spark.yarn.executor.memoryOverhead=600; ```; Without setting a higher than default yarn memory overhead like this we see consistent crashes, it's included in the settings gatk-launch applies already. That should run 5 separate executors with 8 cores each and give each one 32g, so 4g / core. . If you're running in cluster mode you'll have to carve out some memory and cores for the driver. You can set the driver settings with ; ```; --driver-cores 2; --driver-memory 4g; ``` ; or something along those lines. The driver doesn't need much memory or computer for BQSR. In general we've had better luck using the entire cluster for one job and running jobs in sequence rather than trying to run two jobs simultaneously using a subset of the cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738:306,safe,safely,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3465#issuecomment-324064738,1,['safe'],['safely']
Safety,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:1127,timeout,timeout,1127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398,1,['timeout'],['timeout']
Safety,Hi @lbergelson and @droazen: it looks like @lbergelson added a limit to avoid excessive sources. Does that satisfy issues remaining on this PR?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2052269547:72,avoid,avoid,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8752#issuecomment-2052269547,1,['avoid'],['avoid']
Safety,"Hi @lbergelson thank you for looking into this. After a lot of trial and error that's what I figured as well. It would be interesting to know what assumptions are broken and if there is a way to avoid it or even to make the --remove-unused-alternates option compatible with FastaAlternateReferenceMaker.; What I still don't know is if the tool grabs the correct alternate when it sees more than 1 at a site. If yes, the above option is almost unnecessary, but it would be nice if it worked.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077:195,avoid,avoid,195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7433#issuecomment-904755077,1,['avoid'],['avoid']
Safety,"Hi @lbergelson. Thanks for the quick response!. I believe we started with a FASTQ file that had the header I listed above:; `HWI-ST700660_163:1:1101:1243:1870#1@0/1`. Which we later converted to a bam using samtools that contains this header:; `HWI-ST700660_163:1:1101:1243:1870#1@0`; after alignment with BWA-MEM2. . My team thinks it might be the `@` from the FASTQ header since modifying it to use a format without the additional `@`, such as `@SRR5456220.1 1 length=101`, seemed to allow it to work properly in our workflow. Thank you for the suggestions! We will try that out so we can try to avoid detecting and changing each header. We really appreciate it!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8134#issuecomment-1360234528:598,avoid,avoid,598,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8134#issuecomment-1360234528,2,"['avoid', 'detect']","['avoid', 'detecting']"
Safety,"Hi @pieterlukasse. This is a great question and somewhat timely. Funcotator hasn't gotten the attention it needs lately because the engineer who's most involved has been extremely busy with other very high priority projects. We intend to support it going forward but it's unclear if that means bug fixes and reactive support or if we're able to make major upgrades. We're currently in the middle of somewhat of a resource crunch, and we are actively planning how to prioritize our attention in the future. . I think if you're basically happy with features now, you should feel safe investing the time into an output parser. If there are major improvements you need I would wait a week or two and ask again then because we'll have more clarity about what we can do. . If you're interested I would consider contributing back your code the GATK core. There's some existing utility code that could probably help you, but there's a long standing gap in our tooling for users to make sense of the funcotations and we'd welcome improvements or new tools. @jonn-smith @droazen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1378997296:577,safe,safe,577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8154#issuecomment-1378997296,1,['safe'],['safe']
Safety,"Hi @wir963, looking back on this I see that the documentation is a bit ambiguous. `--bwa-score-threshold` is applied first during the microbe alignment step and actually is actually passed directly to `bwa mem` as the `-T` parameter (see http://bio-bwa.sourceforge.net/bwa.shtml). `--min-score-identity` is, as you noted, a fractional value of the read length between 0 and 1 that is applied during the final scoring phase to adjust how stringently aligned reads should be categorized as either known microbial or non-host/non-microbial (unknown). Both will affect sensitivity to microbe detection, but I would generally recommend only adjusting the latter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6818#issuecomment-705021856:588,detect,detection,588,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6818#issuecomment-705021856,1,['detect'],['detection']
Safety,"Hi Ted, ; After we talked offline I went back and read the code again.; I still have some questions regarding the method; `static Cigar findLargeDeletions( final GATKRead read ) `. 1. It seems this will do redundant work in the following sense: for the same query sequence which has several alignments, each `GATKRead` (which is actually an alignment record) will be touched by this method, hence imagine one query sequence with two corresponding `GATKRead`s, the current implementation will emit two new `GATKRead`s. Seems unnecessary and double counting evidence.; 2. I don't see any check on same-chromosome-ness, i.e. `fields[0]`, which is formatted `SA:Z:<CHR_NAME>`.; 3. I see checking of overlap length on the query sequence between two alignment records, but I don't see a check of gap size on the query sequence between two alignment records. As we talked about this offline, it is a hard problem when the two alignment records are gapped away both on the reference and the query sequence, if you want to merge the two records into a single record.; 4. I am still not quite understanding line 1652 about why you are using the clip length to decide orders.; 5. Is there a reason to limit the del length to 2, which is the same as the threshold on `overlapLength`?. For the related method `recplaceCigar`, the method is not checking for negative values on `overlapLength`, which judged from `int Interval.overlapLength(Interval)` is possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406:206,redund,redundant,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6092#issuecomment-522145406,1,['redund'],['redundant']
Safety,"Hi everyone! i partially solved the problem ""WARN GencodeFuncotationFactory - Cannot create complete funcotation for variant at chr__:___:___ due to alternate allele: *"".; The origin of the problem is that we have complex datasets that contain more than one sample. In the set of samples, more than one alternative allele is detected, including the ""*"". The idea is to have one line for each variant because, apparently, Funcotator reads it properly. I applied the following commands and it worked perfectly:. 1) Normalize: ; bcftools norm -m - cohort.vcf > cohort_norm.vcf. 2) Select SNPs (I haven't tried it for indels yet); gatk SelectVariants -R hg38.fa -V ""cohort_norm.vcf"" --select-type SNP -O ""cohort_snp.vcf.gz"". 3) Remove the * variants remaining:; awk -F'\t' '$5 != ""*""' cohort_snp.vcf > filtered_cohort_snp.vcf. 4) Apply Funcotator. At the moment this works perfectly for me. If anyone has a better solution please upload it. Regards",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1733778016:325,detect,detected,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-1733778016,1,['detect'],['detected']
Safety,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:58,predict,predicted,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,4,"['predict', 'safe']","['predicted', 'safely']"
Safety,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:597,abort,aborting,597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494,2,['abort'],"['aborted', 'aborting']"
Safety,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:863,detect,detect,863,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136,1,['detect'],['detect']
Safety,"Hi, ; for those looking to run containers within a multi-user HPC environment, running a container with default root privileges presents a potential data security risk. Adding something like :. RUN useradd -ms /bin/bash gatk; WORKDIR /home/gatk; USER gatk. to the Docker file would greatly reduce the risk and bring the current containers in line with general best practice, e.g https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b. There should be no downsides to running in this manner. Singularity could help but the current configuration will be picked up and prevented from running by any site using a container security scanner, e.g. Aqua.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377:163,risk,risk,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3644#issuecomment-494457377,2,['risk'],['risk']
Safety,"Hi, regarding making --linked-de-bruijn-graph the default, I wanted to share that I had recently run mutect2 (gatkv4.2.6.1) on a larger cohort of samples with that option, some of which had variant calls from a previous mutect2 run (gatkv4.1.0.0) without this option. I noticed that plenty of known cancer drivers (e.g. KRAS p.G12C or PIK3CA p.E545* or BRAF p.V600*) that were present in a substantial number of samples (>10%ish) in the old calls were completely absent in the new calls. I had to add the option --recover-all-dangling-branches to recover those known hotspot mutations. They also all have very sufficient coverage (O(100)x) and high VAF to make them obvious true positives. I'd expect from mutect2 to be always able to call presence or absence of known hotspot mutations, so you should either look into further testing/debugging the linked de Bruijn graph option or also make it a default to recover all dangling branches.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262:514,recover,recover-all-dangling-branches,514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7809#issuecomment-1125355262,3,['recover'],"['recover', 'recover-all-dangling-branches']"
Safety,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:941,detect,detected,941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368,1,['detect'],['detected']
Safety,"Hmm, looks like we lose events 1 and 3 with CollectReadCounts at 250bp using analogous ModelSegments parameters. However, I experimented with tweaking the segmentation to work on the copy ratios (rather than the log2 copy ratios), which seems to recover them. Although one of the goals of having evaluations backed by SV truth sets is to tune such parameters/methods, I'm beginning to think that SV integration might benefit from using the CNV tools in a more customized pipeline---especially if maximizing sensitivity at resolutions of ~100bp jointly with breakpoint evidence is the goal. For example, you might imagine a tool that directly uses CNV backend code to collect coverage over regions specified by `-L`, builds a PoN, denoises, and segments on the fly. Or we can put together a custom WDL optimized for sensitivity. Let's discuss in person?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222:246,recover,recover,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519#issuecomment-372875222,1,['recover'],['recover']
Safety,"How much does count collection cost at the desired bin size? How does this compare to bincov? Perhaps we could eliminate one of these steps if redundant. Note that the read counts are read once and stored in memory, so unless this takes a significant amount of time, then indexing is probably not the highest priority here (although I agree it would be nice to have in general). One related issue, as you mention, is file localization---since each shard only operates on a portion of the counts in each sample, it is a bit wasteful to localize the whole file. But how much does file localization cost? I can't imagine that it is the lowest hanging fruit. One of the more important issues, which you also mention, is optimizing parameters for inference. This includes not only the minimum number of epochs for training, but also things like the learning rate, annealing schedule, iterations per epoch, conditions for epoch convergence, etc. I'll be talking about how to tune these inference parameters---as well as other things in the pipeline---at the next BSV meeting. Let's brainstorm more things to try and prioritize them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932:143,redund,redundant,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5288#issuecomment-427562932,2,['redund'],['redundant']
Safety,"I addressed partially your comments (and fixed a compilation error due to the tests using the previous arguments). One of the major points of discussion are the following:. * `Collection` instead of `List`: I think that the first is more flexible, because a client maybe wants to have a `LinkedHashSet` as the argument to avoid repetition of the same filter. I agree that the abstract class should discourage not honoring the user order.; * Access to methods/fields: I think that the plugin could be used outside GATK in a different way by extending it. I explained some of my usage cases in one of the comments in the code, but just by overriding a simple method the whole plugin could be used very nicely in some of them. I would prefer to do that than copy your code and re-implement the bits that I would like to change. Back to you for your ideas on this, @cmnbroad!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208:322,avoid,avoid,322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2355#issuecomment-275359208,2,['avoid'],['avoid']
Safety,"I addressed the comments, @droazen. But I found that the way to detect the codec for the files is relying on `canDecode`. But even if the a block-compressed file could be read by a codec through the `AbstractFeatureReader`, the `canDecode` for bed files returns false for block-compressed extension. This should be change at the htsjdk level if GATK is planing to read block-compressed feature inputs. The same for the codecs implemented in the framework (I did it for the `SAMPileupCodec`), but for instance the table codec could not be used in a compressed way because you can't create a tabix index for it...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2131#issuecomment-246296390:64,detect,detect,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2131#issuecomment-246296390,1,['detect'],['detect']
Safety,I agree a lot of the detailed content is redundant and not worth maintaining in separate places. Reducing to 2 or 3 lines seems a bit too drastic though -- I would love to see something like a half-page high-level overview to serve as cliff notes for the WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108:41,redund,redundant,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889#issuecomment-484356108,1,['redund'],['redundant']
Safety,"I agree about avoiding `AuthHolder`, so here's a new PR that uses NIO for getting the header from GCS. I haven't set the reference on the `SamReaderFactory` since there is no `Path`-based method. This means that reading CRAMs from GCS will not work, but that's no worse than it is now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-286163712:14,avoid,avoiding,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2450#issuecomment-286163712,1,['avoid'],['avoiding']
Safety,"I also have the same problem here. I don't really care about the GQ field, but I do care about the AD fields. I get over-counting due to overlapping reads. @davidbenjamin what options should I use with Mutect2 to avoid this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-521379039:213,avoid,avoid,213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5436#issuecomment-521379039,1,['avoid'],['avoid']
Safety,"I am also seeing this warning 3x with 4.0.11.0 on a cluster but outside of docker (centos 6). . ```; 18:05:08.861 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg/gatk/4.0.11.0/install/bin/gatk-package-4.0.11.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 24, 2018 6:05:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.NoRouteToHostException: No route to host (Host unreachable); at java.net.PlainSocketImpl.socketConnect(Native Method); at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350); at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206); at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188); at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392); at java.net.Socket.connect(Socket.java:589); at sun.net.NetworkClient.doConnect(NetworkClient.java:175); at sun.net.www.http.HttpClient.openServer(HttpClient.java:463); at sun.net.www.http.HttpClient.openServer(HttpClient.java:558); at sun.net.www.http.HttpClient.<init>(HttpClient.java:242); at sun.net.www.http.HttpClient.New(HttpClient.java:339); at sun.net.www.http.HttpClient.New(HttpClient.java:357); at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220); at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156); at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050); at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:104); at shaded.cloud_nio.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.runningOnComputeEngine(ComputeEngineCredential",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417:431,detect,detect,431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-441873417,1,['detect'],['detect']
Safety,"I am encountering a similar error: ; ```; Using GATK jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar GenotypeGVCFs -R /lustre/haven/proj/UTHSC0013/Tristan_GATK/reference/genome.fa -V gendb:///lustre/haven/proj/UTHSC0013/Tristan_GATK//DB/chr7 -G StandardAnnotation --use-new-qual-calculator -O /lustre/haven/proj/UTHSC0013/Tristan_GATK//gvcf//merged//joint_called_gvcfs_chr7.vcf; 23:15:47.053 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 23:15:47.249 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nics/d/home/hchen3/bin/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jan 07, 2020 11:15:49 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 23:15:49.543 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.545 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.1.2.0; 23:15:49.546 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:15:49.547 INFO GenotypeGVCFs - Executing as hchen3@acf-knl002 on Linux v3.10.0-514.26.1.el7.x86_64 amd64; 23:15:49.548 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:15:49.548 INFO GenotypeGVCFs - Start Date/Time: January 7, 2020 11:15:47 PM EST; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.549 INFO GenotypeGVCFs - ------------------------------------------------------------; 23:15:49.551 INFO GenotypeGVCFs - HTSJDK Version: 2.19.0; 23:15:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057:698,Redund,Redundant,698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6340#issuecomment-571886057,1,['Redund'],['Redundant']
Safety,I am having the same issue with exome capture sequencing on 7 samples. The `CombineGVCFs` always aborts for some reason so `GenotypeGVCFs` fails. . The workaround mentioned above also doesn't work for me since I have to use a `csi` index and then `GenomicsDBImport` fails to find the index. I also have no idea how to define the the whole genome in the `-L` arguement,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-996278538:97,abort,aborts,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6357#issuecomment-996278538,1,['abort'],['aborts']
Safety,"I am still receiving security warnings about GATK 4.4.0.0:. Detected by File Paths: gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Detected by Library: pkg:java/log4j:log4j; CPE: cpe:/a:apache:log4j:1.2.17; Version End of Life Date: August 4th, 2015 at 7:00 PM",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621:60,Detect,Detected,60,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8215#issuecomment-1513816621,2,['Detect'],['Detected']
Safety,"I confirmed gatk-4.4.0.0 HaplotypeCaller results with OpenJDK-17.0.7+07 ( Linux x64 and Linux arm version downloaded from https://adoptium.net/temurin/archive/?version=17 ) on x64 CPU ( AMD Ryzen Threadripper 1950X) and Arm CPU (ARMv8 Cortex-A53) .; A following variant was detected on Arm CPU but not detected on x64. `chr20 29521758 . A G 55.64 . AC=1;AF=0.500;AN=2;BaseQRankSum=0.311;DP=41;ExcessHet=0.0000;FS=8.502;MLEAC=1;MLEAF=0.500;MQ=56.14;MQRankSum=-4.689;QD=1.36;ReadPosRankSum=0.020;SOR=1.886 GT:AD:DP:GQ:PL 0/1:36,5:41:63:63,0,1373`. Additionally, QUAL value of some variants were different between on Arm CPU and x64 CPU. By Modifeing computeLogPenaltyScore in kBestHaplotype.java from Math.log10 to StrictMath.log10 as previous comment, same variant call results were produced on both CPUs.; https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1560470696. I placed vcf files in following URL. You can see these differences.; https://pezycomputing-my.sharepoint.com/:f:/g/personal/sakai_pezy_co_jp/Eo5Gvfau1BpMszGCcfDrD14BOfMgxvk7Mt2JCFqcDfgItQ?e=wzZbpL. On x64: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.vcf; On x64 Modified to StrictMath: PFDATCV2HG002.pz_pipeline.GRCh38.chr20-29520758-29522758.StrictMath.vcf; On ARM: raspberrypi2.vcf; On ARM Modified to StrictMath: raspberrypi2.StrictMath.vcf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558:274,detect,detected,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8338#issuecomment-1563841558,2,['detect'],['detected']
Safety,I don't know... @cmnbroad Is there a way to detect if a cram requires a reference without reading the whole cram?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645554060:44,detect,detect,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6665#issuecomment-645554060,1,['detect'],['detect']
Safety,"I don't see any reason that we need to make redundant disable be an error, since there is no harm in it. (We chose to throw for redundant **enable** because it has a small perf ramification; also, we can't just prune it out since if the redundant filter has args there will be ambiguity). @magicDGS I'd much prefer that we converge on the rules in this ticket before we make any more PRs, and then make one PR with all the changes, rather than fragmenting the fixes across multiple PRs. It makes it much easier to review, more likely that we'll get it right, and won't fragment the discussion across multiple tickets. Thx!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423:44,redund,redundant,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377#issuecomment-276987423,3,['redund'],['redundant']
Safety,"I don't think that hiding/disable arguments would work in every case: sometimes, an argument shouldn't be exposed but still available to set programmatically, or maybe just reduce visibility making it `@Hidden` and/or `@Advance`. What is the problem of making an interface for the top-level argument to the GATK? Changing the interface or the `CommadnLineProgram` has the same effect, but the API user can still behave the same as before. It is much more extensible and downstream-friendly. What's about making the `CLPConfigurationArgumentCollection` an interface always returning defaults to be able to change it in a proper way? The cycle of development of a new argument will be: 1) add a new method to the interface with a default returning what will be expected from the previous behaviour, 2) add and return by the argument in the GATK implementation, 3) use the getter in the CLP for perform the operation. This only adds the first point, and operating in 3 classes instead of 3. For API user it is really easy to maintain the previous behavior when upgrading the dependency by just using their own implementation of the class, or include the top-level new arguments by using the GATK implementation. It is much more flexible and extensible (I always think about GATK also as a library). In addition, I think that this approach is also important for evolving GATK. For example, if a new top-level argument is tagged as experimental (still not supported but requested in Barclay), removing it would allow to keep the interface (no version bump) the same and final users can still operate with the experimental argument. The same applies to the `GATKTool` base class (https://github.com/broadinstitute/gatk/issues/4341), and for downstream projects the aim should be to be able to extend safely the `CommandLineProgram` directly to implement their own toolkit using the powerful GATK framework.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003:1794,safe,safely,1794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-366185003,1,['safe'],['safely']
Safety,"I don't think we've made any guarantees about the thread safety of Funcotator or the associated datasource classes. . Also, this account seems to be a bot and I can't access its listed home pageâ€¦. I can audit the class at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172:57,safe,safety,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7376#issuecomment-891860172,1,['safe'],['safety']
Safety,"I feel like this is going to be problematic in a different way than what @magicDGS is mentioning. We expect many versions of gatk to be compatible with the same python environment. Also for performance reasons we want to start avoid rebuilding the conda environment on every push and bake it into the base docker instead. This change means we definitely have to build it every time. . It feels like we need something more sophisticated. Instead of stamping the conda environment with the gatk version that matches it, maybe we should be stamping the gatk jar and the conda environment with some version based on the conda.env? Maybe we can do something like taking the md5 of the conda.yml and pushing that into both the jar manifest and the conda environment in some way? I'm guessing this scheme has an issue with the actual python code in the gatk since I think that's installed with conda as well? I'd really like to be able to preinstall the various dependencies though and then only update the code that's part of the gatk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721:227,avoid,avoid,227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5081#issuecomment-411214721,1,['avoid'],['avoid']
Safety,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:316,avoid,avoiding,316,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078,2,['avoid'],['avoiding']
Safety,"I have a version of this working in the branch `cw_phase_star_allele`, but am holding off on making a PR until https://github.com/broadinstitute/gatk/pull/6859 can be merged to avoid conflicting changes to integration test files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5651#issuecomment-712158375:177,avoid,avoid,177,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651#issuecomment-712158375,1,['avoid'],['avoid']
Safety,"I have no problems whatsoever with the code, but I do have some concerns about the design:. In the SV group's pipeline, we distribute this multi-gig file from its home in the cloud once at cluster-creation time, and then reuse it for multiple client executions. There are no superfluous copies lying about anywhere, and no redundant copying operations. We can give it any name we wish, and put it anywhere we desire (except that the path must be the same on every worker). This code, if I'm reading it correctly, will redistribute the file from a non-permanent home on the master's local file system or on the HDFS (to which it must be copied redundantly at least once per cluster instantiation), and then it will further be redundantly copied to a temporary location on each worker's local file system with every client execution. I don't know if that's overhead that we can live with, or whether that might prevent us from writing clients with brief execution times. I'm just opening the issue for discussion. We also lose a little flexibility in that the image must live in the same directory as the reference, though I don't think that's a serious drawback -- it's a perfectly logical place for it. However, since we're just appending a fixed extension ("".img"") to the reference name we can only have one image file per reference, which may be a problem because different images need to be created for different versions of bwa and for various options such as the list of alt contigs. We can handle the first problem by insisting that all clients on a particular cluster stick to one version of bwa, which is probably a good idea, anyway, but I think we're stuck if clients need to specify various alt contig lists. It might be better to provide a default path of ""ref-name""+"".img"", but allow that default to be overridden. Also, just to twist the knife a bit, it's too bad we never reviewed my PR for gatk-bwamem-jni, which version-stamped the images for safety. It's now languished since July, a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350:323,redund,redundant,323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3643#issuecomment-333598350,3,['redund'],"['redundant', 'redundantly']"
Safety,"I have not had time to do any profiling, but I have looked at a lot of commits. I think it's likely that my recent changes cause some haplotypes with leading indels to be kept when previously they may have been dropped. It's hard to believe that this could cause a 10-20% slowdown via a commensurate increase in the number of haplotypes assembled. However, haplotypes with leading indels would have a disproportionate pair HMM cost since they would spoil caching of the read-haplotype pair HMM matrix at the very beginning of the matrix. That is, in addition to being particularly expensive haplotypes because they would diverge from the previous haplotype at the first position and therefore not benefit from caching at all, they would also completely destroy whatever caching the previous haplotype would have gotten. We ought to think about haplotypes that start or end with indels. It seems to me that they are bad news and very likely artifacts of assembly windows and/or reads that end in the middle of an STR. I would worry about discarding them outright, because what if all the real variation is attached to haplotypes like this. Therefore, I think the best thing to do is to choose assembly windows more carefully and increase or decrease padding to avoid ending in an STR. Avoiding assembly windows that end in STRs is a wise thing to do regardless, so how about I make a branch for that and we can see if the performance regression goes away?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595:1260,avoid,avoid,1260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6567#issuecomment-623054595,2,"['Avoid', 'avoid']","['Avoiding', 'avoid']"
Safety,"I have the seam problem with app engine and Mac OS ; INFORMACIÃ“N: Failed to detect whether we are running on Google Compute Engine.; I init cloud, I am SU my IDE is Eclipse.; Run as - App Engine, I don't have a problem with ubuntu, just MAC",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-559145335:76,detect,detect,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5447#issuecomment-559145335,1,['detect'],['detect']
Safety,"I have this class from an ancient branch that makes dealing with colors nicer in some ways. It tries to avoid printing colors to non-interactive things and it makes it harder to forget a reset:. ```; /**; * Provides ANSI colors for the terminal output *; */; public final class TerminalColors {. private TerminalColors(){};. private enum TerminalColor{; CYAN(""\u001B[36m""),; RED(""\u001B[31m""),; GREEN(""\u001B[32m""),; WHITE(""\u001B[37m""),; BOLD(""\u001B[1m""),; RESET(""\u001B[0m""); // reset the colors. private final String color;. TerminalColor(String color){; this.color = color;; }. public String getColorString(){; return color;; }. }. public static boolean isInteractive(){; return !(System.console() == null);; }. public static String cyan(String toColor){; return colorString(toColor, TerminalColor.CYAN);; }. public static String red(String toColor){; return colorString(toColor, TerminalColor.RED);; }. public static String green(String toColor){; return colorString(toColor, TerminalColor.GREEN);; }. public static String white(String toColor){; return colorString(toColor, TerminalColor.WHITE);; }. public static String bold(String toBold){; return colorString(toBold, TerminalColor.BOLD);; }. public static String colorString(String toColor, TerminalColor color) {; if(isInteractive()) {; return color.getColorString() + toColor + TerminalColor.RESET.getColorString();; } else {; return toColor;; }; }. public static String stripColorsFromString(String colorString){; String stripped = colorString;; for(TerminalColor color : TerminalColor.values()) {; stripped = stripped.replace(color.getColorString(),"""");; }; return stripped;; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4429#issuecomment-367141169:104,avoid,avoid,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4429#issuecomment-367141169,1,['avoid'],['avoid']
Safety,"I just heard that production is moving to GatherVCFs, which is getting some update that might be pertinent. I know that the workflows say that they are using MergeVCFs because of other issues:. ```; # using MergeVcfs instead of GatherVcfs so we can create indices; # WARNING 2015-10-28 15:01:48 GatherVcfs Index creation not currently supported when gathering block compressed VCFs.; ```. ---. This concern is germane to any WGS analyses and perhaps not concerning for WES analyses. So perhaps our current WDL workflows that use SplitIntervals could expressly state that they are not safe for WGS variant calling analyses.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306888339:584,safe,safe,584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3061#issuecomment-306888339,1,['safe'],['safe']
Safety,I just ran a preliminary test and it appears that the transient attribute field is apparently getting purged between `if (trimmingResult.hasLeftFlankingRegion())` and `if (trimmingResult.hasRightFlankingRegion())` `ReferenceConfidenceModel` calls. That means this should be a safe operation but I would rather be absolutely that this won't cause problems. To that end I would like to explicitly clean the transient attribute fields before every reference confidence call for an added layer of protection.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5908#issuecomment-488329795:276,safe,safe,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908#issuecomment-488329795,1,['safe'],['safe']
Safety,"I like the idea of the modified regexes, that seems like the best balance of usability and flexibility/power. I'd rather avoid having a slew of new special-cased arguments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640:121,avoid,avoid,121,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/588#issuecomment-309815640,1,['avoid'],['avoid']
Safety,"I misread what they said, I blame sleep deprivation. They're releasing THIS month. Probably on the 15th, so I think if we can wait until then to incorporate these changes we can avoid any problems. Are other PR's blocked on this one?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-259194901:178,avoid,avoid,178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247#issuecomment-259194901,1,['avoid'],['avoid']
Safety,"I reached out to our comms team and they fixed the ordering bug in the list of versions so 4.1.6.0 should at least be the first one on the page now. They are aware that there are some issues with the doc being hard to find/search but there's limited developer resources to make changes to the website right now. We're doing what we can to improve things!. So I think arguments that marked as *Deprecated* are listed in the doc, but if an argument is actually *removed* then it's no longer mentioned anywhere. We try to deprecate arguments for a period of time before removing them to give people warning that things might go away, but we don't have a set policy on how long/how many versions. In general we try not to remove stuff very often to avoid this kind of issue, but sometimes people do get caught by it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-613426645:745,avoid,avoid,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6547#issuecomment-613426645,1,['avoid'],['avoid']
Safety,"I reran this with retry enabled. It says it finished successfully. There was an error in the middle but it continued anyways (this isn't shown in the final output, but there was a second bar).; I'm not sure about the output, though. Where is this command supposed to leave `output.bam`? It's not on my desktop. ```; [Stage 1:=====================================> (375 + 2) / 553]17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098; java.io.EOFException: Premature EOF: no length prefix available; 	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2282); 	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:244); 	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:733); 17/03/30 18:18:46 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-369249695-10.240.0.8-1490738675068:blk_1073745922_5098 in pipeline DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK], DatanodeInfoWithStorage[10.240.0.3:50010,DS-a0c20806-0af3-4679-b8cd-9cae6ca25071,DISK]: bad datanode DatanodeInfoWithStorage[10.240.0.4:50010,DS-5596b1b5-b89c-4c39-bbd8-7423614eae0e,DISK]; 17/03/30 20:00:21 INFO org.spark_project.jetty.server.ServerConnector: Stopped ServerConnector@61cda923{HTTP/1.1}{0.0.0.0:4040}; 20:00:21.366 INFO MarkDuplicatesSpark - Shutting down engine; [March 30, 2017 8:00:21 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 165.11 minutes.; Runtime.totalMemory()=1222115328; Job [ac3f4131-f19f-47db-8cc3-82b243ad4b72] finished successfully.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369:959,Recover,Recovery,959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517#issuecomment-290541369,1,['Recover'],['Recovery']
Safety,"I see `Timeout (30 minutes) reached. Terminating ""./gradlew jacocoTestReport""`. It's not clear to me how my changes could have introduced a deadlock or similar problem. . I ran the full test suite (`./gradlew test`) locally to take a look and it passes. Took 20min. Running `SeekableByteChannelPrefetcherTest` by itself also passes, unsurprisingly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501:7,Timeout,Timeout,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-277399501,1,['Timeout'],['Timeout']
Safety,"I see, then it seems safe to use the protobuf java format. We intend to release the 0.6.0 version asap, preferably today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2634#issuecomment-298024935:21,safe,safe,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634#issuecomment-298024935,1,['safe'],['safe']
Safety,"I see. Yes, I think this is closer to the issue at hand -- for instance, we'll have variants that are called, then when we go back to the original alignments there will be no indication of alternate observations at the loci which the variants were called (and vice versa, we'll have original alignments that suggest a variant, but won't get called). In the worst cases scenarios, the haplotype alignments seem ridiculous compared to the original reads (e.g. my GATK forum post: https://gatk.broadinstitute.org/hc/en-us/community/posts/360075749392-Bamout-haplotypes-are-much-worse-than-bamout-tumor-reads-would-suggest). So, right, it's not the realignment of the reads, it's the alignment of the assembled haplotypes that complicates things. Thinking about it in this context though, I see we might be at an impasse (or back at square one with considering the assembly process). . My true fear is that we'll lose what appear to be definite-somatic variants in the original alignments because of the internal assembly + haplotype alignment engine. But I will say, it does seem like we see these problems for lower-coverage areas/variants, so I might be exaggerating the issue at hand. Maybe the solution for testing our expectations is to use higher depths to avoid bad assemblies and to apply some post-processing to the calls. (And wait and see if there are truly troubling cases of ""obvious"" variants getting lost)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-773628190:1260,avoid,avoid,1260,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7064#issuecomment-773628190,1,['avoid'],['avoid']
Safety,"I set TEST_TYPE to ""all"" and was able to run this test without failure. The command I used is:; ```; ./gradlew test --tests org.broadinstitute.hellbender.utils.nio.GcsNioIntegrationTest.openPublicFile; ```; I ran it 10 times and got the same result every time:; `BUILD SUCCESSFUL`. It looks like this was a transient problem: either the internet connection was stalled or the authentication server was down temporarily. As this happens at the very beginning of the execution, it's probably not a very big deal: the user can just retry. Incidentally, PR #2506 that is under review lengthens the connection timeouts, if I am not mistaken. This will probably make the problem less likely to reoccur.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260:605,timeout,timeouts,605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2514#issuecomment-288852260,1,['timeout'],['timeouts']
Safety,"I still got the same error with version 4.1.9.0. . > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/orange/reed/nhouse/Raw_seqs/SEQ9_samples/tmp; 11:30:50.248 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 11:30:50.478 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/apps/gatk/4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Oct 26, 2020 11:30:50 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:30:50.791 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.791 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.1.9.0; 11:30:50.792 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:30:50.792 INFO CombineGVCFs - Executing as nwijewardena@c3a-s8.ufhpc on Linux v3.10.0-1062.18.1.el7.x86_64 amd64; 11:30:50.792 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 11:30:50.792 INFO CombineGVCFs - Start Date/Time: October 26, 2020 11:30:50 AM EDT; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.793 INFO CombineGVCFs - ------------------------------------------------------------; 11:30:50.794 INFO CombineGVCFs - HTSJDK Version: 2.23.0; 11:30:50.794 INFO CombineGVCFs - Picard Version: 2.23.3; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:30:50.794 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:30:50.795 INFO CombineGVCFs - Deflater: IntelDeflater; 11:30:50.795 INFO CombineGVCFs - Inflater: IntelInflater; 11:30:50",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444:193,Redund,Redundant,193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766#issuecomment-716640444,2,"['Redund', 'detect']","['Redundant', 'detect']"
Safety,I talked to travis support and I believe this is now fixed:. From travis support:; > I've happily increased the timeout to 70 minutes on your broadinstitute/gatk repository. Please let me know if you need to increase it further.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2808#issuecomment-306233923:112,timeout,timeout,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2808#issuecomment-306233923,1,['timeout'],['timeout']
Safety,"I think for SQ you could limit the lines you write out to the contigs that are covered in your intervals list, ignore the rest. So you can access that info as soon as you've parsed the command line. Or if you're running without an intervals list you could supply a seq dict file through a separate arg. But the former seems safer. . For other modes: EXTREME would hardcode what we consider required. Then you could potentially do ARBITRARY to allow passing strings for specific attributes that you want to drop. . None of these would have to depend on what's in the calls, I think.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687:324,safe,safer,324,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2233#issuecomment-266059687,1,['safe'],['safer']
Safety,"I think it triggers in certain situations where a firewall is blocking the connection. If the internet is simply unreachable it doesn't happen, so I don't know what the exact error case is. It happened consistently for people inside Intel's firewall or vpn. . An option to disable gcs support isn't a bad idea, it's kind of a hack though, it would be better if we could understand and avoid triggering the problem. If we could only initialize GCS support when we are sure that we actually are accessing files from google that could be a useful, but it doesn't seem like there's any single point we can plug into to detect that, it would have to be spread over everything that uses paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141:385,avoid,avoid,385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491#issuecomment-427432141,4,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"I think it would be OK to make an `ImpreciseVariantDetector` that is only responsible for returning a list of imprecise variants. But I'd rather not hide the fact that it's annotating the breakpoint-detected variants inside that class. Why not break up these two things and push the annotation close to where the breakpoint-detected variants are created (ie. in `discoverSimpleVariants` or something), and then call the imprecise variant detector after that?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357354110:199,detect,detected,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3934#issuecomment-357354110,3,['detect'],"['detected', 'detector']"
Safety,"I think thatâ€™s a great idea, @cmnbroad. I think this avoids most if not all of the pitfalls I mentioned above. Moreover, if each tool also has an argset containing default values for all optional parameters, it probably also solves the problem of having to sync defaults in the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838:53,avoid,avoids,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838,1,['avoid'],['avoids']
Safety,"I think we should do this in 2 PRs. First @asmirnov239 can first add his modification to run multiple samples in case mode. I can do a subsequent PR for ""double-chunking"" intervals to avoid PAPI/quota errors for high-resolution WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583:184,avoid,avoid,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5054#issuecomment-408629583,1,['avoid'],['avoid']
Safety,I tried the latest GATK release and also reported errors.; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx50G -Djava.io.tmpdir=./tmp -jar /public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R /public/home/gaoshibin/B73_REF/Zea_mays.AGPv4.dna.toplevel.fa -V gendb://./CHR7_gvcf_database -G StandardAnnotation --genomicsdb-shared-posixfs-optimizations true -O new_ALL_MATERIALS_chr7.g.vcf.gz; 17:49:50.404 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 17:49:50.653 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/public/home/gaoshibin/software/GATK/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:49:51.271 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.273 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.2.6.1; 17:49:51.273 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:49:51.273 INFO GenotypeGVCFs - Executing as gaoshibin@comput6 on Linux v3.10.0-693.el7.x86_64 amd64; 17:49:51.274 INFO GenotypeGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_211-b12; 17:49:51.274 INFO GenotypeGVCFs - Start Date/Time: 2022å¹´5æœˆ22æ—¥ ä¸‹åˆ05æ—¶49åˆ†50ç§’; 17:49:51.274 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.275 INFO GenotypeGVCFs - ------------------------------------------------------------; 17:49:51.276 INFO GenotypeGVCFs - HTSJDK Version: 2.24.1; 17:49:51.276 INFO GenotypeGVCFs - Picard Version: 2.27.1; 17:49:51.276 INFO GenotypeGVCFs - Built for Spark Version: 2.4.5; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:49:51.277 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_S,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848:613,Redund,Redundant,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7866#issuecomment-1135301848,1,['Redund'],['Redundant']
Safety,"I tried this again carefully (12 runs) and only one failed, due to a ""remote server unavailable"" error. This is the same we see before this PR (we need to retry more aggressively) so I think we can merge safely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631:204,safe,safely,204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2391#issuecomment-281430631,1,['safe'],['safely']
Safety,"I was looking into this because it is useful for me, and I have found that there is going to be redundancy between the `VariantAnnotatorEngine`code and the plugin. Here a couple of suggestions after trying to implement something in this regard time ago:. * Remove/deprecate the private class `AnnotationManager` in favor of the plugin. The current code is performing reflection operations by itself, and this can cause some problems.; * Refactor the `VariantAnnotatorEngine` constructors in favor of a constructor from the barclay plugin and a list of annotations to apply, to avoid the `AnnotationManager` implementation.; * Remove/deprecate static methods for creating an annotator engine (`ofAllMinusExcluded` and `ofSelectedMinusExcluded`) in favor of handling this in the plugin.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922:96,redund,redundancy,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3287#issuecomment-316077922,2,"['avoid', 'redund']","['avoid', 'redundancy']"
Safety,I was push an old branch and needed to pull from master and rebase. So I closed this one to avoid confusion @droazen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5761#issuecomment-469880173:92,avoid,avoid,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5761#issuecomment-469880173,1,['avoid'],['avoid']
Safety,"I was thinking more carefully on this and another option is create methods in `ReadPileup` to fix the overlaps after construction and/or getBaseCounts without overlaps. This won't break the behaviour of LIBS and it is up to the user to change overlaps. But for performance issues, I would like to have a variable in `ReadPileup` for track if the overlaps are corrected/fixed, to avoid recomputation. I can implement this in a different PR or in this one if the basic idea behind this one is not accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555:379,avoid,avoid,379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041#issuecomment-235993555,1,['avoid'],['avoid']
Safety,"I will start working on this off of the work that currently resides in #6034. The proposal will be to perform KBestHaplotype finding for multiple source/sink vertexes and then perform smith waterman on the resulting ""dangling"" haplotypes that are created in order to recover the probable dangling sequence. Hopefully the number of haplotypes will have been brought down by enough that this operation will be tolerable in terms of cost.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376:267,recover,recover,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5957#issuecomment-511024376,1,['recover'],['recover']
Safety,"I would suggest adding the --use-jdk-deflater & --use-jdk-inflater options in all the steps to avoid this kind of error, which seems to be random.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366:95,avoid,avoid,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7582#issuecomment-991514366,1,['avoid'],['avoid']
Safety,"I'd also be hesitant to break the previous expectation that IntervalArgumentCollection contains a non-empty list of intervals. If I understand correctly (and apologies if not, I'm glancing at the repo between paternity-leave duties and am quite sleep deprived!), all calling code would have to add an explicit check that the new option isn't enabled or risk failing ungracefully downstream. For CNV code, this might be as simple as changing the validation method `CopyNumberArgumentValidationUtils.validateIntervalArgumentCollection`, but I wouldn't generally expect it to be so straightforward to add such checks throughout the codebase. I also agree with @lbergelson that the expected behavior might not be immediately clear and that perhaps this could be addressed in the scattering step---seems like shards could just be limited to regions that cover the resource at the outset. Consider also an older comment at https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845 about whether or not we should just use the equivalent Picard tool (horrible glob aside).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687:353,risk,risk,353,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6209#issuecomment-540740687,2,['risk'],['risk']
Safety,"I'm a big fan of the htsjdk precision code: https://github.com/samtools/htsjdk/blob/master/src/main/java/htsjdk/variant/vcf/VCFEncoder.java . That being said, fixed point is fine. We don't really need the precision of scientific for the standard GATK annotations. I would like the decimal point to always be present (somewhere I made an assumption about that in order to avoid a bunch of `instanceof` calls) and total number of digits being constant is good.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-356085053:371,avoid,avoid,371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4047#issuecomment-356085053,1,['avoid'],['avoid']
Safety,"I'm getting a similar error. any solutions?; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314:269,detect,detected,269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6598#issuecomment-1243013314,1,['detect'],['detected']
Safety,"I'm having a similar issue on gatk 4.2.6.1; ```; 17:14:13.170 INFO FuncotateSegments - The following datasources support funcotation on segments:; 17:14:13.171 INFO FuncotateSegments - Gencode 34 CANONICAL; 17:14:13.209 INFO FuncotatorEngine - VCF sequence dictionary detected as B37 in HG19 annotation mode. Performing conversion.; 17:14:13.209 WARN FuncotatorEngine - WARNING: You are using B37 as a reference. Funcotator will convert your variants to GRCh37, and this will be fine in the vast majority of cases. There MAY be some errors (e.g. in the Y chromosome, but possibly in other places as well) due to changes between the two references.; 17:14:13.411 INFO ProgressMeter - Starting traversal; 17:14:13.412 INFO ProgressMeter - Current Locus Elapsed Minutes Features Processed Features/Minute; 17:14:15.391 INFO FuncotateSegments - Shutting down engine; [September 11, 2022 5:14:15 PM GMT] org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments done. Elapsed time: 0.30 minutes.; Runtime.totalMemory()=1752170496; java.lang.IllegalArgumentException: Invalid interval. Contig:chr1 start:917445 end:911649; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.findInclusiveExonIndex(SegmentExonUtils.java:95); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.segment.SegmentExonUtils.determineSegmentExonPosition(SegmentExonUtils.java:63); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2939); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSegmentFuncotations(GencodeFuncotationFactory.java:2914); at o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062:268,detect,detected,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676#issuecomment-1252518062,1,['detect'],['detected']
Safety,"I'm honestly not sure what's going on here. The picard/gatk differences will be resolved soon and then the sed command can go away. I don't think it's worth doing anything until then. We should be able to write vcf.gz so if there's an issue there we want to know about it. There could either be an issue in the TabixIndex writer (which we know for sure has issues, one is a blocker on the 11k project at the moment. ) Alternatively the input file could just be invalid, in which case our vcf writer should be detecting the error and is not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877:509,detect,detecting,509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030#issuecomment-306632877,1,['detect'],['detecting']
Safety,"I'm opposed to including 2 entire references since it will raise our git lfs files to somewhere around 5gb. This is a significant drag on downloading / building / testing gatk and should be avoided if possible. I understand that I may be overruled here, but keeping the test files to a reasonable size was and should remain an important goal of gatk4. . It looks like there may be some options to slim down the existing test files that we should take advantage of if possible. There are a number of large vcfs and fasta files which are NOT currently compressed in our large files. We should compress them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603:190,avoid,avoided,190,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5111#issuecomment-423617603,1,['avoid'],['avoided']
Safety,"I'm surprised none of the variant calling integration tests change. @cmnbroad Would you expect this to change the behavior in any common use cases or is this more of a safety check?. It's admittedly also possible that the MT calling in the Mutect2 integration test does change slightly, but that's a very lenient concordance check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456900212:168,safe,safety,168,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594#issuecomment-456900212,1,['safe'],['safety']
Safety,I've checked the README file (see below). I think it's the latest one because I cannot see the newer version in the ftp site. Funcotator was able to produce some output. So I tried Funcotator with the same VCF input but with only first 5 loci. The program terminated safely. I hypothesized that the bug happened with only some records in the VCF file which was produced from Mutect2. Please help. +---------------------------------------------+; | Data Source Version Information |; +---------------------------------------------+. Version: 1.2.20180329; Source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.2.20180; 329.tar.gz; Alternate Source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.2.20180329.tar.gz,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333:267,safe,safely,267,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333,1,['safe'],['safely']
Safety,"I've implemented the Gaussian-kernel binary-segmentation algorithm from this paper: https://hal.inria.fr/hal-01413230/document This method uses a low-rank approximation to the kernel to obtain an approximate segmentation in linear complexity in time and space. In practice, performance is actually quite impressive!. The implementation is relatively straightforward, clocking in at ~100 lines of python. Time complexity is O(log(maximum number of segments) * number of data points) and space complexity is O(number of data points * dimension of the kernel approximation), which makes use for WGS feasible. Segmentation of 10^6 simulated points with 100 segments takes about a minute and tends to recover segments accurately. Compare this with CBS, where segmentation of a WGS sample with ~700k points takes ~10 minutes---and note that these ~700k points are split up amongst ~20 chromosomes to start!. There are a small number of parameters that can affect the segmentation, but we can probably find good defaults in practice. What's also nice is that this method can find changepoints in moments of the distribution other than the mean, which means that it can straightforwardly be used for alternate-allele fraction segmentation. For example, all segments were recovered in the following simulated multimodal data, even though all of the segments have zero mean:. ![baf](https://user-images.githubusercontent.com/11076296/29100464-ad687946-7c79-11e7-99e4-962ab93709b4.png). Replacing the SNP segmentation in ACNV (which performs expensive maximum-likelihood estimation of the allele-fraction model) with this method would give a significant speedup there. Joint segmentation is straightforward and is simply given by addition of the kernels. However, complete data is still required. Given such a fast heuristic, I'm more amenable to augmenting this method with additional heuristics to clean up or improve the segmentation if necessary. We can also use it to initialize our more sophisticated HMM m",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666:696,recover,recover,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2858#issuecomment-321121666,2,['recover'],['recover']
Safety,"If consolidate ran without issues, that array/contig folder didn't have any fragments that were incomplete. And yes, the duplicates were consolidated down so everything should be fine. . I should add, my last bullet about sanity checks regarding number of fragments does not apply to consolidated arrays. Those will, of course, only have a single fragment.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110:222,sanity check,sanity checks,222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722733110,1,['sanity check'],['sanity checks']
Safety,"If it helps, I have seen this error when using local drives exclusively (not attached to a shared file system). . Twice it has manifested as a core dump that points to ` C [libc.so.6+0xaf4f9] malloc+0x169`: . ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; [dalegre@login4601 fdone]$ head -n 20 hs_err_pid1182729.log; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x000014cfb1d504f9, pid=1182729, tid=1195264; #; # JRE version: OpenJDK Runtime Environment (17.0.3) (build 17.0.3-internal+0-adhoc..src); # Java VM: OpenJDK 64-Bit Server VM (17.0.3-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xaf4f9] malloc+0x169; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h"" (or dumping to /gpfs/gpfs_de6000/home/dalegre/projects/1000-Genomes/jointcalling-test/goast_workflows/JointCalling/test_samples-1000.1.3/core.1182729); #; # If you would like to submit a bug report, please v",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520:242,detect,detected,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8683#issuecomment-1936285520,1,['detect'],['detected']
Safety,If it's easier we should just detect when this happens and add a warning annotation to these records.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3749#issuecomment-376249723:30,detect,detect,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3749#issuecomment-376249723,1,['detect'],['detect']
Safety,"In GATK4, the way to make a tool multithreaded is to implement it as a Spark tool. All Spark tools can be trivially parallelized across multiple threads using the local runner, and across a cluster using spark-submit or gcloud. . We wanted to avoid the complexities of implementing our own map/reduce framework, as was done in previous versions of the GATK, and instead rely on a standard, third-party framework to keep the GATK4 engine as simple as possible.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164:243,avoid,avoid,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273206164,2,['avoid'],['avoid']
Safety,"In a similar vein, would it be feasible to allow sample-matched RNA-seq data to be specified as input, so that the annotation is based on the actual isoform(s) that is (are) transcribed in a particular sample? The same SNV may be annotated in two different ways in two different samples, if the isoform(s) inferred by RNA-seq data differ (e.g. exonic for Patient A, intronic for Patient B). It avoids subjective prioritisation lists like the ones above and is instead data-driven and contextual.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7631#issuecomment-1032216007:394,avoid,avoids,394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7631#issuecomment-1032216007,1,['avoid'],['avoids']
Safety,"In general our germline tools are designed for short variants. I don't think any of them will handle a millions long indel well or at all. The SV or CNV tools sound like a better fit although I'm not sure exactly if they cover your use case exactly. Typically we process short variants and long variants like this separately. . We should be detecting this variant up front on when loading into genomicsDB if it's going to be problematic to retrieve it, and we should be giving a better error message. I don't think we'll be able to handle it through GenotypeGVCFs in any helpful way though. (The best I can imagine it doing is passing it through ungenotyped.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770:341,detect,detecting,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7976#issuecomment-1376029770,1,['detect'],['detecting']
Safety,"In the WGS SV pipeline, for deletions and duplications that the pipeline believes to be biallelic we do the following:. - ALT: `<DEL>` or `<DUP>`; - SVTYPE: `DEL` or `DUP`; - GT: `0/0` or `0/1` or `1/1`. We currently report depth based copy number and quality for these variants in custom format fields `RD_CN` and `RD_GQ` if they are available; we could possibly move those values to the standard `CN` and `CNQ`, but there is some complexity in how to handle events detected by paired end and split reads without good read depth support; ie those under 1kb or so depending on our depth binning size and the coverage. Our depth genotyping module makes estimates of copy number for these sites but sometimes these can be very inaccurate so at the moment we prefer not to report total copy number in those fields. Probably what we _should_ do is fill in CN with 0, 1, or 2 based on the genotype we emitted and set CNQ to the value we computed for GQ. For multiallelic CNVs (i.e. sites where our model is not sure that the variant is bi-allelic) we write:. - ALT: `<CNV>`; - SVTYPE: `CNV`; - GT and GQ: `.`; - CN and CNQ: estimate of total (diploid/unphased) copy number and quality of the depth evidence. I think there are some tradeoffs in completely characterizing the evidence for and quality of each call and enabling easy searching across the whole VCF without having to parse and understand the entire record. Older versions of our pipeline used to put the diploid copy number of the event into the GT field, I think similarly to what's being described above. This is incorrect VCF -- GT values should be indices into the allele list for the variant, and should be a list of length equal to the ploidy. . My view is that if you can confidently infer the alleles present at the site in the sample set you should use a GT value of the form `0/1`, and if you don't know or aren't interested in trying to infer them you should use CN for total copy number and CNQ for the quality. CNF is also availabl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171:467,detect,detected,467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6167#issuecomment-622053171,1,['detect'],['detected']
Safety,"In this context, for a given mutation, there might be a hundred or so reads; and each cell is only contributing one to three reads. For other; mutations, maybe there's less than 10 reads corresponding to less than 10; cells, and it can vary pretty dramatically. The total number of cells; represented in a single sample can be thousands to tens of thousands,; usually - but could be many more as the tech advances. My hack for it at the moment is to encode both the cell barcode and the UMI; information into the read name. Then, for each variant, I query the reads; that overlap that variant in the bam file and analyze each read for; supporting the variant or the REF allele - then I can count the reads; according to the specific cells and also deal with any UMI redundancy per; cell. This works pretty well except for the cases where the HC reassembly; provides evidence for the variant and I can't track it to the originally; aligned reads. Also, mostly I think the difficulty here relates to indels; around homopolymers with our pacbio long isoform reads in our rna-seq; variant pipeline that leverages the gatk rna-seq protocol with HC. On Thu, Feb 29, 2024 at 8:58â€¯AM GÃ¶kalp Ã‡elik ***@***.***>; wrote:. > Since each cell has a barcode wouldn't it be nice to use them as their; > Read Group ID and Sample Name within the BAM so that variant callers will; > distinguish each cell from their Sample Name and produce a multisample VCF; > for that variant site. Once IDs and Sample Names are split per cell you may; > be able to color them differently in IGV to even visually observe those; > events.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971203108>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABZRKX6LYHUXDUMGDU3AIFLYV4ZZLAVCNFSM6AAAAABD4OZKJ6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZRGIYDGMJQHA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971223953:766,redund,redundancy,766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8703#issuecomment-1971223953,1,['redund'],['redundancy']
Safety,"Is the VCF in your screenshot a germline resource file, such as gnomAD? Could you also post the output of `CreateSomaticPanelOfNormals` at these same sites? What samples went into the `ls *.vcf.gz` input in `GenomicDBImport`? How many samples did you use? How did you run `Mutect2` to generate these?. Also, just to be safe, a command like `ls *.vcf.gz` could easily introduce some stray VCFs that happen to be in the same directory. Are you positive that these include only the output of `Mutect2` run on normal samples?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7215#issuecomment-833498893:319,safe,safe,319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7215#issuecomment-833498893,1,['safe'],['safe']
Safety,Is there an easy way to tell what version the index was created with? . It seems like detecting a bad index should happen at the jbwa level instead of GATK itself?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-242818261:86,detect,detecting,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2123#issuecomment-242818261,1,['detect'],['detecting']
Safety,"Is this issue still open? I'm getting a similar error like this:; ```; .; .; .; INFO: Failed to detect whether we are running on Google Compute Engine.; 19:14:42.635 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.635 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.8.1; 19:14:42.635 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:14:42.638 INFO BaseRecalibrator - Executing as XXX on Linux v3.10.0-957.12.2.el7.x86_64 amd64; 19:14:42.638 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_212-b04; 19:14:42.638 INFO BaseRecalibrator - Start Date/Time: September 12, 2020 7:14:42 PM PDT ; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - ------------------------------------------------------------; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Version: 2.23.0; 19:14:42.638 INFO BaseRecalibrator - Picard Version: 2.22.8; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2 ; 19:14:42.638 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:14:42.639 INFO BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:14:42.639 INFO BaseRecalibrator - Deflater: IntelDeflater; 19:14:42.639 INFO BaseRecalibrator - Inflater: IntelInflater; 19:14:42.639 INFO BaseRecalibrator - GCS max retries/reopens: 20; 19:14:42.639 INFO BaseRecalibrator - Requester pays: disabled; 19:14:42.639 INFO BaseRecalibrator - Initializing engine; 19:14:43.472 INFO FeatureManager - Using codec BEDCodec to read file XXX; 19:14:43.726 WARN IndexUtils - Feature file XXX appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 19:14:43.755 INFO BaseRecalibrator - Done ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264:96,detect,detect,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807#issuecomment-691600264,1,['detect'],['detect']
Safety,"It also fails in Mac OS X 10.11.6 x86_64. I'm trying to update my project to the latest version of GATK and this dependency throws the following error with some of my gradle tests and while running an uber-jar (using `--use_jdk_deflater false`):. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x000000011d925644, pid=7088, tid=20739; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libgkl_compression8215566221555962564.dylib+0x1644] Java_com_intel_gkl_compression_IntelDeflater_resetNative+0x164; #; # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/ReadTools/hs_err_pid7088.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. Find attached the log: [hs_err_pid7088.log.txt](https://github.com/broadinstitute/gatk/files/652421/hs_err_pid7088.log.txt). Should I open a different issue for this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689:280,detect,detected,280,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302#issuecomment-267103689,1,['detect'],['detected']
Safety,"It can be removed. I'm working in a new branch and full training was required, thus the predictor-only package is not beneficial.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7839#issuecomment-1122810265:88,predict,predictor-only,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839#issuecomment-1122810265,1,['predict'],['predictor-only']
Safety,"It looks like this is happening because the `GATKAnnotationPluginDescriptor` only propagates pedigree arguments to annotations that derive from the `PedigreeAnnotation` class. In the forum post comments, the one report that includes an input command line specifies the `PossibleDeNovo` annotation, which isn't part of the`PedigreeAnnotation` hierarchy. So it doesn't get properly populated. I assume the other case is similar. This fix is change the hierarchy (or better yet, find a more type-safe way to identify pedigree annotations). Also, the tool doesn't appear in the docs because it doesn't have a `@DocumentedFeature` annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947:493,safe,safe,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947,1,['safe'],['safe']
Safety,It looks like we recommend this docker image in one of our tutorials [(How to part I) Sensitively detect copy ratio alterations and allelic segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments) but I do not recognize the user that created it.; @TatyanaLev could you post this issue on our forum? https://gatk.broadinstitute.org/hc/en-us/community/topics,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6836#issuecomment-696327611:98,detect,detect,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6836#issuecomment-696327611,2,['detect'],"['detect', 'detect-copy-ratio-alterations-and-allelic-segments']"
Safety,"It seems like the patch in 4.1.6 didn't go far enough and that exception needs to be replaced with a `continue` in all cases. This seems to be occurring for haplotypes with long indels inside their assembly padding that don't have enough spanning sequence to resolve. Since the variation is inside the padding, it seems safe to ignore. Increasing padding resolves the issue, alhtough this is at the cost of runtime and should not be necessary. For example, suppose we have a ref haplotype ABCDD, where A, B, and C represent sequences of, say, 100 bases and D is a sequence of 50 bases. Suppose further that A and DD are the padding. Then the cigar of an alt haplotype ABCD gets aligned as a 350 base match that doesn't span the full padded reference region, leading to the error. I still need to figure out why this didn't happen in 4.1.4 (my guess is that elsewhere the code effectively skipped these haplotypes before the exception).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533:320,safe,safe,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6533#issuecomment-607059533,1,['safe'],['safe']
Safety,"It seems to me the `Header definition line` encompasses the information given by the `VCF Field` so this latter is redundant. . It would definitely be useful to categorize INFO (cohort) versus FORMAT (SAMPLE) level annotations. I'm not clear on the significance of the `Type` nor `Category` fields. `Type` might be the groupings, e.g. HaplotypeCaller standard annotations versus Mutect2 standard annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143:115,redund,redundant,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3809#issuecomment-344423143,2,['redund'],['redundant']
Safety,"It's likely that this persists in GATK4, but this isn't high priority because in practice we've found that most of our users ignore the spanning deletion alleles or actively dislike them. There are a variety of known issues surrounding spanning deletions, including filtering of * genotypes when the upstream deletion is filtered. I've attached our b37/GRCh37 WGS interval list (no decoy contig), which is split at Ns in the reference. There are 626 intervals. If recovering all the spanning deletion at shard boundaries is important to you, you can use that list to generate your shards and not subdivide further, though I can't guarantee they will be balanced. [wgs_calling_regions.v1.interval_list.txt](https://github.com/broadinstitute/gatk/files/3116941/wgs_calling_regions.v1.interval_list.txt). I had to add a .txt extension for Github, so you'll want to rename it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930:464,recover,recovering,464,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905#issuecomment-486663930,1,['recover'],['recovering']
Safety,It's some sort of race condition; ```; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:80); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:156); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:134); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:110); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429:76,abort,aborted,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6633#issuecomment-639136429,1,['abort'],['aborted']
Safety,Iâ€™m just now getting around to trying this. . I packed the gatk spark jar into the spark-operatorâ€™s spark docker container. Now Iâ€™m just wondering if itâ€™s possible to stream the genomes from S3 to the executors? Iâ€™m trying to avoid any HDFS usage.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-541462343:226,avoid,avoid,226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6198#issuecomment-541462343,1,['avoid'],['avoid']
Safety,"Java 17.0.12 from [Oracle](https://www.oracle.com/java/technologies/downloads/#java17) seems to display the same behavior. ```; 12:19:27.622 INFO ProgressMeter - Scaffold_1:21175995 247.8 125320 505.8; 12:19:49.612 INFO ProgressMeter - Scaffold_1:21178224 248.1 125330 505.1; 12:20:02.383 INFO ProgressMeter - Scaffold_1:21179909 248.4 125340 504.7; 12:20:14.545 INFO ProgressMeter - Scaffold_1:21183582 248.6 125360 504.4; 12:20:25.422 INFO ProgressMeter - Scaffold_1:21255583 248.7 125670 505.2; 12:20:36.810 INFO ProgressMeter - Scaffold_1:21281660 248.9 125810 505.4; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f4ad4d94291, pid=3638446, tid=3638447; #; # JRE version: Java(TM) SE Runtime Environment (17.0.12+8) (build 17.0.12+8-LTS-286); # Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0.12+8-LTS-286, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/operations/ejaco020/gatk/core.3638446); #; # An error report file with more information is saved as:; # /bigdata/operations/ejaco020/gatk/hs_err_pid3638446.log; #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; ```. I also attempted running within a singularity container, allocating 64GB of memory to the job and specifying -Xmx60G. Still seemed to silently ""crash"". Command I ran was:; ```; singularity run gatk_4.6.0.0.sif gatk HaplotypeCaller --java-options -Xmx60G -R /rhome/ejaco020/bigdata/gatk/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam \ ; -O sing.vcf.gz \ ; -ERC GVCF; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721:600,detect,detected,600,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988#issuecomment-2389450721,1,['detect'],['detected']
Safety,"Jumping in here to add that I'd very much like the opposite behavior in other tools. My understanding is that HaplotypeCaller and GenotypeGVCFs interpret intervals the other way - only emitting variants that start within the `--intervals` given. @yfarjoun pointed out to me that this is desirable when e.g. scatter/gathering WGS samples. But it would be nice for capture data to be able to cause the HC to emit all variants that _overlap_ any of the intervals given. For example if you are capturing a gene panel, it is reasonable to want to see all deletions that delete one or more bases of any exon, regardless of whether the start of the deletion is within the exon. Right now this requires padding the intervals by an estimated ""max deletion length"" to be safe, which then causes more variants that are totally outside the intervals to also be emitted. Might I instead suggest that an argument be introduced like:. `--variant-interval-matching [STARTS_WITHIN|CONTAINED|OVERLAPS]`. that would then have different defaults based on the historical behavior of various tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572626422:761,safe,safe,761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6339#issuecomment-572626422,1,['safe'],['safe']
Safety,"Just for future reference, note that comments in `testVariantRecalibratorSNPMaxAttempts` are also incorrect or out of date. The test passes even if you limit it to one attempt. ```; // For this test, we deliberately *DON'T* sample a single random int as above; this causes; // the tool to require 4 attempts to acquire enough negative training data to succeed; ```. So again, the tests were already ""broken."" But still, rather than attempt to fix them, I think it's best to follow the principle of not changing both production and test code to the extent that it is possible in this scenario. We've already updated enough exact-match expected results to make me a bit uncomfortable!. Someone else may want to tackle fixing the tests in a separate push, but I think it makes sense for me to focus on avoiding these sorts of issues when writing tests for the new tools. EDIT: For the record, I confirmed that the undesired behavior in this test that the RNG hack was trying to avoid was fixed (and hence, the test was ""broken"") in #6425. Probably wasn't noticed because this is the only non-exact-match test and the test isn't strict enough to check that attempts 1-3 fail, it only checks that we succeed by attempt 4. Again, someone else may feel free to examine the actual coverage of this test and whether it's safe to remove it and/or clean up all the duct tape---but at some point, it becomes difficult to tell which pieces of duct tape are load bearing!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628:799,avoid,avoiding,799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7709#issuecomment-1064236628,3,"['avoid', 'safe']","['avoid', 'avoiding', 'safe']"
Safety,"Just noting here that I saw a lot of intermittent 60-minute Travis timeouts for the gCNV case mode WDL tests in #7411. I'm pretty sure that at some point we were running cohort + case together, so not sure why case alone is now hitting the limit. So might be worth investigating and tightening up the tests/data. Also note @ldgauthier's concerns about using a fake dictionary in the simulated data in #6957.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4007#issuecomment-899474100:67,timeout,timeouts,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4007#issuecomment-899474100,1,['timeout'],['timeouts']
Safety,"Just to make sure I understand the issue---will this cause technical problems in the Firecloud environment, or is it more of a style issue?. If the latter, one reason I prefer the use of optional file inputs to trigger tool-level ""modes"" when possible is that it propagates more naturally from the tool level. For example, let's consider a tool that can operate in either tumor-only or matched-pair mode. It is natural at the tool level to make the tumor a required input and the normal optional. The other options are quite awkward: 1) make both inputs required and switch between using the normal or not with a flag (in which case it is very easy for the user to shoot themselves in the foot if they forget to set the flag right, and we'd have to pass a dummy normal every time we want to run tumor only if we don't actually have a pair), 2) leave the normal as optional but add a flag anyway, which would be redundant and require an additional validation (i.e., if the flag is set to matched mode but we don't have a normal, we should fail early), or 3) write separate tools for each mode with the corresponding required inputs. If we accept that optional file input is the way to handle such a scenario at the tool level but not at the workflow level, then we will simply run into the same problems at the workflow level. I'm sure there are more complex scenarios when triggering on file presence/absence doesn't uniquely specify a workflow, in which case flags are a must. But for simple scenarios, I'm not sure why we shouldn't take advantage of the ability to specify optional file inputs in WDL (actually, I'm not sure how else we are supposed to use them?). However, if this is a problem for Firecloud, then I'd like to understand why---and what possible solutions there might be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444:911,redund,redundant,911,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3657#issuecomment-334046444,2,['redund'],['redundant']
Safety,"Just to provide additional context, all of the machinery in the AbstractRecordCollection classes for reading/writing CNV input/output TSVs was meant to make passing metadata (dictionaries, sample names, etc.) from tool to tool as automatic and consistent as possible. This avoids having to re-provide sample names, dictionaries, etc. at each tool/step---which often led to dictionary inconsistencies, contig/sample ordering bugs, etc. in older versions of the pipelines---at the cost of 1) redundantly carrying along this metadata in input/output TSVs, and 2) requiring consistent dictionaries in all initial BAM inputs. I think these are small costs to pay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610:273,avoid,avoids,273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6957#issuecomment-726973610,2,"['avoid', 'redund']","['avoids', 'redundantly']"
Safety,"Karthik;; Thanks for this, I've done that in bcbio so hopefully will avoid the issue going forward. Feel free to close here unless you want to try and trace down further what is happening. Thanks again for the pointer that led us to the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076:69,avoid,avoid,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407512076,1,['avoid'],['avoid']
Safety,"Komal -- thanks for raising this issue and providing so much detail. Karthik -- Thanks for the debugging and pointers on this, I hadn't thought from the core dump to be looking at the annotation fields so this is a really helpful lead. What we can do in bcbio is avoid adding any of these until after running the joint calling so they all get on the final joint VCF rather than the gVCFs. This should hopefully avoid needing to dig too deeply into this and we can take our lesson as: don't annotate gVCFs with too much information. Thank you again for helping with diagnosing the underlying issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407494531:263,avoid,avoid,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5045#issuecomment-407494531,2,['avoid'],['avoid']
Safety,"Let's please repair the branch before merge, rather than risk clobbering master. Squash/rebase does not interact nicely with merge commits in the history, particular if the merge commits contain changes due to conflict resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241:57,risk,risk,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2558#issuecomment-341790241,1,['risk'],['risk']
Safety,"Looking at the code in both GATK's `GenomicsDBImport` and the GenomicsDB library itself, I don't think the sample name map was ever intended as a mechanism to rename samples. It was just added as a way to avoid the up-front download of all the VCF headers. As evidence for this, we have a couple of asserts like this in the code:. ```; assert sampleName.equals(((VCFHeader) reader.getHeader()).getGenotypeSamples().get(0));; ```. However, using the map file to rename samples is a pretty natural thing for clients to want to do. At a minimum, we need to throw if a rename is attempted until sample renaming via the map file is officially supported and tested.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932:205,avoid,avoid,205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3814#issuecomment-343261932,1,['avoid'],['avoid']
Safety,"Looking at the three tools {GetPileupSummaries, CollectAllelicCounts, CollectPerBaseCounts} it definitely seems possible to eliminate a lot of redundancy. I think adding an option to count bases per read as opposed to per fragment, the current default in this tool, would essentially accomplish that---the idea being that you may want to look at read bases for modelling errors. I understand, though, that I might be kibitzing something I know far too little about and there might be many more implications to consider, but those are my general thoughts for the three tools. This seems to agree with the discussion [#4717 (comment)](https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143:143,redund,redundancy,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6545#issuecomment-610591143,1,['redund'],['redundancy']
Safety,"Looks like this failed on travis. I think given that given the lateness of the hour (release wise), we might want to take the original change that removes the libgcc-ng dependency, since that passed on travis, and rely on the simple workarounds for osx, which we'll have to convey out-of-band. Anything that requires changing the docker image seems risky at this point, not to mention that the image is already at 5.2 gig, which is way over our desired target. @samuelklee Any thoughts on this ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086:349,risk,risky,349,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4087#issuecomment-356131086,2,['risk'],['risky']
Safety,"Looks like this is my fault... I didn't realize BWA produces SAM output and the non-spark tool was correcting my mistake automatically (by checking for a magic number). Can we make the error message more informative like: ""BAM file must start with BGZF magic number""? . It would be great to detect whether it's SAM or BAM by checking the file contents, as in non-spark tools that use htsjdk, rather than the extension. Is this easily done?. @lbergelson To clarify I was using the regular BWA binaries not the GATK BWA tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378:291,detect,detect,291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3488#issuecomment-324959378,1,['detect'],['detect']
Safety,M --ba; se-quality-score-threshold 18 --dragstr-het-hom-ratio 2 --dont-use-dragstr-pair-hmm-scores false --pair-hmm-gap-continuation-penalty 10 --expected-mismatch-rate-for-read-disqualification 0; .02 --pair-hmm-implementation FASTEST_AVAILABLE --pcr-indel-model CONSERVATIVE --phred-scaled-global-read-mismapping-rate 45 --disable-symmetric-hmm-normalizing false --disable-cap-base-qu; alities-to-map-quality false --enable-dynamic-read-disqualification-for-genotyping false --dynamic-read-disqualification-threshold 1.0 --native-pair-hmm-threads 4 --native-pair-hmm-use-dou; ble-precision false --flow-hmm-engine-min-indel-adjust 6 --flow-hmm-engine-flat-insertion-penatly 45 --flow-hmm-engine-flat-deletion-penatly 45 --pileup-detection false --pileup-detection-; enable-indel-pileup-calling false --num-artificial-haplotypes-to-add-per-allele 5 --artifical-haplotype-filtering-kmer-size 10 --pileup-detection-snp-alt-threshold 0.1 --pileup-detection-i; ndel-alt-threshold 0.5 --pileup-detection-absolute-alt-depth 0.0 --pileup-detection-snp-adjacent-to-assembled-indel-range 5 --pileup-detection-bad-read-tolerance 0.0 --pileup-detection-pro; per-pair-read-badness true --pileup-detection-edit-distance-read-badness-threshold 0.08 --pileup-detection-chimeric-read-badness true --pileup-detection-template-mean-badness-threshold 0.0; --pileup-detection-template-std-badness-threshold 0.0 --bam-writer-type CALLED_HAPLOTYPES --dont-use-soft-clipped-bases false --override-fragment-softclip-check false --min-base-quality-s; core 10 --smith-waterman JAVA --max-mnp-distance 0 --force-call-filtered-alleles false --reference-model-deletion-quality 30 --soft-clip-low-quality-ends false --allele-informative-reads-o; verlap-margin 2 --smith-waterman-dangling-end-match-value 25 --smith-waterman-dangling-end-mismatch-penalty -50 --smith-waterman-dangling-end-gap-open-penalty -110 --smith-waterman-danglin; g-end-gap-extend-penalty -6 --smith-waterman-haplotype-to-reference-match-value 200 --smith-w,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:6822,detect,detection-absolute-alt-depth,6822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detection-absolute-alt-depth']
Safety,MapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41458,abort,abortStage,41458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616,2,['abort'],['abortStage']
Safety,"Master stats:; Discovered 31120 intervals.; Killed 388 intervals that were near reference gaps.; Killed 174 intervals that had >1000x coverage.; Discovered 9480784 mapped template names.; Ignoring 19200460 genomically common kmers.; Discovered 39739968 kmers.; Discovered 34170333 unique template names for assembly.; Wrote SAM file of aligned contigs.; Discovered 6255 variants.; INV: 239; DEL: 3644; DUP: 1123; INS: 1249; Elapsed time: 47.34 minutes. This PR:; Discovered 31125 intervals.; Killed 390 intervals that were near reference gaps.; Killed 174 intervals that had >1000x coverage.; Discovered 9480874 mapped template names.; Ignoring 19200460 genomically common kmers.; Discovered 39730495 kmers.; Discovered 34154214 unique template names for assembly.; Wrote SAM file of aligned contigs.; Discovered 6234 variants.; INV: 233; DEL: 3635; DUP: 1125; INS: 1241; Elapsed time: 46.77 minutes. We did find a few extra intervals by gluing evidence across partition boundaries. The number of variants detected has decreased by just a little. I think this is likely due to calculating read metadata at the library level, rather than the read group level.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2766#issuecomment-304373035:1006,detect,detected,1006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2766#issuecomment-304373035,1,['detect'],['detected']
Safety,"Meta-comments for reviewers: the new program groups in this PR are based on @sooheelee 's spreadsheet, including some that are placeholders for things that will soon live in Picard, but aren't accessible from there yet. I've left the old program groups intact because they're still being referenced by tools. As the doc PRs are merged in, eventually these will be left dangling with no references, and then we'll remove them. In the meantime we'll have some redundancies (ReadProgramGroup will be replaced by ReadDataProgramGroup, or whatever we settle on).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389:458,redund,redundancies,458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3924#issuecomment-349722389,1,['redund'],['redundancies']
Safety,"More info from @ldgauthier:. ```; Iâ€™ve only been trying the same GenomicsDBImport over and over again. I estimate it to take about; 30 hours if itâ€™s ever successful. The exception happens in different batches every time. Sam F. ; said he saw the exception too but he could eventually resubmit his way through it. His jobs are; shorter running.; ```. So it seems like the error is nondeterministic, but can't be recovered from within the same VM instance / process.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164:411,recover,recovered,411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094#issuecomment-412917164,1,['recover'],['recovered']
Safety,"Most of the scaling issues in Cromwell/Terra have been resolved. Terra still has limitations on workflow metadata size, and passing long file arrays to ever task in large scatters (i.e. the full list of counts files is passed into every gCNV shard) can limit our batch sizes for workflows that embed gCNV (e.g. GatherBatchEvidence in gatk-sv). gCNV workflows also don't call cache reliably (presumably due to timeouts) probably again due to the large file arrays, including 2D arrays.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236:409,timeout,timeouts,409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-928168236,1,['timeout'],['timeouts']
Safety,"My suspicion was wrong. We also include a safety check which cause us to correctly reject most accidental matches. If we detect 2 chromosomes with the same name but different lengths we fail even if we detect otherwise matching chromosomes. I've run all the dictionaries I could find in the gatk bundle against each and only b37 and b37_decoy are compatible with each other which is the desired behavior I believe. | | hg18 | hg19 | b37 | b37_decoy | hg38 |; | -- |------|-----|------|-----------|-------|; | hg18 | âœ… | | | | |; | hg19 | | âœ… | | | |; | b37 | | | âœ… | âœ… | |; | b37_decoy | | | âœ… | âœ… | |; | hg38 | | | | | âœ… |. ```; @DataProvider; public Iterator<Object[]> getComparisons(){; final ArrayList<Object[]> comparisons = new ArrayList<>();; final List<String> dicts = Arrays.asList(""Homo_sapiens_assembly18.dict"",; ""ucsc.hg19.dict"",; ""human_b36_both.dict"",; ""human_g1k_v37.dict"",; ""human_g1k_v37_decoy.dict"",; ""Homo_sapiens_assembly38.dict"");; for( String left : dicts) {; for (String right: dicts){; Path leftDict =Paths.get(""/Users/louisb/Downloads/dicts"", left);; Path rightDict = Paths.get(""/Users/louisb/Downloads/dicts"", right);. comparisons.add( new Object[] {leftDict, rightDict});; }; }; return comparisons.iterator();; }. @Test(dataProvider = ""getComparisons""); public void testSequenceDictionariesAgainstEachother(Path left, Path right){; String leftName = left.getFileName().toString();; String rightName = right.getFileName().toString();; SequenceDictionaryUtils.validateDictionaries(leftName,; SAMSequenceDictionaryExtractor.extractDictionary(left),; rightName,; SAMSequenceDictionaryExtractor.extractDictionary(right));; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193:42,safe,safety,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3754#issuecomment-494924193,3,"['detect', 'safe']","['detect', 'safety']"
Safety,"NIO streaming is currently used to avoid BAM localization in `CollectCounts` and `CollectAllelicCounts`, which is particularly beneficial when running on a limited set of intervals. It is also used in `JointSegmentation` to stream VCFs, which saves on disk space. Additional tasks that could benefit from streaming are those that use counts files inputs: `FilterIntervals`, `GermlineCNVCallerCaseMode`, `GermlineCNVCallerCohortMode`, `DetermineGermlineContigPloidyCaseMode`, and `DetermineGermlineContigPloidyCohortMode`, `CreateReadCountPanelOfNormals`, and `DenoiseReadCounts`. These would require that read counts are in TSV format, which we should move to using exclusively (instead of HDF5).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144:35,avoid,avoid,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144,1,['avoid'],['avoid']
Safety,"No did not. Fereshteh Izadi; ***@***.***?anonymous&ep=bwmEmailSignature> Book time to meet with ***@***.***?anonymous&ep=bwmEmailSignature>; ________________________________; From: mcollodetti ***@***.***>; Sent: Monday, 4 March 2024 19:51; To: broadinstitute/gatk ***@***.***>; Cc: Angel Izadi ***@***.***>; Author ***@***.***>; Subject: Re: [broadinstitute/gatk] I need a PON vcf (Issue #8477). CAUTION: This email originated from outside of Swansea University. Do not click links or open attachments unless you recognise the sender and know the content is safe. RHYBUDD: Daeth yr e-bost hwn o'r tu allan i Brifysgol Abertawe. Peidiwch Ã¢ chlicio ar atodiadau neu agor atodiadau oni bai eich bod chi'n adnabod yr anfonwr a'ch bod yn gwybod bod y cynnwys yn ddiogel. Did it work? I had the same problem and that change didn't do it. â€”; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1977332718>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AUS3HJIINO56ONF7EHSUANDYWTGFLAVCNFSM6AAAAAA3SP5AQKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZXGMZTENZRHA>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1993978579:559,safe,safe,559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8477#issuecomment-1993978579,1,['safe'],['safe']
Safety,"No, this dependency is not from the main xgboost project, it's a 3rd-party library that allows you to make predictions using saved model files. At the time, the plan was to train models in python, then use this java tool to apply the trained predictor. The GQ filtering project trains using a java tool, so I have to bring in the real xgboost library.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189369117:107,predict,predictions,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7950#issuecomment-1189369117,2,['predict'],"['predictions', 'predictor']"
Safety,"Not a bad idea, will look into that tomorrow. Note that you are using Tensorflow 1.4 or 1.5 and that from v1.6 even the; non-Intel optimized build supports only AVX capable machines. On Thu 11 Oct 2018, 21:07 droazen, <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/CNNScoreVariants.java; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>:; >; > > @@ -198,6 +200,13 @@; > return new String[]{""No default architecture for tensor type:"" + tensorType.name()};; > }; > }; > +; > + IntelGKLUtils utils = new IntelGKLUtils();; > + if (utils.isAvxSupported() == false); > + {; > + return new String[]{CNNScoreVariants.AVXREQUIRED_ERROR};; >; > Maybe the answer is for the conda environments to set an extra environment; > variable that would allow GATK to detect which conda environment it's in.; > Then you could have a check in CNNScoreVariants that aborts the tool only; > if AVX is not present AND you're running in the Intel conda environment,; > and point the user to the non-Intel conda environment in the error message.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5291#discussion_r224587026>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG6lr8HM6ItLWqfSaTKeVY4yCp07il29ks5uj6TugaJpZM4XNHdi>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651:915,detect,detect,915,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5291#issuecomment-429109651,2,"['abort', 'detect']","['aborts', 'detect']"
Safety,Note that David R. is only on these issues because he's the one that ported them over from gatk-protected. I think it's safe to close this---I'd hope future denoising models would be more generalizable and able to handle FFPE (even if this might require e.g. FFPE-specific resource tracks).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2847#issuecomment-926784945:120,safe,safe,120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2847#issuecomment-926784945,1,['safe'],['safe']
Safety,"Note to self: we should get rid of the MAX_READ_BATCH check as part of this, since that was motivated by timeouts IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766:105,timeout,timeouts,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766,1,['timeout'],['timeouts']
Safety,"OK - PedigreeValidationType is now set in the constructor and is final. This does not separate the two intertwined codepaths around PedigreeFile vs. FounderIds, but that was a pre-existing problem. It doesnt doesnt change the pre-existing weirdness around the timing of setting pedigreeFile and/or founderIds within GATKAnnotationPluginDescriptor, where PedigreeAnnotation gets special treatment. I dont think this makes that situation any worse. if you still have concerns on this proposal, I actually think I could make our code work if you simply exposed a protected getPedigreeFile() method on PedigreeAnnotation. I can make the SampleDB instance in my code without needed to share code here. It seemed useful to expose some of that code to avoid duplication, but if it's going to over-complicate we can remove it. Also: that one test failure seems potentially unrelated (https://travis-ci.com/github/broadinstitute/gatk/jobs/510624560)? A compile issue with javadoc?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169:745,avoid,avoid,745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7277#issuecomment-853986169,2,['avoid'],['avoid']
Safety,"OK, finally tracked down that original issue from Mehrtash concerning the bundling: https://github.com/broadinstitute/gatk/issues/4397 As we discussed, there was a lot of back and forth to try to resolve this issue, and it was confounded by a lexicographical bug (which may have been reintroduced here). The last chapter in this saga was https://github.com/broadinstitute/gatk/pull/5490. If the matrix transpose is still troublesome and we can avoid it by being more clever with WDL indexing, then maybe we can explore that. Or we can just see if there are analogous existing WDLs and borrow their solution. However, note that @mwalker174 indicated that the *creation* of the matrix itself is troublesome for call caching. If bundling is the only answer and we are willing to pay the cost of localizing all gCNV results to all shards, it might make things easier to first bundle everything up at the end of each gCNV task. Also, would Cromwell be able to handle things if we change the bundling from a) *all* gCNV results (i.e., across all samples and shards) to b) a single bundled global quantity (model + interval lists) + calls bundled (across shards) per sample? Each postprocessing task would then take the global bundle + the bundle containing calls for that sample. That seems like it would resolve Mehrtash's original complaint, while still minimizing the number of files whizzing around. We also discussed batching by sample at the postprocessing task level, but I think we want to keep this task at the per-sample level for parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744:444,avoid,avoid,444,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6607#issuecomment-632303744,1,['avoid'],['avoid']
Safety,"OK, looks like you can get around the compiler lock issues by pointing each invocation of GermlineCNVCaller to a different compilation directory. For example, invoke `gatk` by. `THEANORC=PATH/TO/THEANORC_# gatk GermlineCNVCaller ...`. This uses the `THEANORC` environment variable to set the `.theanorc` configuration file to `PATH/TO/THEANORC_#` for this instance of GATK (where you should fill in `#` appropriately). Each `PATH/TO/THEANORC_#` should be a file containing the following:. ````; [global]; base_compiledir = PATH/TO/COMPILEDIR_#; ````. Where again, `#` is filled in appropriately. The goal is to point each GermlineCNVCaller instance to a different compilation directory. @xysj1989 can you let me know if this works for you?. This is a bit of a hack. We could probably avoid this by changing the GATK code to use a specified or temporary directory for the theano directory without too much effort. However, there is an upside to using a non-temporary directory to avoid recompilation of the model upon subsequent runs. In this case, we'd just want to let the user be able to specify the theano directory (rather than dump things in `~/.theano` unexpectedly). We should think about whether this should be opt-in, i.e., should we preserve the original behavior of using `~/.theano` by default?. @mwalker174 opinions? @droazen or engine team, thoughts on what the policy should be for python/R scripts doing this sort of thing? Is it generally true that the GATK leaves no trace, other than producing the expected output?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809:784,avoid,avoid,784,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235#issuecomment-548430809,2,['avoid'],['avoid']
Safety,"OK, relaxed the exact match to a delta of 1E-6 (chosen because doubles are formatted in somatic CNV outputs as `""%.6f""`) and tests pass on Travis (modulo an unrelated intermittent timeout failure). Note also that I was also able to reproduce locally by switching between Java 8 and 11. Had to add some quick test code for doing the comparisons; not actually sure if we have other utility methods to do so somewhere in the codebase. Another interesting note: I tried to clean up the offending use of log10factorial in AlleleFractionLikelihoods, but this introduced numerical differences at the ~1E-3 level. I think all of the round tripping between log and log10 actually adds up. Some digging revealed that this was introduced way back in gatk-protected in https://github.com/broadinstitute/gatk-protected/commit/aeec297e104db9f5196cb8f8e6691133302474bc#diff-34bd76cb2a416a212e25cbfb11298207265fb9cced775918aefcdb6b91ebc247. Despite the fact that we could easily replace the use of log10factorial with a private logGamma cache, at this point I think it makes more sense to freeze the current behavior. But if similar numerical changes are introduced to ModelSegments in the future, then it might make sense to clean this up at that point as well. Anyway, changed the title of the PR to reflect this update. Should be ready to go!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014:180,timeout,timeout,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7652#issuecomment-1023793014,1,['timeout'],['timeout']
Safety,"OK, thanks @drifty914. Note that the file with num_intervals_per_scatter = 20 is a minimal test case that is run with our continuous integration tests. In real-world use, you want enough intervals in each shard to fit a denoising model---probably 5000 or more is safe. I am wondering if your issue is related to https://github.com/broadinstitute/gatk/issues/4782 and https://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user. It may be that your user ulimit is not high enough for the theano compilation directory?. Let me try to put together a fix for that issue and see if it addresses yours as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-467085960:263,safe,safe,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714#issuecomment-467085960,1,['safe'],['safe']
Safety,"OK, the first test run I tried was with 1kb bins and *no additional normals*. Coverage takes about an hour to collect per BAM and ploidy inference takes about 10 minutes. A few things:. 1) Looks like we are concordant with the truth CN on X for all but 3/40 of the samples. The GQs for these discordant calls are low (~3, 23, and 25 compared with ~400 for most of the others). 2) However, we are striking out on over half of the samples on Y. We mostly call 1 copy when the truth calls 0. Mehrtash thinks this is because a) I didn't mask out any PARs or otherwise troublesome regions on Y and b) I didn't include any other normals. I'll try rerunning with a mask first, then with other normals, and then with both. Hopefully this should clear up with just the mask. 3) There are a few samples where we strike out because the truth calls 2 copies on Y and we call 1. Mehrtash pointed out that this is most likely because the prior table we put together assumes Y can have at most 1 copy. So hopefully these are trivially recovered once we relax this. 4) The GQs are weirdly high on 1, X, and Y compared to the rest of the autosomes. @ldgauthier any idea why this might be? If there's no reason, then something funny is going on within the tool. I haven't gotten a chance to plot any of the counts data yet, either, which may make things more obvious. I'll do this today.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449:1020,recover,recovered,1020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-364234449,2,['recover'],['recovered']
Safety,"OK. However, don't forget that the denoising model is fit independently in each block. So introducing too many blocks could cause overfitting, in a sense. Also, you want to make sure that you have enough bins in each block to learn the model. 10k seems safe, but I'd spot check results first if you want to go down to 1k.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615:253,safe,safe,253,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4397#issuecomment-391071615,2,['safe'],['safe']
Safety,"OK. In GATK3, the sharding size is calculated in `GenomeAnalysisEngine.getShardStrategy`. Since `GenotypeGVCFs` is a RodWalker and does not use input reads (BAM file), the shard size is `1,000,000`. ; This might be more than a thread safety bug (which is easy to fix, by making `GenotypingEngine.calculateOutputAlleleSubset() ` `synchronized`). What worries me is If the cache of upstream deletions spans intervals, this code will not work since the processing is asynchronous. For example, if there are 2 threads and the removed deletion crosses the shard barrier and the downstream interval thread is first to process, it will not see the upstream removed deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197:234,safe,safety,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2326#issuecomment-270467197,1,['safe'],['safety']
Safety,"OfBins) + "" should be >= 0."");; >; > @asmirnov <https://github.com/asmirnov> and @samuelklee; > <https://github.com/samuelklee> are both correct, but for the future in; > cases where you *would* want an IllegalArgumentException you should use; > Utils.validateArg to render this sort of thing a one-liner.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646132>:; >; > > + doc = ""width of the bins"",; > + fullName = WIDTH_OF_BINS_LONG_NAME,; > + shortName = WIDTH_OF_BINS_SHORT_NAME,; > + optional = true,; > + minValue = 1; > + ); > + private int widthOfBins = 1;; > +; > + @Argument(; > + doc = ""width of the padding regions"",; > + fullName = PADDING_LONG_NAME,; > + shortName = PADDING_SHORT_NAME,; > + optional = true,; > + minValue = 0; > + ); > + private int padding = 0;; >; > . . . and if this padding is different from the inherited padding then; > this demands a comment to avoid confusion.; > ------------------------------; >; > In src/main/java/org/broadinstitute/hellbender/tools/copynumber/; > CreateBinningIntervals.java; > <https://github.com/broadinstitute/gatk/pull/3597#discussion_r140646146>:; >; > > +; > + // check if the bin widths are set appropriately; > + if(widthOfBins <= 0) {; > + throw new IllegalArgumentException(""Width of bins "" + Integer.toString(widthOfBins) + "" should be >= 0."");; > + }; > +; > + // get the sequence dictionary; > + final SAMSequenceDictionary sequenceDictionary = getBestAvailableSequenceDictionary();; > + final List<SimpleInterval> intervals = hasIntervals() ? intervalArgumentCollection.getIntervals(sequenceDictionary); > + : IntervalUtils.getAllIntervalsForReference(sequenceDictionary);; > +; > + // create an IntervalList by copying all elements of 'intervals' into it; > + IntervalList intervalList = new IntervalList(sequenceDictionary);; > + intervals.stream().map(si -> new Inte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211:5045,avoid,avoid,5045,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3597#issuecomment-331744211,1,['avoid'],['avoid']
Safety,"Oh nice, that seems to be the ticket. All reads are used when setting that parameter. May I ask what the logic behind performing the downsampling is? Isn't there a risk of removing valid alignments that contribute to low abundance variation events? This would maybe only really be a problem when you are analysing sequences from a population of cells/microbes, but maybe the reward is greater than the risk?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543:164,risk,risk,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7873#issuecomment-1139111543,2,['risk'],['risk']
Safety,"Oh wait, I just checked out the background. I still don't like the redundancy. Can't we just replace `PID` with `PS`?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5318#issuecomment-430747813:67,redund,redundancy,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5318#issuecomment-430747813,1,['redund'],['redundancy']
Safety,"Oh, that's interesting... I wonder if there is a sane way to detect the version mismatch. It's weird that it breaks with a NEWER version of spark. I would expect 2.1.0 to be compatible with 2.0.2. . Incidentally, it would be a good idea to upgrade to the newest spark version. #2555",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290764935:61,detect,detect,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2545#issuecomment-290764935,1,['detect'],['detect']
Safety,"Oh, that's right, I'd forgotten about the SGA license issue. Since we're; about to move to fermi-lite (hopefully), let's just hold off on checking in; the initialization script until that's done, keeping it in the known bucket; location. On Wed, Mar 8, 2017 at 11:37 AM, Steve Huang <notifications@github.com>; wrote:. > @cwhelan <https://github.com/cwhelan> I was actually debating with myself; > about whether to include the initialization script here, as it was living; > in the bucket referred to in the creation script.; > So we could do this:; > always store the initialization script locally with the creation script; > instead of referring to a script living remotely, and makes that a required; > argument. The good: this makes it easier to track changes; The bad:; > initialization script must be removed from the bucket to avoid tracking; > possible different versions.; >; > A non-technical issue: we are ""delivering"" SGA in the initialization; > script, if that comes in to this repo, legal might have a problem with it.; > On the other hand, it the initialization script lives in a place only we; > can access, we are ""installing SGA for our own use"", which is not a problem; > with the GPL license.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285093289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AArTZZPv4WyEYz-yYaZZIIjH8LBMOhZ4ks5rjtlCgaJpZM4MTqFc>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258:834,avoid,avoid,834,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2435#issuecomment-285105258,1,['avoid'],['avoid']
Safety,"Ok, safe to merge now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3961#issuecomment-355113083:4,safe,safe,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3961#issuecomment-355113083,1,['safe'],['safe']
Safety,"Okay apparently there is not a version number in the picard.jar file downloaded from https://github.com/broadinstitute/picard/releases and thus if easybuild detects a cache copy, it will use that instead of downloading it which was an older version. They forced it to download and now version is correct. I will re-try and see if the error persists. Thanks for catching that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633:157,detect,detects,157,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3154#issuecomment-1419783633,1,['detect'],['detects']
Safety,"Okay, I have a new panel for hg38 here: gs://broad-dsde-methods-davidben/mutect2-2023-panel-of-normals/mutect2-hg38-pon.vcf.gz. It has all the variants of the old panel, plus more that arose in more recent versions of Mutect2. It is also generally somewhat more conservative, with a greater bias toward precision than the previous one. This panel is intended to be used at your own risk. I can vouch that it doesn't wreck the results of our own validations but I do not have time to vet it thoroughly enough to put it in the best practices google bucket. Likewise, I cannot promise that it will improve specificity in any particular set of samples. Within several months I hope we are all running the next version of Mutect and never need to see a panel of normals again.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034:382,risk,risk,382,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1430315034,1,['risk'],['risk']
Safety,"One of our goals for alpha (https://github.com/broadinstitute/gatk/issues/961) is actually to wrap `spark-submit` and its many options to make it easier to run hellbender tools on spark. We want users to be able to type a simple command like `./hellbender ToolName [toolArgs] --sparkMaster X`, and have hellbender figure out whether to invoke `spark-submit` or `gcloud dataproc` on their behalf, and provide sensible defaults for all relevant spark options. . Perhaps there is a way in `SparkCommandLineProgram` to detect whether an option has already been set externally, and allow the default to be overridden if it has been?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633:515,detect,detect,515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1070#issuecomment-152538633,1,['detect'],['detect']
Safety,One timeout and one 137 :(,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7624#issuecomment-1004386181:4,timeout,timeout,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7624#issuecomment-1004386181,1,['timeout'],['timeout']
Safety,"Per discussions with @fleharty, we are looking to significantly revamp the automated somatic CNV evaluations in preparation for benchmarking the TH prototype. The existing evaluations use a few unsupported/experimental tools and idiosyncratic/redundant classes (e.g., the `src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/annotatedinterval` class this issue concerns), the functionality of which we can hopefully move to python-based validation code. . The aforementioned code was purposefully decoupled from supported CNV code, but since then it has been incorporated into `Funcotator` tools and `ValidateBasicSomaticShortMutations`, at least. @jonn-smith @davidbenjamin can we discuss a plan for cleaning this code up? Would it be easy to use an existing TSV/XSV class to handle the functionality needed for these tools?. @jonn-smith perhaps we should also discuss the plan for future `FuncotateSegments` development/integration with @fleharty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506:243,redund,redundant,243,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3884#issuecomment-526226506,1,['redund'],['redundant']
Safety,Probably safe then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5678#issuecomment-463780367:9,safe,safe,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678#issuecomment-463780367,1,['safe'],['safe']
Safety,"QandDP,Number=2,Type=Integer,Description=""Raw data (sum of squared MQ and total depth) for improved RMS Mapping Quality calculation. Incompatible with deprecated RAW_MQ for; mulation."">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">. ###and this is the tag for gatk 4.2; ##fileformat=VCFv4.2; ##ALT=<ID=NON_REF,Description=""Represents any possible alternative allele at this location"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another"">; ##FORMAT=<ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing gr; oup"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""The phred-scaled genotype likelihoods rounded to the closest integer"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=ClippingRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Combined depth across samples"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=ExcessHet,Number=1,Type=Float,Des",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789:16363,detect,detect,16363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8574#issuecomment-1793390789,1,['detect'],['detect']
Safety,"Rationale for engine changes:; This tool opens a large number of feature files (TSVs, not VariantContexts) and iterates over them simultaneously. No querying, just a single pass through each.; Issue 1: When a feature file lives in the cloud, it takes unacceptably long (several seconds, typically) to initialize it. A few seconds doesn't seem like a long time, but when there are large numbers of feature files to open, it adds up. This is caused by a large number of codecs (mostly the vcf-processing codecs) opening and reading the first few bytes of the file in the canDecode method. To avoid this I've reversed the order in which we test each codec, checking first if it produces the correct subtype of Feature, and only then calling canDecode. If you don't know what specific subtype you need, you can just ask for any Feature by passing Feature.class. It's much faster that way.; Issue 2: Each open feature source soaks up a huge amount of memory. That's because text-based feature reading is optimized for VCFs, which can have enormously long lines. So huge buffers are allocated. The problem is compounded for cloud-based feature files for which we allocate a large cloud prefetch buffer. (Though that feature can be turned off, which helps a little.) But the biggest memory hog is the TabixReader, which always reads in the index, regardless of whether it's used or not. Tabix indices are very large. To avoid this, I've created a smaller, simpler FeatureReader subclass called a TextFeatureReader that loads the index only when necessary. The revisions allow the new tool to run using an order of magnitude less memory. Faster, too.; Issue 3: The code in FeatureDataSource that creates a FeatureReader is brittle, and tests for various subclasses. To allow use of the new TextFeatureReader, I added a FeatureReaderFactory interface that allows one to ask the codec for an appropriate FeatureReader.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770:590,avoid,avoid,590,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8031#issuecomment-1284340770,4,['avoid'],['avoid']
Safety,"Same here, @jhl667 - good to get additional confirmation. May I ask whether you used the same gold-standard call set or another one, and whether yours was WGS or WES? . Iâ€™m linking @droazen here as well since he acted as the release manager of the affected releases. I think it would be good to get clarity soon, since probably tens of thousands of clinical samples have been processed with the affected versions in the last two years, and a reduction in precision of 10-20% (absolute) may have been relevant for clinical decisions in at least some of these cases. Also, I know how hard it is to avoid such things in what is essentially research software (and such a great one to boot), so this is not about blaming anyone but about fixing the root cause as soon as possible. If it turns out that the issue only affects this particular reference call set and no patients (for some arcane reason), then all the better in my view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171653305:596,avoid,avoid,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921#issuecomment-1171653305,1,['avoid'],['avoid']
Safety,"Samir;; I'm not entirely sure about the fix, but based on where I think it's coming from you'll want to work around the problem by avoiding at least 10bp towards the start/end of a chromosome. That's the amount of sequence context it's filling around the variant. Hope this helps for your problem case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416777409:131,avoid,avoiding,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5130#issuecomment-416777409,1,['avoid'],['avoiding']
Safety,"See #6221. On Fri, Oct 18, 2019 at 9:50 AM droazen <notifications@github.com> wrote:. > Thanks for the clarification @ldgauthier <https://github.com/ldgauthier>.; > Maybe we should add a note to that effect in the docs for the PGT; > annotation to avoid future confusion (eg., the doc string for PGT in; > GATKVCFHeaderLines)? If you submit a one-line PR, I'll approve it :); >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/6220?email_source=notifications&email_token=ABSGC5F5F3TNBJPGRPPGSJ3QPG5KVA5CNFSM4JCHKRSKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBURCTQ#issuecomment-543756622>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABSGC5AQ3YAHJTV5TEASZCDQPG5KVANCNFSM4JCHKRSA>; > .; >. -- ; Laura Doyle Gauthier, Ph.D. (she/her); Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543803156:248,avoid,avoid,248,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543803156,1,['avoid'],['avoid']
Safety,Seems like something like https://github.com/broadinstitute/gatk/issues/4794 could be avoided if we rewrote this. It seems like a pretty simple rewrite too...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481:86,avoid,avoided,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4535#issuecomment-391044481,2,['avoid'],['avoided']
Safety,"Short answer: No, it's not using any information from the genotypes at all. You should get the same results with or without genotypes. Long technical answer: Both of these issues are related to the fact that when combining multiple possible variants at a site (i.e. variants with different alleles from the GGA `-alleles` input as well as any alleles detected in the input sample data), we often need to do remapping of the reference and alternate alleles so that they can all be described in the same context. In the places that caused these bugs we were trying to do this remapping to the alleles directly, but the fact that the variants had genotypes which referred to the original set of non-remapped alleles meant that those original alleles were still kept around and caused an error in the downstream code that tries to put together the final variant record to be emitted at the site. So it was just a case of links within the data structures that hold the genotypes interfering with the intended result, which should be based only on the input set of alternate alleles.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224:351,detect,detected,351,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5355#issuecomment-433468224,1,['detect'],['detected']
Safety,Should I wait for a bug fix ? Can I do something to avoid the error? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388:52,avoid,avoid,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388,1,['avoid'],['avoid']
Safety,"Should this be included here or should we fix it in htsjdk and not do a sanity check here, @cmnbroad ?; This had been addressed in the https://github.com/samtools/htsjdk/pull/835 PR which is now closed",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7069#issuecomment-773392506:72,sanity check,sanity check,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7069#issuecomment-773392506,1,['sanity check'],['sanity check']
Safety,"Since the code isn't reviewed yet I took the liberty of adding one much push with a single change: adding a ""synchronized"" to protect against a potential data race in `SeekableByteChannelPrefetcher`. The contract for `ReadableByteChannel` (which this implements) requires `read` to be thread-safe. I don't know whether this was the cause of #2516. I haven't been able to reproduce it since, but then again even before this change it wasn't easy to trigger.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-290811886:292,safe,safe,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2506#issuecomment-290811886,1,['safe'],['safe']
Safety,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:167,detect,detect,167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477,2,['detect'],['detect']
Safety,"So at the risk of sounding thick, what exact path do I use in my command when I'm calling the docker? Imagine I'm a six year old who doesn't understand what is the internal structure of the GATK docker image or what ""gatk-launch is in the standard docker image in /gatk"" means. Note that right now to call the jar I use /root/gatk.jar",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3334#issuecomment-317457895:10,risk,risk,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3334#issuecomment-317457895,1,['risk'],['risk']
Safety,"So here's what you should do to clean up your workspace:; - first, delete any fragment that doesn't have the full complement of 39 files that it is supposed to have; - next, if there are any duplicate fragments delete all but one of the duplicates. it doesn't matter which one you delete, the fragments have identical data (make sure this is the case by checking md5sums for ALL files in the duplicate fragments). As an aside, this step is optional. That is to say, your workspace is still legal if you leave the duplicate fragments, but you MUST remove the incomplete ones. Not removing the duplicates will just take up extra space. And it'll make the last verification/sanity check a little trickier; - If you've deleted duplicate and incomplete fragments, you should now have the same number of fragments in all your contig folders. If you have fewer fragments in a contig folder, then it is likely that some import process may have failed midway. If you have too many fragments...some other weird corruption may have happened which may be hindering the redundancy check.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722722209:671,sanity check,sanity check,671,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722722209,2,"['redund', 'sanity check']","['redundancy', 'sanity check']"
Safety,"Sorry @magicDGS -- since this is failing tests and needs a rebase/review, and we're extremely pressed for time this morning, this is going to have to wait until the next point release. But don't worry, I think I can safely say that we plan to do point releases frequently -- I'd expect the first one within a couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-356314345:216,safe,safely,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3998#issuecomment-356314345,1,['safe'],['safely']
Safety,"Sorry for the confusion, but assuming the iterator is iterating through an ordered container is not safe, I think. What about this:. ```java; public static <T> Stream<T> stream(final Iterator<T> iterator) {; return stream(() -> iterator);; }; ```. It is reusing the version working with `Iterable`'s.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2330#issuecomment-270465136:100,safe,safe,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2330#issuecomment-270465136,1,['safe'],['safe']
Safety,"Sorry for the delayed response @cmatKhan. The fix here is a little complicated because its a confluence of expected behaviors adding up to this. . Specifically in `-ERC GVCF` mode in HaplotypeCaller we merge adjacent regions with similar `GQ` scores but within those merged reference blocks we track the DP (mean dp) and the MIN_DP. When we go to genotype in GenotypeGVCFs we fall back to Min_DP when reporting the DP since it must be at least that high at the sight in question. For Diploid samples and with the default reference blocking we use, this works fine and for most of the range up to 30+ bases you are able to recover a pretty close approximation of what your DP is in most cases. However for Haploid data the assumptions at play mean that the `GQ` gets very high and starts to max out the field (99) so you end up with extremely long reference confidence blocks with very high confidence and a min_DP of some low value (in your case for the data you shared with us of 4, which is the threshold for hitting GQ=90 in the haploid model). This is also part of why adjusting the intervals for traversal was relevant, as if they were too long they were pulling in bases 1000+ bp away that only have 4fold coverage with reads and torpedoing the reported DP. So far nothing seems to obviously be bugged (we don't test/use haploid calling mode very often so its less well tested and this sort of issue is not surprising). We might be over-confident about reporting `GQ` for haploid reference blocks (though mathematically its sound that you only need a few reference observations to be sure about the ref call). Short of adjusting that model drastically there will always be some threshold where ""after X fold coverage of reads everything is merged into a big adjacent ref block"". Unfortunately with block merging on we never expect the DP to be exact (since it should vary over the block and its expensive to store that information exactly) but the issue is much more pronounced in your haploid s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429:622,recover,recover,622,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8943#issuecomment-2318660429,1,['recover'],['recover']
Safety,"Sorry. I get your thinking, but I think it would be cleaner to pass the param to each of those two classes. It cleans up the dependency tree -- those two classes don't depend on any of the ReadMetadata state but for that one param -- and avoids having to have the SVReadFilter be both Java and Kryo serializable. I'd prefer it, but won't insist upon it.; Adding the filter values to the read metadata output file makes sense, though. Doesn't need to be done now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324467678:238,avoid,avoids,238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469#issuecomment-324467678,1,['avoid'],['avoids']
Safety,"Summary information about this bug:. This issue affects GATK versions 4.3.0.0 through 4.5.0.0, and is fixed in GATK 4.6.0.0. The PR with the fix is: https://github.com/broadinstitute/gatk/pull/8900. This issue also affects Picard versions 2.27.3 through 3.1.1, and is fixed in Picard 3.2.0. This bug is triggered when writing a CRAM file using one of the affected GATK/Picard versions, and both of the following conditions are met:; ; * At least one read is mapped to the very first base of a reference contig; * The file contains more than one CRAM container (10,000 reads) with reads mapped to that same reference contig. When both of these conditions are met, the resulting CRAM file may have corrupt containers associated with that contig containing reads with an incorrect sequence. . Since many common references such as hg38 have N's at the very beginning of the autosomes and X/Y, many pipelines will not be affected by this bug. However, users of a telomere-to-telomere reference, users doing mitochondrial calling, and users with reads aligned to the alt sequences will want to scan their CRAM files for possible corruption. The other mitigating circumstance is that when a CRAM is affected, the signal will be overwhelmingly obvious, with the mismatch rate typically jumping from sub-1% to 80-90% for the affected regions, making it likely to be caught by standard QC processes. A CRAM scanning tool called `CRAMIssue8768Detector` that can detect whether a particular CRAM file is affected by this bug was added in https://github.com/broadinstitute/gatk/pull/8819, and was released as part of GATK 4.6.0.0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437:1451,detect,detect,1451,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437,1,['detect'],['detect']
Safety,"Thank you @mehrzads for your contribution. It would appear that this optimization is aimed to incorporate the knowledge that we could never possibly visit a given vertex more than K times in the first K best paths. Since the graphs are prone to exponential expansion of paths this seems like an important safeguard against this exponential expansion of the graph. . Looking at the code and the algorithm behavior it is intending to copy I see that there is a degenerate case in the current code that can cause the results to be order dependent. My belief is that this code can fall over by virtue of the fact that we refuse to make new incoming edges to a given vertex if there are already too many incoming edges for that vertex. Unfortunately this heuristic doesn't strike me as being valid, because those incoming edges can have any weight, including very high weights because they are bad paths through the graph that we created at a previous step. . I think a more correct optimization would be to limit the number of edges we create LEAVING a given vertex. The logic for this is that while we may not necessarily see all of the incoming edges in the correct weight order we will necessarily see all of the leaving edges in the correct order because those paths are pulled off of the priority queue in the correct order. Thus we can safely ignore any additional paths we see leaving a given edge because by construction as they would necessarily have at least one path that is cheaper than all of the paths leaving the current node.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417:305,safe,safeguard,305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5907#issuecomment-494105417,2,['safe'],"['safeguard', 'safely']"
Safety,"Thank you @mwalker174 . The input bamfile is about 7 GB. If no `--bamPartitionSize` is specified, the job would stuck at the first step `collect at ReadsSparkSource.java:220`, until we killed it. So I tried `--bamPartitionSize 4000000`, and it went through, but the Spark web interface showed errors in `sortByKey` steps:; ![sparkjob](https://user-images.githubusercontent.com/812850/27811313-9000019c-6097-11e7-82ac-aac557be31db.PNG).; And the program failed eventually:; ```; 18:24:57.885 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [July 3, 2017 6:24:57 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 269.29 minutes.; Runtime.totalMemory()=4172283904; org.apache.spark.SparkException: Job aborted due to stage failure: Task 607 in stage 3.0 failed 4 times, most recent failure: Lost task 607.13 in stage 3.0 (TID 14832, 12.9.68.0, executor 24): ExecutorLostFailure (executor 24 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 169939 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363:778,abort,aborted,778,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3186#issuecomment-312758363,1,['abort'],['aborted']
Safety,"Thank you for replying!; I have parallelized the GATK4 Mutect2 using thread pools in Java. I tested the parallelized GATK4 Mutect2 using a WGS data with control. The result came out that, about 0.6% variants were different from the original results. I found that the difference was caused by the random number generator in ReservoirDownsampler. The order of the input intervals after parallelism were different from the original, so the random numbers generated for each position with redundant reads were possibly different. Is there any solutions for this problem?; Thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592:485,redund,redundant,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4325#issuecomment-382586592,1,['redund'],['redundant']
Safety,"Thank you for taking a look into this. I followed recommendation of @gbrandt6 and reduced the --pruning-lod-threshold but this call is still unable to make it to the output of Mutect2. I tried different thresholds from 1.3, 0.7, 0.5 and even 0.1 but it did not lead to any difference in detecting this call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7232#issuecomment-829649061:287,detect,detecting,287,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7232#issuecomment-829649061,1,['detect'],['detecting']
Safety,"Thank you for the speedy review @davidbenjamin. I agree with you that the obvious place to trim the alleles is in `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` as it is the place where we actually edit the output. Indeed my first attempt at this fix was to make that change. Unfortunately, because the `readAlleleLikelihoods` object is constructed with the un-trimmed alleles in the `alleleMapper` was causing failures because the Liklihoods object would have mismatching alleles. To fix `removeAltAllelesIfTooManyGenotypes(ploidy, alleleMapper, mergedVC)` we would have to edit the alleleMapper object, which would be difficult given that I would prefer to just use the allele trimming library object. . Another proposal would have been to just hold onto the `mergedVC` object before we cull the extra alleles and then just compare the alleles at the end. Unfortunately due to engine code optimizations we have enabled an unsafe allele list copy for these alleles in the HaplotypeCaller (to save ourselves the cost of allocating dozens of identical ArrayLists to store Haplotypes every time we use the VariantContextBuilder). . To clarify, it is possible to move the check to the right place its likely to force me to write a non-library implementation of the trimming code that tracks what edits it made and I was trying to avoid doing that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693:946,unsafe,unsafe,946,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044#issuecomment-512355693,2,"['avoid', 'unsafe']","['avoid', 'unsafe']"
Safety,"Thank you so much, I will do that. Sincerely,; Emily. From: ldgauthier ***@***.***>; Sent: Monday, March 28, 2022 2:39 PM; To: broadinstitute/gatk ***@***.***>; Cc: Emily Elizabeth Puckett (puckett3) ***@***.***>; Mention ***@***.***>; Subject: Re: [broadinstitute/gatk] CombineGVCFs: ERROR input alleles must contain <NON_REF> (Issue #7737). CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and trust the content is safe. If I'm reading the process correctly, I don't actually think this should work. CombineGVCFs is specifically for combining GVCFs and it expects GVCFs to have <NON_REF> alleles. If you've already run the data through GenotypeGVCFs then you can't use CombineGVCFs again because the <NON_REF> likelihoods have been applied and those alleles are gone. The vcfcombine tool from bcftools is quite fast if all you want to do is join the samples together. -; Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fbroadinstitute%2Fgatk%2Fissues%2F7737%23issuecomment-1081062021&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Pxg8joQfE51l5e3cUUbKA9bQEYDZjp0AxdX0aqDG1MY%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FALDFEHAXSKZ7YHSFGISLPUTVCIDGZANCNFSM5RZSK5PA&data=04%7C01%7CEmily.Puckett%40memphis.edu%7C51db6aa9f41b483e1ce408da10f2aa5d%7Cae145aeacdb2446ab05a7858dde5ddba%7C0%7C0%7C637840931685525269%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=6Dkb6rbHDZpS05bYUHhlIRHJitgVtR%2FPB5rNHHFMg%2FQ%3D&reserved=0>.; You are receiving this because you were mentioned.Message I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127:504,safe,safe,504,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7737#issuecomment-1082170127,1,['safe'],['safe']
Safety,"Thanks @lbergelson - nice to meet you too. Sorry for the delay here. I had to set up gsutils on my system and am having gdb issues. . Submitting `sudo gdb /nfsdata-tmp/tools/gatk /home/bduser/mepowers/core.114856` I get back . ```; Missing separate debuginfo for the main executable file; Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/6c/../../../jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/bin/java; Core was generated by `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samt'.; Program terminated with signal 6, Aborted.; ```; I did try the yum --enablerepo, but it am getting the same error. . Any quick workarounds? Thanks in advance for the help. Will try again on Monday.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771:568,Abort,Aborted,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690#issuecomment-466588771,1,['Abort'],['Aborted']
Safety,"Thanks @ldgauthier. @gbrandt6 Iâ€™d appreciate it if you want to take a look, but I might ask if you can do it by Friday afternoonâ€”Iâ€™m out after then through all of next week. Would like to merge before I head out to avoid any more rebasing and/or updating of exact-match tests. Happy to look at any changes to the docs you might make in a subsequent PR, though!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-972003949:215,avoid,avoid,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394#issuecomment-972003949,1,['avoid'],['avoid']
Safety,"Thanks @ruqianl, you may want to read through the comments at https://github.com/broadinstitute/gatk/issues/6235 and the corresponding PR https://github.com/broadinstitute/gatk/pull/6244, which both address this issue. See also the following bit of documentation added in that PR:. > Advanced users may wish to set the THEANO_FLAGS environment variable to override the GATK theano configuration. For example, by running THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR"" gatk GermlineCNVCaller ..., users can specify the theano compilation directory (which is set to $HOME/.theano by default). See theano documentation at https://theano-pymc.readthedocs.io/en/latest/library/config.html. So you can specify a unique compilation directory for each of your jobs to avoid the compilelock, e.g., `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/0"" gatk GermlineCNVCaller ...`, `THEANO_FLAGS=""base_compiledir=PATH/TO/BASE_COMPILEDIR/FOR/JOB/1"" gatk GermlineCNVCaller ...`, etc. Alternatively, you can increase `config.compile.timeout` as discussed in those comments.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899:767,avoid,avoid,767,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7411#issuecomment-905070899,2,"['avoid', 'timeout']","['avoid', 'timeout']"
Safety,"Thanks a lot for all your feedback about this @lbergelson and @droazen. From my side this could be close now, although it may be useful to have some of this information in the Wiki to avoid confusion. Thank you very much again!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039:184,avoid,avoid,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2345#issuecomment-273732039,2,['avoid'],['avoid']
Safety,Thanks everyone for working this out. Is there any chance we can detect this problem on load and fail rather than silently giving bad output?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105:65,detect,detect,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-324390105,1,['detect'],['detect']
Safety,"Thanks for adding this! Let me discuss further with @mwalker174 to understand the need and typical use cases (e.g., combining fixed-grid bins) to make sure we don't run into any gotchas downstream. I'll try to review by EOD, but in the meantime, you might want to address a few issues I see at first glance:. 1) Correct the name of the tool (PreprocessIntervals) in the commit message and description.; 2) Add descriptions of the new parameters to the tool Javadoc.; 3) Amend the corresponding WDL task and expose the new parameters in all relevant germline and somatic WDLs.; 4) We should be sure to update the relevant documentation for all germline and somatic WDLs, which emphasizes how PreprocessIntervals should be run differently for WES and WGS, if we plan on changing the default behavior of the tool in the future.; 5) Tests are failing due to a compilation warning about a redundant cast to int.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387:884,redund,redundant,884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701#issuecomment-465978387,1,['redund'],['redundant']
Safety,"Thanks for helping me to understand why you didn't mark the metadata. This may seem like quibbling, but I'd suggest that we mark the metadata with a comment character, and let the pandas/R users remove it. They'll notice if they forget to do that, because the columns won't be named as expected, and they'll have to fix it up. Whereas the risk for automated programs is that they'll simply delete the first row, which might be real data if the file has been reordered for some reason, or if the tool implementing the standard is non-compliant. The resulting bugs will be subtle, and might easily go undetected. Building in behavior to delete lines starting with ""CHROM\t"" seems odd and fraught with peril in a way that building in behavior to strip comments, doesn't. That's my take, anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381:339,risk,risk,339,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381,2,['risk'],['risk']
Safety,"Thanks for making this change! I might include more detail with the note (""Substantially improves results on FFPE samples""), for posterity---it's probably true, but we can't really say anything definitive with only N=1 and without the cross-validation procedure I mentioned on Slack. That is, the higher degree of denoising might just be an artifact of effectively removing more PCs with GC-bias correction (since I'm assuming the same number of PCs were explicitly removed in both cases), but it's possible that removing the optimal number of PCs without GC-bias correction could achieve a better result. Since our correction procedure is relatively naive, there may also be some dependence on bin size. However, I think it's probably not worth a detailed analysis, and that it's generally safe to enable correction by default.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-496933928:791,safe,safe,791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5966#issuecomment-496933928,1,['safe'],['safe']
Safety,Thanks for that explanation. I'd rather set it to a safer value like 100 as a default - @asmirnov239 do you have any sense of how much this affects run time?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-922018024:52,safe,safer,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-922018024,1,['safe'],['safer']
Safety,"Thanks for that info and for sharing the files, @asmirnov239. I suspect that there are essentially two types of bins: ""nice"" and ""not so nice"". The sampling noise in the former is determined by Poisson observation noise, whereas that in the latter is determined by uncertainty in the bias posteriors. This is a bit hard to see in the plots above, and even in this version where I tried to adjust the point size and alpha:. ![image](https://user-images.githubusercontent.com/11076296/137733810-16a79ea9-ea7b-47cc-a42f-40130a949015.png). However, plotting a measure of the difference in the dCRs (from 20 and 200 posterior samples) vs. the dCR is more suggestive:. ![image](https://user-images.githubusercontent.com/11076296/137734587-1b9f6551-74b2-4097-a02c-f51d7341251c.png). As are the dCR histograms:. ![image](https://user-images.githubusercontent.com/11076296/137733867-ce0f5573-a5cc-412c-9060-56fbb09d1ef0.png). I would guess that the nice spike around CR ~ 2 and the fatter base extending up to dCR ~ 100 are distinct populations of bins. So the punchline would be that differences at high dCR are probably just noise within the noise. For ""nice"" bins at dCR ~ few, the sampling noise looks to be <1%. Not really sure what's going on at very high dCR, but I think it's safe to say that these are ""not so nice"" bins!. I've seen this pattern in other WES cohorts when plotting the posterior means vs. std devs for the biases; tried to dig up the plots on Slack, but I can't find them at the moment. Perhaps something along those lines might be worth visualizing in your model-criticism notebooks, if you don't already?. Again, hard to say this is indeed the case from the dCRs alone, but if so, it might be worth baking this sort of mixture into future versions of the model or coming up with other strategies to deal with such bins.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946:1275,safe,safe,1275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754#issuecomment-945731946,1,['safe'],['safe']
Safety,"Thanks for the clarification @ldgauthier. Maybe we should add a note to that effect in the docs for the PGT annotation to avoid future confusion (eg., the doc string for PGT in `GATKVCFHeaderLines`)? If you submit a one-line PR, I'll approve it :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543756622:122,avoid,avoid,122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6220#issuecomment-543756622,1,['avoid'],['avoid']
Safety,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:898,timeout,timeout,898,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151,1,['timeout'],['timeout']
Safety,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:217,avoid,avoid,217,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529,1,['avoid'],['avoid']
Safety,"Thanks for your feedback, @cmnbroad. In my case, I think that `IntegrationTestSpec` is a good way of avoid complicated code to test tool results, but it is true that it have some problems (one that I had was the usage for testing programs where the outputs are determined by a prefix in the command line, but with different suffixes). I think, from the API user point of view, that a class like `IntegrationTestSpec` to facilitate program output testing (including user exceptions) will be nice for developing purposes. Nevertheless, this is just a convenience that I asked for here, but I can try to solve the issues with the `BaseTest` instead. By the way, I would love to have this interface in GATK at least for now, because several of my tools rely on the `IntegrationTestSpecs` for development...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889:101,avoid,avoid,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2122#issuecomment-243124889,2,['avoid'],['avoid']
Safety,Thanks for your suggestion @wir963. The `--min-score-identity` and `--host-min-identity` parameters can be used to tune your desired specificity/sensitivity for microbe read detection. The default settings should guarantee that the identified microbial reads have better alignments to the microbe reference than host.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510:174,detect,detection,174,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6819#issuecomment-705027510,1,['detect'],['detection']
Safety,"Thanks!. Okay I will make sure to do that in the future. Thanks for the help!. On Tue, Aug 14, 2018 at 11:30 AM, Louis Bergelson <notifications@github.com>; wrote:. > @kvinter1 <https://github.com/kvinter1> In future, it's a good idea to; > wait for tests to pass before merging, otherwise you risk the potential; > penalty of having to buy the team beer if test fail once it's in master.; > Doc changes are pretty low risk, but you never know.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412913465>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AoMkWKCRyhFXvfzSUI7C26_4qRZPivIoks5uQu0NgaJpZM4V8n05>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412917430:294,risk,risk,294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5104#issuecomment-412917430,2,['risk'],['risk']
Safety,"That example data from the tutorial is good @sooheelee, but maybe it could be reduced in size to avoid adding it to the large file directory? It will be nice to include that example in the `RealignerTargetCreator` PR (#3112)...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319627989:97,avoid,avoid,97,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3104#issuecomment-319627989,1,['avoid'],['avoid']
Safety,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:535,redund,redundant,535,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926,1,['redund'],['redundant']
Safety,"That's a good point--the documentation should be updated. I think it's safe to do so here before I updated actual workspace itself, although technically it has the potential to create a small window in which the documentation talks about features that are not there. I'll update the ticket in Jira to explicitly mention updating the documentation as well, as it should be among the AC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238401881:71,safe,safe,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8010#issuecomment-1238401881,1,['safe'],['safe']
Safety,"The ""WDL test"" CI failures are not related to the changes in this PR, please see this [sanity check PR](https://github.com/broadinstitute/gatk/pull/8369) which is also currently aflame.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824:87,sanity check,sanity check,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362#issuecomment-1599633824,1,['sanity check'],['sanity check']
Safety,"The DREAM SMC-RNA does not pertain to SNv and indel calling. Rather, it's only for fusion and isoform prediction.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5427#issuecomment-592086085:102,predict,prediction,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5427#issuecomment-592086085,1,['predict'],['prediction']
Safety,"The GATK VCF header issue causing the underlying problem was fixed in #3351, so a new release of GATK4 should work correctly and avoid losing variants during GenomicsDB import/output for joint calling. I agree with Louis that failing with an error would be better than the current silent failures in case of any future issues. Thank you all again for the help debugging this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708:129,avoid,avoid,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3429#issuecomment-325024708,1,['avoid'],['avoid']
Safety,"The `Poisson` arises because we want to our model to generate the *occurrences*, assuming that each *count bin* provides equal weight---rather than the counts themselves. As usual, modeling each bin as Poisson is close enough to modeling all bins as multinomial for our purposes. If we directly use the NB likelihood and simply weight the count likelihood by occurrences, occurrences in the peak will strongly affect the result, adversely so if the count likelihood is actually misspecified there. As an example, consider trying to fit a Poisson to data that is actually zero-inflated Poisson---fitting the histogram will actually result in a more robust estimate for the mean. Another benefit is that truncated data (as we have here) is straightforwardly handled in an unbiased way. In the special case of complete, trivially-binned data, the full, unbinned likelihood is recovered. I think this sort of histogram fitting is pretty standard in the astro/particle community. We can certainly change up the model to include strictly quantized + free-floating states as you describe (rather than the ""fuzzily quantized"" states I use here), but I just wanted to avoid having another level of mixtures/logsumexps for this quick prototype. However, note that modeling mosaicism on the autosomes is desirable, but there we also want the strong diploid prior to nail down the depth and per-contig bias. So we will have to be a little careful about how we introduce free-floating states. Also, since I was not using gcnvkernel, I had to integrate out all discrete parameters. It may be that we can write down a nice model with discrete parameters if we use your inference framework. Finally, I did not further bin the counts here (or rather, the bin size is 1), which already yields the maximum information, but I did use a maximum-count cutoff. If we use the same cutoff for all samples, this allows us to simply pass a non-ragged matrix from Java (with dimensions of samples x contigs x maximum count) as a ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522:873,recover,recovered,873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4371#issuecomment-376307522,2,['recover'],['recovered']
Safety,"The change is because we started using VariantsSparkSink to write VCFs on the cluster, rather than writing them in a single thread from the driver (which doesn't scale). VariantsSparkSink only supports .bgz extensions currently, not .gz. So if you change the output extension to .bgz it will work. Gzip is not splittable so if possible we'd avoid outputting it at all. Hail for example will not load .gz unless the force option is used. (There are actually two force options, one to read it as regular gzip, the other to read it as bgzip, so you have to know which flavour of gzip it is...). We could do one of the following:. 1. Throw an error if the extension is .gz.; 2. Write bgzipped output to the .gz file.; 3. Write regular gzipped output to the .gz file. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307:341,avoid,avoid,341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3725#issuecomment-341689307,1,['avoid'],['avoid']
Safety,"The error comes from two annotations: InbreedingCoeff and ExcessHet. One solution is to add ""-AX ExcessHet -AX InbreedingCoeff"". It doesnt exactly solve the problem, but it avoids hitting the problem code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942:173,avoid,avoids,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938#issuecomment-1238883942,1,['avoid'],['avoids']
Safety,"The functions for getting paired and unpaired reads now go with the second suggestion to partition the reads by read names (rather than use groupBy), then map the entire partitions to a list of paired reads and a list of unpaired reads. . Lots of angle brackets but I think this approach minimizes the number of lines while yielding substantial efficiency gains over the original approach, mainly by avoiding a needless second shuffle.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-300290475:400,avoid,avoiding,400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2664#issuecomment-300290475,1,['avoid'],['avoiding']
Safety,"The last call stack that could be gathered with @droazen pointers to a docker image was -; ```; (gdb) bt; #0 0x00007f1124858067 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56; #1 0x00007f1124859448 in __GI_abort () at abort.c:89; #2 0x00007f1124851266 in __assert_fail_base (fmt=0x7f1124989f18 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", ; assertion=assertion@entry=0x7f112520df60 ""new_prio == -1 || (new_prio >= __sched_fifo_min_prio && new_prio <= __sched_fifo_max_prio)"", file=file@entry=0x7f112520df54 ""tpp.c"", line=line@entry=62, ; function=function@entry=0x7f112520e030 <__PRETTY_FUNCTION__.8458> ""__pthread_tpp_change_priority""); at assert.c:92; #3 0x00007f1124851312 in __GI___assert_fail (; assertion=assertion@entry=0x7f112520df60 ""new_prio == -1 || (new_prio >= __sched_fifo_min_prio && new_prio <= __sched_fifo_max_prio)"", file=file@entry=0x7f112520df54 ""tpp.c"", line=line@entry=62, ; function=function@entry=0x7f112520e030 <__PRETTY_FUNCTION__.8458> ""__pthread_tpp_change_priority""); at assert.c:101; #4 0x00007f112520c5ef in __pthread_tpp_change_priority (previous_prio=previous_prio@entry=-1, ; new_prio=new_prio@entry=6271) at tpp.c:60; #5 0x00007f1125201e5f in __pthread_mutex_lock_full (mutex=0x7f111cd7e340) at ../nptl/pthread_mutex_lock.c:453; #6 0x00007f10f4f83a2d in mutex_lock(pthread_mutex_t*) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #7 0x00007f10f4efe245 in StorageManager::array_close(std::string const&) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #8 0x00007f10f4efe8b7 in StorageManager::array_iterator_finalize(ArrayIterator*) (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #9 0x00007f10f4ef77af in tiledb_array_iterator_finalize (); from /cromwell_root/tmp.034550cf/root/libtiledbgenomicsdb5206530589131345400.so; #10 0x00007f10f4e44730 in GenomicsDBBCFGenerator::~GenomicsDBBCFGenerator() (); from /cromwell_root/t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-436579324:249,abort,abort,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4518#issuecomment-436579324,1,['abort'],['abort']
Safety,The problem is in practice if a read is unmapped along with its mate then there is no way that you can recover that read with any interval given to these tools. If you wish not to lose any reads then it is important not to provide any interval to ApplyBQSR step and run the tool as a single entity to collect all reads together. If you are ApplyBQSR in parallel to collect reads then you may not be able to rescue any unmapped pairs. Mapped reads with an unmapped pair may be rescued but I am not sure how other filters will affect. When I checked my own bam files sorted and bqsr applied bam files have the same number of reads inside already. . Also you may need to check your bam file to see if there are any problematic reads present.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670:103,recover,recover,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8523#issuecomment-1725286670,1,['recover'],['recover']
Safety,"The retry parameters are in BucketUtils.java:setGenerousTimeouts. For the case where we're opening thousands and thousands of connections, it may be helpful to increase those timeouts a little.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746:175,timeout,timeouts,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2685#issuecomment-303784746,1,['timeout'],['timeouts']
Safety,"The segments VCFs contain the event segmentation results for each sample. Exact breakpoints are not given by this tool. Plots can be made using the denoised copy ratio TSV outputs. Currently our germline workflow does not incorporate the B-allele frequency evidence required for LOH detection, although that's a future goal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949:283,detect,detection,283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6320#issuecomment-572176949,1,['detect'],['detection']
Safety,"The sequences of 150,119 genomes in the UK Biobank.](https://pubmed.ncbi.nlm.nih.gov/35859178/; Halldorsson BV, et al. Nature. 2022 Jul;607(7920):732-740. doi: 10.1038/s41586-022-04965-x. Epub 2022 Jul 20.PMID: 35859178. On page 69+ of this pdf, they describe the problem and how they cleverly worked around it. ; ; https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-04965-x/MediaObjects/41586_2022_4965_MOESM1_ESM.pdf. _It should be noted that running GATK out of the box will cause every job to read the entire; gVCF index file (.tbi) for each of the 150,119 samples. The average size of the index files is ; 4.15MB, so each job would have to read 4.15*150,126 = 623GB of data on top of the actual; gVCF slice data. For 60,000 jobs, this would amount to 623GB*60,000 = 37PB or 25.2GB/sec; of additional read overhead if the jobs are run on 20,000 cores in 17 days. This read; overhead will definitely prevent 20,000 cores from being used simultaneously. However,; this problem was avoided by pre-processing the .tbi files and modifying the software; reading the gVCF files from the central storage in a similar fashion as we did for GraphTyper; and the CRAM index files (.crai)._. This explains why chr1 requires more memory than chr22 despite running on the same number of samples. The larger chr1 tbi index is the source of the memory problem. The Decode solution is too limit the reading of the tbi index to the part that indexes the scattered region. There is a long pause at the beginning of the running GenotypeGVCFs which I never understood. GATK must be the reading of all the sample's gvcfs tbi into memory during that pause. So the reblocking of the gvcfs above reduced the memory foot print by decreasing the tbi size. Decode reduced it by chopping up the index so for each scattered region, GATK could only read a small subset of the index needed for that region. The combination of reblocking and chopping up the tbi would help with the memory requirements even more. Ho",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579:1142,avoid,avoided,1142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7968#issuecomment-1374348579,2,['avoid'],['avoided']
Safety,"The test failures in the branch build are clearly related to the recent travis key migration. The PR build (which is the one we care about) passes, so this should be safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491:166,safe,safe,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7393#issuecomment-953279491,2,['safe'],['safe']
Safety,"The tutorial data would be an option, except that it covers reference territory not currently in the repo. It looks like that would add nearly 1 gig just for the reference data, which I think we'd really want to avoid. Ideally we'd have just one PR for each of the (two) tools. If you want to keep `ConstrainedMateFixingManager` and `NWaySAMFileWriter` as separate PRs, thats fine, but I don't think we'd take them until we know there is, or will very soon be, code that depends on them. Certainly using `@Experimental` for the tools could make sense, once we're certain that we have a way forward for test coverage. One other note, it's very helpful to include the original GATK3 file as the first commit, as you did in this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857:212,avoid,avoid,212,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3112#issuecomment-366271857,1,['avoid'],['avoid']
Safety,"The warnings the user is seeing are due to spanning deletion alleles which are currently not annotated with Funcotator. The bug here is what is causing the stack trace. It's in the protein sequence prediction code and I suspect that it has to do with the position of the variant relative to the exon/transcript boundaries. I have not been able to look at it yet, but thanks to the user posting the variants that are causing issues, it should be straight-forward to track down.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980:198,predict,prediction,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6651#issuecomment-644794980,1,['predict'],['prediction']
Safety,"There are many ways of corrupting data in TileDB/GenomicsDB - I am listing the protections available.; * Under the current setup, data is imported once by a single process - the _failIfUpdating_ flag is used in the GATK-4 import tool to ensure this.; * If an importer process crashes in the middle of execution, only the fragments that are fully completed will be visible to downstream queries/reads. Partially written fragments are on disk but ignored. This is achieved by renaming the fragment directory from _.<fragment_id>_ to _fragment_id_. While this is not an atomic operation, under the current use case, I cannot think of any way that will cause issues. At most, you waste some cycles re-importing data. What's not protected:; * Mapping data - sample name to TileDB row id, chromosome name to TileDB column interval. No plans to protect this in the current implementation. Once we have the PostgreSQL based storage for the mapping data, we will be able to detect and prevent inconsistencies.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716:965,detect,detect,965,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2536#issuecomment-305557716,1,['detect'],['detect']
Safety,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:154,avoid,avoids,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189,1,['avoid'],['avoids']
Safety,"There have been a few instances like this though. This one was obviously accidental, but things like the correct spelling of @magicDGS actual name seem like reasonable things to be able to include in the source. Also, testing non-ascii characters seems like something that is going to be increasingly common as we support new versions of the spec so it seems like we should learn to avoid this problem...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095:383,avoid,avoid,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5936#issuecomment-492761095,2,['avoid'],['avoid']
Safety,There's no reason to even risk it becoming a problem in the future -- let's just include the fully-packaged jars in the docker image (since that is our standard binary distribution format anyway).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526:26,risk,risk,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4409#issuecomment-365993526,1,['risk'],['risk']
Safety,These initial results suggest that the savings from a pure-Spark pipeline are in the 15-30% range. @tomwhite Do you attribute these savings mostly to avoiding writing/reading intermediate outputs?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281:150,avoid,avoiding,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3395#issuecomment-341788281,1,['avoid'],['avoiding']
Safety,"They are retried. You're seeing this message because it failed more than `maxChannelReopens` times. The new version which you really want to use for any test at this point is described there:; https://github.com/broadinstitute/gatk/issues/2685#issuecomment-302798685. Among other things, this new version puts in a message about 'retry failed' when it runs out of retries to eliminate the very confusion that you ran into. GenomicsDBImport opens a large number of parallel connections and as a result is getting throttled fairly heavily (by the host GCE machine if nothing else). This results in timeouts and dropped connections. One way forward is to increase the retry delays, another is to find a way to do the same work with fewer parallel open connections.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753:596,timeout,timeouts,596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2749#issuecomment-304123753,1,['timeout'],['timeouts']
Safety,"They discuss that add more ram.; I try with 8 cpu and 500 Go of RAM, but still not working. Error for one bam file:. ```. 15:47:36.554 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/genouest/uni_limoges_fr/jpollet/.conda/envs/myd88/share/gatk4-4.1.4.0-1/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 28, 2019 3:47:37 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 15:47:37.239 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.240 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.4.0; 15:47:37.240 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:47:37.240 INFO Mutect2 - Executing as jpollet@cl1n031.genouest.org on Linux v3.10.0-693.21.1.el7.x86_64 amd64; 15:47:37.240 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 15:47:37.246 INFO Mutect2 - Start Date/Time: 28 novembre 2019 15:47:36 CET; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - ------------------------------------------------------------; 15:47:37.246 INFO Mutect2 - HTSJDK Version: 2.20.3; 15:47:37.246 INFO Mutect2 - Picard Version: 2.21.1; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:47:37.247 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:47:37.247 INFO Mutect2 - Deflater: IntelDeflater; 15:47:37.247 INFO Mutect2 - Inflater: IntelInflater; 15:47:37.247 INFO Mutect2 - GCS max retries/reopens: 20; 15:47:37.247 INFO Mutect2 - Requester pays: disabled; 15:47:37.247 INFO Mutect2 - Initializing engine; 15:47:41.204 INFO Mutect2 - Done initializi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558:489,detect,detect,489,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6271#issuecomment-559553558,1,['detect'],['detect']
Safety,"They now take about the same time, we avoid paying the cost of R installation but pay a cost to build the docker instead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450:38,avoid,avoid,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965#issuecomment-349742450,1,['avoid'],['avoid']
Safety,"This ""fix"" to the timeout issue doesn't work -- closing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466:18,timeout,timeout,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2805#issuecomment-305931466,1,['timeout'],['timeout']
Safety,"This bug has become part of a bigger effort to address how configure the gatk. We're working on a general solution to avoid this sort of issue in the future. We haven't addressed this specific subcase yet though. For now the workaround I described above should work for you. If it doesn't, let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565:118,avoid,avoid,118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2300#issuecomment-274899565,1,['avoid'],['avoid']
Safety,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:249,detect,detect,249,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906,1,['detect'],['detect']
Safety,This might allow us to avoid the expensive final concatenation for outputs in Spark.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855:23,avoid,avoid,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4492#issuecomment-370504855,1,['avoid'],['avoid']
Safety,"This should add support for reading bam files with CSI indexes, as well as porting the FastaReferenceWriter to htsjdk and a lot of other changes to htsjdk. @samuelklee This includes the overlap detector optimizations you wanted as well as the changes to let you write interval file to paths.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651:194,detect,detector,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812#issuecomment-474098651,1,['detect'],['detector']
Safety,"This should be straight-forward once the discussion above is resolved. Just detect if the variant is on a known Mitochondrial contig. If so, use the MT codon table, otherwise use the regular codon table.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996:76,detect,detect,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996,1,['detect'],['detect']
Safety,"This task is to take the training data generated in issue #3092 and learn something from it, for example a regression model that predicts a distribution of artifactual read fractions. Using the learned model in filtering is a separate issue.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993:129,predict,predicts,129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2973#issuecomment-307680993,2,['predict'],['predicts']
Safety,This will be the next bug I look into. It's part of a family of issues that all relate to how the predicted protein change is created.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786:98,predict,predicted,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6289#issuecomment-688996786,1,['predict'],['predicted']
Safety,"To add some commentary to why this is happening: It looks like multiple threads are hitting this line simultaneously and based on the overload of `ArrayList.add()` this error could be triggered by multiple calls to `ensureCapacityInternal()` inside the add method:; ```; final List<ReadsPathDataSource> readSources = new ArrayList<>(threads);; final ThreadLocal<ReadsPathDataSource> threadReadSource = ThreadLocal.withInitial(; () -> {; final ReadsPathDataSource result = new ReadsPathDataSource(readArguments.getReadPaths(), factory);; readSources.add(result);; return result;; });; ```; The fix should be simple you just have to make sure ti synchronize the initialization or swap out the readSources object to one that is itself thread safe. @vruano",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721:739,safe,safe,739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7403#issuecomment-899732721,2,['safe'],['safe']
Safety,Tools are:. - DetectCoverageDropout; - DecomposeSingularValues; - CalculatePulldownPhasePosteriors. Nine related docs are:. - CalculatePulldownPhasePosteriors.java			; - CoverageDropoutDetectorTest.java			; - DecomposeSingularValuesIntegrationTest.java; - CalculatePulldownPhasePosteriorsIntegrationTest.java	; - CoverageDropoutResult.java				; - DetectCoverageDropout.java; - CoverageDropoutDetector.java				; - DecomposeSingularValues.java				; - DetectCoverageDropoutIntegrationTest.java,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381:14,Detect,DetectCoverageDropout,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2809#issuecomment-305998381,3,['Detect'],"['DetectCoverageDropout', 'DetectCoverageDropoutIntegrationTest']"
Safety,"Two high level questions/concerns about the implementationâ€¦ . 1. I think when things are parallelized, we then lose the retries. In order to do that, we might need to get a little more python async-y than I was originally thinking. The retries are pretty important to recover from transient errors in BQ. 2. I think perhaps the ordering/dependecies are lost between queries? It looks like we fire them all off and then wait for them all to complete. But really we need to wait for the vet_new queries to finish before launching the pet queries since they depend on data from the vet_new. If vet_new hasn't started this would failâ€¦ but if vet_new has created the table we'll just get the wrong results",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753:268,recover,recover,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7505#issuecomment-944301753,1,['recover'],['recover']
Safety,"Two more things:. 1) attached is the error log. There error is:. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fb23d5aba19, pid=147396, tid=140403504084736; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode linux-amd64 ); # Problematic frame:; # C [libtiledbgenomicsdb6950059158101672698.so+0x3e3a19] ArraySchema::tile_num(void const*) const+0x79; #; # Core dump written. Default location: /home/groups/MgapGenomicsDb/@files/sequenceOutputPipeline/SequenceOutput_2020-11-04_09-17-57/Job44/core or core.147396; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. ```. 2) I was mistaken on when the error happens. The last line logged by GenomicsDBImport is ""GenomicsDBImport - Importing batch 4 with 33 samples"". There are 4 total batches, with a batchSize of 50. I dont know whether batch 4 finished or if this error is mid-batch import. [hs_err_pid147396.log.txt](https://github.com/broadinstitute/gatk/files/5496539/hs_err_pid147396.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280:98,detect,detected,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6910#issuecomment-722594280,1,['detect'],['detected']
Safety,"Updated plan. ----------; ## Small improvements in new interpretation tool; ; - [x] Output bam instead of sam for assembly alignments; - [x] Instead of creating directory, new interpretation tool writes files (behavior consistent with current interpretation tool); - [x] Prefix with sample name for output files' names; - [x] Add `INSLEN` annotation when there's `INSSEQ`; - [x] Clarify the boundary between `AlignedContig` and `AssemblyContigWithFineTunedAlignments`; - [x] Increase test coverage for `AssemblyContigAlignmentsConfigPicker`; ; ----------; ## Consolidate logic, bump test coverage and update how variants are represented. ### consolidate logic; When initially prototyped, there's redundancy in logic for simple variants, now it's time to consolidate. - [x] `AssemblyContigWithFineTunedAlignments`; - [x] `hasIncompletePicture()`. - [x] `AssemblyContigAlignmentSignatureClassifier`; - [x] Don't make so many splits; - [x] Reduce `RawTypes` into fewer cases; ; - [x] `ChimericAlignment`; - [x] update documentation; - [x] implement a `getCoordinateSortedRefSpans()`, and use in `BreakpointsInference`; - [x] `isNeitherSimpleTranslocationNorIncompletePicture()`; - [x] `extractSimpleChimera()`. ### bump test coverage; Once code above is consolidated, bump test coverage, particularly for the classes above and the following poorly-covered classes; - [x] `ChimericAlignment`; - [x] `isForwardStrandRepresentation()`; - [x] `splitPairStrongEnoughEvidenceForCA()` ; - [x] `parseOneContig()` (needs testing because we need it for simple-re-interpretation for CPX variants) Note that `nextAlignmentMayBeInsertion()` is currently broken in the sense that when using this to filter out alignments whose ref span is contained by another, check if the two alignments involved are head/tail. - [x] `BreakpointsInference` & `BreakpointComplications`. - [x] `NovelAdjacencyAndAltHaplotype`; - [x] `toSimpleOrBNDTypes()`. - [x] `SimpleNovelAdjacencyAndChimericAlignmentEvidence`; - [x] serialization ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021:696,redund,redundancy,696,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4111#issuecomment-375438021,2,['redund'],['redundancy']
Safety,"We already added the functionality needed for the gCNV workflow to IntervalListTools in https://github.com/broadinstitute/picard/pull/1208. The issue is that the tool outputs each scattered interval list to a separate directory if the number of scatters is greater than 1, but it just outputs to a file (essentially a noop) if we don't need to scatter. Not sure the reason for this design, but it makes things difficult from the perspective of WDL. Would be easier if the expected output was always `Array[File]+`. I don't really see why IntervalListTools needs to create those intermediate directories (nor why the naming scheme is determined by `DecimalFormat(""0000"")`---don't think this is documented, either), but I am not sure if that behavior is expected by now or if it is safe to modify it. Pretty sure SplitIntervals is just calling the same backend class used by IntervalListTools. Perhaps that tool might've been spun off before we exposed the Picard tools? See e.g. https://github.com/broadinstitute/gatk/pull/5392#issuecomment-435588845. I think we should try to avoid writing such custom/utility GATK tools unless really warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907:780,safe,safe,780,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599639907,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"We are rewriting that tool in java to prevent issues like this. You could pull that branch from here: https://github.com/broadinstitute/gatk/pull/4800.; Otherwise, I think pysam handles gzipped indexed vcfs better than plain text ones. So if you gzip your resource files with ; `; bgzip -c hapmap_3.3.hg19.sites.vcf > hapmap_3.3.hg19.sites.vcf.gz; `; then index them with tabix:; `; tabix -p vcf hapmap_3.3.hg19.sites.vcf.gz; `; you may avoid this crash.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377:437,avoid,avoid,437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377,1,['avoid'],['avoid']
Safety,"We can add this as an option, but I think there are a few arguments against outright replacement (which may have led to this design decision in the first place):. 1) I don't believe any CNV tools take in sample name as input at the GATK command line, by design. We instead take the BAM basename as the entity ID during the CollectCounts task; this ID is then passed along at the WDL level and is only used to construct the filenames of output files. This obviates the need for tools like GetSampleName and avoids issues with parsing funky sample names at the command line, handling/passing special characters at all levels (WDL, Java, python), etc. (The implicit assumption is that the BAM basename is more likely to be well formed.). 2) I would argue that specifying a sample index is relatively user friendly, especially if we are typically running on all samples. In that case, all you need to know is the total number of samples and that we use zero indexing, and then you simply iterate over all indices. (If you want to run on a single, particular sample, then perhaps using the sample name might be more friendly, but I'd argue that this use case is not typical.). 3) If you want to go ahead and add this option, I would probably keep the directory structure of the GermlineCNVCaller output the same (i.e., with folders named ""SAMPLE_#""), and just check the sample_name.txt files at the PostprocessGermlineCNVCalls step. I don't think this should require GermlineCNVCaller code changes, right?. 4) We may require additional code at the WDL level if we want to both switch over to primarily using sample names but also get rid of bundling (i.e., by passing only the calls for each sample when needed). Locally, you can always just search all output for directories containing the appropriate sample_name.txt. But on the cloud, you'd want to make sure that the postprocessing step for a particular sample gets only its corresponding directories, which would have to happen at the WDL level; the c",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765:506,avoid,avoids,506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6659#issuecomment-644829765,2,['avoid'],['avoids']
Safety,"We can certainly modify IntervalListTools to make the behavior more intuitive (e.g. add a new mode for `INTERVAL_COUNT_WITH_OVREFLOW`), but I'm not sure I understand the issue. We're just trying to avoid calling the GATK command if the scatter is going to be a noop?. The GATK SplitIntervals has slightly different behavior that's helpful in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889:198,avoid,avoid,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502#issuecomment-599618889,2,['avoid'],['avoid']
Safety,"We can implement this, but first can you explain why your `canDecode()` methods can't unambiguously detect your file formats? The VCF/BCF codecs use a magic value at the start of the file to detect the format -- are your codecs guessing as to the intended format? Is there no way to be sure what the format is?. If we have multiple codecs able to decode a file, and only one produces `Features` that match the type parameter of the `FeatureInput`, would it solve your problem if we selected that codec rather than blow up? This would be a bit simpler/easier than a new annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503:100,detect,detect,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1184#issuecomment-163297503,2,['detect'],['detect']
